Identifying and understanding relationships between items is a key component of any modern recommender system. Knowing which items are `similar, ' or which otherwise may be substitutable or complementary, is key to building systems that can understand a user's context, recommend alternative items from the same style _cite_, or generate bundles of items that are compatible _cite_ . Typically, identifying these relationships means defining (or otherwise learning from training data) an appropriate distance or similarity measure between items. This is appropriate when the goal is to learn some notion of `equivalence' between items, e.g.~in order to recommend an item that may be a natural alternative to the one currently being considered. However, identifying such a similarity measure may be insufficient when there is substantial between the items being considered. For example, the characteristics that make clothing items, electronic components, or even romantic partners compatible exhibit substantial heterogeneity: for a pair of such items to be compatible they should be systematically similar in some ways, but systematically different in others. Recently, a line of work has aimed to model such heterogeneous relationships, e.g.~to model co-purchasing behavior between products based on their visual appearance or textual descriptions~ _cite_ . In spite of the substantial heterogeneity in the data used for training (a large dataset of co-purchase `dyads' from) and the complexity of the models used, these works ultimately follow an established metric-learning paradigm: (N) Collect a large dataset of related (and unrelated) items; (N) Propose a parameterized similarity function; and (N) Train the parameterized function such that related items are more similar than non-related items. Such metric-learning approaches can be incredibly flexible and powerful, and have been used to identify similarities between items ranging from music _cite_ to members of the same tribe _cite_ . Such methods work to some extent even in the presence of heterogeneity, since they learn to `ignore' dimensions where similarity should not be preserved. But we argue that ignoring such dimensions discards valuable information that ought to be used for prediction and recommendation. In this paper, we propose new models and algorithms to identify relationships between items in product recommendation settings. In particular, we relax the metricity assumption present in recent work, by proposing more flexible notions of `relatedness' while maintaining the same levels of speed and scalability. Specifically, we hope to overcome the following limitations of previous work: We propose a novel method,, or for short, that addresses the above limitations. We demonstrate our idea in Figure~ _ref_ (we later show an example on real data in Figure~ _ref_) . Here we embed the first item _inline_eq_ (the query) into one space (the `anchor space'), and embed its potential match _inline_eq_ into a series of _inline_eq_ additional spaces. Now, the relatedness between _inline_eq_ and _inline_eq_ is measured in terms of multiple notions, each captured by one of the _inline_eq_ spaces involved. Furthermore, the _inline_eq_ spaces are according to a mixtures-of-experts type framework, determining to what extent each of the _inline_eq_ embeddings is `relevant' to a particular query. Note in particular that the method described in Figure~ _ref_ can learn relationships since we are measuring the distance between two embeddings. The learned relationships are not necessarily symmetric, nor do identity and transitivity necessarily hold; on the other hand the model is flexible enough such that a metric embedding be learned if that was what the data supported. For clarity, our contributions are summarized as follows: Code and data can be found at _url_ .