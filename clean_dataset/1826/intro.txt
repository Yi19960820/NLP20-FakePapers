Deep Convolutional Neural Network has achieved tremendous success for image recognition, since it won the ImageNet image recognition task in N _cite_ . However, the current CNN-based image recognition paradigm is based on a feedforward neural network architecture, with fixed network depth and receptive field size. Such paradigm has several limitations. First, input images are usually resized to a small fixed size (e.g. NxN) as the network input. The resizing process often results in loss of details for small objects, leading to loss of recognition capacity. Although, it's psossible to use global average pooling to deal with larger size of images than the native resolution of the CNN, the receptive field size of the CNN remains the same. Consequently, although the CNN can "see" more local details, it also loses its capability to capture larger scale spatial structures of objects. The only way to increase receptive field size is to further increases network depth, which, to some point, would make the computation of training or inference intractable. Second, it lacks computational scability for large or variable sized images, in the sense that for both large or small images, it has the same computational cost due to the resizing process. It would be more desirable in real application, especially on the mobile and embedded platforms, that for large images we can have certain accuracy-cost tradeoff that can result in better accuracy if more computational resources are used. Third, the feedforward network based paradigm is quite different from human's object recognition system. According to the research of neural science, human's visual perception system involves both feedforward processing and recurrent processing _cite_ . It is also an multi-scale processing mechanism, where the P pathway that involves Midget retinal ganglion cells and Parvocellular cell in LGN is responsible for a fast feedforward process with less details, while the M pathway is a slower feedforward process with more details. Partly inspired by the human visual system for object recognition, this paper explores a different paradigm for image recognition from the traditional CNN-based feedforward network. The paradigm is based on a recurrent network with its recurrence relation defined along image scale rather than the traditional temporal or spatial direction. A CNN base network is embedded into the recurrent network as a feature extractor for different scales. The overall network is named Scale Recurrent Neural Network (SRNN), due to its recurrence relation along the scale direction. The input of a SRNN is a list of images resized with different scales from a single input image. This allows it to process variable-sized images, with large input images usually having more unrolled steps of the SRNN. An important property and advantage of SRNN is that it can both capture fine details of the input image (by the large-sized copies of image) and the large-scale structures of objects (by the small-sized copies) . In other word, it has the capability to significantly increase the range of the receptive field of the neural network. Another benefit of SRNN is its computational scability. The inference process starts from a small-scale image copy with smaller computational cost and lower recognition accuracy. And we can achieve higher accuracy on-demand by feeding additional larger-scale image copies. The inference process can stop at any scale according to the computational budget. The SRNN based paradigm can be viewed as CNN classifier with scale integration. This scale integration process can also be achieved by a straightforward ensemble classifier of averaging inference results of the base CNN on different image scales. It maybe be also achieved by fully connected layers. However, the recurrent network approach allows training and recognition on images with variable sizes, and it also allows us to borrow existing RNN techniques, such as GRU, for further performance boost. In our experiments, we show that SRNN is able to significantly outperform the simple scale ensemble approach. In addition, our proposed GRU variant for SRNN, called half-GRU, also significantly outperform SRNN witih vanilla RNN.