Action recognition and activity understanding in videos are imperative elements of computer vision research. Over the last few years, deep learning techniques dramatically revolutionized research areas such as image classification, object segmentation _cite_ and object detection _cite_ . Likewise, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have been popular in the action recognition task _cite_ . However, various network architectures have been proposed with different strategies on the incorporation of video temporal information. However, despite all these variations, their performance improvements over the finetuned image classification network are still relatively small. Unlike image classification, the most distinctive property of video data is the variable-length. While Images can be readily resized to the same spatial resolution, it is difficult to subsample videos temporally. Therefore, it is difficult for the early ND ConvNet _cite_ to achieve action recognition performance on par with the sophisticated hand-crafted iDT _cite_ representations. In addition, some of the legacy action recognition datasets ({\em e.g.}, KTH _cite_) only contain repetitive and transient actions, which are rarely seen in everyday life and therefore have limited practical applications. With more realistic actions included (with complex actions, background clutter and long temporal duration), the more recent action recognition dataset, {\em e.g.}, YouTube's sports, daily lives videos (UCF-N _cite_) and isolated activities in movies (HMDB-N _cite_), offer much more realistic challenges to evaluate modern action recognition algorithms. Therefore, all experimental results in this paper are based on the UCF-N and HMDB-N datasets. Previous multi-stream architecture, such as the two-stream CNN _cite_, suffers from a common drawback, their spatial CNN stream is solely based on a single image randomly selected from the entire video. For complicated activities and relatively long action videos (such as the ones in the UCF-N and HMDB-N datasets), viewpoint variations and background clutter could significantly complicate the representation of the video from a single randomly sampled video frame. A recent remedy was proposed in the Temporal Segment Network (TSN) _cite_ with a fusion step which incorporates multiple snippets . Inspired by the success of the attention model widely used in natural language processing _cite_ and image caption generation _cite_, the Attention-based Temporal Weighted CNN (ATW) is proposed in this paper, to further boost the performance of action recognition by the introduction of a benign competition mechanism between video snippets. The attention mechanism is implemented via temporal weighting: instead of processing all sampled frames equally, the temporal weighting mechanism automatically focuses more heavily on the semantically critical segments, which could lead to reduced noise. In addition, unlike prior P-CNN _cite_ which requires additional manual labeling of human pose, a soft attention model is incorporated into the proposed ATW, where such additional labeling is eliminated. Each stream of the proposed ATW CNN can be readily trained end-to-end with stochastic gradient descent (SGD) with backpropagation using only existing dataset labels. The major contributions of this paper can be summarized as follows. (N) An effective long-range attention mechanism simply implemented by temporal weighting; (N) each stream of the proposed ATW network can be optimized end-to-end, without requiring additional labeling; (N) state-of-the-art recognition performance is achieved on two public datasets.