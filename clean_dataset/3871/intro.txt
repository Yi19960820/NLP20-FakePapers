Meta learning is a method of learning from learned knowledge that can be used to improve the performance of various learning tasks~ _cite_ . In a typical example where the learning task is classification, meta learning is applied by first training multiple classifiers on the training data. Each classifier may use either all of the training data, or only a subset which may differ from other subsets in the feature space. A test set is then fed into these classifiers and the resulting output is then used as input to train an overall meta classifier such as a majority vote or weighted majority vote. Other variations on meta learning applied to classification exist~ _cite_ . Meta learning can incorporate information about the feature space that is independent of the classifiers such as the Bayes error rate (BER) . Consider the problem of classifying a feature vector _inline_eq_ into one of two classes _inline_eq_ or _inline_eq_ . Denote the class probabilities as _inline_eq_ and _inline_eq_ . The conditional densities of _inline_eq_ given that _inline_eq_ belongs to _inline_eq_ or _inline_eq_ are denoted by _inline_eq_ and _inline_eq_, respectively, and the Bayes classifier assigns _inline_eq_ to _inline_eq_ if and only if _inline_eq_ . If _inline_eq_, the average error rate of this classifier, known as the BER, is The BER is the minimum classification error rate that can be achieved by any classifier on _inline_eq_ 's feature space~ _cite_ . Because of this property, the BER can be used in a meta learning problem where the base classifiers are trained on different feature spaces by weighting the output of the base classifiers based on the Bayes error of the underlying feature space. If a given feature space results in a lower Bayes error than another feature space, then the output of the corresponding classifier would have a higher weight as it would presumably perform better than a classifier on the alternate feature space. The BER can be used at other stages of meta learning such as in the selection of the base classifiers and model selection. This is because the BER provides a benchmark for classification on a given feature space. If a specific classifier applied to the feature space yields an estimated error rate that is significantly above the BER, then it is likely that a different classifier or parameters may result in a lower error rate. On the other hand, if the classifier's estimated error rate is below the BER, then the classifier is likely to be overfitting the data and may not generalize well to new samples from the feature space. A different classifier or parameters may then be chosen. This technique can also be applied in the traditional supervised learning approach where a single classifier is used. The BER can also be beneficial for feature selection in classification problems. The BER is monotonic in the number of features in the sense that increasing the number of features does not decrease the accuracy of the Bayes classifier. However, for many classifiers, including irrelevant features can decrease the prediction accuracy~ _cite_ . Including a large number of features can also be computationally burdensome and create difficulties in storage and memory~ _cite_ . Thus from a practical perspective, using only a subset of the features may result in better performance. If the BER is known for all subsets of features, then a logical method of feature selection would be to choose the smallest subset of features such that the BER of that subset is negligibly larger than the BER of the full feature space~ _cite_ . The eliminated features could be considered redundant or irrelevant since including them in the classification leads to a neglible improvement in accuracy. Unfortunately, computing the BER requires perfect knowledge of the underlying data distributions, which is rarely available. Even for parametric models of the densities, Eq.~ _ref_ requires multi-dimensional integration and has no closed form solution for many models. Evaluating the BER in these cases involves computationally intensive numerical integration, especially for high dimensions. For these reasons, many feature selection algorithms have focused on other optimality criteria such as minimizing the prediction error of a specific classifier~ _cite_ or maximizing the statistical dependency between the feature subset and class assignments via some criterion such as mutual information or correlation~ _cite_ . However, selecting features by minimizing the classifier prediction error can be computationally intensive and only provides a solution for the specified classifier. Additionally, other methods based on maximizing statistical dependency can be too restrictive or otherwise problematic~ _cite_ . Given these problems, many bounds on the BER have been derived that are related to _inline_eq_-divergences~ _cite_ . These bounds have been used in applications involving the BER including feature selection~ _cite_ . Accurate estimation of these bounds on the BER requires accurate estimation of an _inline_eq_-divergence functional, often in a nonparametric setting. Until recently, little has been known about the properties of nonparametric _inline_eq_-divergence estimators such as convergence rates and the asymptotic distribution. In Moon and Hero~ _cite_, it was shown that the bias of simple density plug-in estimators of _inline_eq_-divergence converges very slowly to zero when the dimension of the feature space is high, which limits their utility. Nguyen et al~ _cite_ proposed an _inline_eq_-divergence estimation method based on estimating the likelihood ratio of two densities that achieves the parametric mean squared error (MSE) convergence rate when the densities are sufficiently smooth. However, this method can be computationally intensive for large sample sizes and the asymptotic distribution of the estimator is currently unknown. Berisha et al~ _cite_ also proposed a consistent estimator of specific bounds on the BER based on the construction of a minimal spanning tree (MST) that does not require density estimation. However, the convergence rate of this estimator is unknown and it is restricted to specific BER bounds instead of _inline_eq_-divergences in general. Finally, other _inline_eq_-divergence estimators have been proposed that achieve the parametric rate when the densities are sufficiently smooth~ _cite_ . However, some of these estimators are restricted to certain subsets of _inline_eq_-divergences, and they require an optimal kernel which can be difficult to implement and compute. Many of these problems can be countered effectively by using meta learning. While meta learning was described above in the classification setting, it can also be applied to estimation to improve the convergence rates. This is typically done by taking a weighted sum of base estimators that individually converge slowly. Then by an appropriate choice of weights, the weighted ensemble estimator converges rapidly to the true value. For example, Sricharan et al~ _cite_ derived a nonparametric estimator of generalized entropy functionals that converges at the parametric rate by using simple plug-in density estimators as the base estimators. More recently, similar theory was applied by Moon and Hero~ _cite_ to obtain a nonparametric _inline_eq_-divergence functional estimator based on a weighted ensemble of _inline_eq_-nearest neighbor (nn) estimators. This estimator enjoys the advantages of being simple to implement and achieving the parametric convergence rate when the densities are sufficiently smooth. In this paper, we focus on estimating multiple bounds on the Bayes error derived from _inline_eq_-divergence functionals in a nonparametric setting using the weighted _inline_eq_-nn estimator from~ _cite_ . We first estimate the bounds on simulated data where the true BER is computable. This gives a guide for the empirical utility of each bound. We then apply this to real data by estimating the bounds on the BER for the classification of sunspot images using the features derived in~ _cite_ . This gives a measure of the utility of the derived feature space in this supervised setting. We also compare the results to those obtained usinge the MST estimator~ _cite_ . The paper is outlined as follows. Section~ _ref_ describes the weighted _inline_eq_-nn estimator of _inline_eq_-divergence functionals while Section~ _ref_ provides the bounds on the Bayes error and their relation to _inline_eq_-divergences. In Section~ _ref_, the simulated results are presented. Section~ _ref_ describes the sunspot data and presents the estimated bounds on the BER. Section~ _ref_ concludes.