Generating realistic images has been actively pursued in the machine learning community in recent years. Achieving this goal requires true understanding of images, including the structure, semantics and so on. Deep generative models (DGMs) ~ _cite_ have attracted considerable attention recently because they provide us a new perspective to deeply understand vision data. Among various DGMs, generative adversarial networks (GANs) ~ _cite_ have gained the most interest as they learn a deep generative model for which no explicit likelihood function is required, but only a generative process~ _cite_ . Towards the goal of generating realistic images, various GANs have been proposed with varying degrees of success~ _cite_ . Most of them are non-structured and can be roughly categorized into two categories: single-generator based and multi-generator based, depending on the number of generators employed. Single-generator based approaches, for example~ _cite_, try to modify the objective of GANs, or the optimization strategies to guide the training process. Multiple-generator based approaches, for example, the work proposed by~ _cite_, employ multiple generators to generate more diverse images. However, both categories of approaches ignore that realistic generation essentially depends on truly understanding data, especially the structure. A meaningful step forward in the regard was made by the Graphical GAN (GMGAN) ~ _cite_, which employs Bayesian networks to model the structured generative process of images. However, GMGAN only defines a single generative process (\ie~generator) transforming from mixture of Gaussian noise to images. In fact, real-world images, such as images in the CIFAR-N and ImageNet datasets, are highly complex and usually have multi-modality. For such complex data, a single generative process is almost impossible to fit for all images, resulting in problems like mode collapse and dropping~ _cite_ . To address these issues, we propose a multi-modal generative process for images based on GANs and use a probabilistic graphical model to represent the generation process, as illustrated in Fig.~ _ref_ . Our key idea lies in introducing an underlying generative mode _inline_eq_ for each sample, denoting which generative sub-process (\ie~generator _inline_eq_) it belongs to. To achieve more precise representation capability, we suppose the mode distribution _inline_eq_ could be distinct for each sample, while sampled from the same Dirichlet prior distribution~ _cite_ . This line of thinking leads to a new structured and implicit generative model: latent Dirichlet allocation based GANs (LDAGAN), which not only the natural multi-modality structure of image, but also can be more interpretable as a topic model~ _cite_ . Given the strong representation power of LDAGAN, a natural question then arises: To this end, we take an important step by presenting a variational inference and expectation-maximization (EM) algorithm in an adversarial process. Specifically, we utilize the discriminator in GANs to formulate the likelihood function for model parameters. In adversarial training, we maximize the above likelihood with respect to model parameters by virtue of EM algorithm. We make stochastic variation inference to ensure the training of LDAGAN is not time consuming. The main contributions of this paper are summarized as follows: (\romannumeralN) We build a structured GANs exploring multimodal generative process of images. (\romannumeralN) We present a variational EM algorithm for Bayesian network parameter estimation in adversarial training. (\romannumeralN) We achieve state-of-the-art performance on CIFAR-N~ _cite_, CIFAR-N~ _cite_ and ImageNet~ _cite_ datasets. For example, our method has achieved a value of _inline_eq_ for Fr chet Inception Distance on the ImageNet dataset, which is currently the best reported result with standard CNN architecture in literature to our knowledge.