\noindent Recently, deep structured networks such as deep convolutional (CNN) and recurrent (RNN) neural networks have become increasingly popular in artificial intelligence, showing remarkable performance on many real-world problems, including scene classification~ _cite_, speech recognition~ _cite_, gaming~ _cite_, semantic segmentation~ _cite_, and image captioning~ _cite_ . However, like most machine learning techniques, current deep learning approaches are based on conventional statistics and require the problem to be formulated in a structured way. In particular, they are designed to learn a model for a distribution (or a function) that maps a structured input, typically a vector, a matrix, or a tensor, to a structured output. Consider the task of image classification as an example. The goal here is to predict a label (or a category) of a given image. The most successful approaches address this task with CNNs, \ie by applying a series of convolutional layers followed by a number of fully connected layers~ _cite_ . The final output layer is a fixed-sized vector with the length corresponding to the number of categories in the dataset (\eg~N in the case of the ILSVR Challenge~ _cite_) . Each element in this vector is a score or probability for one particular category such that the final prediction corresponds to a probability distribution over all classes. The difficulty arises when the number of classes is unknown in advance and in particular varies for each example. Then, the final output is generated heuristically by a discretization process such as choosing the _inline_eq_ highest scores~ _cite_, which is not part of the learning process. This shortcoming concerns not only image tagging but also other problems like detection or graph optimization, where connectivity and graph size can be arbitrary. We argue that such problems can be naturally expressed with sets rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. Therefore, learning approaches built on conventional statistics cannot be the right choice for these problems. In this paper, we propose a learning approach based on point processes and finite set statistics to deal with sets in a principled manner. More specifically, in the presented model, we assume that the input (the observation) is still structured, but the output is modelled as a set. Our approach is inspired by a recent work on set learning using deep neural networks~ _cite_ . The main limitation of that work, however, is that the approach employs two sets of independent weights (two independent networks) to generate the cardinality and state distributions of the output set. In addition, to generate the final output as a set, sequential inference has to be applied instead of joint inference. In this paper, we derive a principled formulation for performing both learning and inference steps jointly. The main contribution of the paper is summarised as follows: