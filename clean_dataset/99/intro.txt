High Dynamic Range imaging (HDRI) emerged in recent years as a major research topic in the context of computational photography, where the main purpose is to bridge the gap between the dynamic range native to the scene and the relatively limited dynamic range of the camera. As a result, the level of details in the captured LDR image is significantly enhanced, so that the final image presents a balanced contrast and saturation in all parts of the scene. The most common approach to render an HDR image with a camera presenting a limited dynamic range is called ~ _cite_ . It relies on merging several LDR images of the same scene captured with different exposures. By alternating the exposure settings between under-exposure and over-exposure, the input stack of LDR images contains various sets of details in different areas of the scene. These details are combined together into a single HDR image by estimating the inverse of the (CRF) . The resulting image finally undergoes a tone-mapping operation for display purposes. Another alternative is called (EF) ~ _cite_ . The main difference between both approaches is that exposure fusion directly merges the input LDR images to produce a final high-quality LDR image without the usage of the CRF. The visual characteristics of the resulting fused image are similar to a tone-mapped HDR image (pseudo-HDR image) . The direct merging of the input images represents a clear advantage over exposure bracketing since prior information about the exposure settings are not needed and no estimation of the inverse CRF is required. This results in a decrease of the computational complexity while yielding high quality enhancement results. Both exposure bracketing and exposure fusion are based on the assumption that the input LDR images are aligned. However, misalignment due to camera-of scene-motion will almost always occur, especially when the input images are captured sequentially. As a result, the output image contains strong artifacts where several instances of the same object can be seen. These artifacts are known as the . Whether the HDRI system is based on exposure bracketing or exposure fusion, removing these artifacts from the final image is a very challenging task. In this work, we aim at taking advantage of the latest advances achieved by ConvNets in classification and image enhancement topics. In a nutshell, our main goal is to combine the tasks of details enhancement and the removal of motion-induced ghost artifacts into a single framework. This is achieved by creating an end-to-end mapping which learns the exposure fusion for dynamic scenes . In other words, our trained model yields a final artifact-free image which disposes of a wider range of details, based on input LDR images presenting motion-related scene differences and color/exposure differences. Similar to exposure fusion, the output of our trained model is a LDR image, as no true HDR transformation is occurring. However, the visual attributes of the resulting image allows it to be labeled as a pseudo-HDR image. We test our learnable exposure fusion approach for dynamic scenes on several indoor and outdoor scenes and we show that the quality of our results improves upon state-of-the-art approaches. We show as well that our approach is capable of handling extreme cases in terms of motion and exposure difference between the input images, while maintaining a very low execution time. This makes it suitable for low-end capturing devices such as smartphones.