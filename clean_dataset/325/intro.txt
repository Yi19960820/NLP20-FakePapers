For the past few years, deep learning models have been used extensively to solve various machine learning tasks. One of the underlying assumptions is that deep, hierarchical models such as convolutional networks create useful representation of data (), which can then be used to distinguish between available classes. This quality is in contrast with traditional approaches requiring engineered features extracted from data and then used in separate learning schemes. Features extracted by deep networks were also shown to provide useful representation () which can be, in turn, successfully used for other tasks () . Despite their importance, these representations and their corresponding induced metrics are often treated as side effects of the classification task, rather than being explicitly sought. There are also many interesting open question regarding the intermediate representations and their role in disentangling and explaining the data () . Notable exceptions where explicit metric learning is preformed are the variants (), in which a contrastive loss over the metric induced by the representation is used to train the network to distinguish between similar and dissimilar of examples. A contrastive loss favours a small distance between pairs of examples labeled as similar, and large distances for pairs labeled dissimilar. However, the representations learned by these models provide sub-par results when used as features for classification, compared with other deep learning models including ours. Siamese networks are also sensitive to calibration in the sense that the notion of similarity vs dissimilarity requires context. For example, a person might be deemed similar to another person when a dataset of random objects is provided, but might be deemed dissimilar with respect to the same other person when we wish to distinguish between two individuals in a set of individuals only. In our model, such a calibration is not required. In fact, in our experiments here, we have experienced hands on the difficulty in using Siamese networks. We follow a similar task to that of . For a set of samples _inline_eq_ and a chosen rough similarity measure _inline_eq_ given through a training oracle (e.g how close are two images of objects semantically) we wish to learn a similarity function _inline_eq_ induced by a normed metric. Unlike 's work, our labels are of the form _inline_eq_ for triplets _inline_eq_ of objects. Accordingly, we try to fit a metric embedding and a corresponding similarity function satisfying: _eq In our experiment, we try to find a metric embedding of a multi-class labeled dataset. We will always take _inline_eq_ to be of the same class as _inline_eq_ and _inline_eq_ of a different class, although in general more complicated choices could be made. Accordingly, we will use the notation _inline_eq_ and _inline_eq_ instead of _inline_eq_ . We focus on finding an _inline_eq_ embedding, by learning a function _inline_eq_ for which _inline_eq_ . Inspired from the recent success of deep learning, we will use a deep network as our embedding function _inline_eq_ . We call our approach a . A similar approach was proposed in _cite_ for the purpose of learning a ranking function for image retrieval. Compared with the single application proposed in _cite_, we make a comprehensive study of the triplet architecture which is, as we shall argue below, interesting in and of itself. In fact, we shall demonstrate below that the triplet approach is a strong competitor to the Siamese approach, its most obvious competitor.