Human activity recognition has been used in many real-world applications. In environments that require a higher level of security, surveillance systems can be used to detect and prevent abnormal or suspicious activities such as robberies and kidnappings. In addition, human activity recognition can be employed in systems for video retrieval, so that a user is able to search for videos containing specific activities. Another type of application is in health care, such as activities of daily living monitoring systems. Surveillance applications have traditionally relied on network cameras monitored by human operators that must be aware of the activities carried out by people who are in the camera field of view. With the recent growth in the number of cameras to be analyzed, the efficiency and accuracy of human operators has reached the limit~ _cite_ . Therefore, security agencies have attempted computer vision-based solutions to replace or assist the human operator. Automatic recognition of suspicious activities is a problem that has attracted the attention of researchers in the area~ _cite_ . A significant portion of the progress on activity recognition task has been achieved with the design of discriminative feature descriptors exploring temporal information. Such information is based on motion analysis and is very important to represent the video in a more discriminative space, allowing the improvement of activity recognition. Over the last decade, most of the works focused on designing handcrafted local feature descriptors~ _cite_ or on encoding schemes using mid-level representations, such as Bag-of-Words (BoW) ~ _cite_ or Fisher vector (FV) ~ _cite_, followed by Support Vector Machines (SVM) classifier. Nowadays, large efforts have been directed to the employment of deep Convolutional Neural Networks (CNNs) . These architectures learn hierarchical layers of representations to perform pattern recognition and have demonstrated impressive results on many pattern recognition tasks (e.g., image classification~ _cite_ and face recognition~ _cite_) . Although the excellent improvements achieved in such tasks, activity recognition lacks on performance when using CNNs. Many works~ _cite_ point that the potential reason behind such gap falls in two cases: (i) current datasets do not have enough videos for training and are too much noisy; and (ii) current CNN architectures are still not able to handle temporal information (or to take full advantage of it), consequently letting spatial (appearance) information prevail. A major breakthrough spatiotemporal information representation was achieved by Simonyan and Zisserman~ _cite_, who directly incorporated motion information by using optical flow instead of learning it from scratch, showing significant improvement over other approaches. Known as two-stream network, their architecture is composed of two stream of data: (i) spatial network, which takes as input the raw RGB pixels; and (ii) temporal network, which takes as input dense optical flow displacement fields (vertical and horizontal components) computed across the frames. The final predictions are computed as the average of the output scores from the two streams. To further improve the representation of spatiotemporal information, this work introduces a new temporal stream for the two-stream networks to perform activity recognition, named (MOS) . The method is based on non-linear transformations on the optical flow components to generate input images for the temporal stream. Our hypothesis is based on the assumption that the motion information on a video sequence can be described by the spatial relationship contained on the local neighborhood of magnitude and orientation extracted from the optical flow. More specifically, we assume that the motion information is adequately specified by fields of magnitude and orientation. In view of that, our method captures not only the displacement, by using orientation, but also magnitude providing information regarding the velocity of the movement. In the literature, magnitude and orientation information are often used to describe motion information in various local handcrafted-based features, such as Motion Boundary Histogram (MBH) ~ _cite_, Histogram of Oriented Flow (HOF) ~ _cite_, Histograms of Optical Flow Orientation and Magnitude (HOFM) ~ _cite_ and Optical Flow Co-occurrence Matrices (OFCM) ~ _cite_ . However, none of the aforementioned methods used such information on an end to end learning scheme with a CNN, as the proposed approach does. According to the experimental results, our proposed temporal stream used as input to existing neural network architectures is able to recognize activities accurately on two well-know datasets (UCFN~ _cite_ and HMDBN~ _cite_) outperforming the results achieved by the original two-stream network as well as other deep networks available in the literature.