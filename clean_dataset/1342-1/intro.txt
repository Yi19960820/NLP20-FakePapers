Scene understanding is a fundamental yet challenging task in computer vision, which has a great impact on other applications such as autonomous driving and robotics. Classic tasks for scene understanding mainly include object detection, instance segmentation and semantic segmentation. This paper considers a recently proposed task named {\em panoptic segmentation} ~ _cite_, which aims at finding all foreground (FG) objects (named {\em things}, mainly including countable targets such as {\em people}, {\em animals}, {\em tools}, {\em etc.}) at the instance level, meanwhile parsing the background (BG) contents (named {\em stuff}, mainly including amorphous regions of similar texture and/or material such as {\em grass}, {\em sky}, {\em road}, {\em etc.}) at the semantic level. The benchmark algorithm~ _cite_ and MS-COCO panoptic challenge winners~ _cite_ dealt with this task by directly combining FG instance segmentation models~ _cite_ and BG scene parsing~ _cite_ algorithms, which ignores the underlying relationship and fails to borrow rich contextual cues between {\em things} and {\em stuff} . In this paper, we present a conceptually simple and unified framework for panoptic segmentation. To facilitate information flow between FG {\em things} and BG {\em stuff}, we combine conventional instance segmentation and semantic segmentation networks, leading to a unified network with two branches. This strategy brings an immediate improvement in segmentation accuracy as well as higher efficiency in computation (because the network backbone can be shared) . This implies that panoptic segmentation benefits from complementary information provided by FG objects and BG contents, which lays the foundation of our approach. Going one step further, we explore the possibility of integrating higher-level visual cues ({\em i.e.}, beyond the features extracted from the end of the backbone) towards the more accurate segmentation. This is achieved via two attention-based modules working at the object level and the pixel level, respectively. For the first module, we refer to the regional proposals, each of which indicates a possible FG {\em thing}, and adjusts the probability of the corresponding region to be considered as FG {\em things} and BG {\em stuff} . For the second module, we take out the FG segmentation mask, and use it to refine the boundary between FG {\em things} and BG {\em stuff} . In the context of deep networks, these two modules, named the Proposal Attention Module (PAM) and Mask Attention Module (MAM), respectively, are implemented as additional connections across FG and BG branches. Within MAM, a new layer named {\em RoIUpsample} is designed to define an accurate mapping function between pixels in the fixed-shape FG mask and the corresponding feature map. In practice, all additional connections go from the FG branch to the BG branch, mainly due to the observation that FG segmentation is often more accurate . Furthermore, BG {\em stuff}, while being refined by FG {\em things}, also gives feedback via gradients. Consequently, both FG and BG segmentation accuracies are considerably improved. The overall approach, named Attention-guided Unified Network ({\bf AUNet}), can be easily instantiated to various network backbones, and optimized in an end-to-end manner. We evaluate AUNet in two popular segmentation benchmarks, namely, the MS-COCO~ _cite_ and Cityscapes~ _cite_ datasets, and claim {\bf the state-of-the-art performance} in terms of PQ, a standard metric integrating accuracies of both {\em things} and {\em stuff} ~ _cite_ . In addition, the benefits brought by joint optimization and two attention-based modules are verified through an extensive ablation study~ _ref_ . The major contribution of this research is to present a simple and unified framework for both FG and BG segmentation, which reaches the top performance in MS-COCO~ _cite_ and Cityscapes~ _cite_ datasets. Furthermore, this work also investigate the complementary information delivered by FG objects and BG contents. While panoptic segmentation serves as a natural scenario of studying this topic, its application lies in a wider range of visual tasks. Our solution, AUNet, is a preliminary exploration in this field, yet we look forward to more efforts along this direction. The remainder of this paper is organized as follows. Section~ _ref_ briefly reviews related work. Section~ _ref_ elaborates the proposed AUNet, including two attention-based modules. After experiments are shown in Section~ _ref_, we conclude this work in Section~ _ref_ .