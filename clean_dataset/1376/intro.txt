Vectorial image representation is a fundamental problem in computer vision field. In many visual analysis systems, the visual content in an image is usually represented into a fix-sized vector for convenience of the followed processing. In recent years a lot of effort has been made on first designing the handcraft visual features _cite_ and then aggregating the visual features into a single vector _cite_ . The bag-of-visual-words (BoVW) model is one of the famous methods to construct image representation. In the BoVW model, firstly, a set of local invariant visual features are extracted on the detected image patches or the densely sampled grids. Then an image is represented into a visual word histogram based on the quantization results of local features with an off-line trained visual vocabulary. The visual vocabulary is usually trained with the unsupervised clustering algorithm, such as the standard _inline_eq_-means, hierarchical _inline_eq_-means _cite_, approximate _inline_eq_-means _cite_ . Usually the quantization is performed by the nearest neighbor or the approximate nearest neighbor method. Namely each local invariant visual feature is quantized to its nearest or approximate nearest visual word in the vocabulary, which is a kind of hard vector quantization. Instead of the hard vector quantization, in _cite_, Wang et al. proposed a locality linear coding approach to quantize each local visual feature. Kernel method is another alternative to transform a set of features into a vectorial representation, such as Fisher kernel _cite_, and democratic kernel _cite_ . Fisher kernel models the joint probability distribution of the visual features detected in an image. The vectorial representation is constructed based on the derivatives in the parameter space. Besides the quantization results in the BoVW model, Fisher kernel also includes the residual information between the local visual features and their visual words _cite_ . Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications _cite_ . One non probabilistic version of Fisher kernel is carefully investigated in _cite_, which is named as vector of locally aggregated descriptors (VLAD) . Instead of designing the handcraft visual features, such as SIFT _cite_, SURF _cite_, and HOG _cite_, deep convolutional neural network (CNN) _cite_ learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet _cite_ . With the learned non-linear transformation model, each image can be transformed to a feature vector _cite_ . With deep nets to learn from large-scale dataset, the CNN model can well discriminate diverse visual content, which is desired in many visual information processing systems. With breakthrough in many computer vision tasks, the CNN model has made a milestone in visual representation and become a new benchmark baseline _cite_ . A lot of efforts have been made to understand the representation ability of convolutional neural network _cite_ . In _cite_, Goodfellow et al. test the invariance of deep networks with a natural video dataset and find that the ``deep" structure can obtain more invariance than the ``shallow" ones. In _cite_, Zeiler and Fergus try to understand why deep convolutional neural network works very well. They propose to visualize the patterns activated by the intermediate layers with a deconvolutional network. It is revealed that some complex patterns can be captured by top layers, which is very amazing. In _cite_, Lenc et al. study the mathematical properties of equivariance, invariance, equivalence of image representations such as SIFT or CNN from the theoretical perspective. In _cite_, Cimpoi et al. conduct a range of experiments on material and texture attribute recognition and find that CNN can also obtain excellent result on this topic. In _cite_, Long et al. study the learned correspondence at a fine level of CNN and reveal that good keypoint prediction can be obtained with the learned intermediate CNN features. More specifically, in _cite_, Razavian et al. demonstrate that local spatial information of image is also conveyed by CNN and this local information can be used to perform facial landmark prediction, semantic segmentation, and object keypoints detection. However, CNN is suitable to describe these images with a single object localized at the center, namely those roughly aligned images as shown in Fig. _ref_ . For a complex image with multiple objects, it is unsuitable to extract a single global CNN feature as shown in Fig. _ref_ because there may exits geometric transformations on these objects. As a more reasonable alternative, we can firstly align the content of the image and then construct the global vectorial representation. Hence, inspired by the invariant representation via pooling local features, in this paper we propose to represent image with local CNN to address the translation and re-sizing invariance issue and pool the transformed CNN feature to achieve a fix-sized rotation-invariant representation, which we call the kernelized convolutional neural network (KCNN) in the following. Specifically, we first detect some object-like patches from the given image. Then for each detected object-like patch, we extract CNN feature to describe the object in it. Finally to form a vectorial representation of the whole image, we aggregate these object-level CNN features with kernel function as shown in Fig. _ref_ . We organize the rest of the paper as follows. In Section _ref_, we present some studies on the sensitivity of global CNN feature to three specific transformations. In Section _ref_, we introduce our algorithm in detail. The experimental results are presented in Section _ref_ . Finally we make conclusions in Section _ref_ .