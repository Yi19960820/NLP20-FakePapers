How-to videos on sites such as YouTube and Vimeo, have enabled millions to learn new skills by observing others more skilled at the task. From drawing to cooking and repairing household items, learning from videos is nowadays a commonplace activity. However, these loosely organized collections normally contain a mixture of contributors with different levels of expertise. The querying person needs to decide who is better and who to learn from. Furthermore, the number of how-to videos is only likely to increase, fueled by more cameras recording our daily lives. An intelligent agent that is able to assess the skill of the subject, or rank the videos based on the skill displayed, would enable us to delve into the wealth of this on-line resource. In this work, we attempt to determine skill for a variety of tasks from their video recordings. We base this work on two assumptions, first-for tasks where human observers {\it consistently} label one video as displaying more skill than another, there is enough information in the visual signal to automate that decision; and second-the same framework for determining skill can be used for a variety of tasks ranging from surgery to drawing and rolling pizza dough. We propose to determine skill using a pairwise deep ranking model, which characterizes the difference in skill displayed between a pair of videos, where one is ranked higher than the other by human annotators ~ (Fig~ _ref_) . We use a Siamese architecture where each stream is made up of a two-stream (spatial and temporal) convolutional neural network (NS-CNN) . This Siamese architecture is trained using a novel ranking loss function that considers the extent of the task within the video, and includes pairs of videos where the skill level is indistinguishable. By assigning videos a relative score of skill for the given task, we can predict a skill ranking for a set of videos. Our main contributions are as follows: i) ~We present the first method to determine skill in videos for a wide variety of tasks. ii) We propose a novel ranking loss function which considers the extent of the video and incorporates pairwise similarities in training. This loss function outperforms the standard ranking loss on all datasets by up to N \%. iii) ~We present pairwise skill annotations for three datasets, two of which are newly recorded. iv) ~We evaluate our approach on four datasets (two public) ; one surgical-for which there is authoritative expert ranking, another on rolling pizza dough, as well as two newly introduced datasets for the tasks of drawing and using chopsticks. Newly recorded datasets and annotations are available from the authors' webpages.