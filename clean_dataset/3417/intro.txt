Action Units (AUs) are the basic facial movements that work as the building blocks in formularizing multiple facial expressions. The successful detection of AUs will greatly facilitate the analysis of the complicated facial actions or expressions. AU detection has been studied for decades as one of the basic facial computing problems and many interesting approaches have been proposed. Classical approaches in AU detection either focus on facial landmark-based local features or appearance-based global features. A number of deep learning approaches have also been proposed to learn deeper facial representations that result in better AU detection. However, some essential problems are still not solved completely. Due to different features for different facial components, individual AUs may need to be considered separately. One image may include multiple AUs, therefore whether training single AU or multi-label AUs has to be analyzed. Since all actions appear in a temporal instead of just static mode, fusing temporal information becomes necessary. So, to achieve the best AU detection performance, all the three aspects need to be considered. Since CNNs have proved to be a powerful tool in solving many image-based tasks and several novel deep structures and frameworks have been proposed, we choose these deep learning models to tackle the AU detection problems. Recently, region-based processing is used in the fast/faster RCNN for prediction of object's bounding box or objectiveness probability in _cite_ . This inspired us to design separate networks to learn features for different regions of interest. The success in applying LSTM (long and short term memory) in image caption generation _cite_ and human action recognition _cite_ led us to believe that it is a good temporal information fusing kernel which may be also useful for facial AU detection. After identifying the three problems and being inspired by these RCNN and LSTM approaches, we designed an adaptive region cropping based multi-label learning deep recurrent net. The structure of the proposed neural network is shown in Figure _ref_ . There are some unique features of the proposed network. Unlike conventional CNNs where the same convolutional filters are shared within the same convolutional layers, we crop individual regions of interest (ROIs) based on facial landmarks from all the feature maps. The red circle, for instance, represents an area of interest. So, these ROIs are learned individually and therefore important areas will be able to receive special attention. To fuse the temporal information of expressions, the features from the final fully connected layer are fed to several stacks (two in the figure for illustration purpose only) of LSTM layers. Then, the temporal features are used to predict all AUs simultaneously. Through this structure, our network can handle both the adaptive region learning and the temporal fusing problems. Comparing to existing approaches, our approach has the following unique contributions: N) A set of adaptive ROI cropping nets (ROI Nets) is designed to learn regional features separately. In the proposed network, each ROI has a local convolutional neural network. The convolutional filters will only be trained for corresponding regions. N) Multi-label learning is employed to integrate the outputs of those individual ROI cropping nets, which learns the inter-relationships of various AUs and acquires global features across sub-regions for AU detection. Multi-label and single AU based methods are compared. With additional AU correlations and richer global features, the multi-label learning approach shows slightly better performance. N) An LSTM-based temporal fusion recurrent net (LSTM Net) is proposed to fuse static CNN features, which makes the AU predictions more accurate than with static images only. This paper is organized as the follows. In Section N, we have introduced the problems in AU detection and the basic idea of our proposed approach. In Section N, we review the related work on AU detection including both traditional and deep learning approaches. We then explain our proposed region learning based CNN network in Section N. Section N describes the way the temporal information of the CNN features is fused with the LSTM layers. Experimental results are included in Section N where we evaluate our proposed approach in terms of regions cropping, multi-label learning and temporal fusion, and performance comparison against baseline approaches are also given. We conclude the paper in Section N.