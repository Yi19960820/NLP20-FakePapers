Video classification based on contents like human actions or complex events is a challenging task that has been extensively studied in the research community. Significant progress has been achieved in recent years by designing various features, which are expected to be robust to intra-class variations and discriminative to separate different classes. For example, one can utilize traditional image-based features like the SIFT _cite_ to capture the static spatial information in videos. In addition to the static frame based visual features, motion is a very important clue for video classification, as most classes containing object movements like the human actions require the motion information to be reliably recognized. For this, a very popular feature is the dense trajectories~ _cite_, which tracks densely sampled local frame patches over time and computes several traditional features based on the trajectories. In contrast to the hand-crafted features, there is a growing trend of learning robust feature representations from raw data with deep neural networks. Among the many existing network structures, Convolutional Neural Networks (CNN) have demonstrated great success on various tasks, including image classification~ _cite_, image-based object localization~ _cite_, speech recognition~ _cite_, etc . For video classification, Ji et al. ~ _cite_ and Karparthy~ et al. ~ _cite_ extended the CNN to work on the temporal dimension by stacking frames over time. Recently, Simonyan~ et al. ~ _cite_ proposed a two-stream CNN approach, which uses two CNNs on static frames and optical flows respectively to capture the spatial and the motion information. It focuses only on short-term motion as the optical flows are computed in very short time windows. With this approach, similar or slightly better performance than the hand-crafted features like~ _cite_ has been reported. These existing works, however, are not able to model the long-term temporal clues in the videos. As aforementioned, the two-stream CNN~ _cite_ uses stacked optical flows computed in short time windows as inputs, and the order of the optical flows is fully discarded in the learning process (cf. Section N) . This is not sufficient for video classification, as many complex contents can be better identified by considering the temporal order of short-term actions. Take ``birthday'' event as an example---it usually involves several sequential actions, such as ``making a wish'', ``blowing out candles'' and ``eating cakes''. To address the above limitation, this paper proposes a hybrid deep learning framework for video classification, which is able to harness not only the spatial and short-term motion features, but also the long-term temporal clues. In order to leverage the temporal information, we adopt a Recurrent Neural Networks (RNN) model called Long Short Term Memory (LSTM), which maps the input sequences to outputs using a sequence of hidden states and incorporates memory units that enable the network to learn when to forget previous hidden states and when to update hidden states with new information. In addition, many approaches fuse multiple features in a very ``shallow" manner by either concatenating the features before classification or averaging the predictions of classifiers trained using different features separately. In this work we integrate the spatial and the short-term motion features in a deep neural network with carefully designed regularizations to explore feature correlations. This method can perform video classification within the same network and further combining its outputs with the predictions of the LSTMs can lead to very competitive classification performance. Figure~ _ref_ gives an overview of the proposed framework. Spatial and short-term motion features are first extracted by the two-stream CNN approach~ _cite_, and then input into the LSTM for long-term temporal modeling. Average pooling is adopted to generate video-level spatial and motion features, which are fused by the regularized feature fusion network. After that, outputs of the sequence-based LSTM and the video-level feature fusion network are combined as the final predictions. Notice that, in contrast to the current framework, alternatively one may train a fusion network to combine the frame-level spatial and motion features first and then use a single set of LSTM for temporal modeling. However, in our experiments we have observed worse results using this strategy. The main reason is that learning dimension-wise feature correlations in the fusion network requires strong and reliable supervision, but we only have video-level class labels which are not necessarily always related to the frame semantics. In other words, the imprecise frame-level labels populated from the video annotations are too noisy to learn a good fusion network. The main contributions of this work are summarized as follows: The rest of this paper is organized as follows. Section~N reviews related works. Section~N describes the proposed hybrid deep learning framework in detail. Experimental results and comparisons are discussed in Section~N, followed by conclusions in Section~N.