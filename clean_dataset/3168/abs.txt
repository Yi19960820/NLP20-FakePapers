In vision problems, multiple input sources can be used in combination to provide complementary prediction that may be redundant but convey information more effectively. Similarly, many semantic video analysis tasks can benefit from multiple, heterogenous signals. For example, sequences of RGB images and optical flow are usually processed simultaneously to boost the performance of human action recognition in videos. To learn from these heterogenous inputs, existing methods reply two-stream architectures that contain independent, parallel streams of networks. However, two-stream networks do not fully exploit the reciprocal information contained in the multiple signals, let alone exploit it in a recurrent manner. Therefore, we propose, in this paper, a novel recurrent architecture, termed Coupled Recurrent Network (CRN), to deal with multiple input sources. In CRN, the parallel streams of Recurrent Neural Networks (RNNs) are intertwined with each other using Recurrent Interpretation Block (RIB) and Recurrent Adaptation Block (RAB) . RIB supports learning of reciprocal representations from multiple signals. RAB makes the features adapted to the next recurrence. Different from the training of typical RNNs which stack the loss at each time step or the last time step, we propose an effective and efficient training strategy for CRN. Experiments show the efficacy of the proposed CRN. In particular, we achieve the new state-of-the-art on the benchmark datasets of human action recognition and multi-person pose estimation.