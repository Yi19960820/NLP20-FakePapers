Deep convolutional neural networks (CNNs) have been shown to substantially improve common image analysis tasks in computer vision and (bio-) medical imaging. They have in particular advanced research in automatic segmentation and image classification. Dense prediction based on fully-convolutional network (FCN) architectures _cite_ enables very accurate voxel-wise segmentation by a single forward pass of the input image through a trained CNN architecture _cite_ . However, FCNs also come with tremendous demand for memory and computational resources that can rarely be satisfied in clinical scenarios in particular when envisioning a mobile application of computer-assisted diagnosis and interventions. Furthermore, the translation of deep learning into interactive clinical workflows will require processing times of few seconds, which up-to date were only achievable using power-demanding GPUs. Surprisingly little research has been undertaken in deep learning for medical image analysis that attempts to limit model complexity. In this work, we address these challenges and present a new technique to advance state-of-the-art CNN and FCN approaches by introducing the TernaryNet--a versatile end-to-end trainable deep learning architecture that drastically reduces computational and memory demand for inference. We achieve this goal by replacing floating point matrix multiplications with ternary convolutions (based on sparse binary kernels), with both activations and weights restricted to values of _inline_eq_ . They can be calculated using a masked Hamming distance, a XOR / XNOR operation followed by a, and reduce computational demand by up to a factor of N. Our approach is not merely motivated by gains in computational performance, but also to explore the theoretical advantages of explicit sparsity promotion to reduce the risk of overfitting (as detailed in the following subsection) and learn more plausible neural network models. Our work extends recent approaches from computer vision that relied on binary convolutions _cite_, ternary weight networks _cite_, hashing by continuation _cite_ and our initial work on sparse binary convolutions _cite_ . The presented approach is to the best of our knowledge the first to use binary convolutions for semantic segmentation and the very first to propose ternary convolutions (and not only ternary weights since activations are also restricted) based on masked Hamming distances. The TernaryNet can be employed for any given image analysis task, e.g. landmark regression or image-level classification, but we chose to demonstrate its applicability to medical imaging for the automatic voxel-accurate segmentation of the pancreas in CT scans, which is a particularly demanding task. Pancreas segmentation is very important for computer assisted diagnosis of inflammation (pancreatitis) or cancer and furthermore to provide image-based navigational guidance for interventions, including endoscopy _cite_ . In the following, we will motivate the use of sparse binary kernels in deep convolutional networks and discuss related work for the use of quantisation in image analysis in particular in deep networks. Section _ref_ contains the detailed explanation of ternary quantisation and convolutions. Starting with a short discussion of current work on CT pancreas segmentation, we describe our experimental setup in Section _ref_ and compare different strategies and choices for model complexity reduction. We discuss our results, potentials for further research and future implications of our novel ternary convolution concept in Section _ref_ and end with some concluding remarks. Motivation for sparse binary kernels: Convolutional neuronal networks excel in image recognition tasks by mimicking the visual cortex of mammals. The visual information is detected by photoreceptor cells and transmitted and processed using multiple layers of neurons interconnected by synapses. Computational models have the capacity to replicate these mechanisms and can furthermore represent neural activations up to extremely high numerical precision (up to N decimal points) . However, in nature the simple structure of neural cells and environmental influences severely limit the accuracy of subtle changes in activation and in addition the need to conserve energy may lead to a sparse as possible use of neural activity. Ohlshausen \& Field _cite_ and Lee et al. _cite_ therefore established the idea of sparse coding for pattern recognition and neural networks. Those works demonstrate that powerful convolutional filters can be learned using few non-zero values by means of sparsity inducing LN norms and a feature sign searching algorithm. Furthermore, we observe that the non-zero elements of these synthetic models of VN cells tend to be close to values + N and-N. Therefore, a ternary approximation of weights leads to only minor degradation of representational power (see Fig. _ref_) . Related work: Due to their computational efficiency binary codes and their comparison using the Hamming distance (which counts the number of dissimilar bits in a long binary vector) are becoming increasingly popular for demanding image analysis tasks. They have been employed for hashing based large-scale image retrieval _cite_, nearest-neighbour based segmentation _cite_ and image registration _cite_ . In computer vision binary descriptors are frequently used for realtime applications, e.g. tracking using BRIEF features _cite_ . There are, however, also cases where binarisation led to inadequate loss in representation quality as e.g. reported for lung nodule classification in _cite_ . In our recent prior work _cite_, we proposed the use of sparse binary kernels with very large receptive fields inspired by BRIEF features and dilated convolutions _cite_ that enabled highly accurate segmentations without complex network architectures. Similarly and concurrently _cite_ proposed local binary convolutions that are derived from local binary patterns. A key limitation of these works is, however, that their design does not allow us to automatically train non-zero elements within binary kernels. Instead, they have to be chosen once at random (with a similar manual design as proposed in _cite_) . We also did not realise binary or ternary activations thus the use of efficient computations without floating point arithmetic was not possible. An alternative solution that has recently been proposed is the use of trained ternary filter weights _cite_ . In particular ternary weight networks _cite_ use a very simple, yet powerful, approximation and learning strategy based on the mild assumption of Gaussian statistics. They generalise the earlier ideas of _cite_ for learning binary weights and clearly demonstrate that ternarisation drastically reduces the accuracy gap to high precision weights. Another related approach by Liu et al. _cite_ employs decomposition methods for sparsification of convolution filters and proposes a new implementation for fast sparse matrix multiplication. While weight quantisation has quickly matured, another important aspect that has so far been only insufficiently addressed is the quantisation or sparsification of activations. Setting approximately half of the activations to zero using a rectifying linear unit (ReLU) is common practice in deep learning. Yet more drastic quantisation e.g. using the sign function as non-linear activation leads to strong artefacts during forward passes and no gradient for backpropagation. Courbariaux et al. _cite_ therefore proposes an adhoc solution that employs a rectangle (boxcar) function as a replacement, which was later also used in _cite_ . The downside of this approach is the fact that since two different functions are used during forward and backward propagation the training behaviour is ill-defined and potentially unstable. Cao et al. _cite_ propose a more justifiable approach based on the continuation of the hyperbolic tangent, which approaches the sign function with increasing slope _inline_eq_ in its limit: They prove the convergence of this optimisation when employing a sequence of increasing values of _inline_eq_ during training. They limit the use of this function to the final layer within a framework for supervised hashing. In our work, we extend this concept to a ternary hyperbolic tangent as explained in detail in the following section and apply this function as nonlinearity throughout--for every activation--in our deep network models.