Stereo matching consists in matching every point from an image taken from one viewpoint to its physically corresponding one in the image taken from another viewpoint. The problem has applications in robotics~ _cite_, medical imaging~ _cite_, remote sensing~ _cite_, virtual reality and ND graphics and computational photography~ _cite_ . Recent developments in the field have been focused on stereo for hard / uncontrolled environments~ (wide-baseline, low-lighting, complex lighting, blurry, foggy, non-lambertian) ~ _cite_, usage of high-order priors and cues~ _cite_, and data-driven, and in particular, deep neural network based, methods~ _cite_ . This work improves on this latter line of research. The first successes of neural networks for stereo matching were achieved by substitution of hand-crafted similarity measures with deep metrics~ _cite_ inside a legacy stereo pipeline for the post-processing (often~ _cite_) . Besides deep metrics, neural networks were also used in other subtasks such as predicting a smoothness penalty in a CRF model from a local intensity pattern~ _cite_ . In~ _cite_ a ``global disparity'' network smooth the matching cost volume and predicts matching confidences, and in~ _cite_ a network detects and fixes incorrect disparities. End-to-end deep stereo . Recent works attempt at solving stereo matching using neural network trained end-to-end without post-processing~ _cite_ . Such a network is typically a pipeline composed of {\bf embedding}, {\bf matching}, {\bf regularization} and {\bf refinement} modules: The {\bf embedding} module produces image descriptors for left and right images, and the (non-parametric) {\bf matching} module performs an explicit correlation between shifted descriptors to compute a cost volume for every disparity~ _cite_ . This matching module may be absent, and concatenated left-right descriptors directly fed to the {\bf regularization} module~ _cite_ . This strategy uses more context, but the deep network implementing such a module has a larger memory footprint as shown in Table~ _ref_ . In this work we reduce memory use without sacrificing accuracy by introducing a matching module that compresses concatenated left-right image descriptors into compact matching signatures. The {\bf regularization} module takes the cost volume, or the concatenation of descriptors, regularizes it, and outputs either disparities~ _cite_ or a distribution over disparities~ _cite_ . In the latter case, sub-pixel disparities can be computed as a weighted average with SoftArgmin, which is sensitive to erroneous minor modes in the inferred distribution. This {\bf regularization} module is usually implemented as a hourglass deep network with shortcut connections between the contracting and the expanding parts~ _cite_ . It composed of ND convolutions and not treat all disparities symmetrically in some models~ _cite_, which makes the network over-parametrized and prohibits the change of the disparity range without modification of its structure and re-training. Or it can use ND convolutions that treat all disparities symmetrically~ _cite_ . As a consequence these networks have less parameters, but their disparity range is still is non-adjustable without re-training due to SoftArgmin as we show in~ \S~ _ref_ . In this work, we propose to use a novel sup-pixel MAP approximation for inference which computes a weighted mean around the disparity with maximum posterior probability. It is more robust to erroneous modes in the distribution and allows to modify the disparity range without re-training. Finally, some methods~ _cite_ also have a {\bf refinement} module, that refines the initial low-resolution disparity relying on attention map, computed as left-right warping error. The training of end-to-end networks is usually performed in fully supervised manner~ (except of~ _cite_) . All described methods~ _cite_ use modest-size image patches during training. In this work, we show that training on a full-size images boosts networks ability to utilize large context and improves its accuracy. Also, the methods, even the ones producing disparity distribution, rely on _inline_eq_ loss, since it allows to train network to produce sub-pixel disparities. We, instead propose to use more ``natural'' sub-pixel cross-entropy loss that ensures faster converges and better accuracy. Our contributions can be summarize as follows: In the experimental section, we validate our contributions. In \S~ _ref_ we show how the reduced memory footprint allows to train on full-size images and to leverage large image contexts to improve performance. In \S~ _ref_ we demonstrate that, thanks to the proposed sub-pixel MAP and cross-entropy, we are able to modify the disparity range without re-training, and to improve the matching accuracy. Than, in \S~ _ref_ we compare our method to state-of-the-art baselines and show that it has smallest N-pixels error~ (NPE) and second smallest mean absolute error~ (MAE) on the FlyingThingsND set and ranked third and fourth on KITTI'N and KITTI'N sets respectively.