Essentially, Deep learning (DL) is motivated by the artificial intelligent (AI) research, where the objective is to replicate the human brain capability, i.e. to observe, learn, analyze and make a decision, particularly for complex problems _cite_ . DL is about learning the representation of a hierarchical feature, and it contains a variety of methods, such as neural network, hierarchical of probabilistic models, and supervised as well as unsupervised learning algorithms. _cite_ . The current good reputation of DL is due to the decrease in the price of computer hardware, improvement in the computational processing capabilities, and advanced research in the Machine Learning and Signal Processing _cite_ . In general, DL models can be classified into discriminative, generative, and hybrid models _cite_ . Recurrent neural network (RNN), deep neural networks (DNN), and convolutional neural networks (CNN) are some examples of Discriminative models. Examples of generative models are deep Boltzmann machine (DBM), regularized autoencoder, and deep belief network (DBN) . In the case of the hybrid model, it refers to a combination of generative and discriminative models. An example of such hybrid model is a pre-trained deep CNN using DBN, where it can improve the performance of deep CNN better than if it uses only random initialization. Among all of these DL techniques, this paper focuses on CNN. Although the good reputation of DL for solving any learning problem is known, how to train it is challenging. The successful proposal to optimize this technique using layered-wise pre-training was proposed by Hinton and Salakhutdinov _cite_ . Some other methods are Hessian-free optimization suggested by Marten _cite_, and Krylov Subspace Descent by Vinyal et al. _cite_ Recently, some of the metaheuristic algorithms have been used to optimize DL, especially CNN. Some papers _cite_ _cite_ _cite_ _cite_ _cite_ report that these methods can improve the accuracy of CNN. Metaheuristic is a powerful method to solve difficult optimization problems, and it has been used in almost all research area of engineering, science, and even industrial application _cite_ . In general, this method works with three main objectives, i.e. solving big problems, solving the problem faster, and finding robust algorithms _cite_ . Besides, they are not difficult to be designed, flexible, and relatively easy to be applied. Almost all metaheuristics algorithms inspired by nature, which is based on several principles of phenomena in physics, biology, and ethology. Some examples of biology phenomena are Differential Evolution (DE), Evolution Strategy (ES), Genetic Algorithm (GA) . Phenomena of physics are Threshold Accepting method (TA), Microcanonical Annealing (MA), Simulated Annealing (SA), and Ethology phenomena are Ant Colony Optimization (ACO), Firefly Algorithm (FA), Particle Swarm Optimization (PSO) _cite_ . Another metaheuristic phenomenon is inspired by music, such as Harmony Search algorithm _cite_ . Classifications of metaheuristic can also be based on single-solution based metaheuristic (S-metaheuristic) and population-based metaheuristic (P-metaheuristic) . Examples of S-metaheuristic are SA, TA, MA, Guided Local Search, and Tabu Search. In the case of P-metaheuristic, it can be divided into Swarm Intelligent (SI) and Evolutionary Computation (EC) . Examples of SI are FA, PSO, ACO, Bee Colony Optimization and examples of EC are GA, ES, DE _cite_ . Of the various types of the metaheuristic algorithm, in this paper we use the MA, with the consideration that the S-Metaheuristic is simple to implement on DL, and to the best of our knowledge, has never been used for optimizing CNN. The Macrocanonic algorithm is the variant of Simulated Annealing. Uses an adaptation of the Metropolis algorithm, the conventional SA algorithm aims to bring a system to equilibrium at decreasing temperatures _cite_ . On the other hand, MA based on Creutz’s microcanonical simulation technique, where the system’s evolution is controlled by its internal energy, not by its temperature. The advantages Creutz algorithm over the Metropolis algorithm is since it does not require the generation of quality random numbers or the evaluation of transcendental functions, thus allowing much faster implementation. Experiments on the Creutz method indicate that, it can be programmed to run an order of magnitude faster than the conventional Metropolis method for discrete systems _cite_ . A further significant advantage is that microcanonical simulation does not require high-quality random numbers. The organization of this paper is as follows: Section N is an introduction; Section N provides an overview of Microcanonical Annealing; Section N describes the method convolutional neural network; Section N presents the proposed method; Section N gives the results of the experiment, and lastly, Section N presents the conclusion of this paper.