In recent years, we have witnessed the great success of convolutional neural networks (CNN) _cite_ in a wide range of visual applications, including image classification _cite_, object detection _cite_, age estimation _cite_, etc. This success mainly comes from deeper network architectures as well as the tremendous training data. However, as the network grows deeper, the model complexity is also increasing exponentially in both the training and testing stages, which leads to the very high demand in the computation ability. For instance, the N-layer AlexNet _cite_ involves NM parameters and requires over NM FLOPs \footnotemark to classify a single image. Although the training stage can be offline carried out on high performance clusters with GPU acceleration, the testing computation cost may be unaffordable for common personal computers and mobile devices. Due to the limited computation ability and memory space, mobile devices are almost intractable to run deep convolutional networks. Therefore, it is crucial to accelerate the computation and compress the memory consumption for CNN models. For most CNNs, convolutional layers are the most time-consuming part, while fully-connected layers involve massive network parameters. Due to the intrinsical difference between them, existing works usually focus on improving the efficiency for either convolutional layers or fully-connected layers. In _cite_, low-rank approximation or tensor decomposition is adopted to speed-up convolutional layers. On the other hand, parameter compression in fully-connected layers is explored in _cite_ . Overall, the above-mentioned algorithms are able to achieve faster speed or less storage. However, few of them can achieve significant acceleration and compression simultaneously for the whole network. In this paper, we propose a unified framework for convolutional networks, namely Quantized CNN (Q-CNN), to simultaneously accelerate and compress CNN models with only minor performance degradation. With network parameters quantized, the response of both convolutional and fully-connected layers can be efficiently estimated via the approximate inner product computation. We minimize the estimation error of each layer's response during parameter quantization, which can better preserve the model performance. In order to suppress the accumulative error while quantizing multiple layers, an effective training scheme is introduced to take previous estimation error into consideration. Our Q-CNN model enables fast test-phase computation, and the storage and memory consumption are also significantly reduced. We evaluate our Q-CNN framework for image classification on two benchmarks, MNIST _cite_ and ILSVRC-N _cite_ . For MNIST, our Q-CNN approach achieves over N _inline_eq_ compression for two neural networks (no convolution), with lower accuracy loss than several baseline methods. For ILSVRC-N, we attempt to improve the test-phase efficiency of four convolutional networks: AlexNet _cite_, CaffeNet _cite_, CNN-S _cite_, and VGG-N _cite_ . Generally, Q-CNN achieves N _inline_eq_ acceleration and _inline_eq_ compression (sometimes higher) for each network, with less than N \% drop in the top-N classification accuracy. Moreover, we implement the quantized CNN model on mobile devices, and dramatically improve the test-phase efficiency, as depicted in Figure _ref_ . The main contributions of this paper can be summarized as follows: