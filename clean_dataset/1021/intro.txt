Deep neural networks (DNNs) have recently been setting new state of the art performance in many fields including computer vision, speech recognition as well as natural language processing. Convolutional neural networks (CNNs), in particular, have outperformed traditional machine learning algorithms on computer vision tasks such as image recognition, object detection, semantic segmentation as well as gesture and action recognition. These breakthroughs are partially due to the added computational complexity and the storage footprint, which makes these models very hard to train as well as to deploy. For example, the Alexnet _cite_ involves NM floating point parameters and NM high precision multiply-accumulate operations (MACs) . Current DNNs are usually trained offline by utilizing specialized hardware like NVIDIA GPUs and CPU clusters. But such an amount of computation may be unaffordable for portable devices such as mobile phones, tablets and wearable devices, which usually have limited computing resources. What's more, the huge storage requirement and large memory accesses may hinder efficient hardware implementation of neural networks, like FPGAs and neural network oriented chips. To speed-up test-phase computation of deep models, lots of matrix and tensor factorization based methods are investigated by the community recently _cite_ . However, these methods commonly utilize full-precision weights, which are hardware-unfriendly especially for embedded systems. Moreover, the low compression ratios hinder the applications of these methods on mobile devices. Fixed-point quantization can partially alleviate these two problems mentioned above. There have been many studies working on reducing the storage and the computational complexity of DNNs by quantizing the parameters of these models. Some of these works _cite_ quantize the pretrained weights using several bits (usually N _inline_eq_ N bits) with a minimal loss of performance. However, in these kinds of quantized networks one still needs to employ large numbers of multiply-accumulate operations. Others _cite_ focus on training these networks from scratch with binary (+ N and-N) or ternary (+ N, N and-N) weights. These methods do not rely on pretrained models and may reduce the computations at training stage as well as testing stage. But on the other hand, these methods could not make use of the pretrained models very efficiently due to the dramatic information loss during the binary or ternary quantization of weights. In this paper, we propose a unified framework called Fixed-point Factorized Network (FFN) to simultaneously accelerate and compress DNN models with only minor performance degradation. Specifically, we propose to first directly factorize the weight matrix using fixed-point (+ N, N and-N) representation followed by recovering the (pseudo) full precision submatrices. We also propose an effective and practical technique called weight balancing, which makes our fine-tuning (retraining) much more stable. We demonstrate the effects of the direct fixed-point factorization, full precision weight recovery, weight balancing and whole-model performance of AlexNet _cite_, VGG-N _cite_, and ResNet-N _cite_ on ImageNet classification task. The main contributions of this paper can be summarized as follows: