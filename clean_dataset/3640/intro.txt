Much of deep learning success in computer vision tasks such as image categorization and object detection stems from the availability of large annotated databases like ImageNet and MS-COCO. However, for SLAM-like pose tracking and reconstruction problems, there instead exists a fragmented ecosystem of smaller device-specific datasets such as the Freiburg-TUM RGBD Dataset _cite_ based on the Microsoft Kinect, the EuRoC drone/MAV dataset _cite_ based on stereo vision cameras and IMU, and the KITTI driving dataset~ _cite_ . We frequently ask ourselves: Obtaining accurate ground-truth pose measurements for a large number of environments and scenarios is difficult. Getting accurate alignment between ground-truthing sensors and a standard set of Visual SLAM sensors takes significant effort; it is expensive and is difficult to scale across variations in cameras. Category labeling in a crowd-sourced or pay-per-label Amazon Mechanical Turk fashion, as is commonly done for ImageNet-like datasets, suddenly seems a lot more fun. Photorealistic rendering is potentially useful, as all relevant geometric variables for SLAM tasks can be recorded with N \% accuracy. Benchmarking SLAM on photorealistic sequences makes a lot of sense, but training on such rendered images often suffers from domain adaptation issues. Our favorite deep nets seem to overfit. Datasets have been created with this intent in mind, but as the research community always demands results on real-world datasets, the benefit of photorealistic rendering for automatic SLAM training is still a dream. Since a public ImageNet-scale SLAM dataset does not exist today and photorealistic rendering brings its own set of new problems, how are we to embrace the data-driven philosophy of deep learning while building an end-to-end Deep SLAM system? Our proposed solution comes from a couple of key insights. First, recent ego-motion estimation has shown that it is possible to train deep convolutional neural networks on the task of image prediction. Compared to direct supervision (i.e., regressing to the ground truth N DoF pose), supervision for frame prediction comes ``for free.'' This new insight that the move away from strong-supervision might bear more fruits is rather welcoming for SLAM, an ecosystem already plagued with a fragmentation of datasets. Systems such as _cite_ perform full frame prediction, and we will later see that our flavor of the prediction problem is more geometric, as we focus on . Second, SLAM models must be lean or they will not run at a large scale on embedded platforms such as those in robotics and augmented reality. Our desire to focus on geometric consistency as opposed to full frame prediction comes from a dire need to deploy such systems in production. While it is fulfilling to watch full frame predictions made by a deep learning system, we already known from previous successes in SLAM (e.g., _cite_ and _cite_) that predicting/aligning points is sufficient for metric-level pose recovery. So why solve a more complex full frame prediction task than is necessary for SLAM?