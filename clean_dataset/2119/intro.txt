Given an image, an object localization method aims to recognize and locate interesting objects within the image. The ability to localize objects in images and videos efficiently and accurately opens up a lot of applications like automated vehicular systems, searching online shopping catalogues, home and health-care automation among others. Objects can occur in images in varying conditions of occlusion, illumination, scale, pose and context. These variations make object detection a challenging problems in the field of computer vision. The current state of the art in object detection includes methods which involve `strong' supervision. In the context of object detection, strong supervision entails annotating localization and pose information about present objects of interest. Generating such rich annotations is a time-consuming process and is expensive to perform over large data-sets. Weak supervision lends itself to large-scale object detection for data-sets where only image-level labels are available. Effective localization under weak supervision enables extensions to new object classes and modalities without human-generated object bounding box annotations. Also, such methods enable generation of inexpensive training data for training object detectors with strong supervision. Deep Convolutional Neural Networks (CNNs) _cite_, _cite_ have created new benchmarks in the object recognition challenge _cite_ . CNNs for object recognition are trained using image-level labels to predict the presence of objects of interest in new test images. A common paradigm in analyzing CNNs has emerged where the convolutional layers are considered as data-driven feature extractors and the subsequent fully-connected layers constitute hyperplanes which delineate object categories in the learnt feature space. Non-linearities through Rectified Linear Units (ReLU) and sigmoidal transfer functions have helped to learn complex mapping functions which relate images to labels. The convolutional layers encode both semantic and spatial information extracted from training data. This information is represented by activations from the convolutional units in the network which are commonly termed as Feature Maps. In this paper, we present a method that exploits correlation between semantic information present in Feature Maps and localization of an object of interest within an image. An example of such correlation can be seen in Figure _inline_eq_ . Note that crudely localized image-patches with the objects of classes, `chair', `person' and `tv monitor', generate high classification scores for the corresponding classes. This suggests that one can coarsely localize objects solely by image classification scores in this context. CNN based classifiers are trained for the task of image recognition on large image classification data-sets _cite_, _cite_, _cite_ . The learnt convolutional filters compute spatially localized activations across layers for a given test image _cite_ . We examine the activation values in the outermost convolutional layer and propose localization candidates (or bounding boxes) which maximize classification scores for a class of interest. Class scores vary across localization candidates because of the aforementioned local nature of the convolutional filters. We then progressively explore smaller and smaller regions of interest till a point is reached where the classifier is no longer able to discriminate amongst the classes of interest. The localization candidates are organized in a search tree, the root node being represented by the entire test image. As we traverse from the root node towards the leaf nodes, we consider finer regions of interest. To approximate the search for optimal localization candidates, we adopt a beam search strategy where the number of candidate bounding boxes are restricted as we progress to finer localizations. This strategy enables efficient localization of multiple objects of multiple classes in images. We outperform the state-of-the-art in localization accuracy by a significant margin of up to N mAP on two standard data-sets with complex scenes, PASCAL VOC N _cite_ and the much larger MS COCO _cite_ . The main contributions of this paper are: