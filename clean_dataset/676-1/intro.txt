Automatic removal of specific content in an image is a task of practical interest, as well as of intellectual appeal. There are many situations in which a part of an image needs to be erased and replaced. This may include text (whether present in the scene or overimposed on the image), which may have to be removed, for example to protect personal information; people, such as undesired passersby in the scene; or other objects that, for any reasons, one may want to wipe off the picture. While these operations are typically performed manually by skilled Photoshop editors, substantial cost reduction could be achieved by automatizing the workflow. Automatic content removal, though, is difficult, and an as yet unsolved problem. Removing content and inpainting the image in a way that it looks ``natural'' entails the ability to capture, represent, and synthesize high-level (``semantic'') image content. This is particularly true of large image areas infilling, an operation that only recently has been accomplished with some success~ _cite_ . Image inpainting algorithms described in the literature normally require that a binary ``mask'' indicating the location of the area to be synthesized be provided, typically via manual input. In contrast, an automatic content removal system must be able to accomplish two tasks. First, the pattern or object of interest must be segmented out, creating a binary mask; then, the image content within the mask must be synthesized. The work described in this paper is born from the realization that, for optimal results, these two tasks (segmentation and inpainting) should {\em not} be carried out independently. In fact, only in few specific situations can the portion of the image to be removed be represented by a binary mask. Edge smoothing effects are almost always present, either due to the camera's point spreading function, or due to blending, if the pattern (\eg text) is overimposed on the image. Although one could potentially recover an alpha mask for the foreground content to be removed, we believe that a more appropriate strategy is to {\em simultaneously} detect the foreground {\em and} synthesize the background image. By doing so, we do not need to resort to hand-made tricks, such as expanding the binary mask to account for inaccurate localization. Our algorithm for content removal and inpainting relies on conditional generative adversarial networks (cGANs) _cite_, which have become the tool of choice for image synthesis. Our network architecture is based on an encoder-decoder scheme with skip layers (see Fig. _ref_) . Rather than a single decoder branch, however, our network contains two parallel and interconnected branches: one ({\em dec-seg}) designed to extract the foreground area to be removed; the other ({\em dec-fill}) tasked with synthesizing the missing background. The {\em dec-seg} branch interacts with the {\em dec-fill} branch via multiple {\em neglect nodes} (see Fig. _ref_) . The concept of neglect nodes is germane (but in reverse) to that of mixing nodes normally found in {attention networks} _cite_ . Mixing nodes highlight a portion of the image that needs to be attended to, or, in our case, neglected. Neglect nodes appear in all layers of the architecture; they ensure that the {\em dec-fill} branch is aware of which portions of the image are to be synthesized, without ever committing to a binary mask. A remarkable feature of the proposed system is that the multiple components of the network (encoder and two decoder branches, along with the neglect nodes) are all trained at the same time. Training seeks to minimize a global cost function that includes a conditional GAN component, as well as _inline_eq_ distance components for both foreground segmentation and background image. This optimization requires ground--truth availability of {\em foreground} segmentation (the component to be removed), {\em background} images (the original image without the foreground), and {\em composite} images (foreground over background) . By jointly optimizing the multiple network components (rather than, say, optimizing for the {\em dec-seg} independently on foreground segmentation, then using it to condition optimization of {\em dec-fill} via the neglect nodes), we are able to accurately reconstruct the background inpainted image. The algorithm also produces the foreground segmentation as a byproduct. We should emphasize that this foreground mask is {\em not} used by the {\em dec-fill} synthesizing layer, which only communicates with the {\em dec-seg} layer via the neglect nodes. To summarize, this paper has two main contributions. First, we present the first (to the best of our knowledge) truly automatic semantic content removal system with promising results on realistic images. The proposed algorithm is able to recover high-quality background without any knowledge of the foreground segmentation mask. Unlike most previous GAN--based inpainting methods that assume a rectangular foreground region to be removed _cite_, our system produces good result with any foreground shape, even when it extends to the image boundary. Second, we introduce a novel encoder-decoder network structure with two parallel and interconnected branches ({\em dec-seg} and {\em dec-fill}), linked at multiple levels by mixing (neglect) nodes that determine which information from the encoder should be used for synthesis, and which should be neglected. Foreground region segmentation and background inpainting is produced in one single forward pass.