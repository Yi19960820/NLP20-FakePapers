In recent years, {\it data-driven} image reconstruction techniques based on machine learning, in particular {deep learning} (DL) ~ _cite_, have gained tremendous success in solving complex inverse problems~ _cite_, and can often provide results surpassing those using state-of-the-art {\it model-based} techniques. Traditionally, solving an inverse problem involves first explicitly formulating the imaging model and incorporating domain and prior knowledge (e.g. via the use of regularization techniques), and then finding an analytical solution (e.g. through an optimization procedure) ~ _cite_ . Unlike model-based approaches, the `end-to-end' DL framework does not explicitly utilize any models or priors, and instead relies on large datasets to `learn' the underlying inverse problem. The outcome of this DL approach consists of two important components. First, the result from the training stage is a CNN that corresponds to {\it a} plausible underlying mapping function relating the measurement to the solution. Second, the trained CNN can be used to make `predictions' when presenting it with new measurements that were unused in the training stage. This second part comes with major practical benefits in computational cost and speed in typical image reconstruction problems, since the prediction process simply involves the feedforward computation of the CNN that typically takes no more than a few seconds on a normal grade GPU. In contrast, most of modern model-based techniques rely on iterative algorithms~ _cite_ that require much higher computational cost and longer running time; the same lengthy process needs to be repeated every time for each new measurement. Here, we distinguish two classes of imaging problems: those involve {\it independent} datasets from often static objects, and those dealing with {\it sequential} datasets that are temporally correlated, from dynamic objects. In {\it independent} problems, CNNs have been demonstrated to provide superior performance to solve many challenging imaging problems, such as image super-resolution~ _cite_, denoising~ _cite_, segmentation~ _cite_, deconvolution~ _cite_, compressive imaging~ _cite_, tomography~ _cite_, digital labeling~ _cite_, holography~ _cite_, phase recovery~ _cite_, and imaging through diffusers~ _cite_ . What's common in this class of problems is that {\it independently} prepared input-output pairs (i.e. measurement and solution), obtained by repeating the same imaging process, are presented to the CNN at the training stage to optimize the network's parameters. In {\it sequential} problems, the temporal correlation of a dynamic process contains additional information, and is often recorded in video datasets. Various CNN frameworks have been proposed to learn the additional temporal information. For example, spatial super-resolution has been demonstrated by training a CNN on both spatial and temporal dimensions of videos~ _cite_ . Temporal super-resolution on recurring processes is achieved by learning the underlying temporal statistics~ _cite_ . The motion information of dynamic objects is learned with an optical-flow based CNN~ _cite_ . Motion artifacts can be removed by jointly learning the blurring point-spread-function (PSF) and deconvolution operation~ _cite_ . In all these cases, CNNs are designed to process {\it a video sequence} in order to extract the temporal information. The downside is that the CNN architectures inevitably become more complicated that require more computational resources, as compared to those used in the independent problems. Fundamentally, the complication stems from that any single frame from the imaging techniques used does not contain sufficient temporal statistical information. In this work, we develop a CNN architecture to reconstruct video sequence of dynamic live cells captured with a computational microscopy technique based on Fourier ptychographic microscopy (FPM) ~ _cite_ . The unique feature of the FPM is its ability to quantitatively reconstruct phase information with both wide field-of-view (FOV) and high spatial resolution, i.e., a large space-bandwidth product (SBP) . This is not possible for traditional techniques which must trade spatial or temporal resolution for FOV. For live-cell imaging applications, this allows one to simultaneously image a large cell population (e.g. more than N in a single frame in ~ _cite_) . Cells of the same type undergo similar morphological changes during different cell states, which then repeat over each cell cycle. If one records only a few cells at a time using conventional microscopy techniques~ _cite_, capturing the full dynamics would require a large sequence of measurements to cover the entire cell cycle (typically ranging from a few hours to days) . Our proposed technique is based on the observation that, in any live cell experiment without precise cell synchronization~ _cite_, at any instant of time, a large cell population would contain samples covering all cell states. {\it In other words, it is possible to gather sufficient temporal statistical information of a single cell by imaging a large spatial ensembles simultaneously} . Based on this idea, we propose a CNN that is trained using only a single frame from the FPM. We then show that this trained CNN is able to reconstruct large-SBP phase videos with high fidelity using datasets taken in a time-series live cell experiments. Existing FPM techniques are limited by their long acquisition times, which are limited by the FPM algorithms that require at least _inline_eq_ overlap in the Fourier coverage of the images captured from neighboring LEDs~ _cite_ . Several illumination multiplexing techniques have been demonstrated to improve the acquisition speed~ _cite_ . However, the amount of data reduction is still limited by the Fourier overlap requirement. Here, we show that, similar to prior work on CNN for FPM on static objects~ _cite_, our CNN can be sufficiently trained using much fewer images than that needed by the model-based FPM algorithms for dynamic live-cell samples. Distinct from computer vision applications, a particular challenge in applying DL to biomedical microscopy is the difficulty in gathering ground truth data needed for training the network. Various strategies have been proposed, including synthetic data from simulations built with physical imaging models~ _cite_, semi-synthetic data that uses experimental data to guide simulations~ _cite_, experimental data captured with a different modality~ _cite_, and experimental data captured with the same modality~ _cite_ . Here, we propose to use the traditional FPM reconstructed phase images as the ground truth for training. Since our technique requires only a single frame for training, this does not add much overhead in data acquisition or computation. When using experimental data as the ground truth, they inevitably are contaminated with noise. In FPM, the quality of the phase reconstruction is limited by spatially variant aberrations, system mis-alignment, and intensity-dependent noise~ _cite_ . Robust learning using noisy labelled data has been demonstrated for image classification and segmentation~ _cite_ . In essence, CNN captures the invariants while filtering out the random fluctuations~ _cite_ . Here, we show that our proposed CNN is also robust to phase noise in the `ground truth' data for solving the inverse problem of FPM. We build a CNN based on the conditional generative adversarial network (cGAN) framework, consisting of two sub-networks, the generator and the discriminator. The generator network uses the UNet architecture ~ _cite_ with densely connected convolutional blocks (DenseNet) ~ _cite_ to output high-resolution phase image. The discriminator network distinguishes if the output is real or fake. We compare five variants of the network, which differ by the input measurements using different illumination patterns corresponding to different Fourier coverages. Similar to the traditional FPM, the darkfield measurements lead to spatial resolution improvement in the reconstruction. To further refine the network, we introduce a mixed loss function that takes a weighted Fourier domain loss, in addition to the standard image domain loss for the generator and the adversarial loss for the discriminator. We show that this novel weighted Fourier domain loss leads to improved recovery of high frequency information. We demonstrate our technique using live Hela cell FPM video data from ~ _cite_ . We quantitatively assess the performance of our CNN technique over time against those from traditional FPM results, and found that the `generalization' degradation of the reconstructed phase is small over the entire time course (>N hours) . The training is performed on a PC Intel core iN, N GB RAM, NVIDIA GeForce Titan XP for _inline_eq_ N hours using Keras/Tensorflow framework. Once the network is trained, reconstructing a N _inline_eq_ N pixels phase image requires only _inline_eq_ N seconds, which is approximately N _inline_eq_ faster than the model-based FPM algorithm~ _cite_ . Our technique demonstrates a promising deep learning approach to continuously image large live-cell populations over extended time and gather spatial and temporal information with sub-cellular resolution. Compare to existing FPM~ _cite_, this CNN approach significantly improves the overall throughput by reducing both the acquisition and computation times, and with less data requirement. The CNN reconstructed phase image provides high spatial resolution, wide FOV, and low noise-induced artifacts. We also show the flexibility in reconstructing other cell types using transfer learning, which makes our technique appealing to broad applications.