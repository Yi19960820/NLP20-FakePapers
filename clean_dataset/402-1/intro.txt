Super resolution (SR) aims to retrieve a high resolution (HR) image from a given low resolution (LR) image. SR is widely used in many computer vision applications such as surveillance, face and iris recognition, and medical image processing. There have been many SR algorithms proposed recently _cite_ . Some early methods referred to interpolations, statistical image priors _cite_ or contour features _cite_ . Later, the example-based methods were proposed based on learning a dictionary of patch correspondences, such as neighbor embedding _cite_ and sparse coding _cite_ . Recently, convolutional neural networks (CNNs) have been widely used due to their significant improvement of the accuracy. The pioneering work SRCNN _cite_ introduced CNN as a solution to the SR problem. The accuracy of SRCNN is limited due to the N-layer structure and its small context receptive field. To improve the accuracy, some researchers propose to use more layers _cite_, deep recursive structure _cite_, or other network architectures such as ResNet _cite_ . However, most state of art methods could not be executed in real-time using practical hardware due to their large network size. In addition, with deeper networks, it becomes more difficult to tune the training parameters, such as the weight initialization, the learning rate, and the weight decay rate. As a result, the training can be stuck into a local minimum or does not converge at all. Hence, increasing depth might lead to accuracy saturation _cite_ or degradation for image classification _cite_ . In this paper, we propose the Cascade Trained Super Resolution Convolutional Neural Network (CT-SRCNN) as a solution to the above problem. Different from existing approaches that train all the layers at once with unsupervised weight initialization, we start by training a small network of N layers. When the rate of decrease in the training loss diminishes, newly initialized layers are gradually inserted into the trained network to make it deeper, and the training is continued. With this cascade training strategy, the convergence is improved, and the accuracy could be consistently increased with more layers. So it is `the deeper the better'. In addition, all the weights of the new layers in the CT-SRCNN are randomly initialized, and the learning rate is fixed. This is a great advantage compared to existing approaches _cite_, which need much time spent on tuning the parameters. When the CT-SRCNN depth becomes N layers, the accuracy is competitive compared to the state-of-the-art image SR networks, while using only _inline_eq_ of the parameters. The speed is also more than N times faster. Next, we explore how to remove the redundant filters from the SR network to improve the efficiency. We first discuss a one-shot trimming scheme, where all layers are made slimmer by removing filters with less importance. Then a cascade trimming method is proposed, which results in the same slim network but less accuracy loss compared to the one-shot trimming. Similar to cascade training, cascade trimming also consists of several stages. At each trimming stage, some layers are made slimmer by removing some of their filters, while keeping the other layers intact, and is followed by fine-tuning the network. Different layers are trimmed at different stages in a cascade manner. Our trimming strategy is different and much simpler than training a whole newly initialized thin network as in FitNets _cite_ . Our cascade trimming methodology is also very different from existing pruning algorithms _cite_, where some parameters are set to zero but keeping layer sizes the same. For the same number of non-zero parameters, trimmed networks have lower computational complexity, compressed network size, and are more hardware friendly than pruned networks which tend to have random pruning patterns. We show that by using cascade trimming together with cascade training we can train an SR network with concurrently high accuracy, good computational efficiency, and significantly smaller size. As summary, our contributions are two folds: N. A cascade training strategy is utilized. Cascade training does not need much work on parameter tuning. The resulting CT-SRCNN can achieve the state-of-the-art accuracy at N layers, with more than N times less complexity compared to existing deep CNN models; N. A network trimming scheme to reduce the SR network size is proposed. The trade-off between network size and accuracy is explored by the proposed cascade trimming. The redundant filters could be removed with minor loss on the accuracy;