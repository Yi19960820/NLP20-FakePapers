Can you recognize the scenes in Figure _ref_, even though they are depicted in different modalities? Most people have the capability to perceive a concept in one modality, but represent it independently of the modality. This cross-modal ability enables people to perform some important abstraction tasks, such as learning in different modalities (cartoons, stories) and applying them in the real-world. Unfortunately, representations in computer vision do not yet have this cross-modal capability. Standard approaches typically learn a separate representation for each modality, which works well when operating within the same modality. However, the representations learned are not aligned across modalities, which makes cross-modal transfer difficult. Two modalities are strongly aligned if, for two images from each modality, we have correspondence at the level of objects. In contrast, weak alignment is if we only have global label that is shared across both images. For instance, if we have a picture of a bedroom and a line drawing of a different bedroom, the only thing that we know is shared across these two images is the scene type. However, they will differ in the objects and viewpoint inside. In this paper, our goal is to learn a representation for scenes that has strong alignment using only data with weak alignment. We seek to learn representations that will connect objects (such as bed, car) across modalities (e.g., a picture of a car, a line drawing of a car, and the word ``car'') without ever specifying that such a correspondence exists. To investigate this, we assembled a new cross-modal scene dataset, which captures hundreds of natural scene types in five different modalities, and we show a few examples in Figure _ref_ . Using this dataset and only annotations of scene categories, we propose to learn an aligned cross-modal scene representation. We present two approaches to regularize cross-modal convolutional networks so that the intermediate representations are aligned across modalities, even when only weak alignment of scene categories is available during training. Figure _ref_ visualizes the representation that our full method learns. Notice that our approach learns hidden units that activate on the same object, regardless of the modality. Although the only supervision is the scene category, our approach enables alignment to emerge automatically. Our approach builds on a foundation of domain adaptation _cite_ and multi-modal learning _cite_ methods in computer vision. However, our focus is learning cross-modal representations when the modalities are significantly different (e.g., text and natural images) and with minimal supervision. In our approach, the only supervision we give is the scene category, and no alignments nor correspondences are annotated. To our knowledge, the adaptation of intermediate representations across several extremely different modalities with minimal supervision has not yet been extensively explored. We believe cross-modal representations can have a large impact on several computer vision applications. For example, data in one modality may be difficult to acquire for privacy, legal, or logistic reasons (eg, images in hospitals), but may be abundant in other modalities, allowing us to train models using accessible modalities. In search, users may wish to retrieve similar natural images given a query in a modality that is simpler for a human to produce (eg, drawing or writing) . Additionally, some modalities may be more effective for human-machine communication. The remainder of this paper describes and analyzes our cross-modal representations in detail. In section N, we first discuss related work that our work builds upon. In section N, we introduce our new cross-modal scene dataset. In section N, we present two complementary approaches to regularize convolutional networks so that intermediate representations are aligned across modalities. In section N, we present several visualizations and experiments in cross-modal retrieval to evaluate our representations.