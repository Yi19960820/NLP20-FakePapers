In recent years, we have made significant advances in standard recognition tasks such as image classification~ _cite_, detection~ _cite_ or segmentation~ _cite_ . Most of these gains are a result of using feed-forward end-to-end learned ConvNet models. Unlike humans where visual reasoning about the space and semantics is crucial~ _cite_, our current visual systems lack any context reasoning beyond convolutions with large receptive fields. Therefore, a critical question is how do we incorporate both and reasoning as we build next-generation vision systems. Our goal is to build a system that can not only extract and utilize hierarchy of convolutional features, but also improve its estimates via spatial and semantic relationships. But what are spatial and semantic relationships and how can they be used to improve recognition? Take a look at Fig.~ _ref_ . An example of spatial reasoning (top-left) would be: if three regions out of four in a line are ``window'', then the fourth is also likely to be ``window''. An example of semantic reasoning (bottom-right) would be to recognize ``school bus'' even if we have seen few or no examples of it--just given examples of ``bus'' and knowing their connections. Finally, an example of spatial-semantic reasoning could be: recognition of a ``car'' on road should help in recognizing the ``person'' inside ``driving'' the ``car''. A key recipe to reasoning with relationships is to build up estimates. Recently, there have been efforts to incorporate such reasoning via top-down modules~ _cite_ or using explicit memories~ _cite_ . In the case of top-down modules, high-level features which have class-based information can be used in conjunction with low-level features to improve recognition performance. An alternative architecture is to use explicit memory. For example, Chen \& Gupta~ _cite_ performs sequential object detection, where a is used to store previously detected objects, leveraging the power of ConvNets for extracting dense context patterns beneficial for follow-up detections. However, there are two problems with these approaches: a) both approaches use stack of convolutions to perform pixel-level reasoning~ _cite_, which can lack a reasoning power that also allows regions farther away to directly communicate information; b) more importantly, both approaches assume enough examples of relationships in the training data--so that the model can learn them from scratch, but as the relationships grow exponentially with increasing number of classes, there is not always enough data. A lot of semantic reasoning requires learning from few or no examples~ _cite_ . Therefore, we need ways to exploit additional structured information for visual reasoning. In this paper, we put forward a generic framework for both spatial and semantic reasoning. Different from current approaches that are just relying on convolutions, our framework can also learn from structured information in the form of knowledge bases~ _cite_ for visual recognition. The core of our algorithm consists of two modules: the local module, based on spatial memory~ _cite_, performs pixel-level reasoning using ConvNets. We make major improvements on efficiency by parallel memory updates. Additionally, we introduce a global module for reasoning beyond local regions. In the global module, reasoning is based on a structure. It has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to classes. Taking advantage of such a structure, we develop a reasoning module specifically designed to pass information on this graph. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other in order to refine estimates. Note that, local and global reasoning are not isolated: a good image understanding is usually a compromise between background knowledge learned and image-specific observations. Therefore, our full pipeline joins force of the two modules by an attention~ _cite_ mechanism allowing the model to rely on the most relevant features when making the final predictions. We show strong performance over plain ConvNets using our framework. For example, we can achieve _inline_eq_ absolute improvements on ADE~ _cite_ measured by per-class average precision, where by simply making the network deeper can only help _inline_eq_ .