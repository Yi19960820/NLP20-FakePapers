One of the basic problems in robotics is data association, where sensor readings have to be associated with previous measurements, as the combination of sensor data reduces noise and improves robot understanding of the world. Autonomous Underwater Vehicles (AUVs) constantly struggle with data association, as the underwater environment is very hostile for sensing. Some common robot tasks that require data association are tracking and simultaneous localization and mapping (SLAM) . Object detection/recognition can also benefit from data association in the form of matching images if the task is to locate an object and only a single training sample is available. Acoustic sensing (Sonar) is used in underwater environments as sound can travel large distances on water with little attenuation. Optical cameras are not an option as light is attenuated and absorbed by particles in the water column. Interpretation of acoustic images is not trivial as unwanted reflections, noise, and low signal-to-noise ratio (SNR) degrades the amount of information that the AUV can gather. For sonar, matching image patches to known objects or landmarks in the environment is an important problem. Matching can also be formulated for other tasks such as mosaicing _cite_, where sonar images must be registered before being combined to improve SNR ratio and image resolution. Matching sonar images is difficult due to viewpoint dependence. In this work, we propose the use of Convolutional Neural Networks (CNN) to learn a matcher for sonar images. Our objective is to produce a function that takes two sonar image patches and makes a binary decision: both images correspond to different views of the same object, or not. Matching should be possible even as insonification varies due to AUV or sensor movement, different views, and object rotation or translation. CNNs have obtained very good results _cite_ in different tasks that use optical color images, such as object recognition _cite_ and transfer learning _cite_ . We have previously evaluated CNNs for object recognition in sonar images and found that they also improve the state of the art _cite_ . CNNs have also been used to match patches from color images _cite_ with high accuracy. These results motivate us to use CNNs for sonar data, as the trained network can learn sonar-specific information directly from the data. We show that we can build and train a CNN that matches Forward-Looking Sonar (FLS) image patches with high accuracy (AUC N), surpassing the state of the art keypoint matchers such as SIFT and SURF (with AUC in the range N-N) . Our contributions are: we propose an algorithm to generate matching pairs from labeled objects for training, we learn the matching function directly from labeled data, without any manual feature engineering, we show that it is possible to match sonar images with relatively high accuracy.