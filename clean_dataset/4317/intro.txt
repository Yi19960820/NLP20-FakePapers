neural networks trained on large-scale labeled datasets could achieve excellent performance across varieties of tasks, such as sentiment analysis~ _cite_, image classification~ _cite_ and semantic segmentation~ _cite_ . Yet they usually fail to generalize well on novel tasks because the transferability of features decreases as the distance between the base and target tasks increases~ _cite_ . A convincing explanation is that there exists a domain shift between training data and testing one~ _cite_ . To alleviate the negative effect caused by a domain shift, domain adaptation (DA) is proposed to utilize labeled data from a source domain to generalize models generalize well on a target domain~ _cite_ . Domain adaptation, which is a field belonging to transfer learning, has long been utilized to make it possible to exploit the knowledge learned in one specific domain to effectively improve the performance in a related but different domain. Earlier methods of DA aim to learn domain-invariant feature representations from data by jointly minimizing a distance metric that actually measures the adaptability between a pair of source and target domains, such as Transfer Component Analysis~ _cite_, Geodesic Flow Kernel~ _cite_, and Transfer Kernel Learning ~ _cite_ . In order to learn transferable features well, researchers apply deep neural networks to DA models~ _cite_ . A feature extractor neural network is trained by reducing ``distance" between distributions of two different domains, on the assumption that the classifier trained by source data also works well in a target domain. In this kind of methods, Maximum Mean Discrepancy (MMD) loss is widely used for mapping different distributions~ _cite_ . For example, Deep Adaptation Networks (DAN) ~ _cite_, Joint Adaptation Networks~ _cite_ and Residual Transfer Networks~ _cite_ apply MMD loss to several layers whereas Large Scale Detection through Adaptation~ _cite_ adds a domain adaptation layer that is updated based on MMD loss. Recently, the idea of Generative Adversarial Networks (GANs) ~ _cite_ has been widely applied to DA. The methods of using GANs~ _cite_ to transform source images to target ones are proposed and their classifiers are trained with the generated target images. However, when distributions of source and target domains are totally different, adversarial training has poor performance because of a gradient vanishing phenomenon. Alternative methods train GANs on features of source and target domains. Their generator is acted as a feature extractor, and discriminator as a domain classifier. There are symmetric and asymmetirc adaptation architectures in adversarial domain adaptation, which can effectively adapt source and target distributions. The former's features in the source and target domains are generated from the same network~ _cite_, while the latter's from different networks~ _cite_ . It is well-recognized that the former is poor at generalization whereas the latter is difficult to train. To solve the above problems, in this work, we propose a novel feature-shared model for adversarial domain adaptation, which achieves the flexibility of asymmetric architecture and can be easily trained. In the proposed framework as shown in Fig.~ _ref_, a weight-shared feature extractor distills features from different domains, and a feature-shared transform network maps features from the source domain to the space of target features. Adversarial learning is completed with the losses from the label and domain classifiers. Note that we design residual connections between the extractor and the network to ease the learning of distribution mapping by sharing features. In addition, in order to avoid getting stuck into local minima, we construct a regularization term to ensure that the model at least knows a vague, if not exact, direction to match different distributions and overcome a gradient vanishing problem. The main contributions of this work are as follows: Section~ _ref_ reviews some related work on unsupervised domain adaptation. In Section~ _ref_, the proposed method is described. Several experiments are reported in Section~ _ref_ . Section~ _ref_ concludes this paper.