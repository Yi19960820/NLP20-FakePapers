In robot-assisted minimally-invasive surgery, surgeon expertise directly affects overall surgical performance and patient safety. Technical training programs are necessary to ensure trainees develop adequate skills to teleoperate robots and perform complex operations proficiently. Due to the steep learning curves, an objective measure of trainee performance and automated identification of surgical activity are of prominent concerns towards an efficient training and intelligent robot autonomy in order to further enhance surgery outcomes~ _cite_ . Current techniques for objective surgical skill assessment include descriptive statistics (time, path length, smoothness, etc.) ~ _cite_, gesture segmentation-based analysis such as Hidden Markov Models (HMM) ~ _cite_, and feature-based modelings such as k-nearest neighbor (_inline_eq_ NN), support vector machine (SVM) ~ _cite_ . Given an observation of motion data from robot end-effectors, local segmented gestures or hand-crafted features are extracted and fed into a classifier to assess trainee skills and performance. Similarly, these techniques are applied to understand underlying surgical task structures and workflow~ _cite_ . However, the aforementioned approaches are limited in several ways. First, it is time-consuming and strenuous to manually design meaningful representations to uncover hidden pattens of complex motion. Gesture segmentation is task-dependent, limited to specific operations and requires significant prior knowledge of particular structures and pre-processing to decompose motion sequences. Moreover, a common deficiency is that most classifications can only obtained at the level of trial, which requires an entire observation of each training operation. These drawbacks make previous approaches less efficient for an online automatic feedback system. A sequence distance-based method, such as dynamic time warping (DTW), has been proposed to provide feasible online classifying for surgical task and gesture recognition~ _cite_ . However, higher computational loads involved in practice, as well as the role of DTW in the skill analysis still remains unknown. In this paper, we aim at developing an end-to-end surgery motion analytic framework based on a multi-output deep learning architecture, SATR-DL, for online trainee skill analysis and task recognition (Figure~ _ref_) . In particular, by integrating a Convolutional Neural Network (CNN) ~ _cite_ and Gated Recurrent Unit (GRU) network~ _cite_, our proposed deep model can simultaneously learn both spatial (convolutional) abstract representations within the interval of input frame, as well as the temporal dynamics of multiple channels at each time step in raw motion data. By exploring these intrinsic properties of kinematic sequences, SATR-DL can effectively characterize the nature of surgery motion relative to both the trainee experience and operation activity. Crucially, this work does not assume any prior knowledge of primitive gestures and does not require pre-defined features. We find that our SATR-DL significantly enhances both the efficiency and accuracy when compared to other techniques in the study of robotic surgery assessment.