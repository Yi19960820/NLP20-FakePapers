The problem of recovering depth information from images has been widely studied in computer vision. Traditional approaches operate by considering multiple observations of the scene of interest, \eg derived from two or more cameras or corresponding to different lighting conditions. More recently, the research community has attempted to relax the multi-view assumption by addressing the task of monocular depth estimation as a supervised learning problem. Specifically, given a large training set of pairs of images and associated depth maps, depth prediction is casted as a pixel-level regression problem, \ie a model is learned to directly predict the depth value corresponding to each pixel of an RGB image. In the last few years several approaches have been proposed for addressing this task and remarkable performance has been achieved thanks to deep learning models _cite_ . Recently, various Convolutional Neural Network (CNN) architectures have been proposed, tackling different sub-problems such as how to jointly estimate depth maps and semantic labels~ _cite_, how to build models robust to noise or how to combine multi-scale features~ _cite_ . Focusing on the latter issue, recent works have shown that CRFs can be integrated into deep architectures _cite_ and can be exploited to optimally fuse the multi-scale information derived from inner layers of a CNN _cite_ . Inspired by these works, in this paper we also propose to exploit the flexibility of graphical models for multi-scale monocular depth estimation. However, we significantly depart from previous methods and we argue that more accurate estimates can be obtained operating not only at the prediction level but exploiting directly the internal CNN feature representations. To this aim, we design a novel CRF model which automatically learns robust multi-scale features by integrating an attention mechanism. Our attention model allows to automatically regulate how much information should flow between related features at different scales. Attention models have been successfully adopted in computer vision and they have shown to be especially useful for improving the performance of CNNs in pixel-level prediction tasks, such as semantic segmentation _cite_ . In this work we demonstrate that attention models are also extremely beneficial in the context of monocular depth prediction. We also show that the attention variables can be jointly estimated with multi-scale feature representations during CRF inference and that, by employing a structured attention model _cite_ (\ie by imposing similarity constraints between attention variables for related pixels and scales), we can further boost performance. Through extensive experimental evaluation we demonstrate that our method produces more accurate depth maps than traditional approaches based on CRFs _cite_ and multi-scale CRFs _cite_ (Fig. _ref_) . Moreover, by performing experiments on the publicly available NYU Depth VN _cite_ and on the KITTI _cite_ datasets, we show that our approach outperforms most state of the art methods. In summary, we make the following contributions: (i) We propose a novel deep learning model for calculating depth maps from still images which seamlessly integrates a front-end CNN and a multi-scale CRF. Importantly, our model can be trained end-to-end. Differently from previous works _cite_ . (ii) Our approach benefits from a novel attention mechanism which allows to robustly fuse features derived from multiple scales as well as to integrate structured information. (iii) Our method demonstrates state-of-the-art performance on the NYU Depth VN _cite_ dataset and is among the top performers on the more challenging outdoor scenes of the KITTI benchmark _cite_ . The code is made publicly available .