Humans can acquire knowledge by watching instructional videos online. A typical situation is that people confused by specific problems try to look for solutions in related instructional videos. For example, while learning to cook new dishes, they may wonder why a specific ingredient is added, and what happens between the two procedures . Watching instructional videos can often clarify these questions and hence, assist humans in accomplishing tasks. We hereby propose the question: can machines also understand instructional videos as humans do, which requires not only accurate recognition of objects, actions, and events but also the higher-order inference of any relations therein, e.g., spatial, temporal, correlative and casual? Here we use higher-order inference to refer to the inference that cannot be completed immediately by direct observations and thus requires stronger semantics for video modeling (see Fig.~ _ref_) . Current instructional video understanding studies focus on various tasks e.g., reference resolution~ _cite_, procedure localization~ _cite_, dense captioning~ _cite_, activity detection~ _cite_ and visual grounding~ _cite_ . Despite the rich literature and applications, question-answering (QA) task in instructional videos explored in our work is less developed, which acts as a proxy to benchmark the higher-order inference in machine intelligence. Previous works, e.g., ImageQA~ _cite_ and VideoQA~ _cite_, also leverage the QA task as automatic evaluation method, but QA on instructional videos has never been tackled before. Observing the lack of suitable dataset on instructional videos, we propose YouCook Question Answering (YouQuek) dataset based on YouCookN~ _cite_ which is the largest instructional video dataset. Our YouQuek dataset is the first reasoning-oriented dataset aimed for instructional videos. We employ question-answering as intuitive interpretations for various styles of reasoning. Figure~ _ref_ presents two exemplar QA pairs in our dataset along with the corresponding example human reasoning procedure involved to answer the questions. YouQuek dataset contains N, N manually-collected QA pairs that are divided into different categories regarding different reasoning styles, e.g., counting, ordering, comparison, and changing of properties. Upon the newly built dataset, we explore in two directions. The first one concerns effective representations of modeling instructional videos. The videos in our consideration have an average length of N min and as instructional videos, they are structured and have step-by-step procedure constraining the understanding task. By modeling the temporal relations among different procedures, we are expecting valuable information to be extracted from the instructional videos, for which we study various model structures and propose a novel Recurrent Graph Convolutional Network (RGCN) . The RGCN deals with complex reasoning by message passing in the graph, but also maintains the sequential ordering information by a supporting RNN. In this design, graph and RNN can boost each other since the information can be swapped between the two pathways. Second, we explore the use of different modalities in video modeling. Apart from visual information, temporal boundaries, descriptions for each procedure, and transcripts are explored. In this direction, we want to test the effect of combining various types of available annotations with our developed video models on understanding instructional videos. Given that modeling instructional videos from vision alone is hard, combining such information approximates better the human learning experiences and it, in turn, gives us a hint for devising better models for machine intelligence. We conduct extensive experiments on the YouQuek dataset. In the ablation study, we find that attention mechanism helps boost the performance. Our proposed RGCN model outperforms all other models with respect to the overall accuracy, even without attention. From the multi-modality perspective, modeling instructional videos using temporal boundaries together with descriptions can help dig more valuable information from videos. We also conduct human quiz on the QAs in our dataset. Results show that machines still have a large gap to human performance in that even without visual information, humans still can answer some questions correctly using life experience, or common sense, which hints us that incorporating the external knowledge with video models will be helpful for future works. Our main contributions are summarized as follows. The rest of the paper is organized as the following. We first discuss some related works in Sec.~ _ref_, and introduce the proposed YouQuek dataset in Sec.~ _ref_ . Then in Sec.~ _ref_, we set up series of baseline models for the dataset, and propose RGCN as a new model for instructional video reasoning. In Sec.~ _ref_, we demonstrate and discuss the experiment results. Conclusions are drawn in Sec.~ _ref_ . The YouQuek dataset and our code for all methods will be released upon acceptance.