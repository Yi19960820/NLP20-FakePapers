Localization of monocular images is a fundamental problem in computer vision and robotics. Camera localization forms the basis of many functions in computer vision where it is an important component of the Simultaneous Localization and Mapping (SLAM) process and has direct application, for example, in the navigation of autonomous robots and drones in first-response scenarios or the localization of wearable devices in assistive living applications. The most common means of performing N-DOF pose estimation using visual data is to make use of specially-built models, which are constructed from a vast number of local features that have been extracted from the images captured during mapping. The ND locations of these features are then found using a Structure-from-Motion (SfM) process, creating a many-to-one mapping from feature descriptors to ND points. Traditionally, localizing a new query image against these models involves finding a large set of putative correspondences. The pose is then found using RANSAC to reject outlier correspondences and optimize the camera pose on inliers. Although this traditional approach has proven to be incredibly accurate in many situations, it faces numerous and significant challenges. These methods rely on local and unintuitive hand-crafted features, such as SIFT keypoints. Because of their local nature, establishing a sufficient number of reliable correspondences between the image pixels and the map is very challenging. Spurious correspondences arise due to both ``well-behaved'' phenomena such as sensor noise and quantization effects as well as pure outliers which arise due to the local correspondence assumptions not being satisfied _cite_ . These include inevitable environmental appearance changes due to, for example, changing light levels or dynamic elements such as clutter or people in the frame or the opening and closing of doors. These aspects conspire to give rise to a vast number of suprious correspondences, making it difficult to use for any purpose but the localization of crisp and high-resolution images. Secondly, the maps often consists of millions of elements which need to be searched, making it very computationally intensive and difficult to establish correspondences in real-time. Recently, however, it has been shown that machine learning methods such as random forests _cite_ and convolutional neural networks (CNNs) _cite_ have the ability to act as a regression model which directly estimates pose from an input image with no expensive feature extraction or feature matching processes required. These methods consider the input images as being entirely uncorrelated and produce independent pose estimates that are incredibly noisy when applied to image sequences. On most platforms, including smart-phones, mobile robots and drones, image-sequences are readily obtained and have the potential to greatly enhance the accuracy of these approaches and promising results have been obtained for sequence-based learning for relative pose estimation _cite_ . Therefore, in this paper we consider ways in which we can leverage the temporal smoothness of image-sequences to improve the accuracy of N-DoF camera re-localization. Furthermore, we show how we can in essence unify map-matching, model-based localization, and temporal filtering all in one, extremely compact model. \noindent Map-matching Map matching methods make use of a map of a space either in the form of roads and traversable paths or a floor-plan of navigable and non-navigable areas to localize a robot as it traverses the environment. Map-matching techniques are typified by their non-reliance on strict data-association and can use both exteroceptive (eg. laser scans) or interoceptive (odometry, the trajectory or the motion of the platform) sensors to obtain a global pose estimate. The global pose estimate is obtained through probabilistic methods such as sequential Monte Carlo (sMC) filtering _cite_ or hidden Markov models (HMMs) _cite_ . These methods inherently incorporate sequential observations, but accuracy is inferior to localizing against specialised maps, such as a ND map of sparse features. \noindent Sparse feature based localization When a ND model of discriminative feature points is available (eg. obtained using SfM) then the poses of query images can be found using camera re-sectioning. Matching against large ND models is generally very computationally expensive and requires lots of memory space to store the map. A number of approaches have been proposed to improve the efficiency of standard ND-to-ND feature matching between the image and the ND model _cite_ . For example, _cite_ propose a quantized feature vocabulary for direct ND-to-ND matching with the camera pose being found using RANSAC in combination with a PnP algorithm and in _cite_ an active search method is proposed to efficiently find more reliable correspondences. _cite_ propose a client-server architecture where the client exploits sequential images to perform high-rate local N-DoF tracking which is then combined with lower-rate global localization updates from the server, entirely eliminating the need for loop-closure. The authors propose various methods to integrate the smooth local poses with the global updates. While this approach exploits a temporal stream of images, it does not use this information to improve the accuracy of global localization. In _cite_ the authors consider means of improving the global accuracy by introducing temporal constraints into the image registration process by regularizing the poses trough smoothing. \noindent CNN features Deep learning is quickly becoming the dominant approach in computer vision. The many layers of a pre-trained CNN form a hierarchical model with increasingly higher level representations of the input data as one moves up the layers. It has been shown that many computer vision related tasks benefit from using the output from these upper layers as feature representations of the input images. These features have the advantage of being low-level enough to provide representations for a large number of concepts, yet are abstract enough to allow these concepts to be recognized using simple linear classifiers _cite_ . They have shown great success applied to a wide range of tasks including logo classification _cite_, and more closes related to our goals, scene recognition _cite_ and place recognition _cite_ . \noindent Posenet _cite_ demonstrated the feasibility of estimating the pose of a single RGB image by using a deep CNN to regress directly on the pose. For practical camera relocalization, Posenet is far from ideal. For example, on the Microsoft N-Scenes dataset it achieves a _inline_eq_ error where the model space is only _inline_eq_ . We argue that this can be partly attributed to the fact that the design of the network makes no attempt to capture the structure of the scene or map in which it is attempting to localize and thus would require an excessively large and well sampled training set to generalize adequately. \noindent Scene coordinate regression forests of Shotton et al. _cite_ use a regression forest to learn the mapping between the pixels of an RGB-D input image and the scene co-ordinates of a previously established model. In essence the regression forest learns the function _inline_eq_ . To perform localization, a number of RGB-D pixels from the query image are fed through the forest and a RANSAC-based pose computation is used to determine a consistent and accurate final camera pose. To account for the temporal regularity of image sequences, the authors consider a frame-to-frame extension of their method. To accomplish this, they initialize one of the pose hypotheses with that obtained from the previous frame, which results in a significant improvement in localization accuracy. Although extremely accurate, the main disadvantage of this approach is that it requires depth images to function and does not eliminate the expensive RANSAC procedure. In this paper, we propose a recurrent model for reducing the pose estimation error by using multiple frames for the pose prediction. Our specific contributions are as follows: