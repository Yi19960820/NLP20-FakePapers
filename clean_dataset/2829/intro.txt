Advances in deep learning have led to tremendous success in a wide variety of applications in visual and audio perception such as image classification~ _cite_, object detection~ _cite_, and speech recognition~ _cite_ . What's interesting about deep convolutional neural networks is that it brings together the notions of feature extraction, feature projection, and prediction within an end-to-end learning framework to produce more coherent and more discriminative predictors. The excitement around deep learning and recent findings that increasing network depth~ _cite_ typically results in greater modeling capacity has led researchers to focus on designing deeper and more complex deep neural networks to improve modeling accuracy. Although having deeper architectures was demonstrated to provide better modeling performances, a number of challenges arise as we increase network depth. Besides becoming more prone to overfitting and becoming more difficult to train to convergence, deeper neural networks also result in a significant increase in not only the number of parameters in the network, but also dramatically increases the computational cost of network inference. A number of strategies have been proposed to tackle the various challenges associated with deeper neural network architectures. For example, Szegedy {\it et al.} ~ _cite_ introduced the concept of inception modules which helps to increase the depth of deep neural networks while maintaining the number of parameters. Specifically, inception modules consist of several convolutional layers with different receptive field sizes fed by the same inputs. This architecture helps the model to extract better features with fewer number of computations. While this new module mitigates the computational complexity to some extent and extends the possibility of having deeper networks with fewer parameters, deeper network architectures still suffer from vanishing gradient issues and thus a degradation in learning. He {\it et al.} ~ _cite_ took a different strategy to address the problem and tackled the former issue of degradation in learning deeper neural networks (e.g., vanishing gradient) by introducing the concept of residual learning, where learning is based on the residual mapping rather than directly on the unreferenced mapping. This novel idea brought forth the possibility of much larger and deeper networks by easing the training of such networks. Following that, Xie~ {\it et al.} ~ _cite_ incorporated the idea behind inception modules (i.e., split-transform-merge strategy) within a residual block structure to provide better subspace modeling while resolving the degradation problem at the same time, resulting in a ResNext architecture that achieved improved modeling accuracy. Several other architectures have been proposed based on these observations to provide better modeling accuracy. For example, the DenseNet~ _cite_ architecture connects each layer to every other layer in a feed-forward fashion. In this network architecture, the feature maps of all preceding layers are used as inputs to the next layer, thus alleviating the vanishing gradient issue. Zoph~ {\it et al.} ~ _cite_ devised an evolutionary algorithm to search through a huge set of possible computational blocks and found the most optimized block architecture to design a every deep neural network with. The parameters of the computational block are trained during the search procedure and the whole network are fine-tuned with the training data as well. The computational cost associated with deep neural networks remain a significant bottleneck for deployment in many industrial applications. Although some applications can leverage high-performance computing units such as GPUs to enable real-time operation, this comes at a very high financial cost in the form of cloud computing costs or on-premise equipment and power costs. Furthermore, there are a large number of industrial applications where access to high-performance computing devices is simply not possible. As such, mechanisms for reducing computational cost of deep neural networks while retaining modeling accuracy is highly desired. To tackle the issue of computational cost, a wide variety of methods have been proposed. One common strategy is precision reduction~ _cite_, where the data representation of a network is reduced from the typical N-bit floating point precision to low-bit fixed-point or integer precision. This technique is suitable mainly for the specialized hardware with the faster lower precision arithmetic calculation. Another common strategy is model compression~ _cite_, which involves leveraging traditional data compression methods such as weight thresholding, hashing, and Huffman coding. Such compression methods are mostly beneficial in storage reduction unless the hardware used supports accelerated sparse multiplications. Other strategies include the use of teacher-student strategies~ _cite_, where a larger teacher network is used to train a smaller student network, as well as the use of evolutionary algorithms~ _cite_ for evolving the architecture of deep neural networks over generations to be more compact. More recently, conditional computation~ _cite_ and early prediction~ _cite_ methods have been proposed to tackle this issue, which involve the dynamic execution of different modules within a network. Conditional computation methods have largely been motivated by the idea that residual networks can be considered as an ensemble of shallower networks. As such, these methods take advantage of skip connections to determine which residual modules are necessary to be executed, with most leveraging reinforcement learning. For example, a controller is typically trained within a reinforcement framework where the controller plays the role of deciding which network block needs to be executed based the input image. The controller is usually a shallower network. Early prediction techniques, on the other hand, divide the network into several partitions, with fully-connected layers (i.e., classification layers) integrated into the network at the end of each partition. In previous work~ _cite_, the new network is then trained based a multi-loss function with respect to all of the integrated fully-connected layers. After the network and all of the integrated fully-connected layers are trained, a threshold is calculated for each of the integrated fully-connected layer classifiers based on the outputs of the Softmax layer to determine whether the sample can be predicted in the current fully-connected layer or requires prediction after the next partition. A particular limitation to past early prediction approaches is that the fully-connected layers are trained based on a cross-entropy loss, which has been shown to produce predictions that may not be as reliable as desired . Furthermore, not only are such previous techniques quite difficult to set up, they also require the network to be trained from scratch with these techniques integrated for strong performance. In this study, we explore the idea of early prediction but instead draw inspiration from the soft-margin support vector~ _cite_ theory for decision-making. Specifically, we introduce the concept of decision gates (d-gate), modules that are trained via hinge loss to decide whether a sample needs to be projected into a deeper embedding or if an early prediction can be made at the d-gate, thus enabling the conditional computation of dynamic representations at different depths. The proposed d-gate modules can be integrated with any deep neural network without the need to train networks from scratch, and thus reduces the average computational complexity of the deep neural networks while maintaining modeling accuracy.