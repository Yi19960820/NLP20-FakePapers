Automated personal identification using palmprint images has been widely studied and employed for a range of law-enforcement and e-security applications. However contactless palmprint identification is relatively new area of research and offers more attractive solution for the deployments as it can address serous concerns relating to the hygiene while offering higher convenience and user security. In addition, the contactless palmprint imaging also enables deformation free acquisition of palmprint features, or the ground truth information, which can enable higher matching accuracy than those using contact-based imaging. Despite strong motivation and desire to develop contactless palmprint identification solutions, there are several challenges that needs to be addressed by researchers. Firstly, the palmprint matching accuracy degrades relatively for the contactless images as such palmprint images generally presents higher imaging variations. Therefore more advanced matching techniques needs to be developed to improve the matching accuracy from the contactless palmprint images. Secondly, the detection of contactless palmprint images (region of interest) from the presented hands is quite challenging as the background during such imaging is expected to be dynamic or less stable. Available research on contactless palmprint images addresses such challenges by acquiring contactless palmprint images with fixed background that can enable key point detection using pixel-wise operators to segment the palmprint images. Deep learning capabilities offer enormous potential to address these two challenges and are considered in this report. In recent years, deep learning has emerged as the dominant approach for a range of computer vision related problems and has delivered state-of-the-art performance for object detection _cite_, face recognition _cite_, iris recognition _cite_ and image classification. However, unlike for the face recognition, there is almost nil attention to incorporate remarkable capabilities from the deep learning for palmprint identification and achieve superior performance than popular or state-of-the-art palmprint recognition methods. This report proposes a new, deep learning based, contactless palmprint identification framework which not only offers accurate matching capabilities but also exhibits outstanding generalization capabilities on different public databases. With the design of effective residual feature network, our model can enlarge the receptive field _cite_ for matching contactless palmprint images and learn comprehensive palmprint features which generalizes very well on other databases. We develop a soft-shifted triplet loss function to accommodate frequent contactless palmprint imaging variations and offer meaningful supervision for learning effective palmprint features from limited size of training samples. We also introduce a contactless palm detector to automatically detect palm images, from the presented hands under complex backgrounds, and design of such palm detectors is critical for the success contactless palmprint identification during deployments. The main contributions in this technical report can be summarized as follows: (a) We develop a new deep learning based contactless palmprint identification framework with high generalization capability for operating on different contactless palmprint databases that can represent diverse deployment scenarios. A new Soft-Shifted Triplet Loss (SSTL) function has been developed to successfully address the nature of contactless palmprint patterns for learning comprehensive palm features (please see more details in section N) . Our work therefore presents significant advances to bridge the gap between deep learning and contactless palmprint matching techniques available today; (b) Under fair comparison, our approach consistently outperforms several state-of-the-art methods on publicly available contactless palmprint databases. Even under challenging scenario without incorporating any parameter tuning on the target dataset, our model can still achieve superior or competing performance over the state-of-art methods that have been subjected to extensive parameter tuning. This report also demonstrates how the faster-R-CNN _cite_ architecture can be adapted to build an online palm detector, which can robustly detect palm images from the presented hands under complex backgrounds. Such advancement is highly desirable, in the current literature, for the success of online and contactless palmprint identification applications. Completely automated matching of contactless palmprint images has received lot of attention and a range of palmprint matchers have been introduced in the literature. Detected or segmented palm images can be characterized by major/minor curved lines and creases that can be observed from low resolution (_inline_eq_ dpi) images and additional flexion ridges _cite_ that are observed from high resolution (_inline_eq_ dpi, not focus of this work like for _cite_) images. Therefore a range of texture matching methods have been introduced in the literature _cite_ . Encoding palmprint features using the dominant orientation of lines/creases in _cite_ is one of the most effective method for matching palmprint images. More recent work in matching contactless palmprint images appear in T-PAMIN _cite_ where an ordinal measurement based descriptor, i.e., difference of normal (DoN), has shown to outperform a range of methods introduced for matching contactless palmprint images using publicly available databases. This approach benefits from the contactless palm image acquisition modeling and introduces specialized masks to encode projective ordinal measurements. Therefore, this method has also been used to ascertain the effectiveness of approach developed in this technical report and serves as a reasonable choice as other methods have not yet shown to offer superior performance than from _cite_ in the best of our knowledge. Automated detection of palm images, or the region of interest from the hands presented by users, is inherently required for the success of automated palmprint identification systems. Most popular methods for palmprint detection are based on the extraction of key-points representing finger joints and extract a fixed region of interest relative to the orientation and/or the distance _cite_ between the key points. This approach works very well for the contact-based imaging setups but poses a range of problems for contactless palmprint images as its very difficult to robustly detect these key-points under background changes which are inherent during the contactless imaging even with the cooperative users attempting access. Therefore developed contactless palmprint databases _cite_ (in public domain) have been acquired using relatively fixed or stable background to primarily address the open problem of detecting palm images under user friendly contactless imaging setup. More work to detect contactless palmprint under contactless imaging is highly desirable and is also considered in our work. Despite promising performance indicated in the literature for matching palmprint images, conventional palmprint descriptors have several limitations. Summary of earlier work presented in _cite_ indicate that existing methods offer quite accurate performance but this performance needs to be further improved (especially on large contactless databases e.g. _cite_) to meet expectations for a wide range of deployments. Conventional palmprint descriptors, such as CompCode _cite_ or DoN _cite_, RLOC _cite_ or Ordinal _cite_, are based on empirical models, which apply hand crafted filters for the generation of features. Therefore these models heavily rely on the parameter selection when incorporated for matching performance for other/different databases or those acquired under different imaging environments. This situation can also be observed from _cite_, where eight different combination of parameters or N different databases are employed by extensive tuning. Commonly employed techniques in the the palmprint literature _cite_ for the automated detection of palm images, or the region of interest from the hands presented by users interested to access the system, often fails when the hand images are acquired under complex backgrounds. Such failure can be attributed to the nature of algorithms that relies on the detection of key-point using pixel-based operators that are dependent to differentiate gray-levels from skin and the background. The deep learning based approaches have potential to address above outlined limitations, since the parameters in deep neural networks are not empirically set instead self-learned from the data and the deep learning architectures are known to offer high generalization capabilities. However, any direct application of such architectures, e.g. _cite_, is expected to deliver limited performance or cannot match performance offered from state-of-art techniques such as those from _cite_ . This is due to the fact that new challenges emerge while incorporating typical deep learning architectures (e.g. CNN) for the palmprint recognition, which can primarily be attributed to the nature of palmprint patterns. Unlike the face, palmprint patterns are known to reveal little structured information or meaningful hierarchies. The palmprint texture is widely considered to be more accurate methods in the literature _cite_ which mainly employed small sized filters or block based operators to extract palmprint features. Therefore, we can infer that the most discriminative information from palmprint patterns is extracted from the local intensity distributions in region of interest (palm) images rather than from (if any) global features. The CNNs are known to be effective in recovering features from low level to the high level, and from local to global, due to the combination of convolutional and fully connected layers _cite_ . However as outlined earlier, the high level and global features extracted from such networks may not be optimal for the accurate matching of palmprint patterns. This report attempts to develop a more accurate and robust deep learning based palmprint feature representation framework and makes significant contributions towards fully discovering the potential from the deep learning for the contactless palmprint identification. Such objectives are yet to be pursued in the literature. Different from _cite_, this technical report develops a novel deep network and customized loss function, which are highly optimized to extract discriminative palmprint features and has been comparatively evaluated with several state of art methods using multiple public contactless palmprint databases.