In recent years, the technology of self-driving cars has made dramatic progress. One of the critical challenges of this emerging technology is the safety of both car occupants and other road users. The current prototype of autonomous cars are equipped with advanced sensors such as ultrasonic, vision, radar and LIDAR. These sensors along with sophisticated data fusion algorithms are able to detect and track obstacles in real-time with very good resolution. When an obstacle is detected in the planned path, either its planned route should be modified or the vehicle should come to stop. Depending on the traffic situation and vehicle speed, this policy could cause collision with other vehicles. Therefore, obstacle avoidance may not always be the safest action. Similar challenge has been discussed in _cite_ . The intuitive solution would be to recognize the object before taking an action. The intelligent unit should predict whether it is safe to pass over the object or it should inevitably follow avoiding policy. A sample video for each scenario is downloaded from Youtube and a few frames are shown in the Figure _ref_ . In the first video, an empty plastic container is bouncing in the road which is safe to pass. In the second video, a heavy object is falling out of the front car which should definitely be avoided. The immediate solution that one might consider is to formulate the problem as a regular image classification task and collect a dataset of collision safe and unsafe objects. While there is much progress in object detection/recognition methods _cite_, this approach has several challenges which makes it ineffective for this particular application. First, collecting a dataset that contains different objects in different lighting conditions and viewpoints is a difficult task in itself. Second, it is almost impossible to infer the weight of an object by its visual cue; for example, two very similar boxes with one of them filled with metal pieces and the other one which is empty have similar images. Finally, there is a high possibility of recognition failure because the image resolution is usually poor for far away objects. Also, the classifier should decide in a short period of time, where motion blur might make the problem even more challenging. For example, the white plastic container in the first column of Figure _ref_ could be classified as a gas cylinder. These challenges are easily resolved by a human by observing the trajectory of empty box versus heavy box (e.g. plastic container versus a gas cylinder) . Therefore, assuming that the real-time trajectory of the dynamic object is available _cite_, we claim that motion pattern provides strong cue to infer the object dynamics accurately and to classify it as a "safe to pass over" or "must avoid" object.