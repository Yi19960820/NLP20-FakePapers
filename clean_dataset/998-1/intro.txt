Today's digital contents are inherently multimedia: text, audio, image, video and so on. Images and videos, in particular, become a new way of communication between Internet users with the proliferation of sensor-rich mobile devices. This has encouraged the development of advanced techniques for a broad range of multimedia understanding applications. A fundamental progress that underlies the success of these technological advances is representation learning. Recently, the rise of Convolutional Neural Networks (CNN) convincingly demonstrates high capability of learning visual representation especially in image domain. For instance, an ensemble of residual nets _cite_ achieves N \% top-N error on the ImageNet test set, which is even lower than N \% of the reported human-level performance. Nevertheless, video is a temporal sequence of frames with large variations and complexities, resulting in difficulty in learning a powerful and generic spatio-temporal representation. One natural way to encode spatio-temporal information in videos is to extend the convolution kernels in CNN from ND to ND and train a brand new ND CNN. As such, the networks have access not only the visual appearance present in each video frame, but also the temporal evolution across consecutive frames. While encouraging performances are reported in recent studies _cite_, the training of ND CNN is very computationally expensive and the model size also has a quadratic growth compared to ND CNN. Take a widely adopted N-layer ND CNN, i.e., CND _cite_ networks, as an example, the model size reaches NMB which is even larger than that (NMB) of a N-layer ND ResNet (ResNet-N) _cite_, making it extremely difficult to train a very deep ND CNN. More importantly, directly fine-tuning ResNet-N with frames in Sports-NM dataset _cite_ may achieve better accuracy than CND trained on videos from scratch as shown in Figure _ref_ . Another alternative solution of producing spatio-temporal video representation is to utilize pooling strategy or Recurrent Neural Networks (RNN) over the representations of frames, which are often the activations of the last pooling layer or fully-connected layer in a ND CNN. This category of approaches, however, only build temporal connections on the high-level features at the top layer while leaving the correlations in the low-level forms, e.g., edges at the bottom layers, not fully exploited. We demonstrate in this paper that the above limitations can be mitigated by devising a family of bottleneck building blocks that leverages both spatial and temporal convolutional filters. Specifically, the key component in each block is a combination of one _inline_eq_ convolutional layer and one layer of _inline_eq_ convolutions in a parallel or cascaded fashion, that takes the place of a standard _inline_eq_ convolutional layer. As such, the model size is significantly reduced and the advantages of pre-learnt ND CNN in image domain could also be fully leveraged by initializing the _inline_eq_ convolutional filters with _inline_eq_ convolutions in ND CNN. Furthermore, we propose a novel Pseudo-ND Residual Net (PND ResNet) that composes each designed block in different placement throughout a whole ResNet-like architecture to enhance the structural diversity of the network. As a result, the temporal connections in our PND ResNet are constructed at every level from bottom to top and the learnt video representations encapsulate information related to objects, scenes and actions in videos, making them generic for various video analysis tasks. The main contribution of this work is the proposal of a family of bottleneck building blocks that simulates ND convolutions in an economic and effective way. This also leads to the elegant view of how different blocks should be placed for learning very deep networks and a new PND ResNet is presented for video representation learning. Through an extensive set of experiments, we demonstrate that our PND ResNet outperforms several state-of-the-art models on five different benchmarks and three different tasks.