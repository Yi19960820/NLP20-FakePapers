Measure of similarity plays an important role in applications such as content-based recommendation, image search and speech recognition. Therefore a number of techniques to {\em learn} a measure of similarity from data have been proposed~ _cite_ . When the measure of distance is induced by an inner product in a low-dimensional space as is done in many studies, learning a distance metric is equivalent to learning an {\em embedding} of objects in a low-dimensional space. This is useful for visualization as well as using the learned representation in a variety of down-stream tasks that require fixed length representations of objects as has been demonstrated by the applications of word embeddings~ _cite_ in language. Among various forms of supervision for learning distance metric, similarity comparison of the form `object _inline_eq_ is more similar to _inline_eq_ than to _inline_eq_ '', which we call {\em triplet comparison}, is extremely useful for obtaining an embedding that reflects a {\em perceptual similarity} _cite_ . Triplet comparisons can be obtained by crowdsourcing, or it may also be derived from class labels if available. The task of judging similarity comparisons, however, can be challenging for human annotators. Consider the problem of comparing three birds as seen in Fig.~ _ref_ . Most annotators will say that the head of bird _inline_eq_ is more similar to the head of _inline_eq_ while the back of _inline_eq_ is more similar to _inline_eq_ . Such ambiguity leads to noise in annotation and results in poor embeddings. A better approach would be to tell the annotator the desired view or the perspective of the object to use for measuring similarity. Such view-specific comparisons are not only easier for annotators, but they can also enable precise feedback for human ``in the loop'' tasks, such as, interactive fine-grained recognition~ _cite_, thereby reducing the human effort. The main drawback of learning view specific embeddings {\em independently} is that the number of similarity comparisons scales linearly with the number of views. This is undesirable as even learning a single embedding of _inline_eq_ objects may require _inline_eq_ triplet comparisons~ _cite_ in the worst case. We propose a method for learning embeddings {\em jointly} that addresses this drawback. Our method exploits underlying correlations that may exist between the views allowing a better use of the training data. Our method models the correlation between views by assuming that each view is a of a common embedding. Our model can be seen as a matrix factorization model in which local metric is defined as _inline_eq_, where _inline_eq_ is a matrix that parametrizes the common embedding and _inline_eq_ is a positive semidefinite matrix parametrizing the individual view. The model can be efficiently trained by alternately updating the view specific metric and the common embedding. We experiment with a synthetic dataset and two realistic datasets, namely, poses of airplanes, and crowd-sourced similarities collected on different body parts of birds (CUB dataset; Welinder et al.,) . On most datasets our joint learning approach obtains lower triplet generalization error compared to the independent learning approach or naively pooling all the views into a single one, especially when the number of training triplets is limited. Furthermore, we apply our joint metric learning approach to the multi-task metric learning setting studied by _cite_ to demonstrate that our method can also take input features and class labels into account. Our method compares favorably to the previous method on ISOLET dataset.