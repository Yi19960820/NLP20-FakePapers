Deep Neural Networks are powerful at solving classification problems in computer vision. However, learning classifiers with these models requires a large amount of labeled training data, and recent approaches have struggled to adapt to new classes in a data-efficient manner. There is interest in quickly learning new concepts from limited data using one-shot learning methods~ _cite_ . One-shot image classification is the problem of classifying images given only a single training example for each category~ _cite_ . We propose to undertake One-Shot Semantic Image Segmentation . Our goal is to predict a pixel-level segmentation mask for a semantic class (like horse, bus, \etc) given only a single image and its corresponding pixel-level annotation. We refer to the image-label pair for the new class as the support set here, but more generally for _inline_eq_-shot learning, support set refers to the _inline_eq_ images and labels. A simple approach to performing one-shot semantic image segmentation is to fine-tune a pre-trained segmentation network on the labeled image~ _cite_ . This approach is prone to over-fitting due to the millions of parameters being updated. It also introduces complications in optimization, where parameters like step size, momentum, number of iterations, \etc may be difficult to determine. Recent one-shot image categorization methods~ _cite_ in contrast, meta-learn a classifier that, when conditioned on a few training examples, can perform well on new classes. Since Fully Convolutional Neural Networks (FCNs) ~ _cite_ perform segmentation as pixel-wise classification, we could extend these one-shot methods directly to classify at the pixel level. However, thousands of dense features are computed from a single image and one-shot methods do not scale well to this many features. We illustrate this issue by implementing an extension to the Siamese Network from _cite_ as a baseline in Section _ref_ . We take inspiration from few-shot learning and propose a novel two-branched approach to one-shot semantic image segmentation. The first branch takes the labeled image as input and produces a vector of parameters as output. The second branch takes these parameters as well as a new image as input and produces a segmentation mask of the image for the new class as output. This is illustrated in Figure _ref_ . Unlike the fine tuning approach to one-shot learning, which may require many iterations of SGD to learn parameters for the segmentation network, the first branch of our network computes parameters in a single forward pass. This has several advantages: the single forward pass makes our method fast; our approach for one-shot learning is fully differentiable, allowing the branch to be jointly trained with the segmentation branch of our network; finally, the number of parameters _inline_eq_ is independent of the size of the image, so our method does not have problems in scaling. To measure the performance for one-shot semantic segmentation we define a new benchmark on the PASCAL VOC N dataset~ _cite_ (Section _ref_) . The training set contains labeled images from a subset of the PASCAL classes and the testing set has annotations of classes that were not present in training. We show significant improvements over the baselines on this benchmark in terms of the standard meanIoU (mean Intersection over Union) metric as described in Section~ _ref_ . We extend to _inline_eq_-shot learning by applying our one-shot approach for each of the _inline_eq_ images independently to produce _inline_eq_ segmentation masks. We then aggregate these masks by performing a logical-OR operation at the pixel level. This approach, apart from being easy to implement and fast, requires no retraining to generalize to any number of images in the support set. We show its effectiveness in terms of increasing meanIOU accuracy per added image to the support set in section _ref_ . \ PASCAL VOC contains only N classes, which is small when compared to standard datasets used for training one-shot classification methods like Omniglot (N) _cite_ and ImageNet (N) (_cite_) . Simulating the one-shot task during training, even with such a limited number of classes performs well. This is in contrast to the common notion that training models for few-shot learning requires a large number of classes. We hypothesize that part of our algorithm's ability to generalize well to unseen classes comes from the pre-training performed on ImageNet, which contains weak image-level annotations for a large number of classes. We perform experiments on the pretraining in section _ref_ . This paper makes the following contributions: (N) we propose a novel technique for one-shot segmentation which outperforms baselines while remaining significantly faster; (N) we show that our technique can do this without weak labels for the new classes; (N) we show that meta-learning can be effectively performed even with only a few classes having strong annotations available; and (N) we set up a benchmark for the challenging _inline_eq_-shot semantic segmentation task on PASCAL.