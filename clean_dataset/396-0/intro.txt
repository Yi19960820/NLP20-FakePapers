Deep convolutional neural networks (CNNs) have improved performance of many tasks in computer vision, such as image recognition _cite_, object detection _cite_, and semantic segmentation _cite_ . However, existing works mainly design network architectures to solve the above problems on a single domain, for example, improving scene parsing on the real images of Cityscape dataset _cite_ . When these networks are applied to the other domain of this scene parsing task, such as the virtual images of GTAN dataset _cite_, their performance would drop notably. This is due to the appearance gap between the images of these two datasets, as shown in Fig. _ref_ (a) . A natural solution to solve the appearance gap is by using transfer learning. For instance, by finetuning a CNN pretrained on Cityscapes using the data from GTAN, we are able to adapt the features learned from Cityscapes to GTAN, where accuracy can be increased. But even so, the appearance gap is not eliminated, because when applying the finetuned CNN back to Cityscapes, the accuracy would be significantly degraded. How to address large diversity of appearances by designing deep architectures? It is a key challenge in computer vision. The answer is to induce appearance invariance into CNNs. This solution is obvious but non-trivial. For example, there are many ways to produce the property of spatial invariance in deep networks, such as max pooling~ _cite_, deformable convolution~ _cite_, which are invariant to spatial variations like poses, viewpoints, and scales, but are not invariant to variations of image appearances. As shown in Fig. _ref_ (b), when the appearance variance of two datasets are simple and known beforehand, such as lightings and infrared, they can be reduced by explicitly augmenting data. However, as shown in Fig. _ref_ (c), when appearance variance are complex and unknown, such as arbitrary image styles and virtuality, the CNNs have to learn to reduce them by introducing new component into their deep architectures. To this end, we present IBN-Net, a novel convolutional architecture, which learns to and appearance variance, while discrimination of the learned features. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, enhancing both its learning and generalization capacity. It has two appealing benefits that previous deep architectures do not have. First, different from previous CNN structures that isolate IN and BN, IBN-Net unifies them by delving into their learned features. For example, many recent advanced deep architectures employed BN as a key component to improve their learning capacity in high-level vision tasks such as image recognition _cite_, while IN was often combined with CNNs to remove variance of images on low-level vision tasks such as image style transfer _cite_ . But the different characteristics of their learned features and the impact of their combination have not been disclosed in existing works. In contrast, IBN-Net shows that combining them in an appropriate manner improves both learning and generalization capacities. Second, our IBN-Net keeps IN and BN features in shallow layer and BN features in higher layer, inheriting from the statistical merit of feature divergence under different depth of a network. As shown in Fig. _ref_, the x-axis denotes the depth of a network and the y-axis shows feature divergence calculated via symmetric KL divergence. When analyzing the depth-vs-divergence in ImageNet original with its Monet version (blue bars), the divergence decreases as layer depth increases, manifesting the appearance difference mainly lies in shallow layers. On the contrary, compared with two disjoint ImageNet splits (orange bar), the object level difference attributes to majorly higher layer divergence and partially low layer ones. Based on these observations, we introduce IN layers to CNNs following two rules. Firstly, to reduce feature variance caused by appearance in shallow layers while not interfering the content discrimination in deep layers, we only add IN layers to the shallow half of the CNNs. Secondly, to also preserve image content information in shallow layers, we replace the original BN layers to IN for a half of features and BN for the other half. These give rise to our IBN-Net. Our contributions can be summarized as follows: (N) A novel deep structure, IBN-Net, is proposed to learning and generalization capacities of deep networks. For example, IBN-NetN achieves N N \% and N N \% topN/topN errors on the original validation set of ImageNet~ _cite_ and a new validation set after style transformation respectively, outperforming ResNetN by N N \% and N N \%, when they have similar numbers of parameters and computational cost. (N) By delving into IN and BN, we disclose the key characteristics of their learned features, where IN provides visual and appearance, while BN accelerates training and preserves features. This finding is important to understand them, and helpful to design the architecture of IBN-Net, where IN is preferred in shallow layers to remove appearance variations, whereas its strength in deep layers should be reduced in order to maintain discrimination. The component of IBN-Net can be used to re-develop many recent deep architectures, improving both their learning and generalization capacities, but keeping their computational cost unchanged. For example, by using IBN-Net, DenseNetN _cite_, ResNetN _cite_, ResNeXtN _cite_, and SE-ResNetN _cite_, outperform their original versions by N \%, N \%, N \%, and N \% on ImageNet respectively. These re-developed networks can be utilized as in many tasks in future researches. (N) IBN-Net significantly improves performance across domains. By taking scene understanding as an example under a cross-evaluation setting, \ie training a CNN on Cityscapes and evaluating it on GTAN without finetuning and vice versa, ResNetN integrated with IBN-Net improves its counterpart by N \% and N \% respectively. It also notably reduces sample size when finetuning GTAN pretrained model on Cityscapes. For instance, it achieves a segmentation accuracy of N \% when finetuning using just N \% training data from Cityscapes, compared to N \% of ResNetN alone, which is finetuned using all training data.