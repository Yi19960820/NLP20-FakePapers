Action understanding remains an intensely studied problem-space, e.g., action recognition _cite_, action detection _cite_ and action labeling _cite_ . These works all emphasize instantaneous or short term actions, which clearly play a role in understanding short or structured videos~ _cite_ . However, for long, unconstrained videos, such as user-uploaded instructional videos of complex tasks---preparing coffee _cite_, changing tires ~ _cite_---learning the steps of accomplishing these tasks and their dependencies is essential, especially for agents' automatic acquisition of language or manipulation skills from video _cite_ . We define procedure as the sequence of necessary steps comprising such a complex task, and define each individual step as a procedure segment, or simply segment for convenience, inspired by~ _cite_ . For example, there are N segments in the making a BLT sandwich video shown in Fig.~ _ref_ . We represent these segments by their start and end temporal boundaries in a given video. Note that one procedure segment could contain multiple actions, but it should be conceptually compact, i.e., described with a single sentence. The number of procedure segments and their locations reflect human consensus on how the procedure is structured. Can this human-consensus procedure structure be learned by an agent? To that end, we define the Procedure Segmentation problem as: automatically segment a video containing a procedure into category-independent procedure segments. Although this is a new problem, there are two related, existing problems: event proposal and procedure learning. The event proposal problem~ _cite_ is to localize category-independent temporal events from unconstrained videos. Both event proposals and procedure segments can contain multiple actions. However, the event proposal problem emphasizes the recall quality given a large amount of proposals, rather than the identification of a procedure (sequence of segments) from limited but necessary proposals. Events might overlap and are loosely-coupled but procedure segments barely overlap, are closely-coupled and usually have long-term dependencies. The existing work in procedure learning is less-supervised than that of event proposals (no labels are given for the segments) . It emphasizes video-subtitle alignment~ _cite_ and discovery of common procedure steps of a specific process _cite_ . However, the methods proposed in these works make restrictive assumptions: they typically assume either language is concurrently available, e.g., from subtitles, or the number of procedure steps for a certain procedure is fixed, or both. Such assumptions are limited: extra textual input is unavailable in some scenarios; the subtitles or action sequences automatically generated by machines, e.g., YouTube's ASR system, are inaccurate and require manual intervention; and many procedures of a certain type, such a specific recipe, will vary the number of steps in different instances (process variation) . Unfortunately, work in neither of these two problems sheds sufficient light on understanding procedure segmentation, as posed above. In this paper, we directly focus on procedure segmentation. We propose a new dataset of sufficient size and complexity to facilitate investigating procedure segmentation, and we present an automatic procedure segmentation method, called ProcNets . Our new dataset, called YouCookN, contains N videos from N recipes with a total length of N hours. The procedure steps for each video are annotated with temporal boundaries and described post-hoc by a viewer/annotator with imperative English sentences (see Fig. _ref_) . To reflect the human consensus on how a procedure should be segmented, we annotate each video with two annotators, one for the major effort and the other one for the verification. To the best of our knowledge, this dataset is more than twice as large as the nearest in size and is the only one available to have both temporal boundary annotation and imperative English sentence annotation for the procedure segments. We then propose an end-to-end model, named Procedure Segmentation Networks or ProcNets, for procedure segmentation. ProcNets make neither of the assumptions made by existing procedure learning methods: we do not rely on available subtitles and we do not rely on knowledge of the number of segments in the procedure. ProcNets segments a long, unconstrained video into a sequence of category-independent procedure segments. ProcNets have three pieces: N) context-aware frame-wise feature encoding; N) procedure segment proposal for localizing segment candidates as start and end timestamps; N) sequential prediction for learning the temporal structure among the candidates and generating the final proposals through a Recurrent Neural Network (RNN) . The intuition is: when humans are segmenting a procedure, they first browse the video to have a general idea where are the salient segments, which is done by our proposal module. Then they finalize the segment boundaries based on the dependencies among the candidates, i.e., which happens after which, achieved by our sequential prediction module. For evaluation, we compare variants of our model with competitive baselines on standard metrics and the proposed methods demonstrate top performance against baselines. Furthermore, our detailed study suggests that ProcNets learn the structure of procedures as expected. Our contributions are three-fold. First, we introduce and are the first to tackle the category-independent procedure segmentation problem in untrimmed and unconstrained videos. Second, we collect and distribute a large-scale dataset for procedure segmentation in instructional videos. Third, we propose a segment-level recurrent model for proposing semantically meaningful video segments, in contrast to state-of-the-art methods that model temporal dependencies at the frame-level _cite_ . The output procedure segments of ProcNets can be applied for other tasks, including full agent-based procedure learning~ _cite_ or smaller-scale video description generation~ _cite_ .