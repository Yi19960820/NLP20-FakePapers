Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too resource demanding for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets (F acebook-B erkeley-Net s), a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves N \% top-N accuracy on ImageNet with NM FLOPs and N ms latency on a Samsung SN phone, N smaller and N faster than MobileNetVN-N _cite_ with similar accuracy. Despite higher accuracy and lower latency than MnasNet _cite_, we estimate FBNet-B's search cost is Nx smaller than MnasNet's, at only N GPU-hours. Searched for different resolutions and channel sizes, FBNets achieve N \% to N \% higher accuracy than MobileNetVN. The smallest FBNet achieves N \% accuracy and N ms latency (N frames per second) on a Samsung SN. Over a Samsung-optimized FBNet, the iPhone-X-optimized model achieves a N speedup on an iPhone X. FBNet models are open-sourced at _url_ .