It is widely known that machine learning has brought a massive benefit to many areas over the last decades. In many scenarios, computer vision relies on vast amount of data to train machine learning algorithms. Recently, several datasets of top-view plant images were publicly released _cite_, allowing the computer vision community to propose new methodologies for leaf counting _cite_ and leaf segmentation _cite_ . However, these algorithms can perform even better when provided with more training data, such that the generalisation capabilities of the trained model are increased, while also reducing overfitting. As of now, a main issue affecting current plant phenotyping datasets is the limited quantity of labelled data _cite_ . Typically, the computer vision community has been employing dataset augmentation to increase the amount of data using artificial transformations. In fact, artificially perturbing the original dataset with affine transformations (e.g., rotation, scale, translation) is considered a common practice. However, this approach has limits: the augmented data only capture the variability of the training set (e.g., if a plant with N leaves is missing from the training set, this particular instance will not ever be learnt) . For this reason, herein we present preliminary work on generating artificial images of plants, using a recent generative model that learns how to create new images. An attempt to generate rosettes was done in _cite_, where an empirical model was created analysing N Arabidopsis plants of about N leaves. Sigmoidal growth models were fitted based on the sets of plants under their study. Then, organs were dissected and leaves were used to fit B-splines to obtain vector images. The issues of this model can be summarised as follows: (i) the approach is confined on the manual observation of morphological traits of a limited set of plants; and (ii) there is a lack of realism in the generated images, for example in the absence of texture on the leaves. The manual acquisition of morphological data from plants is a tedious and error-prone process _cite_ and we aim to alleviate this process, by using neural networks that can learn those parameters from data. Recently, several generative models were proposed to generate realistic images. In the literature it is possible to find different generative models to create artificial images. For example, in _cite_ the authors synthesise images of fingerprints, reproducing the orientation model of fingerprint lines. Another method to generate images employs genetic programming _cite_ . However, recent interest in neural networks, has brought new methodologies to generate synthetic images. In fact, convolutional neural networks (CNNs) were used to generate images of photorealistic chairs _cite_ . In _cite_, the authors introduce the Deep Recurrent Attentive Writer (DRAW) network, which combines LSTM _cite_ layers to draw images, using a selective attention model that, at each iteration, finds the next location to draw new pixels. Despite its impressive results, this method is challenged by natural image data. The Generative Adversarial Network (GAN) _cite_ has been proven to be successful at generating synthetic images also on natural images. In a GAN, there are two models competing with each other: the Generator (G), which creates artificial images; and the Discriminator (D), which is trained to classify images coming from the training set (real) and the generator (fake) . The spirit of the GAN is to improve G to create more realistic images, whilst D is trained to distinguish between real and generated images. Training works by improving in alternating fashion G or D, until an equilibrium is obtained. Generally speaking, the generator and the discriminator can be any network that satisfies the following criteria: (i) D needs to take as input an image and has to output `N' and `N' (real/not real) ; (ii) G needs to take as input random noise (e.g. drawn from an uniform or normal distribution) and has to give as output an image. LAPGAN _cite_ was proposed, which was able to produce better quality images using Laplacian pyramids. A new successful adversarial network providing outstanding results is Deep Convolutional GAN _cite_ . The benefits of this model mostly stem from the use of convolutional/deconvolutional layers for discriminator and generator respectively and the lack of pooling/upsampling layers. Although adversarial networks have brought many benefits, a main limitation is the lack of direct control over the images generated. For instance, in the case where we want to train a GAN to generate images of handwritten digits--the MNIST dataset _cite_ is a typical benchmark dataset in computer vision and machine learning--, it would be reasonable to have control over which digit to generate each time. For this reason Conditional GAN _cite_ was proposed to overcome such limitation. In this new formulation, generator and discriminator networks are endowed with an additional input, allowing to be trained under certain conditions. In _cite_, the authors propose StackGAN, a two-stage GAN conditioned on image captions. Specifically, Stage-I generates coarse images, which are fed to Stage-II to obtain more realistic images. In this paper, we show how to generate Arabidopsis plants, using a model inspired by _cite_, trained on the CVPPP N dataset. The network learns how to map random noise _inline_eq_ into an Arabidopsis plant, under a condition _inline_eq_ . For our purposes, _inline_eq_ encodes the number of leaves that the artificially generated plant should have. The employed model, which we call Arabidopsis Rosette Image Generator (through) Adversarial Network (ARIGAN), is able to create _inline_eq_ RGB images of Arabidopsis plants, as shown in \figurename~ _ref_ . We evaluate our model by creating an Ax (using the CVPPP dataset name convention) and provide the generated data to a state-of-the-art leaf counting algorithm _cite_ to augment the training dataset. The remainder of this paper is organised as follows. In Section _ref_ we show the proposed methodology on how to train and generate Arabidopsis plants. In Section _ref_ we report the results of our experiments. Finally, Section _ref_ concludes the paper.