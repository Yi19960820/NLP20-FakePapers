Video understanding is a fundamental problem in computer vision and has drawn increasing interests over the past few years due to its vast potential applications in surveillance, robotics, etc. While fruitful progress~ _cite_ has been made on activity detection to recognize and localize temporal segments in videos, such approaches are limited to work on pre-defined lists of simple activities, such as playing basketball, drinking water, etc. This restrains us from moving towards real-world unconstrained activity detection. To solve this problem, we tackle the natural language moment retrieval task. Given a verbal description, our goal is to determine the start and end time (\ie localization) of the temporal segment (\ie moment) that best corresponds to this given query. While this formulation opens up great opportunities for better video perception, it is substantially more challenging as it needs to model not only the characteristics of sentence and video but also their complex relations. On one hand, a real-world video often contains multiple moments of interests. Consider a simple query like ``The child touches the ground the second time", shown in Figure _ref_, a robust model needs to scan through the video and compare the video context to find the second occurrence of ``child touches the ground". This raises the first challenge for our task: semantic misalignment . A simple ordinal number will result in searching from a whole video, where a naive sliding approach will fail. On the other hand, the language query usually describes complex temporal dependencies. Consider another query like "Child is running away after is closest to the camera", different from the sequence described in sentence, the "close to the camera" moment happens before "running away". This raises the second challenge for our task: structural misalignment . The language sequence is often misaligned with video sequence, where a naive matching without temporal reasoning will fail. These two key challenges we identify: semantic misalignment and structural misalignment have not been solved in existing methods~ _cite_ for the natural language moment retrieval task. Existing methods sample candidate moments by scanning videos with varying sliding windows, and compare the sentence with each moment individually in a multi-modal common space. Although simple and intuitive, this individualist representations of sentence and video make it hard to model semantic and structural relations among two modalities. To address the above challenges, we propose an end-to-end Moment Alignment Network (MAN) for the natural language moment retrieval task. The proposed MAN model directly generates candidate moment representations aligned with language semantics, and explicitly model temporal relationships among different moments in a graph-structured network. Specifically, we encode the entire video stream using a hierarchical convolutional network and naturally assign candidate moments over different temporal locations and scales. Language features are encoded as efficient dynamic filters and convolved with input visual representations to deal with semantic misalignment. In addition, we propose an Iterative Graph Adjustment Network (IGAN) adopted from Graph Convolution Network (GCN) ~ _cite_ to model relations among candidate moments in a structured graph. Our contributions are as follows: