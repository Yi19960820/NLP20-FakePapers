dictionary based sparse representation (SR) finds successful application in various signal processing domains, such as image denoising~ _cite_, image recognition~ _cite_, face recognition~ _cite_, speaker identification/verification~ _cite_, and fingerprint identification _cite_ . In SR domain, existing data driven dictionary learning techniques can be broadly divided into three categories: supervised, semi-supervised and unsupervised. The dictionary learned utilizing class labels are referred to as supervised. Whereas those learned using weak supervision in form of any assumed structure/constraint are termed as semi-supervised dictionary. Both these kinds of dictionaries produce more discriminative sparse codes than the unsupervised ones, thus result in better classification performance. In SR domain, usually redundant (over-complete) dictionaries are preferred. Such dictionaries have more columns (atoms) than rows (data dimensionality) . Sometimes, lesser number of examples than the data dimensionality involved are available for learning the dictionary. Thus, only under-complete dictionary could be learned unless we project the data to appropriate low-dimensional space. Nevertheless, the use of under-complete dictionaries have been reported in SR based classification tasks~ _cite_ . In recent past, the dictionary learning has received a lot of attention in SR domain. Combining the K-means clustering and the singular value decomposition (SVD), a widely used dictionary learning approach is proposed and is referred to as KSVD~ _cite_ . In learning of the KSVD dictionary, the reconstruction error is minimized under the constraint on sparsity for the given data. Though not being optimized for producing discriminative sparse codes, some works have explored the KSVD dictionaries in the classification task~ _cite_ . In the literature, a few classification-driven dictionaries are also proposed such as supervised KSVD (S-KSVD) ~ _cite_ dictionary and label-consistent KSVD (LC-KSVD) ~ _cite_ dictionary. The S-KSVD algorithm incorporates the Fisher discriminant criterion in dictionary learning. Whereas in the LC-KSVD algorithm, a linear transformation that maps the sparse code to more discriminative one is also learned along with the dictionary. Though yielding enhanced classification performance, these supervised dictionaries neither use any block structure nor explicitly minimize the within-class redundancy. As the result of that, such dictionaries are found to yield inconsistent sparse codes for the same class data. In addition to supervised dictionary, some block-structured dictionaries are also proposed. The introduction of block structure in a dictionary is noted to enhance not only its reconstruction ability~ _cite_ but also its classification ability~ _cite_ . Initial works simply exploit the known block structures in sparse coding with no emphasis on learning such dictionaries~ _cite_ . The block-KSVD (BKSVD) ~ _cite_ dictionary is probably the first attempt towards learning an unsupervised block-structured dictionary. It employs a sparse agglomerative clustering (SAC) algorithm for estimating the unknown block structure. Given a dictionary, the SAC algorithm estimates the block structure by iteratively grouping its atoms based on sparse coding. As the SAC employs orthogonal matching pursuit (OMP) ~ _cite_ for sparse coding, the grouped atoms happen to be diverse (less correlated) . Thus, if the given dictionary comprises of correlated atoms those are less likely to be grouped together in the SAC approach. This affects the classification performance due to inconsistency in sparse coding. Addressal the above mentioned weakness in the estimated block structure is the prime motivation behind this work. Further, in the context of image recognition, we come across a proposal of supervised block-structured dictionary learning approach that employs intra-block coherence suppression for reducing the redundancy and is referred to as the IBCS~ _cite_ dictionary. The minimization of intra-block coherence in a dictionary is critical for consistency in the resulting sparse codes. In that work, the block structure is initialized in a supervised manner and is kept fixed during the dictionary learning. It would be interesting to explore the adaptation of block structure while retaining the class supervision. Motivated by these works, we propose a classification-driven dictionary learning approach and contrast its performance with existing approaches on synthetic data as well as real data. The main contributions of this work are as follows: The remainder of the paper is organized as follows. First, the prior work on the dictionary learning using the block structure is discussed in Section~ _ref_ . The proposed correlation based greedy clustering algorithm is discussed in Section~ _ref_ . In Section~ _ref_, we formulate the classification driven dictionary learning approach. Section~ _ref_ and Section~ _ref_ present the evaluation of the proposed approach on synthetic and real data, respectively. The paper is concluded in Section~ _ref_ .