~ ~ Purpose ~ We propose a novel multi-modal and multi-task architecture for simultaneous low level gesture and surgical task classification in Robot Assisted Surgery (RAS) videos. ~ ~ Methods ~ Our end-to-end architecture is based on the principles of a long short-term memory network (LSTM) that jointly learns temporal dynamics on rich representations of visual and motion features, while simultaneously classifying activities of low-level gestures and surgical tasks. ~ ~ Results ~ Our experimental results show that our approach is superior compared to an architecture that classifies the gestures and surgical tasks separately on visual cues and motion cues respectively. We train our model on a fixed random set of N gesture video segments and use the rest N for testing. This results in around N, N gesture frames sampled for training and N, N for testing. For a N split experimentation, while the conventional approach reaches an Average Precision (AP) of only N \% (N \%), our architecture reaches an AP of N \% (N \%) for N tasks and N possible gesture labels, resulting in an improvement of N \% (N \%) . ~ ~ Conclusions ~ Our architecture learns temporal dynamics on rich representations of visual and motion features that compliment each other for classification of low-level gestures and surgical tasks. Its multi-task learning nature makes use of learned joint relationships and combinations of shared and task-specific representations. While benchmark studies focus on recognizing gestures that take place under specific tasks, we focus on recognizing common gestures that reoccur across different tasks and settings and significantly perform better compared to conventional architectures.