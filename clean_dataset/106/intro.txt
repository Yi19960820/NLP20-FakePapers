The frontiers of computer vision have been reshaped profoundly in the last few years due to the ever wider deployment of convolutional neural networks (CNNs) . A cornucopia of CNNs have been developed and now define the state of the art for many vision tasks. On the other hand, manifold learning has been a central concept for many learning algorithms over decades, with applications to tasks as diverse as dimensionality reduction~ _cite_, hashing~ _cite_, feature encoding~ _cite_, clustering~ _cite_, semi-supervised learning~ _cite_, model imitation~ _cite_, and visualization~ _cite_ . Thus, it stands to reason to devise methods to utilize manifold structures more effectively in training CNNs. The main challenge to train a CNN with manifold structures is to incorporate the latter's structure onto the former's architecture. Manifold structures are often expressed by a graph or an affinity matrix of all data samples. This is inconvenient to use with many CNNs, as these are tailored for classification tasks with crisp class labels. Systems do exist for learning CNNs on graphs~ _cite_, which accommodate affinity graphs directly as training data. However, the scalability of this strand of method is limited, since affinity graphs can potentially be large. In order to seamlessly couple manifold structures with the architecture of modern CNNs, we propose to segment data manifold. It is segmented so that samples that are close to each other fall into the same group or `pseudo-class'. The corresponding pseudo-labels are fed to the CNN to train it for classification: grouping similar samples and separating dis-similar ones. This is in line with the aim of manifold learning. Yet, the labels obtained from the segmentation can be noisy due to its unsupervised nature. To mitigate this, we propose ensemble manifold segmentation (EMS) to create an ensemble of segmentations that are accurate individually and mutually diverse. EMS leads to an ensemble of pseudo classification tasks, which results in an ensemble-task architecture featuring an ensemble of loss functions. Figure~ _ref_ shows the architecture of our method. It consists of two copies of the same network, with shared weight parameters. The right stream is trained with unlabeled data and their pseudo-labels as generated from ensemble clustering; the stream on the left is trained under `real' supervision, with training samples whose real target labels are available. The method is dubbed ManifoldNet. ManifoldNet can be trained with only the right stream or with the two streams jointly, depending on the nature of the tasks. For instance, for unsupervised learning tasks such as dimensionality reduction, hashing and unsupervised network imitation (distillation), one can use only the right stream. For tasks such as semi-supervised classification, the two streams are trained with different training sets, one labeled, one unlabeled. This flexibility greatly increases the applicability of the method. ManifoldNet translates manifold structures to crisp labels, which gives representational and training advantages with modern CNNs. Apart from being intuitive and easy to implement, ManifoldNet has additional benefits. Compared to manifold learning methods~ _cite_, it comes naturally with an out-of-sample ability: the trained CNNs can be used for many tasks, in the same way as standard CNNs can, \eg as a feature extractor; Compared to deep embedding~ _cite_, ManifoldNet is better scalable as it can be trained in a point-wise manner. ManifoldNet is a very general framework and can be easily applied to many different tasks. As Figure~ _ref_ shows, much of the work needed to apply it to a new task lies in adopting a task-specific network architecture. The method is orthogonal to the methods~ _cite_ for self-learning feature representations. ManifoldNet is evaluated on two different tasks: network imitation and semi-supervised classification. Experiments show that it effectively utilizes manifold structures for both unsupervised and semi-supervised learning.