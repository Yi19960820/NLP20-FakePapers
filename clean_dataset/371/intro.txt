We deal with the problem of supervised human action recognition from unconstrained `real-world' videos. The objective is to determine an action (one per time instance) performed in a given video. In the scenario where large number of action classes are present such as HMDBN _cite_ and UCFN _cite_ with N and N classes respectively, five Major categories _cite_ can be classified as shown in Table _ref_ . The major discriminating information between two categories such as (N) and (N) is the objects with which the human are interacting! The discriminating information for action classes within a category such as in (N)--Shoot ball, Shoot bow, Shoot gun--is the objects information. Further, if the objects were to be non-moving, such as gun, bow, they would not be detected by the spatio-temporal interest points or trajectories. To overcome these limitations, we use Convolution Neural Networks (CNN) pre-trained on the Imagenet _cite_ N object categories. CNNs are very efficient to train, faster to apply and better in accuracy as objects detectors _cite_ . These deep nets learn the invariant representation and object classification result simultaneously by back-propagating information, through stacked convolution and pooling layers, with the aid of a large number of labelled examples. In this context, we investigate the following questions We study and present our results on the large-scale action datasets HMDBN and UCFN containing at least N-N different action classes. In the remainder of the paper, Section _ref_ contains a review of the related works. Section _ref_ describes the framework and details the local feature descriptors, codebook generation, object detectors, different feature encoding techniques, classifier and datasets. Section _ref_ presents and discusses the results obtained on the benchmark datasets. Finally, conclusions are drawn in Section _ref_ .