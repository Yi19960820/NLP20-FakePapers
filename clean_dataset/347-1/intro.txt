the five basic human senses, touch is an important perceptional modality for understanding the relationship between our surroundings and us. It offers complementary information that helps comprehend the environment. From this perspective, touch or tactile sensing has been an attractive subject of research in robotics and haptics for many years _cite_ _cite_ _cite_ _cite_ _cite_ . The main physical property needed for grasping and interacting with objects is the force of interaction. When a robotic hand attempts to grasp an object, a contact-type haptic sensor is used to measure the force of interaction between the device and the object. This improves the success rate of the gripping and enables precise hand manipulations~ _cite_ . In the case of humans, the visual information sensed through the eyes is used in addition to tactile sensations when grasping an object. Through visual information, we perceive the shape, appearance, and texture of an object, and infer the tactile memory learned through the past experience. From the perspectives of neuroscience and psychophysics, Ernst and Banks~ _cite_ investigated the mechanism of information sharing between the senses of vision and touch. Newell et al.~ _cite_ showed that the human brain employs shared models of objects across multiple sensory modalities, e.g., vision and tactile sensing, so that knowledge can be transferred from one to another. Inspired by the knowledge transfer from vision to touch~ _cite_, we propose a vision sensor-based method that simulates tactile sensing, which has a different modality, using only visual sensing information. When humans try to touch an object, they can recall how it feels before touching it by summoning past experiences. Specifically, if we know what an object is, and we can observe how its appearance changes by a finger pressing, we can predict the interaction force between the object and the finger from past experiences. Another focus of the proposed method is that compared with contact-type haptic sensors, a non-contact-type sensing method can help constantly measure the haptic force because the camera sensor is not worn out even when it is thus used for a long time. Moreover, as an additional touch sensor does not need to be attached to the instrument, the mechanism of the instrument can be miniaturized. In this paper, our computational approach is based on learning haptic information from past human experiences. The following are pivotal rules: to predict the interaction forces exerted by the target object using only sequential images, and for simplicity, we assume that the target objects have been touched in advance like past experiences. For this purpose, we collected more than N, N images of different objects under a variety of conditions and used the corresponding databases to train and validate the proposed method. From the viewpoint of predicting haptic information from images, the basic deep learning architecture is developed using a convolutional neural network (CNN)-based recurrent neural network (RNN) ~ _cite_ . As in human perception processes, we used the CNN to analyze types of target objects and changes in their appearances using the images, analyzed the images over time, and used temporal changes in them as inputs to the RNN to eventually estimate the force of interaction. To construct the network, we believe that the attention mechanism~ _cite_ _cite_, which focuses only on regions of importance in images for visual question answering (VQA) ~ _cite_, helps improve the accuracy of prediction of the force. However, the main difference between the proposed method and previously developed attention networks~ _cite_ _cite_, which commonly have been designed for a single image-based attention mechanism, is that we use a temporal dynamics-based attention method using sequential images to predict the interaction forces. Because the appearance changes of the target object between the sequential images play a pivotal role in inferring haptic information. As the number of CNN features increase due to the sequential images, there is an increasing need for a method to efficiently process a large amount of information generated. For this purpose, we propose a sequential image-based attention method consisting of a sequential spatial attention module (SSAM) and a sequential image-based channel attention module (SCAM) to attain higher accuracy for predicting haptic information. By developing the attention module based on sequential images independently of the RNN, as shown in Fig.~ _ref_, the concentrated information can be inferred clearly to predict the haptic force based on changes in the appearance of the target object. Moreover, we use spatial and channel attention modules, respectively, and each attention module is based on a proposed weighted average pooling (WAP) method for handling successfully a large amount of information generated by the sequential images. The main contributions of this paper are as follows: (N) We propose a computational method for predicting the haptic interaction force only from visual information without a haptic sensor. (N) The sequential image-based attention modules are proposed for efficiently processing the increased convolutional features due to the sequential images and for obtaining more accurate haptic information at the same time. (N) We collected a large number of sequential images of objects and the corresponding information concerning the forces of interaction on the objects by using an automatic mechanism.