Image classification is a fundamental and challenging problem in computer vision and many research efforts have been devoted to this topic during the past years . The majority of these contributions focus on the problem of object recognition and scene recognition, partially due to the simplicity of object and scene concepts and the availability of large-scale datasets (ImageNet, Places) . On the other hand, event recognition in static images also is important for semantic image understanding. Being able to selectively retrieve event images helps us keep nice memories of particular episodes of our lives or to more effectively find relevant illustrations, helps to locate where images were taken or to analyze people's culture and so on. In general, an event captures the complex behavior of a group of people, interacting with multiple objects, and taking place in a specific environment. As illustrated in Figure~ _ref_, the characterization of the concept `event' is relatively complicated compared with the concepts of objects and scenes. Images from the same event category may vary even more in visual appearance and structure. Multiple high-level semantic cues, such as interacting objects, scene context, human poses, and garments, can provide useful cues for event understanding and should be exploited. Recently, Convolutional Neural Networks (CNNs) have delivered great successes in large-scale image classification, in particular for object recognition and scene recognition . Large-scale image datasets (more than N million images) with supervised labels have proven of great importance in this. However, for event recognition, the current public datasets are relatively small and could be easily over-fitted by deep models. Moreover, the inherent complexity of the concept of `event' increases the difficulty of training an event CNN from raw images. Therefore, transferring deep models successfully trained on other datasets to the case of event recognition comes out to be a practical approach. Specifically, in this paper, we aim to study and . In this vein, we make three contributions to event recognition in still images that are described in the next paragraphs. First, we empirically investigate the correlation among object, scene, and event recognition. As part of this effort, we identify a subset of object and scene categories which are discriminative for event recognition. Essentially, events are complex social phenomena composed of multiple people in the presence of relevant objects and within a specific type of scene. Therefore, an event class will co-occur with certain objects and scenes, while having low correlation with others. Hence, we explore these inherent relationships between objects, scenes, and events in a quantitative way, and provide some evidence that it is a good choice to transfer object and scene deep models to event recognition. Specifically, thanks to the large-scale object and scene datasets, we utilize these pre-trained deep object and scene models to test the images of different event classes. The resulting prediction scores enable us to study the joint distribution of objects, scenes, and events, and analyze the correlation of these N concepts. Furthermore, based on these empirical observations, we propose a method to select a subset of object and scene classes, that have a strong discriminative capacity to recognize event classes. This subset of object and scene categories will allow us to better transfer the deep object and scene models to the target of event recognition. Second, we propose new frameworks to transfer these deep representations learned on the ImageNet dataset and the PlacesN dataset to event recognition. Due to the smaller size of the training datasets, it would be extremely difficult to train a deep network from scratch for event recognition. Therefore, we propose three different transferring techniques: (N) initialization-based transferring, (N) knowledge-based transferring, and (N) data-based transferring. In the scenario of initialization-based transferring, we simply copy the weights of the convolutional layers from the object or scene models to those of event models and randomly initialize the fully-connected layers. After this, we carefully fine-tune the whole networks with event supervision information from the target dataset. In the latter two scenarios, we design a multi-task learning framework to regularize the fine-tuning procedure of the event networks and improve their generalization ability. For instance, in knowledge-based transfer, we utilize the object and scene models to guide the fine-tuning of event networks, and we leverage the soft outputs of object and scene CNNs as supervision signals to assist with the training of event networks. In the case of data-based transfer, we exploit the large-scale object and scene datasets to regularize the training process of event networks. We propose to jointly train the event networks with object and scene networks under a weight-sharing scheme for common convolutional layers. Finally, based on these learned event CNNs, coined ``OSNE-CNNs'', we propose a simple yet effective event recognition pipeline. Specifically, we design an end-to-end event recognition method without utilizing explicit encoding methods or learning additional classifiers. First, we design a multi-ratio and multi-scale image cropping strategy and transform each image into a set of regions. This multi-ratio and multi-scale strategy is able to deal with scale variations and aspect ratio differences in event images. Then, these image regions are fed into OSNE-CNNs for event recognition. The two-stream OSNE-CNNs can incorporate visual cues for both scenes and objects. Finally, the prediction scores from multiple image regions are averaged to obtain the final recognition result. Based on our proposed event recognition pipeline, we perform experiments on three event recognition datasets: the ChaLearn Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition (WIDER), and the UIUC Sports Event dataset . The experimental results demonstrate that our event recognition method obtains the state-of-the-art performance on all these challenging datasets. \iffalse This paper follows on from our previous two papers at the ChaLearn Looking at People (LAP) workshop, which are the _inline_eq_ and _inline_eq_ winners of the CVPR and ICCV ChaLearn LAP challenges, respectively. We have made substantial extensions over this previous work, which can be summarized as follows: {\bf LUC: THE FOLLOWING SUMMARY IS HIGHLY REPETITIVE AS THESE THINGS HAVE BEEN SAID TWICE BEFORE (ABSTRACT AND INTRO THUS FAR) . IT IS ALSO CONFUSING, AS THESE ELEMENTS WERE SPECIFIED AS THE CONTENT OF THIS PAPER, AND ARE NOW ALSO THE DELTA WITH RESPECT TO EARLIER WORK... SO EVERYTHING IN THIS PAPER IS NEW WHEN COMPARED TO THE EARLIER PAPERS? BUT WHY WOULD THIS ONE BE SAID TO FOLLOW ON FROM THE OTHER TWO THEN?} \fi The remainder of this paper is organized as follows. In Section _ref_, we review work related to ours. Then, we present an empirical study of the correlation of objects and scenes with events in Section _ref_ . Section _ref_ gives the description of our proposed transferring techniques of adapting object and scene CNNs for event recognition. We propose a simple yet effective event recognition pipeline with our learned OSNE-CNNs in Section _ref_ . The experimental evaluation and exploration is described in Section _ref_ . Finally, we discuss our method and offer some conclusions in Section _ref_ .