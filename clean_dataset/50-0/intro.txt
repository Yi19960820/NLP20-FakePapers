Deep neural networks (DNNs) are powerful and flexible models that can extract hierarchical and discriminative features from large amounts of data. Despite their success in many supervised tasks such as image classification _cite_, speech recognition _cite_ and machine translation _cite_, DNNs are data hungry which limits their applications in many domains where abundant annotated data are not available. This motivates us to explore almost infinite amount of unlabeled data for obtaining good representations that generalizes across tasks. Learning from unlabeled data, often named as unsupervised learning, can be divided into four levels: learning from unlabeled images, from unlabeled videos, from virtual environment, or from the real environment directly. In this paper, we focus on unsupervised learning from label-free videos. One of the main challenges of unsupervised learning is to derive good training signals. For supervised learning, we use the difference between the prediction from a DNN and the annotated ground truth as training signal. For unsupervised learning, there are two ways to obtain such signal: one is to design auxiliary tasks such as reconstructing the input image _cite_, predicting the next fewer frames given the first fewer frames in a video _cite_, reordering the shuffled video frames _cite_, generating fake images/videos in a generative adversarial network settings (GAN) _cite_, etc; another is to provide constraints _cite_ that describe the desired structure of the output from a DNN. Our work belongs to the latter one. Inspired by human visual system that can learn invariant representations and structures of objects from temporal experiences _cite_, we design an objective function that constrains the output from a DNN to be temporally consistent meanwhile avoiding degenerated cases. We evaluate our proposed algorithm in both synthetic and natural video settings. Experiments with end-to-end training on unlabeled-videos and applications to head orientation estimation and moving object localization demonstrated the effectiveness of the proposed algorithm. The contributions of this paper are the following: The rest of the paper is organized as follows. We briefly review related work in Section N. The detailed description of the proposed methods is given in Section N followed by experimental results in Section N and conclusion in Section N.