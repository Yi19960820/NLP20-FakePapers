Person re-identification (ReID) aims at identifying pedestrian identities across disjoint camera views. It suffers from various difficulties such as large variations of pose, viewpoint, and illumination conditions. Despite that person ReID tasks have been receiving increasing popularity, it remains a very challenging problem, especially in real-world application scenarios. Recently, many inspiring works~ _cite_ have been proposed to tackle issues such as part misalignment and viewpoint changes. However, despite that these models have achieved remarkable performance on several person ReID benchmarks, two obvious, but as yet, unanswered questions are seldom valued by these approaches: N) does the image resolution discrepancies in the training set affect the performance of person ReID? and N) how to prevent a model from being prone to certain resolution combinations when the training data reflects the natural image distribution partially. As shown in Fig.~ _ref_, the image resolution discrepancy problem is common in both public datasets and real-world applications. We argue that these discrepancies are caused by arbitrarily rescaling training images with different resolutions to a uniform size. The original resolutions of pedestrian image patches are diverse due to three reasons. First, the graphical perspective leads to various sizes of pedestrians in images. Second, configurations of surveillance cameras are different in both public datasets and real-world applications. Some old surveillance cameras can only produce low-resolution images while other modern cameras generate high-resolution images. Third, to the best of our knowledge, almost all deeply-learned ReID models require rescaling image patches to a uniform size in both training and testing. This procedure will inevitably lead to the image resolution discrepancy problem. For a person ReID model, sufficient training data with different resolutions is vital for improving its generalization ability. For each image in the training set, if we get all its antithetical counterparts that have the same content but with different resolutions, it will help a ReID model to gain a better generalization ability. However, there is almost no chance of finding a pair of images in which the image from the low-resolution camera has a higher image resolution than the one from the high-resolution camera. It means that the resolution discrepancies in the actual training set are biased since certain resolution combinations are missing. Previous methods cover up this problem with carefully designed training hyperparameters~ _cite_ or sophisticated image pre-processing method~ _cite_ . Unlike these methods, we propose a generic and straightforward framework called deep antithetical learning that directly tackles the resolution discrepancy problem. The first step is the image quality assessment. Since the resolution changes of training images are mostly caused by manually rescaling images into a uniform size, we adopt the No-reference Image Quality assessment (NR-IQA) ~ _cite_ and measure the image resolution in the frequency domain. In the second step, we generate an antithetical training set in which the resolution of images is antithetical to their counterparts in the original training set. Image counterparts of lower resolutions can be easily generated by randomly downsampling, while approaches for enhancing the image resolution are limited. Generative adversarial networks (GANs) provide a practical approach for that purpose. However, neither CycleGAN~ _cite_ nor SRGAN~ _cite_ has the ability to enhance the image resolution to a specific level. Despite that we can split the original training set into multiple subsets, we cannot guarantee that every image has counterparts in every subset. Therefore, we roughly split the entire training set into two subsets: one with high-resolution (HR) images and another with low-resolution (LR) images. We then generate an antithetical training set in which the resolution of images is antithetical to their counterparts in the original training set. Specifically, for those HR images in the original set, we generate their LR counterparts by downsampling them randomly. And for those LR images in the original set, a GAN-based model is utilized for recovering fine texture details from them. These recovered images, along with the aforementioned manually blurred images, form the antithetical training set. Apart from generating a new training set for better representing the natural image distribution, training the ReID model with proper objective functions is also crucial. We analyze the widely-used identification + verification paradigm~ _cite_ and find that the triplet loss with online hard negative mining (OHM) has a tendency to select training triplets of certain resolution combinations. This selection bias makes the ReID model suffer from resolution discrepancies and severely damages the performance. We address this problem by proposing a novel Contrastive Center Loss (CCL) . The intuition behind is that rather than designing a sophisticated strategy for handling resolution differences between positive image pairs and negative ones, it is much easier to consider positive samples and negative samples separately. During the training procedure, the proposed CCL simultaneously clusters images of same identities and pushes the centers of different clusters away. To summarize, our contribution is three-fold: In conclusion, we present a high-performance person ReID system. Extensive experimental analyses and evaluations are conducted to demonstrate its effectiveness. Without bells and whistles, the proposed approach outperforms previous state-of-the-art methods on three large benchmarks by a large margin.