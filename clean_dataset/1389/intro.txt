\indent \par The use of Deep Learning and deep neural networks has spread in a large variety of Computer Vision applications due to their increasingly effectiveness in solving many difficult visual tasks. Specifically a Convolutional Neural Network, presented in _cite_, is a deep learning model which is used extensively in image recognition. A Convolutional Neural Network (CNN) consists of successive layers, where the network processes the input patterns in different scales. These multiple levels of representation remove the need for complex feature extraction, which transforms the raw data into a feature vector, because a CNN can accept the raw data as input and learn how to extract the important features internally in its first few layers. In addition, these types of neural networks take advantage of the locality of the patterns in an image by using convolutional layers. \par However, neural networks and subsequently CNNs are vulnerable to certain inputs as shown in _cite_, the adversarial examples. These inputs, although are not easily perceived by humans can lead a CNN to produce faulty results. In the case of CNNs that are used in image classification, where the input is an image, we are also referring to adversarial examples as adversarial images. In this case, an original image, which is classified correctly by a CNN, is slightly perturbed to produce the adversarial image. Despite the fact that the adversarial image seems similar to the original image, according to the perception of a human, it is classified into a different category. This means that a user can alter the output of the network by perturbing the input in a way that it is not detected by humans. \par The existence of adversarial examples indicates that existing CNNs, although in some specific applications can achieve near human accuracy, they do not perceive the input in the same way as humans do. As a result, by studying adversarial examples we can improve the models used, in order to create models that are closer to the human perception, which we consider as the ideal solution. In fact, as shown in _cite_, by improving the behavior of a neural network against adversarial examples we can also improve the accuracy of that network for real inputs. Also adversarial examples are a vulnerability that can be abused from a malicious user to influence the behavior of a system that uses a vulnerable neural network. For example a physical world adversarial example _cite_, can alter the perception of a self-driving car that uses cameras to navigate through the urban environment. \par There are two main approaches for combating adversarial examples. The first approach aims at making the neural network more robust against adversarial examples _cite_, _cite_, by changing the network's architecture and the learning procedure. The second approach assumes that the neural network is already trained and tries to detect whether a new input is an adversarial example or it is a real input _cite_, _cite_, _cite_, _cite_, _cite_ . \par In this paper, we focus on the second approach and we propose methods that aim to detect adversarial inputs. Specifically, we propose three different methods for detecting adversarial examples generated for a CNN that performs image classification. After we analyze and compare their performance, we propose different ways of combining their best aspects to develop a more robust approach. The first method is based on the regularization of the feature vectors which are produced by the network. Using the regularized feature vectors we retrain the last layer of the CNN similarly to the adversarial training proposed in _cite_ . We then can detect if a new input is an adversarial example by comparing the output of the original network with the output of the retrained network. The second method creates histograms using the absolute values of the outputs of the network's hidden layers. Then by combining these histograms, this method creates a vector which is used by an SVM classifier to classify the input either as real or as adversarial. For the third method we assume that in a neighborhood of the input space the CNN acts as an affine classifier. Using that assumption we introduce the concept of the residual image, which contains information about the parts of the input pattern that are ignored by the network. This information is then used to perturb the input image in order to detect whether this image is a real or an adversarial input . \par We use these methods and their combination to detect adversarial examples generated for a LeNet _cite_ network trained on the MNIST _cite_ dataset. The combination of the three proposed methods offers some improvements over similar state of the art approaches, on the detection of adversarial examples on the MNIST dataset.