Image compression aims to encode image with less bits, and can provide an effective solution to save storage requirement and transmission bandwidth. Based on whether distortion is allowed in the reconstructed image or not, image compression can be classified into lossy and lossless methods. For both the two categories of approaches, lossless entropy encoding is an important building block for generating the compressed representation. One key issue of entropy encoding is to predict the probability of the current symbol to be encoded. In some earlier approaches, such as JPEG, the frequency of the symbols is directly counted and the classical Huffman coding approach is used to compress the codes. While, recent approaches tend to take benefit from the image context information for better predicting the probability of symbols. In JPEG N, the EBCOT coder~ _cite_ is employed to model the context and approximate the probability, and the Binary Arithmetic Coding-MQ-Coder is utilized to compress the codes. In H.N/AVC, the context-adaptive binary arithmetic coding (CABAC) ~ _cite_ is introduced to model the context of the preceding two encoded symbols for compressing the codes with arithmetic coding. CABAC adopts look-up table (LUT) to predict the probability of the current symbol based on its context information. The PixelCNN and PixelRNN models have validated the effectiveness of deep neural networks (DNNs) for capturing highly complex and long-range dependence between pixels. However, since the un-decoded bits can not be utilized for probability prediction in the decoding process, standard DNNs can not be directly applied for entropy coding in image compression. Toderici et al.~ _cite_ present a recurrent neural network (RNN) based model, where a binary RNN is trained to estimate the probability of bits for better arithmetic coding. However, their model needs to conduct one forward pass to predict each bit, making the RNN based encoder~ _cite_ computationally expensive. Recently, Li et al.~ _cite_ replace the unavailable values in the context cuboid with pre-defined values and train a shallow CNN to predict the probability of current bit. Even though the CNN solution~ _cite_ speeds up the RNN based encoder, it also performs probability prediction independently for the neighboring symbols and remains inefficient. In this paper, we propose a trimmed convolutional network for arithmetic encoding (TCAE) which can greatly speed up the process of arithmetic encoding. Specifically, TCAE introduces a new class of trimmed convolution operations to accelerate the probability prediction step in the entropy coding process. Compared with the standard convolution operations which incorporate all the surrounding information to generate the output, trimmed convolution utilize a binary mask to avoid the adopting of certain input values. Equipped with trimmed convolution kernels, the proposed TCAE approach is able to avoid the involvement of non-encoded symbols for probability prediction. Consequently, a fully convolutional network architecture can be directly adopted for probability prediction, making our TCAE highly efficient for arithmetic encoding. In Fig.~ _ref_ (a) and (c), we illustrate the application of trimmed convolution kernels on the the input layer and hidden feature maps, respectively. For the current bit to be encoded in the input layer (red pixel in Fig.~ _ref_ (a)), it should be excluded from the context for probability prediction. While, for the value in the same location of hidden feature map, as it only conveys the information from the corresponding context area, we should include it in the coding process. We will give more details on mask settings for sophisticated multiple filters and ND data in Section~ _ref_ . Furthermore, to accelerate the decoding speed, we introduce a slope TCAE schedule, which divides the codes of ND code maps into several blocks by requiring that the codes inner one block are independent. Therefore, we can decode the codes in one block simultaneously, while the code blocks still need to be decoded in order. Concretely, we set _inline_eq_ is the coordinate of the ND code map _inline_eq_ . The _inline_eq_-th code block is defined as _inline_eq_ . In this paper, we evaluate our TCAE and slope TCAE on both the image lossless and lossy compression tasks. For the lossless compression task, we adopt the proposed algorithms to compress the gray images, and achieve higher compression ratio than the existing lossless compression methods, such as PNG, JPEG-LS and JPEGN-LS. While, for the lossy compression task, TCAE and slope TCAE is adopted to compress the intermediate codes of the lossy compression system. We take a recently proposed CNN-based system~ _cite_ as an example, and utilize our methods to compress the binary codes and importance map generated by the encoder. Compared with~ _cite_, our TCAE can not only improve the compression performance due to the consideration of large context, but also be significantly faster in terms of encoding speed. Benefited from trimmed convolution, the compression system~ _cite_ with slope TCAE can encode the image in real time, and _inline_eq_ speed up the decoding process. To sum up, the contribution of this work is four-fold: