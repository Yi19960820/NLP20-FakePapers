The success of Convolutional Neural Networks (CNNs) _cite_ is highly tied to the availability of large-scale annotated datasets, \eg, ImageNet _cite_ . However, large-scale datasets with high-quality label annotations are not always available for a new domain, due to the significant time and effort it takes for human experts. There exist several cheap but imperfect surrogates for collecting labeled data, such as crowd-sourcing from non-experts or annotations from the web, especially for images (\eg, extracting tags from the surrounding text or query keywords from search engines) . These approaches provide the possibility to scale the acquisition of training labels, but invariably result in the introduction of some noisy (incorrect) labels. Moreover, even high-quality datasets are likely to have noisy labels, as data labeling can be subjective and error-prone. The presence of noisy labels for training samples may adversely affect representation learning and deteriorate prediction performance _cite_ . Training accurate CNNs against noisy labels is therefore of great practical importance. We will refer to samples whose classes are mislabeled/incorrectly annotated as {\em noisy samples} and denote their labels as {\em noisy labels} . Such noisy labels can fall into two types, {\em closed-set} and {\em open-set} . More specifically, a {\em closed-set noisy label} occurs when a noisy sample possesses a true class that is contained within the set of known classes in the training data. While, an {\em open-set noisy label} occurs when a noisy sample possesses a true class that is not contained within the set of known classes in the training data. The former scenario has been studied in previous work, but the latter one is a new direction we explore in this paper. Figure~ _ref_ provides a pictorial illustration of noisy labels, where we have an image dataset with two classes, jasmine (the plant) and cat (the animal) . The closed-set noisy labels occur when cat and jasmine are mislabeled from one category to the other, but the true labels of these images are still cat or jasmine. The open-set noisy labels occur for those images labeled as cat or jasmine, but their true labels are neither cat nor jasmine, \eg, the zoo map and the cartoon character. Table _ref_ demonstrates all the possible cases on how different samples are labeled in this problem. The leftmost column specifies the true class and the other columns specify the type of label in the dataset. Previous work has addressed the noisy label problem explicitly or implicitly in a closed-set setting, via either loss correction or noise model based clean label inferring _cite_ . However, these methods are vulnerable in the more generic open-set scenario, as loss or label correction may be inaccurate since the true class may not exist in the dataset. Open-set noisy labels are likely to occur for scenarios where data are harvested rapidly, or use approximate labels (\eg, using a search engine query to retrieve images and then labeling the images according to the query keyword that was used) . To the best of our knowledge, how to address the open-set noisy label problem is a new challenge. In this paper, we propose an iterative learning framework that can robustly train CNNs on datasets with open-set noisy labels. Our model works iteratively with: (N) a noisy label detector to iteratively identify noisy labels; (N) a Siamese network for discriminative feature learning, which imposes a representation constraint via contrastive loss to pull away noisy samples from clean samples in the deep representation space; and (N) a reweighting module on the softmax loss to express a relative confidence of clean and noisy labels on the representation learning. A simplified illustration of the proposed framework is presented in Figure~ _ref_ . Our main contributions can be summarized as follows: (N) We identify the open-set noisy label problem as a new challenge for representation learning and prediction. (N) We propose an iterative learning framework to robustly train CNNs in the presence of open-set noisy labels. Our model is not dependent on any assumption of noise. (N) We empirically demonstrate that our model significantly outperforms state-of-the-art noisy label learning models for the open-set setting, and has a comparable or even better performance under the closed-set setting.