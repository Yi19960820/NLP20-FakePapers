are many famous ensembles (e.g., Bagging~ _cite_, Boosting~ _cite_, Stacking~ _cite_, Random Forests~ _cite_ and neural network ensembles~ _cite_) and also many recent ensemble methods~ _cite_, many of which have been widely applied in numerous real-world applications. Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers~ _cite_ . Generally speaking, ensemble of diverse classifiers can allow us to get higher accuracy, which is often not achievable by a single model. Consequently, how to diversely combine classifiers plays an important role and becomes a main topic in ensemble classifier. Given a diversity measure formulation, most conventional diversity-based ensemble methods combine classifiers by calculating and evaluating this fixed measure directly, e.g. _cite_ . Several diversity measures are extensively acknowledged in classifier ensemble, e.g.,,,, and ~ _cite_ . Obviously, the diversity is useful, and an ensemble should combine diverse component classifiers for improving accuracy; in an extreme case, ensembling all same classifiers has no any improvement. However, how to adaptively use diversity and suitably optimize combination is still unclear and challenging in the literature. Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles~ _cite_ . Some researchers even argued that in practice it was not possible to define and use a diversity measure that is clearly linked to the ensemble accuracy~ _cite_ . The point is ``how to use the useful diversity" . Currently, this seems like a ``fast knot" in ensemble learning. From a different view to conventional diversity-based ensembles, firstly, we argue that the diversity (called as adaptive diversity with data) should be strongly correlated to the accuracy, not directly measured on the training data but adaptively on the latent data distribution. Currently, given data samples, all existing diversity measures are calculated on the validation set without considering data noise and measure space, although there are a few related descriptions. For example, Ruta and Gabrys observed that certain diversity measures (e.g., Q statistics) performed well on artificial data, but was inadequate for realistic datasets~ _cite_ . Secondly, we also believe that diversity-based ensembles, i.e., ensembles with both accuracy and (adaptive) diversity, can improve performance of the final classification system. Consequently, we introduce a new term in classifier ensemble, learning to diversify ~, which should learn to adaptively select and combine a subset of classifiers by considering both accuracy and diversity, given a list of available component classifiers. There are two main essential tasks of this learning to diversify strategy: (N) First and obviously, same as diversity-based ensembles, the finally selected classifiers in an ensemble should be more diverse than the general combination strategy, e.g., averaging; (N) second but more importantly, according to different data under various situations, learning to diversify should adaptively evaluate the diversity with the given formulation, i.e., adaptive diversity with data. Specifically, within a linear classifier ensemble framework, we propose a unified learning-to-diversity approach, Learning TO Diversify via Weighted Kernels (LNDWK) . For the first task, we propose to learn classifier weights by optimizing a direct but simple criterion: maximizing the accuracy and the diversity simultaneously. Moreover, we formulate this procedure in a convex minimization problem. For the second task, different from the conventional methods where the diversity is directly calculated on the component classifiers' outputs from the validation samples, we seek adaptive diversity with data in the view of measure space (with kerneling) and data noise (with weighting) . The diversity in our approach is adaptively evaluated with a set of weights (called as kernel weights) on the samples' outputs within a kernel, i.e., the classifiers' outputs are converted into another space with weighted kernels, and the kernel weights are learned automatically according to the contribution of validation samples. Furthermore, we combine these two tasks into a unified framework with a self-training algorithm. In this algorithm, first, given the kernel and weights (kernel weights), the classifier weights for combination are learned by minimizing a convex loss function, which maximizes accuracy and diversity of the ensemble simultaneously. Then, the kernel weights are automatically updated with a dynamically damped learning trick. This two-step optimization is then repeated until convergence or the maximum number of iterations is exceeded. We argue that this diversity measure via weighted kernels is adaptive and robust for noise data in different situations, and thereby classifier ensemble based on learning to diversify can produce a subset of important and diverse classifiers. To some extent, we hopefully arrive the above point: `` to use the useful diversify " with an adaptively learning strategy. Our approach is extensively evaluated and verified on a large set of _inline_eq_ typical UCI classification datasets, and consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen~ _cite_, Regularized Selective Ensemble~ _cite_, and Ensemble Pruning via Semi-Definite Programming~ _cite_ . The rest of the paper is organized as follows. Related work is presented in Section~ _ref_ . Section~ _ref_ describes some notations used in this paper. Our proposed method, Learning TO Diversify via Weighted Kernels (LNDWK), is presented in Section~ _ref_ in detail. Section~ _ref_ shows extensive experimental results on _inline_eq_ UCI benchmark datasets. Finally, some conclusions are drawn in Section~ _ref_ .