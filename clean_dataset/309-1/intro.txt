While large-scale, high-dimensional multimedia big data are pervasive in search engines and social networks, cross-modal retrieval has attracted increasing attention, which enables approximate nearest neighbors (ANN) search across different modalities with computation efficiency and search quality. As relevant data from different modalities (image and text) may endow semantic correlations, it is important to support cross-modal retrieval that returns semantically-relevant results of one modality in response to a query of different modality. A promising solution to the cross-modal retrieval is hashing methods _cite_, which transform high-dimensional data into compact binary codes and generate similar binary codes for similar data. This paper focuses on cross-modal hashing that builds data-dependent hash coding for efficient cross-media retrieval _cite_ . Due to large volumes and the semantic gap _cite_, effective cross-modal hashing remains a challenge. Existing cross-modal hashing methods construct correlation across different modalities in the process of hash function learning and indexes cross-modal data into an isomorphic Hamming space _cite_ . They can be categorized into unsupervised methods and supervised methods. While unsupervised methods are general and can be trained without semantic labels or relevance feedbacks, they are restricted by the semantic gap _cite_ that high-level semantic description of an object differs from low-level feature descriptors. Supervised methods can incorporate semantic labels or relevance feedbacks to mitigate the semantic gap _cite_ and improve the hashing quality, i.e. achieve accurate search with shorter codes. Recently, deep hashing methods _cite_ have shown that both feature representation and hash coding can be learned more effectively using deep neural networks _cite_, which can naturally encode nonlinear hashing functions. Other cross-modal retrieval models via deep learning _cite_ have shown that deep models can capture nonlinear cross-modal correlations more effectively and yielded state-of-the-art results on many benchmarks. However, a crucial disadvantage of these cross-modal deep hashing methods is that the quantization error is not statistically minimized hence the feature representation is not optimally compatible with binary hash coding. Another potential limitation is that they generally do not adopt principled pairwise loss function to link the pairwise Hamming distances with the pairwise similarity labels which is crucial to close the gap between the Hamming distance on binary codes and the metric distance on continuous representations. Therefore, suboptimal representation and hash coding may be produced by existing cross-modal deep hashing methods. This paper presents Correlation Hashing Network (CHN), a hybrid deep architecture for cross-modal hashing. CHN jointly learns good image and text representations tailored to hash coding and formally controls the quantization error, which constitutes four components: (N) an image network with multiple convolution-pooling layers to extract good image representations, and a text network with multiple fully-connected layers to extract good text representations; (N) two hashing layers to generate compact hash codes for each modality; (N) a cosine max-margin loss for capturing cross-modal correlation structure; and (N) a new quantization max-margin loss for controlling the quality of the binarized hash codes. Extensive experiments show that CHN yields state-of-the-art results on standard cross-modal retrieval datasets.