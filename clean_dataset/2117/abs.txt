Recognizing high level semantic concepts and complex events in consumer videos has become an important research topic with great application needs in the past few years. In this work, we apply Deep Convolution Network (DCN), the state-of-the-art static image recognition algorithm, to recognize semantic concepts in unconstrained consumer videos. Our preliminary studies show that the annotation of video corpora is not sufficient to learn robust DCN models. The networks trained directly on the video dataset suffer from significant overfitting. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from weakly labeled images corpus to videos. The image corpus helps to learn important visual patterns for natural images, while these patterns are ignored by the models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. By transfer learning from image to video, we can learn a video recognizer with only Nk videos. Because the image corpus is weakly labeled, the entire learning process requires only Nk annotated instances, which is far less than the million scale image and video datasets used in previous works. The same approach can be applied to other visual recognition tasks where only scarce training data is available. Our extensive experiments on network configurations also lead to better understanding about the effect of various meta-parameters in DCN, which enable better network architecture design for future researches and applications.