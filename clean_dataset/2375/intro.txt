A fundamental requirement of pattern recognition is the ability to ignore irrelevant variations in the input . Most visual recognition problems are thwarted by variations in position, size, pose, lighting, and other viewing conditions that can bring objects from different classes closer, while increasing within-class variability, and the construction of representations that are invariant to these variations remains an active area of research. Recent examples of learned visual representations have proven highly effective for recognition, but a precise understanding of exactly what they represent remains elusive. And although these representations are hypothesized to be invariant to various identity-preserving deformations, apart from a few exceptions, these claims are rarely tested directly . Image synthesis provides a powerful methodology for examining the invariances of arbitrary representations. It has been used to explore and refine texture models, incrementally augmenting the representation with new statistical constraints until images synthesized with matching parameters are indistinguishable to human observers . When applied to deep recognition networks, synthesis has revealed failures in the form of ``adversarial examples": images that appear entirely different to a human observer, and yet are identified by the network as belonging to the same category . In these cases, samples from the equivalence class of images that map to the same representation vector provide a means of verifying or falsifying the hypothesis that the invariances of the representation are also invariances for human observers. But the synthesis test, in which human observers try to discriminate synthesized images, is one-sided: failures (i.e. \visually distinct images) can reveal {\em inappropriate} invariances of a representation, but successes can mask a lack of {\em desired} invariances. Consider the standard case of translation-invariance. The Fourier amplitude spectrum (i.e., the set of magnitudes of Fourier transform coefficients) provides a well-known example of a translation-invariant representation, but it is invariant to far more than translations, and this is immediately revealed by a synthesis test (figure _ref_, top right) . On the other hand, simply representing an image with its raw pixel values (the identity representation) will trivially produce visually perfect synthetic examples (figure _ref_, top center) despite the fact that it has no invariance properties at all. We seek a more general method of evaluation that penalizes a model for discarding too much information (as with synthesis) but also for discarding too little information. Each of these failures can be seen as an inadequacy of the image {\em metric} induced by the representation. Specifically, an image representation deforms the input space, bringing some images closer to each other while spreading others out, and thus inducing a new metric in image space. We can expose properties of this image metric by generating a geodesic sequence of images. Specifically, given an initial and final image, we synthesize a sequence of images that follow a minimal-length path in the response space of the representation. In the absence of any other constraints, this path will be a straight line connecting the representations of the two images; more generally, it will be the straightest path connecting the two points. In the case where the two images differ by a simple transformation (e.g. \a translation, figure _ref_, left column) that is not linearized by the representation (i.e. mapped to the straightest path connecting the two representations), the geodesic will differ from the original transformation connecting the images (figure _ref_, middle column) . Similarly, if the representation is invariant to many transformations, the geodesic may correspond to a path that uses a mixture of transformations, and thus differ from the ground truth path (figure _ref_, right column) . As a result, by visualizing whether a representation has linearized the action of various deformations, representational geodesics can reveal both excessive and insufficient invariance in an image model. We develop an algorithm for synthesizing geodesic sequences for a representation, and use it to examine whether learned representations linearize various real-world transformations such as translation, rotation, and dilation. We find that a current state-of-the-art object recognition network fails to linearize these basic transformations. However, these failures point to a deficiency in the representation, leading to a simple way of improving it. We show that the improved representation is able to linearize a range of parametric transformations as well as generic distortions found in natural image sequences.