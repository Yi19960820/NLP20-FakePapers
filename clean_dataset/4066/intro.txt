Deep learning methods~ _cite_ have demonstrated superior results in many computer vision tasks, including image recognition even on the very-large-scale benchmark such as ImageNet~ _cite_ . Most deep learning methods exploit the strong representation capacity of convolutional neural networks (CNNs) and are often supervised by the softmax loss (SoftMax) . This loss function, however, does not explicitly drive CNNs to learn discriminative features. One recent trend to make better use of the strong representation capacity of CNNs is to explicitly encode the discriminant into the model. For example, Hadsell et al. proposed a Siamese network with a contrastive loss for dimensionality reduction~ _cite_, which learned a mapping such that similar points in the input space were mapped to nearby points in the low dimensional manifold. Schroff et al. extended the contrastive loss to a triplet loss by introducing a third anchor point which was sampled from the training set, and minimized the distance between an anchor and a positive of the same identity and meanwhile maximized the distance between the anchor and a negative of different identity~ _cite_ . However, training samples needed to be carefully selected, as the number of training sample grows at the rate of _inline_eq_, where _inline_eq_ is the number of training samples. In this paper, we propose a novel approach to train deep CNNs by enforcing unequivocal intra-class compactness and inter-class separability. In contrast to prior approaches, ours explicitly encourages CNNs to learn discriminative features without relying on sample pairs or triplets, and can be efficiently optimized with batch stochastic gradient descent (SGD) . This is achieved by introducing to our objective function the notion of anchors, based on which a Nearest Class Mean (NCM) loss is raised. Here, anchors refer to predefined vectors that are fixed during training. The anchors are regarded as class centers, each of which is assigned to a particular class. We foster the learned features to gather around the anchors of the same label, and meanwhile expect the anchors to be as separate as possible. Specifically, we derive anchors using the following two principals: ~~ Our objective function, which will be discussed in detail in Section N, involves computing the distance measure between the two points, for which the only requirement is it being differentiable. We consider two common and simple distances, Euclidean and cosine distances, which further leads to two novel loss functions. One toy example is shown in Figure~ _ref_, highlighting the effect of anchor-based cosine and Euclidean distances on the random deep features. Note that, the proposed method is not limited to the Euclidean or cosine distance and any differentiable distance metric functions can be readily adopted. The approach of Wen et al.~ _cite_ also relies on center-based Euclidean distance to learn discriminant features. Unlike our method, it performed updates of class centers based on mini-batch. However, the number of training samples for each class in a training batch is random and thus the centers of any two different class may be very close to each other or have very large difference in norm. These two factors result in large perturbations on the class centers and further lead to slow convergence. Furthermore, it alone cannot learn meaningful features due to the trivial solutions like all deep features being mapped to the origin. By contrast, our method fixes the anchors and avoids such perturbations, yielding discriminative features as well as promising results. Also, our method requires no tunable hyper-parameters, making it easy to be deployed in practice. The approach of Liu et al.~ _cite_, on the other hand, introduces a large-margin softmax loss (L-Softmax), which requires the angle between different classes to exceed a margin. It requires a huge amount of hyper-parameters that are difficult to tune. Also, its backward propagation becomes very demanding when the angle margin is large as the triangle formula must be expanded. Our method however directly constrains the angle between the deep features and anchors, which avoids the tricky strategy of updating the deep features to accelerate convergence. Further, our anchors are selected from the deep feature space to ensure minimal inter-class angular margin between classes. Our contribution is therefore, introducing the novel anchor-based loss functions into the deep neural works, which does not require sample pairs or triplets and can be efficiently optimized with batch SGD. It is free of tunable parameters and thus very easy to be deployed. Our method leads to very promising results on several standard benchmarks, demonstrating the effectiveness of the proposed method.