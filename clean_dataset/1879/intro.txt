Pedestrian detection has attracted broad attentions _cite_ . This problem is challenging because of large variation and confusion in human body and background scene, as shown in Fig. _ref_ (a), where the positive and hard negative patches have large ambiguity. Current methods for pedestrian detection can be generally grouped into two categories, the models based on hand-crafted features _cite_ and deep models _cite_ . In the first category, conventional methods extracted Haar _cite_, HOG _cite_, or HOG-LBP _cite_ from images to train SVM _cite_ or boosting classifiers _cite_ . The learned weights of the classifier (\eg SVM) can be considered as a global template of the entire human body. To account for more complex pose, the hierarchical deformable part models (DPM) _cite_ learned a mixture of local templates for each body part. Although they are sufficient to certain pose changes, the feature representations and the classifiers cannot be jointly optimized to improve performance. In the second category, deep neural networks achieved promising results _cite_, owing to their capacity to learn middle-level representation. For example, Ouyang \etal _cite_ learned features by designing specific hidden layers for the Convolutional Neural Network (CNN), such that features, deformable parts, and pedestrian classification can be jointly optimized. However, previous deep models treated pedestrian detection as a single binary classification task, they can mainly learn middle-level features, which are not able to capture rich pedestrian variations, as shown in Fig. _ref_ (a) . To learn high-level representations, this work jointly optimizes pedestrian detection with auxiliary semantic tasks, including pedestrian attributes (\eg `backpack', `gender', and `views') and scene attributes (\eg `vehicle', `tree', and `vertical') . To understand how this work, we provide an example in Fig. _ref_ . If only a single detector is used to classify all the positive and negative samples in Fig. _ref_ (a), it is difficult to handle complex pedestrian variations. Therefore, the mixture models of multiple views were developed in Fig. _ref_ (b), \ie pedestrian images in different views are handled by different detectors. If views are treated as one type of semantic tasks, learning pedestrian representation by multiple attributes with deep models actually extends this idea to extreme. As shown in Fig. _ref_ (c), more supervised information enriches the learned features to account for combinatorial more pedestrian variations. The samples with similar configurations of attributes can be grouped and separated in the high-level feature space. Specifically, given a pedestrian dataset (denoted by _inline_eq_), the positive image patches are manually labeled with several pedestrian attributes, which are suggested to be valuable for surveillance analysis _cite_ . However, as the number of negatives is significantly larger than the number of positives, we transfer scene attributes information from existing background scene segmentation databases (each one is denoted by _inline_eq_) to the pedestrian dataset, other than annotating them manually. A novel task-assistant CNN (TA-CNN) is proposed to jointly learn multiple tasks using multiple data sources. As different _inline_eq_ 's may have different data distributions, to reduce these discrepancies, we transfer two types of scene attributes that are carefully chosen, comprising the shared attributes that appear across all the _inline_eq_ 's and the unshared attributes that appear in only one of them. The former one facilitates the learning of shared representation among _inline_eq_ 's, whilst the latter one increases diversity of attribute. Furthermore, to reduce gaps between _inline_eq_ and _inline_eq_ 's, we first project each sample in _inline_eq_ 's to a structural space of _inline_eq_ and then the projected values are employed as input to train TA-CNN. Learning TA-CNN is formulated as minimizing a weighted multivariate cross-entropy loss, where both the importance coefficients of tasks and the network parameters can be iteratively solved via stochastic gradient descent _cite_ . This work has the following main contributions . (N) To our knowledge, this is the first attempt to learn high-level representation for pedestrian detection by jointly optimizing it with semantic attributes, including pedestrian attributes and scene attributes. The scene attributes can be transferred from existing scene datasets without annotating manually. (N) These multiple tasks from multiple sources are trained using a single task-assistant CNN (TA-CNN), which is carefully designed to bridge the gaps between different datasets. A weighted multivariate cross-entropy loss is proposed to learn TA-CNN, by iterating among two steps, updating network parameters with tasks' weights fixed and updating weights with network parameters fixed. (N) We systematically investigate the effectiveness of attributes in pedestrian detection. Extensive experiments on both challenging Caltech _cite_ and ETH _cite_ datasets demonstrate that TA-CNN outperforms state-of-the-art methods. It reduces miss rates of existing deep models on these datasets by _inline_eq_ and _inline_eq_ percent, respectively. We review recent works in two aspects. Models based on Hand-Crafted Features The hand-crafted features, such as HOG, LBP, and channel features, achieved great success in pedestrian detection. For example, Wang \etal _cite_ utilized the LBP + HOG features to deal with partial occlusion of pedestrian. Chen \etal _cite_ modeled the context information in a multi-order manner. The deformable part models _cite_ learned mixture of local templates to account for view and pose variations. Moreover, Doll r \etal proposed Integral Channel Features (ICF) _cite_ and Aggregated Channel Features (ACF) _cite_, both of which consist of gradient histogram, gradients, and LUV, and can be efficiently extracted. Benenson \etal _cite_ combined channel features and depth information. However, the representation of hand-crafted features cannot be optimized for pedestrian detection. They are not able to capture large variations, as shown in Fig. _ref_ (a) and (b) . Deep Models Deep learning methods can learn features from raw pixels to improve the performance of pedestrian detection. For example, ConvNet _cite_ employed convolutional sparse coding to unsupervised pre-train CNN for pedestrian detection. Ouyang \etal _cite_ jointly learned features and the visibility of different body parts to handle occlusion. The JointDeep model _cite_ designed a deformation hidden layer for CNN to model mixture poses information. Unlike the previous deep models that formulated pedestrian detection as a single binary classification task, TA-CNN jointly optimizes pedestrian detection with related semantic tasks, and the learned features are more robust to large variations, as shown in Fig. _ref_ (c) and (d) .