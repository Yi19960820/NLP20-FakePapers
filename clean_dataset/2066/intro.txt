Estimating dense optical flow is one of the longstanding problems in computer vision, with a variety of applications. Though numerous approaches have been proposed over the past decades, \eg~ _cite_, realistic benchmarks such as MPI Sintel _cite_ or KITTI _cite_ continue to challenge traditional energy minimization approaches. Motivated by the recent successes of end-to-end deep learning, convolutional neural networks (CNNs) have been suggested for addressing these challenges, and recently started to outperform many traditional flow methods in public benchmarks _cite_ . Still, most CNN-based methods rely on the availability of a large amount of data with ground truth optical flow for supervised learning. Due to the difficulty of obtaining ground truth in real scenes, such networks are trained on synthetically generated images, for which dense ground truth is easier to obtain in large amounts _cite_ . However, because of the intrinsic differences between synthetic and real imagery and the limited variability of synthetic datasets, the generalization to real scenes remains challenging. We posit that for realizing the potential of deep learning in optical flow, addressing this issue will be crucial. In order to cope with the lack of labeled real-world training data, recent work _cite_ has proposed optical flow networks based on unsupervised learning. They sidestep the need for ground truth motion as only image sequences are required for training. Though unsupervised learning seems to be a promising direction for breaking the dependency on synthetic data, one may argue that these approaches fall short of the expectations so far, as they neither match nor surpass the accuracy of supervised methods despite the domain mismatch between training and test time. In this paper, we introduce an end-to-end unsupervised approach that demonstrates the effectiveness of unsupervised learning for optical flow. Building on recent optical flow CNNs _cite_, we replace the supervision from synthetic data by an unsupervised photometric reconstruction loss similar to _cite_ . We compute bidirectional optical flow (\ie, both in forward and backward direction, see Fig.~ _ref_) by performing a second pass with the two input images exchanged. Then, we design a loss function leveraging bidirectional flow to explicitly reason about occlusion _cite_ and make use of the census transform to increase robustness on real images. Through a comprehensive ablation study, we validate the design choices behind our unsupervised loss. On the challenging KITTI benchmarks _cite_, our unsupervised model outperforms previous unsupervised deep networks by a very large margin. Perhaps more surprisingly, it also surpasses architecturally similar supervised approaches trained exclusively on synthetic data. Similar observations hold, for example, on the Middlebury dataset _cite_ . After unsupervised training on a large amount of realistic domain data, we can optionally make use of sparse ground truth (if available) to refine our estimates in areas that are inherently difficult to penalize in an unsupervised way, such as at motion boundaries. Our fine-tuned model achieves leading accuracy on KITTI, thus demonstrating that our unsupervised learning approach also enables pre-training for supervised methods on domains with limited amounts of ground truth data. We thus present a step toward eliminating the need for careful engineering of synthetic datasets, as our networks can be trained on other domains with the expectation of achieving competitive accuracy.