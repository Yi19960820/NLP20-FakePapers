Deep convolutional neural networks have already achieved tremendous success on a variety of computer vision tasks such as image classification _cite_, object detection _cite_, segmentation _cite_, video analysis _cite_, human pose estimation _cite_ among many others. The performance on these different tasks are dramatically boosted by sophisticated neural network structures such as AlexNet _cite_, NIN (Network In Network) _cite_, VGG-Net _cite_, Inception Network _cite_, and ResNet _cite_ . It is clear that the networks are going deeper and deeper from AlexNet to ResNets. On the other hand, due to the need in mobile/embedded applications, there is a new trend of going smaller while retaining the performance of large and deep CNNs. While Inception Nets and ResNets have tried to reduce the model size by reducing the size of convolution kernels, using _inline_eq_ convolutions and trimming the fully connected layers, they are still too large to meet the demanding requirement for mobile and embedded devices (e.g., FPGA) . For example, ResNet-N has NMB and GoogLeNet has NMB. However, FPGAs often have less than NMB of on-chip memory and no off-chip memory or storage~ _cite_ . To further reduce the model size, various compressing techniques have been introduced to deep CNNs, including parameter quantization, binarization, sharing, pruning, hashing, Huffman coding, etc~ _cite_ . There also emerge few studies recently attempting to design small and compact networks, including the SqueezeNets~ _cite_ and the MobileNets~ _cite_ . Nevertheless, the performance drop of smaller networks is still a critical concern for many designs. For example, the authors of~ _cite_ have designed an extremely small network with less than N and achieved N \% top-N accuracy on ImageNet, which is considerably less than state-of-the-art results of deep CNNs (e.g., N \% of GoogLeNet according to our implementation) . In this paper, we address this concern by proposing several new techniques in the two aforementioned directions for reducing the model size. {\bf First}, we consider parameter binarization-a simple and effective method for reducing the model size. While many previous works try to quantize or binarize all weights in deep CNNs, we propose a novel treatment of _inline_eq_ kernels and _inline_eq_ kernels (e.g., _inline_eq_) . In particular, we only binarize _inline_eq_ convolutional kernels (with _inline_eq_) . This design is motivated by the difference between _inline_eq_ convolutions and _inline_eq_ convolutions and the communityâ€™s prior knowledge about them. Unlike _inline_eq_ convolutions that explicitly extract features in a spatial manner, _inline_eq_ convolutions serve as data projection and transformation. In this sense, _inline_eq_ convolutions need to preserve the information as much as possible and _inline_eq_ convolutions is only required to extract abstract patterns from images. In addition, many works in computer vision have used binary convolutions to extracted features from images~ _cite_, while sparse projection has been reported with performance drop compared with dense projection~ _cite_ . The different treatments of _inline_eq_ and _inline_eq_ kernels also has several benefits in terms of computation: (i) _inline_eq_ convolutions using floating points is cheaper and simpler than _inline_eq_ convolutions; (ii) this splitting is very suitable for FPGAs where logic blocks can efficiently handle the binarized convolutions and DSP units can handle the _inline_eq_ convolutions. {\bf Second}, we propose a simple new design of small networks by stacking up several layers of a novel module, which is built on a new block codenamed pattern residual block. The idea of the pattern residual block is to add transformed feature maps generated by _inline_eq_ convolutions to the pattern feature maps generated by _inline_eq_ convolutions, which resembles but generalizes the skip connection in ResNets. The new pattern residual block is well suited to the design of small networks for increasing the model capacity and more importantly to the binarized pattern networks for offsetting the effect of pattern binarization. Using N, our designed small network (termed as SEP-Net) achieves _inline_eq_ top-N accuracy, beating that of the SqueezeNet (N, N \%) and the MobileNet (N, N \%) with simlar sizes. Leveraging our pattern binarization, we reduce our model size to N while maintaining N \% top-N accuracy. By further quantizing _inline_eq_ filters using _inline_eq_ bits, we achieve N \% top-N accuracy with a model size N