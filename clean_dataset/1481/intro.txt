Recently, deep convolutional neural networks (CNNs) have been an important tool that achieves state-of-the-art performances in many computer vision tasks _cite_ . Its goal is to build a model to address a target problem with a sequence of convolutional layers that are developing from low-level features to more abstract representations. Deep networks can also learn robust representations that are suitable for other task _cite_ . Deep Distance Metric Learning (DML) approaches explore ways to construct such representations that maintain better similarity/distance measurement for, \eg, verification, retrieval or clustering tasks. While supervision of traditional objective functions (\eg Softmax Loss) yield successful results, comparative loss functions (i.e. Triplet Loss) are shown to be more suitable for semi-supervised deep DML tasks _cite_ . Beside sample-based supervision which processes each sample individually, one can benefit from the captured information by considering a set of images as a unified entity. An image set is a collection of instances of the same object/person from varying viewpoints, illuminations, poses and exhibits different characteristics. A set contains richer information of the target than a single image and is potentially more useful for problems like object or scene classification, face recognition and action analysis. As the authors of _cite_ illustrated, set-based supervision can learn discriminative features rather than just separable features like sample-based approaches would learn. This paper makes the following contributions: The rest of the paper is organized as follows: In Section N, we provide an overview of the related work about sample-, set-based deep metric learning for face recognition. Section N describes existing and new set-based loss functions and other strategies. In Section N, we provide some more information on the set-up and techniques used in the experiments. We then present and discuss some experimental results. Finally, we draw conclusion and elaborate on future works in Section N.