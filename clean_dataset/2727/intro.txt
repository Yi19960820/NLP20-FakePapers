Pedestrian detection is an active research area in computer vision and has rapidly progressed through the recent decade. There are many benchmark pedestrian detection datasets for learning and evaluation _cite_ . One common setting in these datasets is that the camera's y-axis is roughly aligned to the direction of gravity. This means that pedestrians are captured in the vertical direction because pedestrians usually stand upright on the ground. This ``upright assumption'' in benchmark datasets distinguishes pedestrians from many objects in the scene. Much work has been devoted to designing features _cite_ or model architectures _cite_ to learn the appearance of upright pedestrians. However, this upright assumption may not always be true in real-world situations where the camera orientation can be highly dynamic. For example, when recording a video with a mobile phone camera, the angle of the camera can vary wildly as one walks or runs. For cameras installed on construction vehicles, the upright assumption is easily invalidated when recording video over rough terrain. In both of these examples, the projection of pedestrians in the image can be at extreme angles of rotation. Detecting pedestrians in such situations is difficult with current state-of-the-art models. One straightforward way to achieve rotational robustness for pedestrian detection is to simply increase the size of the training data to include more instances of pedestrians imaged at an angle. When new data is not available, one can also augment the data _cite_ by creating new training examples from rotated images. While such a brute-force approach can certainly lead to improvements, simply creating more data does not necessarily address the fundamental problem of understanding and modeling image rotations. An alternative solution is to attempt to infer the rotational distortion of the image and to remove the effect of that distortion prior to detection within a unified rotation-invariant detection network. However, estimating rotational changes in an image is difficult with the current paradigm of convolutional feature extraction because they are based on a rectangular spatial decomposition of the image. In other words, rotational changes in image content can produce very different feature responses in the upper convolutional layers of a convolutional neural network (CNN) . In order to facilitate a smoother change in convolutional feature responses due to image rotation, we propose the use of a novel Global Polar Pooling (GP-Pooling) operator which converts rectangular convolutional feature responses to a polar grid system. Using this polar coordinate system, rotations of the input images only result in translational shifts of the polar features maps, making it easier for higher level convolution layers to model image rotation. In this way, our proposed GP-Pooling operator gives a deep neural network the ability to encode image rotations more effectively. In order to obtain rotational invariance during detection we propose a rotational rectification network (RNN) that can be flexibly inserted into an intermediate layer of a general detection network. The RNN uses a CNN with a GP-Pooling layer to estimate the rotation angle present in the images. Then the estimated rotation _inline_eq_ is passed to a spatial transformer network to undistort the image features. An overview of the overall network architecture is illustrated in Figure _ref_ . We show that after removing the effect of rotation inside a network, the general detector can be easily adapted to work on pedestrians imaged at arbitrary rotation angles. The main contributions of our work are summarized as follows: (N) we propose a Global Polar Pooling (GP-Pooling) operator, which can be used to encode the radial distribution of features within a general CNN architecture and (N) we propose a rotational rectification network (RNN) that can be inserted into a wide range of CNN-based detectors to achieve rotational invariance.