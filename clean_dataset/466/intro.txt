The availability of large labeled image datasets~ _cite_ has been one of the key factors for advances in recognition. These datasets, which have been largely curated from internet images, have not only helped boost performance~ _cite_, but have also fostered the development of new techniques~ _cite_ . However, compared to images, videos seem like a more natural source of training data because of the additional temporal continuity they offer for both learning and labeling. So ideally we should have large labeled internet video datasets. In general, the human effort required for labeling these vision datasets is huge, e.g., ImageNet~ _cite_ required N man-years to label bounding boxes in the N million images harvested from the internet. Consider the scale of internet videos--YouTube has N hours of video (N million frames) uploaded every minute. It seems unlikely that human per-image labeling will scale to this amount of data. Given this scale of data and the associated annotation problems~ _cite_, which are more pronounced in videos, it is no surprise that richly annotated large video recognition datasets are hard to find. In fact, the available video datasets~ _cite_ lack the kind of annotations offered by benchmark image datasets~ _cite_ . One way to tackle the labeling problem is using semi-supervised learning (SSL) . Starting with only a few annotated examples, the algorithm can label more examples automatically. However, a major challenge for any kind of SSL technique is to constrain the learning process to avoid semantic drift, i.e., added noisy samples cause the learner to drift away from the true concept. Recent work~ _cite_ has shown ways to constrain this learning process for images. In this paper, we present an approach to constrain the semi-supervised learning process~ _cite_ in videos. Our technique constrains the SSL process by using-appearance, motion, temporal etc., in video data and automatically learns . Intuitively, algorithms dealing with videos should use appearance and temporal cues using detection and tracking, respectively. One would expect a simple combination of detection and tracking to constitute a semi-supervised framework that would prevent drift since both of these processes would ideally cancel each others' errors. However, as we show in our experiments (Sec.~ _ref_), a na ve combination of these two techniques performs poorly. In the long run, the errors in both detection and tracking are amplified in a coupled system. We can also consider pure detection approaches or pure tracking approaches for this problem. However, pure detection ignores temporal information while pure tracking tends to stray away over a long duration. We present a scalable framework that discovers objects in video using SSL (see Figure~ _ref_) . It tackles the challenging problem of localizing new object instances in long videos starting from only a few labeled examples. In addition, we present our algorithm in a realistic setting of ``sparse labels''~ _cite_, i.e., in the few initial ``labeled'' frames, not all objects are annotated. This setting relaxes the assumption that in a given frame, all object instances have been exhaustively annotated. It implies that we do not know if any unannotated region in the frame belongs to the object category or the background, and thus cannot use any region from our input as negative data. While much of the past work has ignored this type of sparse labeling (and), we show ways to overcome this handicap. Figure~ _ref_ presents an overview of our algorithm. Our proposed algorithm is different from the rich body of work on tracking-by-detection. Firstly, we do not attempt to solve the problem of . Our framework does not try to identify whether it has seen a particular instance before. Secondly, since it works in the regime of, it does not assume that negative data can be sampled from around the current box. Thirdly, we limit expensive computation to a subset of the input frames to scale to a million frames. \par \noindent Contributions: We present a semi-supervised learning framework that in videos. Starting from few objects, it iteratively labels new and useful training examples in the videos. Our key contributions are: N) We tackle the SSL problem for discovering multiple objects in sparsely labeled videos; N) We present an approach to constrain SSL by combining multiple weak cues in videos and exploiting decorrelated errors by modeling data in multiple feature spaces. We demonstrate its effectiveness as compared to traditional tracking-by-detection approaches; N) Given the redundancy in video data, we need a method that can automatically determine the relevance of training examples to the target detection task. We present a way to include in each iteration of the SSL process, leading to a scalable algorithm.