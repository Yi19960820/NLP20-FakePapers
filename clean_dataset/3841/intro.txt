Humans are able to recover the occlusion relationships of objects from single images. This has long been recognized as an important ability for scene understanding and perception~ _cite_ . As shown on the left of Fig.~ _ref_, we can use occlusion relationships to deduce that the person is holding a dog, because the person's hand occludes the dog and the dog occludes the person's body. Electrophysiological~ _cite_ and fMRI~ _cite_ studies suggest that occlusion relationships are detected as early as visual area VN. Biological studies~ _cite_ also suggest that occlusion detection can require feedback from higher level cortical regions, indicating that long-range context and semantic-level knowledge may be needed. Psychophysical studies show that there are many cues for occlusion including edge convexity~ _cite_, edge-junctions, intensity gradients, and texture~ _cite_ . Computer vision researchers have also used similar cues for estimating occlusion relations. A standard strategy is to apply machine learning techniques to combine cues like convexity, triple-points, geometric context, image features like HOG, and spectral features, e.g.~ _cite_ . These methods, however, mostly rely on hand-crafted features and have only been trained on the small occlusion datasets currently available. But in recent years, fully convolutional deep convolutional neural networks (FCN) ~ _cite_ that exploit local and non-local cues, and trained on large datasets, have been very successful for related visual tasks such as edge detection~ _cite_ and semantic segmentation~ _cite_ . In addition, visualization of deep networks~ _cite_ show that they can also capture and exploit the types of visual cues needed to estimate occlusion relations. This motivates us to apply deep networks to estimate occlusion relationships, which requires constructing a large annotated occlusion dataset. This also requires making design choices such as how to represent occlusion relations and what type of deep network architecture is best able to capture the local and non-local cues required. We represent occlusion relations by a per-pixel representation with two variables: (i) a binary edge variable to indicate if a pixel in on a boundary, and (ii) a continuous-valued occlusion orientation variable (at each edge pixel) in the tangent direction of the edge whose direction indicates the occlusion relationship using the left rule (i.e. the region to the left of the edge is in front of the region to the right) . Our DOC network architecture is based on recent fully convolutional networks~ _cite_ and is multi-scale so that it can take into account local and non-local image cues. More specifically, we design two versions of DOC based on~ _cite_ and~ _cite_ respectively. To construct our dataset, we select PASCAL VOC images~ _cite_ where many of the object boundaries have already been annotated~ _cite_ . This simplifies our annotation task since we only have to label the occlusion orientation variable specifying border ownership. Our Pascal Instance Occlusion Dataset (PIOD) consists of N, N images and is two orders of magnitude larger than existing ones such as the BSDS border ownership~ _cite_ (N images) and GeoContext~ _cite_ (N images) . We note that the NYU depth dataset~ _cite_ (N indoor images) can also be used to test occlusion relations, but restricted to indoor images. This paper makes two main contributions: (N) We design a new representation and corresponding loss for FCN architecture showing that it performs well and is computationally efficient (N) . (N) We create a large occlusion boundary dataset over the PASCAL VOC images, which is a new resource for studying occlusion. We will release our models, code and dataset.