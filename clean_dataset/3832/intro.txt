Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet _cite_, VGG _cite_, Inception _cite_ to Residual _cite_ networks, corresponding to improvements in many image recognition tasks. The superiority of deep networks has been spotted in several works in the recent years _cite_ . However, training deep neural networks has several difficulties, including exploding/vanishing gradients and degradation. Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies _cite_, better optimizers _cite_, skip connections _cite_, knowledge transfer _cite_ and layer-wise training _cite_ . The latest residual networks _cite_ had a large success winning ImageNet and COCO N competition and achieving state-of-the-art in several benchmarks, including object classification on ImageNet and CIFAR, object detection and segmentation on PASCAL VOC and MS COCO. Compared to Inception architectures they show better generalization, meaning the features can be utilized in transfer learning with better efficiency. Also, follow-up work showed that residual links speed up convergence of deep networks _cite_ . Recent follow-up work explored the order of activations in residual networks, presenting identity mappings in residual blocks _cite_ and improving training of very deep networks. Successful training of very deep networks was also shown to be possible through the use of highway networks _cite_, which is an architecture that had been proposed prior to residual networks. The essential difference between residual and highway networks is that in the latter residual links are gated and weights of these gates are learned. Therefore, up to this point, the study of residual networks has focused mainly on the order of activations inside a ResNet block and the depth of residual networks. In this work we attempt to conduct an experimental study that goes beyond the above points. By doing so, our goal is to explore a much richer set of network architectures of ResNet blocks and thoroughly examine how several other different aspects besides the order of activations affect performance. As we explain below, such an exploration of architectures has led to new interesting findings with great practical importance concerning residual networks. Width vs depth in residual networks . The problem of shallow vs deep networks has been in discussion for a long time in machine learning _cite_ with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits. The authors of residual networks tried to make them as thin as possible in favor of increasing their depth and having less parameters, and even introduced a <<bottleneck>> block which makes ResNet blocks even thinner. We note, however, that the residual block with identity mapping that allows to train very deep networks is at the same time a weakness of residual networks. As gradient flows through the network there is nothing to force it to go through residual block weights and it can avoid learning anything during training, so it is possible that there is either only a few blocks that learn useful representations, or many blocks share very little information with small contribution to the final goal. This problem was formulated as diminishing feature reuse in _cite_ . The authors of _cite_ tried to address this problem with the idea of randomly disabling residual blocks during training. This method can be viewed as a special case of dropout _cite_, where each residual block has an identity scalar weight on which dropout is applied. The effectiveness of this approach proves the hypothesis above. Motivated by the above observation, our work builds on top of _cite_ and tries to answer the question of how wide deep residual networks should be and address the problem of training. In this context, we show that the widening of ResNet blocks (if done properly) provides a much more effective way of improving performance of residual networks compared to increasing their depth. In particular, we present wider deep residual networks that significantly improve over _cite_, having N times less layers and being more than N times faster. We call the resulting network architectures . For instance, our wide N-layer deep network has the same accuracy as a N-layer thin deep network and a comparable number of parameters, although being several times faster to train. This type of experiments thus seem to indicate that the main power of deep residual networks is in residual blocks, and that the effect of depth is supplementary. We note that one can train even better wide residual networks that have twice as many parameters (and more), which suggests that to further improve performance by increasing depth of thin networks one needs to add thousands of layers in this case. Use of dropout in ResNet blocks. Dropout was first introduced in _cite_ and then was adopted by many successful architectures as _cite_ etc. It was mostly applied on top layers that had a large number of parameters to prevent feature coadaptation and overfitting. It was then mainly substituted by batch normalization _cite_ which was introduced as a technique to reduce internal covariate shift in neural network activations by normalizing them to have specific distribution. It also works as a regularizer and the authors experimentally showed that a network with batch normalization achieves better accuracy than a network with dropout. In our case, as widening of residual blocks results in an increase of the number of parameters, we studied the effect of dropout to regularize training and prevent overfitting. Previously, dropout in residual networks was studied in _cite_ with dropout being inserted in the identity part of the block, and the authors showed negative effects of that. Instead, we argue here that dropout should be inserted between convolutional layers. Experimental results on wide residual networks show that this leads to consistent gains, yielding even new state-of-the-art results (\eg, N-layer-deep wide residual network with dropout achieves N \% error on SVHN) . In summary, the contributions of this work are as follows: