Before they can speak or understand language, babies have a grasp of some natural phenomena. They realize that if they drop the spoon it will fall to the ground (and their parent will pick it up) . As they grow older, they develop understanding of more complex notions like object constancy and time. Children acquire much of this knowledge by observing and interacting with the world. In this paper we seek to learn computational models of how the world works through observation. Specifically, we learn models for natural transformations from video. To enable this learning, we collect time-lapse videos demonstrating four different natural state transformations: melting, blooming, baking, and rotting. Several of these transformations are applicable to a variety of objects. For example, butter, ice cream, and snow melt, bread and pizzas bake, and many different objects rot. We train models for each transformation--irrespective of the object undergoing the transformation--under the assumption that these transformations have shared underlying physical properties that can be learned. To model transformations, we train deep networks to generate depictions of the future state of objects. We explore several different generation tasks for modeling natural transformations. The first task is to generate the future state depiction of an object from a single image of the object (Sec~ _ref_) . Here the input is a frame depicting an object at time t, and output is a generated depiction of the object at time t + k, where k is specified as a conditional label input to the network. For this task we explore two auto-encoder based architectures. The first architecture is a baseline algorithm built from a standard auto-encoder framework. The second architecture is a generative adversarial network where in addition to the baseline auto-encoder, a discriminator network is added to encourage more realistic outputs. For our second and third future prediction tasks, we introduce different ways of encoding time in the generation process. In our two-stack model (Sec~ _ref_) the input is two images of an object at time t and t + m and the model learns to generate a future image according to the implicit time gap between the input images (ie generate a prediction of the object at time t + Nm) . These models are trained on images with varying time gaps. Finally, in our last prediction task, our goal is to recursively generate the future states of an object given a single input image of the object. For this task, we use a recurrent neural network to recursively generate future depictions (Sec~ _ref_) . For each of the described future generation tasks, we also explore the effectiveness of pre-training on a large set of images, followed by fine-tuning on time-lapse data for improving performance. Future prediction has been explored in previous works for generating the next frame or next few frames of a video _cite_ . Our focus, in comparison, is to model general natural object transformations and to model future prediction at a longer time scale (minutes to days) than previous approaches. We evaluate the generated results of each model both quantitatively and qualitatively under a variety of different training scenarios (Sec~ _ref_) . In addition, we perform human evaluations of model variations and image retrieval experiments. Finally, to help understand what these models have learned, we also visualize common motion patterns of the learned transformations. These results are discussed in Sec~ _ref_ . The innovations introduced by our paper include: N) A new problem of modeling natural object transformations with deep networks, N) A new dataset of N time-lapse videos depicting N common object transformations, N) Exploration of deep network architectures for modeling and generating future depictions, N) Quantitative, qualitative, and human evaluations of the generated results, and N) Visualizations of the learned transformation patterns. Previous works~ _cite_ have looked at the problem of recognizing attributes, which has significant conceptual overlap with the idea of object state recognition. For example ``in full bloom'' could be viewed as an attribute of flowers. Parikh and Grauman~ _cite_ train models to recognize the relative strength of attributes such as face A is ``smiling more'' than face B from ordered sets of images. One way to view our work is as providing methods to train relative state models in the temporal domain. Most relevant to our work, given a set of object transformation terms, such as ripe vs unripe, _cite_ learns visual classification and regression models for object transformations from a collection of photos. In contrast, our work takes a deep learning approach and learns transformations from video--perhaps a more natural input format for learning temporal information. Timelapse data captures changes in time and has been used for various applications. _cite_ hallucinates an input scene image at a different time of day by making use of a timelapse video dataset exhibiting lighting changes in an example-based color transfer technique. _cite_ presents an algorithm that synthesizes timelapse videos of landmarks from large internet image collections. In their follow-up work, ~ _cite_ imports additional camera motion while composing videos to create transformations in time and space. Future prediction has been applied to various tasks such as estimating the future trajectories of cars _cite_, pedestrians _cite_, or general objects _cite_ in images or videos. In the ego-centric activity domain, _cite_ encodes the prediction problem as a binary task of selecting which of two video clips is first in temporal ordering. Given large amounts of unlabeled video data from the internet, _cite_ trains a deep network to predict visual representations of future images, enabling them to anticipate both actions and objects before they appear. Generative models have attracted extensive attention in machine learning~ _cite_ . Recently many works have focused on generating novel natural or high-quality images. _cite_ applies deep structure networks trained on synthetic data to generate ND chairs. _cite_ combines variational auto-encoders with an attention mechanism to recurrently generate different parts of a single image. Generative adversarial networks (GANs) have shown great promise for improving image generation quality~ _cite_ . GANs are composed of two parts, a generative model and a discriminative model, to be trained jointly. Some extensions have combined GAN structure with multi-scale laplacian pyramid to produce high-resolution generation results~ _cite_ . Recently _cite_ incorporated deep convolutional neural network structures into GANs to improve image quality. _cite_ proposed a network to generate the contents of an arbitrary image region according to its surroundings. Some related approaches~ _cite_ have trained generation models to reconstruct input video frames and/or generate the next few consecutive frames of a video. We also explore the use of DCGANs for future prediction, focusing on modeling object transformations over relatively long time scales.