High-dimensional data often have a small intrinsic dimension. For example, in the area of computer vision, face images of a subject _cite_, _cite_, handwritten images of a digit _cite_, and trajectories of a moving object _cite_, can all be well-approximated by a low-dimensional subspace of the high-dimensional ambient space. Thus, multiple class data often lie in a union of low-dimensional subspaces. The problem is to partition high-dimensional data into clusters corresponding to their underlying subspaces. Standard clustering methods such as k-means in general are not applicable to subspace clustering. Various methods have been recently suggested for subspace clustering, such as Sparse Subspace Clustering (SSC) _cite_ (see also its extensions and analysis in _cite_), Local Subspace Affinity (LSA) _cite_, Local Best-fit Flats (LBF) _cite_, Generalized Principal Component Analysis _cite_, Agglomerative Lossy Compression _cite_, Locally Linear Manifold Clustering _cite_, and Spectral Curvature Clustering _cite_ . A recent survey on subspace clustering can be found in _cite_ . Low-dimensional intrinsic structures, which enable subspace clustering, are often violated for real-world computer vision observations (as well as other types of real data) . For example, under the assumption of Lambertian reflectance, _cite_ shows that face images of a subject obtained under a wide variety of lighting conditions can be approximated accurately with a N-dimensional linear subspace. However, real-world face images are often captured under pose variations; in addition, faces are not perfectly Lambertian, and exhibit cast shadows and specularities _cite_ . Therefore, it is critical for subspace clustering to handle corrupted underlying structures of realistic data, and as such, deviations from ideal subspaces. When data from the same low-dimensional subspace are arranged as columns of a single matrix, this matrix should be approximately low-rank. Thus, a promising way to handle corrupted data for subspace clustering is to restore such low-rank structure. Recent efforts have been invested in seeking transformations such that the transformed data can be decomposed as the sum of a low-rank matrix component and a sparse error one _cite_ . _cite_ and _cite_ are proposed for image alignment (see _cite_ for the extension to multiple-classes with applications in cryo-tomograhy), and _cite_ is discussed in the context of salient object detection. All these methods build on recent theoretical and computational advances in rank minimization. In this paper, we propose to robustify subspace clustering by learning a linear transformation on subspaces using matrix rank, via its nuclear norm convex surrogate, as the optimization criteria. The learned linear transformation recovers a low-rank structure for data from the same subspace, and, at the same time, forces a high-rank structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separations between the subspaces for more accurate subspace clustering. This paper makes the following main contributions: The proposed approach can be considered as a way of learning data features, with such features learned in order to reduce rank and encourage subspace clustering. As such, the framework and criteria here introduced can be incorporated into other data classification and clustering problems.