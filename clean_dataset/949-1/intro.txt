The ability to infer a ND model of an object from a single image is necessary for human-level scene understanding. Despite the large success of deep learning in computer vision and the diversity of tasks being approached, ND representations are not yet in the focus of deep networks. Can we make deep networks learn such ND representations? In this paper, we present a simple and elegant encoder-decoder network that infers a ND model of an object from a single image of this object, see Figure~ _ref_ . We represent the object by what we call "multi-view ND model"~--the set of all its views and corresponding depth maps. Given an arbitrary viewpoint, the network we propose generates an RGB image of the object and the depth map. This representation contains rich information about the ND geometry of the object, but allows for more efficient implementation than voxel-based ND models. By fusing several views from our multi-view representation we get a full ND point cloud of the object, including parts invisible in the original input image. While technically the task comes with many ambiguities, humans are known to be good in using their prior knowledge about similar objects to guess the missing information. The same is achieved by the proposed network: when the input image does not allow the network to infer the parts of an object--for example, because the input only shows the front view of a car and there is no information about its back--it fantasizes the most probable shape consistent with the presented data (for example, a standard sedan car) . The network is trained end-to-end on renderings of ND models from the ShapeNet dataset~ _cite_ . We render images on the fly during network training, with random viewpoints and lighting. This makes the training set very diverse, thanks to the size of ShapeNet, and effectively infinite. We make the task more challenging and realistic by pasting the object renderings on top of random background images. In this setup, the network learns to automatically segment out the object. Moreover, we show that networks trained on synthetic images of this kind yield reasonable predictions for real-world images without any additional adaptation. Contributions. First, we largely improve on the visual quality of the generated images compared to previous work. Second, we achieve this with a simpler and thus more elegant architecture. Finally, we are the first who can apply the network to images with non-homogeneous background and natural images.