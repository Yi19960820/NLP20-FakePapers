Video based human action recognition has many applications in human-computer interaction, surveillance, video indexing and retrieval. Actions or movements generate varying patterns of spatio-temporal appearances in videos that can be used as feature descriptors for action recognition. Based on this observation, several visual representations have been proposed for discriminative human action recognition such as space-time pattern templates~ _cite_, shape matching~ _cite_, spatio-temporal interest points~ _cite_, and motion trajectories based representation~ _cite_ . Especially, dense trajectory based methods~ _cite_ have shown impressive results for action recognition by tracking densely sampled points through optical flow fields. While these methods are effective for action recognition from a common viewpoint, their performance degrades significantly under viewpoint changes. This is because the same action appears different and results in different trajectories when observed from different viewpoints. A practical system must recognize human actions from unknown and more importantly unseen viewpoints. One approach for recognizing actions across different viewpoints is to collect data from all possible views and train a separate classifier for each case. This approach does not scale well as it requires a large number of labelled samples for each view. To overcome this problem, some techniques infer ND scene structure and use geometric transformations to achieve view invariance _cite_ . These methods often require robust joint estimation which is still an open problem in real-world settings. Other methods focus on view-invariant spatio-temporal features~ _cite_ . However, the discriminative power of these methods is limited by their inherent structure of view-invariant features~ _cite_ . Knowledge transfer-based methods~ _cite_ have recently become popular for cross-view action recognition. These methods find a view independent latent space in which features extracted from different views are directly comparable. For instance, Li and Zickler~ _cite_ proposed to construct virtual views between action descriptors from source and target views. They assume that an action descriptor transforms continuously between two viewpoints and the virtual path connecting two views lies on a hyper-sphere (see Fig.~ _ref_-(a)) . Thus, ~ _cite_ computes virtual views as a sequence of linearly transformed descriptors obtained by making a finite number of stops along the virtual path. This method requires samples from both source and target views during training to construct virtual views. To relax the above constraint on training data, Wang et al.~ _cite_ used a set of discrete views during training to interpolate arbitrary unseen views at test time. They learned a separate linear transformation between different views for each human body part using a linear SVM solver as shown in Fig.~ _ref_-(b), thereby limiting the scalability and increasing the complexity of their approach. Existing view knowledge transfer approaches are unable to capture the non-linear manifolds where realistic action videos generally lie, especially when actions are captured from different views. This is because they only seek a set of linear transformations to construct virtual views between the descriptors of action videos captured from different viewpoints. Furthermore, such methods are either not applicable or perform poorly when recognition is performed on videos acquired from unknown and, more importantly, unseen viewpoints. In this paper, we propose a different approach to view knowledge transfer that relaxes the assumptions on the virtual path and the requirements on the training data. We approach cross-view action recognition as a non-linear knowledge transfer learning problem where knowledge from multiple views is transferred to a shared compact high-level space. Our approach consists of three phases. Figure _ref_ shows an overview of the first phase where a Robust Non-linear Knowledge Transfer Model (R-NKTM) is learned. The proposed R-NKTM is a deep fully-connected network with weight decay and sparsity constraints which learns to transfer action video descriptors captured from different viewpoints to a shared high-level representation. The strongest point of our technique is that we learn a {\em single} R-NKTM for mapping all action descriptors from all camera viewpoints to a shared compact space. Note that the labels used in Fig.~ _ref_ are dummy labels where every sequence is given a unique label that does not correspond to any specific action. Thus, action labels are not required while R-NKTM learning or while transferring training and test action descriptors to the shared high-level space using the R-NKTM. The second phase is training where action descriptors from unknown views are passed through the learned R-NKTM to construct their cross-view action descriptors. Action labels of training data are now required to train the subsequent classifier. In the test phase, view-invariant descriptors of actions observed from unknown and previously unseen views are constructed by forward propagating their view dependent action descriptors through the learned R-NKTM. Any classifier can be trained on the cross-view action descriptors for classification in a view-invariant way. We used a simple linear SVM classifier to show the strength of the proposed R-NKTM. Our R-NKTM learning scheme is based on the observation that similar actions, when observed from different viewpoints, still have a common structure that puts them apart from other actions. Thus, it should be possible to separate action related features from viewpoint related features. The main challenge is that these features cannot be linearly separated. The second challenge comes from learning a non-linear model itself which requires a large amount of training data. Our solution is to learn the R-NKTM from action trajectories of synthetic ND human models fitted to real motion capture (mocap) data. By projecting these ND human models to different views, we can generate a large corpus of synthetic trajectories to learn the R-NKTM. We use k-means to generate a general codebook for encoding the action trajectories. The same codebook is used to encode dense trajectories extracted from real action videos in the training and test phases. The major contribution of our approach is that we learn a {\em single} Robust Non-linear Knowledge Transfer Model (R-NKTM) which can bring any action observed from an unknown viewpoint to its compact high-level representation. Moreover, our method encodes action trajectories using a general codebook learned from synthetic data and then uses the same codebook to encode action trajectories of real videos. Thus, new action classes from real videos can easily be added using the same learned NTKM and codebook. Comparison with eight existing cross-view action recognition methods on four benchmark datasets including the IXMAS~ _cite_, UWAND Multiview Activity II~ _cite_, Northwestern-UCLA Multiview ActionND~ _cite_, and UCF Sports~ _cite_ datasets shows that our method is faster and achieves higher accuracy especially when there are large viewpoint variations. This paper is an extension of our prior work _cite_ where we transferred a given action acquired from any viewpoint to its canonical view. Knowledge of the canonical view was required for NKTM learning in _cite_ . This is a problem because the canonical view is not only action dependent, it is ill-defined. For example, what would be the canonical view of a person walking in a circle? Another limitation of _cite_ is that cylinders were fitted to the mocap data to approximate human limbs, head and torso. The trajectories generated from such models do not accurately represent human actions. In this paper, we extend our work by removing both limitations. Firstly, we no longer require identification of the canonical view for learning the new R-NKTM and use dummy labels instead. Secondly, we fit realistic ND human models to the mocap data and hence generate more accurate trajectories. Using ND human models also enables us to vary, and hence model, the human body shape and size. Besides these extensions, we also perform additional experiments on two more datasets namely, the UWAND Multiview Activity II~ _cite_ and UCF Sports~ _cite_ datasets. We denote our prior model _cite_ by NKTM and the one proposed in this paper by R-NKTM.