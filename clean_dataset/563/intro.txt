Severe weather conditions, such as rain _cite_, haze _cite_ and snow _cite_, impact not only human visual perception but also outdoor computer vision systems _cite_ . In rainy days, both dynamic rain streaks and static raindrops adhered to the camera lens can significantly degrade the captured image's quality. The research on rain streaks removal, e.g., _cite_ has been well explored. However, these methods cannot be directly used for raindrop removal, since the appearance and physical imaging process of raindrops are completely different from those of rain streaks. Compared with rain streaks, raindrops have more obvious and irregular geometric structures, which makes this problem more challenging. Since most deep networks _cite_ for high-level vision tasks are trained on clean images, their performances are easily affected by adherent raindrops. Thus, designing effective and efficient algorithms for raindrop removal is desirable for a wide range of practical vision applications. To date, many methods have been proposed to handle single image raindrop removal. We briefly review these methods and classify them into two categories: model-based methods and learning-based methods. Model-based methods. Model-based methods are based on physical imaging process or raindrop geometric appearance to model the distribution of raindrops. In _cite_, the authors attempt to model the shape of adherent raindrops by a sphere section. Furthermore, in _cite_, Bessel curves are used to obtain higher modeling accuracy. Since raindrops have various shapes and sizes, the above models can cover only a small portion of raindrop shapes. To simplify the raindrop removal problem, several hardware constraints, e.g., multiple cameras _cite_, a stereo camera system _cite_ and pan-tilt surveillance cameras _cite_, are exploited. However, these methods cannot work with single cameras. In _cite_, the authors detect raindrops by using motion and intensity temporal derivatives of input videos. Since this method requires consecutive video frames to extract raindrop features, it is not suitable for processing single images. Learning-based methods. Learning-based methods use large amounts of data to learn and explore the characteristics of raindrops. In _cite_, the authors detects raindrops on a windshield using raindrop features learned by PCA. When objects in the background are similar to raindrops, PCA cannot effectively extract the characteristics of raindrops and causes false detection. Recently, due to the large amount of available training data and computing resources, deep learning has become the most popular learning-based methods. In _cite_, the authors build a N layers network to extract features of static raindrops and dirt spots from synthetic images. While this method works well on small and sparse rain spots, it cannot remove large and dense raindrops. Recently, an AttentiveGAN _cite_ is proposed to simultaneously detect and remove raindrops. This method employs a recurrent network to detect raindrops and generate corresponding attention maps. These maps are further injected into the following networks to boost the reconstruction performance. Since raindrops contain various shapes, sizes and appearances, most existing methods cannot simultaneously achieve performance and robustness. The recent AttentiveGAN _cite_ can well remove raindrops from single images, as shown in Figure _ref_ (c) . However, this network contains a relatively large number of parameters and requires complex recurrent operations, which limits its potential value in practical applications with limited computing resources. In this paper, we aim to effectively remove the raindrops from single raindrop-degraded images with a lightweight network, as shown in Figure _ref_ (d) . To well learn the mapping function between the degraded image and the clean one, not only high-level semantic features but also rich spatial information _cite_ are required. Therefore, to achieve this goal, we design a new feature aggregating operation to better fuse spatial and semantic information across adjacent layers. Furthermore, we find that raindrops are short focal length convex lenses with the function of converging light. The light passing through a raindrop and converge to a point, causing a significant change in the luminance of the raindrop occluded area, while the chrominance will not be affected much. Based on this observation, we force our network to focus on the luminance channel of the YUV color space. Compared with the conventional way of processing all the RGB channels, this color space transformation can significantly simplify the learning process. The contributions of this work are threefold: