Current automatic speech recognitions (ASR) systems perform very well for clean audio but not when noise is present _cite_, which limits their use in practical situations. On top of that, the best current systems use complex neural networks which require lots of memory and computations, making implementation on embedded systems difficult. For practical embedded applications, resource-efficient systems with high noise tolerance are needed. To improve the performance under noise, several techniques have been proposed. Some systems estimate the noise envelope from the signal and subtract this from the noisy speech (Spectral Subtraction) . _cite_ achieved N \% phoneme accuracy on the TIMIT dataset for N dB audio SNR, compared to N \% for clean audio. In _cite_ an algorithm is introduced to detect and compensate for different noise circumstances using a time-varying probability model of the spectra of the clean speech, noise and channel distortion. As an example, their method reduces the Word Error Rate (WER) on the Wall Street Journal dataset from N \% to N \%, compared to a clean audio WER of N \%. These techniques can improve ASR performance but often only under certain circumstances, and their performance still lags significantly behind clean audio performance. \par An alternative way of tackling this problem is to use visual information by performing lipreading in addition to possibly noisy acoustic signals, a technique humans use as well. Several authors have proposed automated lip-reading ASR systems. A common approach is to first generate feature vectors from video frames, which are then processed by a Hidden Markov Model (HMM) . Lan et al _cite_ take this approach and compare several feature extraction techniques, such as Active Appearance Modeling (AAM), sieve features, eigenlips and DCT-based techniques. The best system achieves N \% word error rate (WER) on the GRID lipreading dataset. More recently, Deep Learning methods have been applied for visual ASR. These allow learning directly from raw data and do not require expert knowledge, while showing better performance than hand-crafted feature recognizers. An often used dataset is GRID, even though it is not suited for real-life speech recognition due its very simple sentence structure and its lack of phoneme labels. In _cite_ Long-Short-Term-Memory (LSTM) neural networks were used for lipreading, achieving a N \% WER on GRID, while Assael et al. _cite_ use spatio-temporal Convolutional Neural Networks (CNN) and achieved a WER of N \%. Chung et al. _cite_ use a CNN-LSTM architecture for lipreading, achieving a WER of N \% on GRID. They also perform audio-visual speech recognition on their own LRS dataset, combining the lipreading CNN-LSTM with an LSTM for audio recognition through an attention network. This work uses a similar setup, but takes the system's required resources in terms of necessary operations and memory into account. Furthermore, it analyzes the performance of the audio-visual system under noisy audio conditions. In Section _ref_, LSTM networks for acoustic only ASR are evaluated on the TIMIT dataset. The section discusses the performance versus resource requirements trade-off in different network architectures. Section _ref_ discusses CNNs for lipreading, which are then combined with LSTM networks in order to model temporal information _cite_ . Also here, a solution is optimal when it requires the least amount of resources. In Section _ref_, acoustic and visual networks using the least resources are combined through an attention network.