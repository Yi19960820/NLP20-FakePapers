Image captioning _cite_ aims at generating natural language descriptions of images. Advanced by recent developments of deep learning, many captioning models rely on an encoder-decoder based paradigm _cite_, where the input image is encoded into hidden representations using a Convolutional Neural Network (CNN) followed by a Recurrent Neural Network (RNN) decoder to generate a word sequence as the caption. Further, the decoder RNN can be equipped with spatial attention mechanisms _cite_ to incorporate precise visual contexts, which often yields performance improvements empirically. Although the encoder-decoder framework can be effectively trained with maximum likelihood estimation (MLE) _cite_, recent research _cite_ have pointed out that the MLE based approaches suffer from the so-called exposure bias problem. To address this problem, _cite_ proposed a Reinforcement Learning (RL) based training framework. The method, developed on top of the REINFORCE algorithm _cite_, directly optimizes the non-differentiable test metric (BLEU _cite_, CIDEr _cite_, METEOR _cite_), and achieves promising improvements. However, learning with RL is a notoriously difficult task due to the high-variance of gradient estimation. Actor-critic _cite_ methods are often adopted, which involves training an additional value network to predict the expected reward. On the other hand, _cite_ designed a self-critical method that utilizes the output of its own test-time inference algorithm as the baseline to normalize the rewards, which leads to further performance gains. Beside to the high-variance problem, we notice that there are two other drawbacks of RL-based captioning methods that are often overlooked in the literature. First, while these methods can directly optimize the non-differentiable rewards and achieve high test scores, the generated captions contain many repeated trivial patterns, especially at the end of the sequence. Table _ref_ shows examples of bad-endings generated by a self-critical based RL algorithm (model details refer to Section _ref_) . Specifically, N \% generated captions end with phrases as ``with a'', ``on a'', ``of a'', (for detailed statistics see Appendix _ref_), on the MSCOCO _cite_ validation set with the standard data splitting by _cite_ . The reason is that the shaped reward function biases the learning. In Figure _ref_, we see these additive patterns at the end of captions, although make no sense to humans, yield to a higher reward. Empirically, removing these endings results in a huge performance drop of around N \%. _cite_ has also reported that in abstractive summarization, using RL only achieves high ROUGE _cite_ score, yet the human-readability is very poor. The second drawback is that RL-based text generation is sample-inefficient due to the large action space. Specifically, the search space is of size _inline_eq_, where _inline_eq_ is a set of words, _inline_eq_ is the sentence length, and _inline_eq_ denotes the cardinality of a set. This often makes training unstable and converge slowly. In this work, to tackle these two issues, we propose a simple yet effective solution by introducing coherent language constraints on local action selections in RL. Specifically, we first obtain word-level _inline_eq_-gram _cite_ model from the training set and then use it as an effective prior. During the action sampling step in RL, we reduce the search space of actions based on the constitution of the previous word contexts as well as our _inline_eq_-gram model. To further promote samples with high rewards, we sample multiple sentences during the training and update the policy based on the best-rewarded one. Such simple treatments prevent the appearance of bad endings and expedite the convergence while maintaining comparable performance to the pure RL counterpart. In addition, the proposed framework is generic, which can be applied to many different kinds of neural structures and applications.