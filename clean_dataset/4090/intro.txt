The underlying representation of scene geometry is a crucial element of any localisation and mapping algorithm. Not only does it influence the type of geometric qualities that can be mapped, but also dictates what algorithms can be applied. In SLAM in general, but especially in monocular vision, where scene geometry cannot be retrieved from a single view, the representation of geometrical uncertainties is essential. However, uncertainty propagation quickly becomes intractable for large degrees of freedom. This difficulty has split mainstream SLAM approaches into two categories: SLAM ~ _cite_ which represents geometry by a sparse set of features and thereby allows joint probabilistic inference of structure and motion (which is a key pillar of probabilistic SLAM _cite_) and or {semi-dense} SLAM ~ _cite_ that attempts to retrieve a more complete description of the environment at the cost of approximations to the inference methods (often discarding cross-correlation of the estimated quantities and relying on alternating optimisation of pose and map~ _cite_) . However, the conclusion that a dense representation of the environment requires a large number of parameters is not necessarily correct. The geometry of natural scenes is not a random collection of occupied and unoccupied space but exhibits a high degree of order. In a depth map, the values of neighbouring pixels are highly correlated and can often be accurately represented by well known geometric smoothness primitives. But more strongly, if a higher level of understanding is available, a scene could be decomposed into a set of semantic objects (e.g. \a chair) together with some internal parameters (e.g. \size of chair, number of legs) and a pose, following a direction indicated by the SLAM + + system~ _cite_ towards representation with very few parameters. Other more general scene elements which exhibit simple regularity such as planes can be recognised and efficiently parametrised within SLAM systems (e.g.~ _cite_) . However, such human-designed dense abstractions are limited in the fraction of natural, cluttered scenes which they can represent. In this work we aim at a more generic compact representation of dense scene geometry by training an auto-encoder on depth images. While a straightforward auto-encoder might over-simplify the reconstruction of natural scenes, our key novelty is to condition the training on intensity images. Our approach is planned to fit within the common and highly scalable keyframe-based SLAM paradigm~ _cite_, where a scene map consists of a set of selected and estimated historical camera poses together with the corresponding captured images and supplementary local information such as depth estimates. The intensity images are usually required for additional tasks, such as descriptor matching for place recognition or visualisation, and are thus readily available for supporting the depth encoding. The depth map estimate for a keyframe thus becomes a function of the corresponding intensity image and an unknown compact representation (henceforth referred to as `code') . This allows for a compact representation of depth without sacrificing reconstruction detail. In inference algorithms the code can be used as dense representation of the geometry and, due to its limited size, this allows for full joint estimation of both camera poses and dense depth maps for multiple overlapping keyframes. We might think of the image providing local details and the code as supplying more global shape parameters which are often not predicted well by `depth from single image' learning. Importantly though, these global shape parameters are not a designed geometric warp but have a learned space which tends to relate to semantic entities in the scene, and could be seen as a step towards enabling optimisation in general semantic space. Our work comes at a time when many authors are combining techniques from deep learning with estimation-based SLAM frameworks, and there is an enormously fertile field of possibilities for this. Some particularly eye-catching pieces of work over the past year have focused on supervised and self-supervised training of surprisingly capable networks which are able to estimate visual odometry, depth and other quantities from video~ _cite_ . These methods run with pure feed forward network operation at runtime, but rely on geometric and photometric formulation and understanding at training time to correctly formulate the loss functions which connect different network components. Other systems are looking towards making consistent long-term maps and for instance combine learned normal predictions with photometric constraints at test time _cite_ . Such systems are able to refine geometric estimates, and this is the domain in which we are particularly interested here. In CNN-SLAM~ _cite_ single image depth prediction and dense alignment are used to produce a dense ND map and this gives a promising result, but it is not possible to optimise the predicted depth maps further for consistency when multiple keyframes overlap as it is in our approach. To summarise, the two key contributions of our paper are: In the rest of this paper, we will first explain our method for depth learning and prediction, and then show the applicability of this approach in a SLAM setting.