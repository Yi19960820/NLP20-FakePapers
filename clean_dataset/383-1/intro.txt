crucial part of an AVR algorithm is the feature selection for both audio and visual modalities, which has a direct impact on the performance of the audio-visual recognition task. Regarding the speech modality, most speech recognition systems employ Hidden Markov Models~ (HMMs) to extract the temporal information of speech and Gaussian Mixture Models (GMMs) to discriminate between different HMMs states for acoustic input representation. However deep learning has recently been employed as a mechanism for unsupervised speech feature extraction~ _cite_ .~Beyond speaker and speech recognition, deep learning has also been used for feature extraction of unlabeled facial images~ _cite_ . Similar approaches have been employed in the analysis of multi-modal voice and face data, which resulted in an improvement of speech recognition performance~ _cite_ . The inference based on common sense is that the lip motions and the heard voice which is represented by speech features are highly correlated as a human is usually able to match the heard sound to a given set of lip motion. However, the visual lip motions and their corresponding audio stream still can have non-negligible uncorrelated information. Decision fusion has been shown to be effective in which the final decision is made by fusing the statistically independent decisions from different modalities with the emphasize on uncorrelated characteristics between different modalities _cite_ . However, data fusion in early stages, demonstrated more promising results as it creates a joint representation between two modalities based on the cross-modality correlations _cite_ . As the corresponding audio-visual streams have correlated and uncorrelated information, we propose an architecture based on Deep Neural Networks~ (DNNs) as a discriminative model between the two modalities in order to simultaneously distinguish between the correlated and uncorrelated components. Alongside with the audio stream, lip motions can also contain speaker-related information. Some research efforts applied both modalities for Speaker Identification~ (SI) and Speaker Verification~ (SV) mainly based on decision fusion and MFCC features _cite_ . The speaker dependent systems are generally aimed to recognize the speech or speaker identity based on speaker-dependent characteristics. However, speaker-independent systems must be able to recognize the part of speech regardless of who is speaking. The SV has two general categories of text-dependent and text-independent types. In text-dependent setup, a fixed text is used for all experiments. On the other hand, in text-independent SV, no prior information or restrictions are considered for the utterances. It makes the text-independent to be more challenging that text-dependent scenario. Most of the previous research efforts for audio-visual for the aforementioned problems have been conducted in the text-dependent scenario. In contrast, we conduct our experiments in speaker-independent and text-independent mode to deliberately investigate the most challenging scenario. There is a significant amount of literature describing audio-visual recognition in a variety of applications, including speech recognition in noisy environments using lip motions as auxiliary features _cite_, as well as the converse where speech data is leveraged for the purpose of lip reading _cite_ . However, there is a lack of research on concurrently incorporating the spatial and temporal audio-visual information to address the root problem of whether or not the audio stream and the visual stream match.~As an example, in a multi-speaker scenario, if the features connecting audio and video can be found, ~the speakers' lip motions could be determined by the audio stream and vice versa. In this paper, we investigate the main problem of audio-visual matching. In another word, the main problem is to recognize whether the visual lip motions of a speaker corresponds to the accompanying speech signal. The aforementioned root problem is the precedent to audio-visual synchrony verification, as recognizing the consistency between the audio-visual streams is desired. The problem of audio-visual synchrony recognition has been addressed in different research efforts such as _cite_ for identity verification, and liveness recognition of the audio-visual streams _cite_ . To address the problem, we propose to use the ND Convolutional Neural Networks models that have recently been employed for action recognition, scene understanding, and speaker verification and demonstrated promising results~ _cite_ .~ND CNNs concurrently extract features from both spatial and temporal dimensions, so the motion information is captured and concatenated in adjacent frames. We use ND CNNs to generate separate channels of information from the input frames. The combination of all channels of correlated information creates the final feature representation. The focus of the research effort described in this paper is to implement two non-identical ND-CNNs for audio-visual matching~ (Section~ _ref_) . The goal is to design nonlinear mappings that learn a non-linear embedding space between the corresponding audio-video streams using a simple distance metric. This architecture can be learned by evaluating pairs of audio-video data and later used for distinguishing between pairs of matched and non-matched audio-visual streams.~One of the main advantages of our audio-visual model is the noise-robust audio features, ~which are extracted from speech features with a locality characteristic~ (Section~ _ref_), ~and the visual features, which are extracted from spatial and temporal information of lip motions. Both audio-visual features are extracted using ND CNNs, allowing the temporal information to be treated separately for better decision making. The contributions of this paper are as follows: To the best of our knowledge, this is the first attempt to use ND convolutional neural networks for audio-visual matching in which a bridge between spatio-temporal features has been established to build a common feature space between audio-visual modalities. Our source code has been released online as an open source project _cite_ .