Convolutional neural networks (CNNs) have achieved tremendous progress on many pattern recognition tasks, especially large-scale images recognition problems _cite_ . However, on one hand, CNNs still make mistakes easily. _cite_ reveals that adding adversary noise to an image in a way imperceptible to humans can cause CNNs mislabel the image. _cite_ shows some related results: it is easy to use evolutionary algorithms to generate images that are completely unrecognizable to humans, but the state-of-the-art CNNs can classify the image to some categories with N \% confidence. On the other hand, it is still unclear how CNNs learn suitable features from the training data and what a feature map represents _cite_ . This dimness of CNNs promote recent development of visualization of CNNs, also known as deep visualization _cite_ . Deep visualization aims to reveal the internal mechanism of CNNs by generating an image that activates some specific neurons, which could provide meaningful and helpful insights for designing more effective architectures _cite_ . There are many deep visualization techniques available for understanding CNNs. Perhaps the simplest way is displaying the response of a specific layer, or several special feature maps . However, these feature maps only provide limited and unintuitive information about filters and images. For instance, although it is possible to find some filters that respond to a specific object such as “face” in _cite_, this method is heuristic and not universal. A major deep visualization technique is activation maximization _cite_, which finds an image that activates some specific neurons most intensively to reveal what feature these neurons response to. _cite_ shows the object conception learned by AlexNet by maximizing the neuron activation at the last layer. _cite_ generates similar results by applying the activation maximization to a single feature map. _cite_ generates many fantastic images by intensifying the activated neurons of input images at high layers as well as low layers, which is called ``deep dream''. However, the generated images are rough. So a series of subsequent works concentrated on improving generated images quality by adding natural priors, such as _inline_eq_ norm _cite_, total variation _cite_, jitter _cite_, Gaussian blur _cite_ and data-driven patch priors _cite_ . Besides, _cite_ also uncovers the different types of features learned by each neuron with a priori input image. Another major deep visualization technique calls code inversion _cite_, which generates an image whose activation code is similar to the target activation code at a particular layer produced by a specific image. It reveals which features are extracted by filters from the input image. Code inversion could be realized by training another neural network and directly predicting the reconstructed image _cite_, or by iteratively optimizing an initial noisy image _cite_, or transposing CNNs to project the feature activations back to the input pixel space with deconvnet _cite_ . These {} inversion methods could also be extended to statistical property of code. _cite_ visualizes Gram matrix of feature maps and finds that it represents image texture. _cite_ utilize Gram matrix to do image style transfer. Compared with activation maximization, code inversion could intuitively reveal the specific feature extracted by filters from given images. Many previous works in deep visualization have revealed some valuable explanations about a single neuron _cite_, a feature map _cite_ or the code _cite_ at different layers. The CNNs are not totally black boxes anymore. However, to our best knowledge, there is still no work about visualizing what exactly every filter tries to capture from an image. Understanding of filters could help improve architecture of CNNs. In this paper, we introduce (FMI) to deal with the aforementioned problem. For a filter of interest, FMI enhances the corresponding feature map and weakens the rest feature maps at the same time. Then classical code inversion algorithm is applied to the modified code and generate inversion images. Our experimental results show that every filter in CNNs extracts a specific texture. The texture at higher layers contains more colours and more intricate structures (Fig. _ref_) . In addition, we find that style of an image could be a combination of hierarchical texture primitives. Two methods are proposed to generate images of diverse styles by inversing modified code. In particular, we change code by reallocating the sum of each feature map randomly, and according to target code purposefully. With these results, we provide an explanation about why Gram matrix of feature maps _cite_ could represent image style. Since every filter extracts a specific texture, the combination weights of feature maps decides image style. Like the sum of feature maps along channel axis, Gram matrix also guides the energy of every feature map of generated image. Our experiments were conducted based on the open-source deep learning framework Mxnet _cite_ and is available at https: //github.co-m/xzqjack/FeatureMapInversion.