Automatically describing a video in natural language is an important challenge for computer vision and machine learning. This task, called video captioning, is a crucial achievement towards machine intelligence and also the support of a number of potential applications. Indeed, bringing together vision and language, video captioning can be leveraged for video retrieval, to enhance content search on video sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a video. Before targeting videos, captioning has been tackled for images, where the task was that of generating a single sentence which described a static visual content~ _cite_ . Later, image captioning approaches have been extended to short videos with a single action, object, or scene, initially using very similar approaches to image captioning, and then with solutions to account for the temporal evolution of the video~ _cite_ . After having been applied to highly constrained or user generated videos~ _cite_, video captioning is moving to more complex and structured kinds of video, thanks to the spread of movie description datasets~ _cite_ . So far, video captioning algorithms have relied on the use of Recurrent Neural Networks or Long Short-Term Memory (LSTM) ~ _cite_ layers, which can naturally deal with sequences of frames and, in principle, learn long-range temporal patterns. However, it has been proved that LSTMs show good learning capabilities on sequences which are between N and N frames long~ _cite_, shorter than the ones used in video captioning. Furthermore, the plain nature of recurrent networks can not deal with the layered structure of videos. This is the case of edited video, such as movies. Long edited video can be segmented into short scenes, using Descriptive Video Services or with deep learning techniques~ _cite_ ; however video scenes contain several shots that, although temporally consistent, have a different appearance. As an example, in Figure~ _ref_ two shots of a dialogue are depicted. In this case we want to prevent the network from mixing the memory of the two shots; conversely, if the network could be aware of the presence of a temporal boundary, it could reset its internal status creating a new output independent to the one of the previous shot. This also applies to user-generated video, where events can be composed by a sequence of actions in a single shot (e.g. a player runs and shoots the ball) . An effective encoder should consider the temporal dependencies both intra-action and inter-actions. In this paper, we propose a novel video encoding scheme for video captioning capable of identifying temporal discontinuities, like action or appearance changes, and exploiting them to get a better representation of the video. Figure~ _ref_ shows the hierarchical structure of our sequence-to-sequence architecture: frames, described by features computed by a CNN, enter into our time boundary-aware LSTM. The awareness of the presence of an appearance or action discontinuity automatically modifies the connectivity through time of the LSTM layer: the result is a variable length and adaptive encoding of the video, whose length and granularity depends on the input video itself. The outputs of the first boundary-aware layer are encoded through an additional recurrent layer into a fixed length vector, which is then used for generating the final caption through a Gated Recurrent Unit (GRU) layer. The contributions of the paper are summarized below.