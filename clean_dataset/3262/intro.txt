The problem of recovering a geometric representation of the ND world given a set of images is classically defined as multi-view ND reconstruction in computer vision. Traditional pipelines such as Structure from Motion (SfM) and visual Simultaneous Localization and Mapping (vSLAM) typically rely on hand-crafted feature extraction and matching across multiple views to reconstruct the underlying ND model. However, if the multiple viewpoints are separated by large baselines, it can be extremely challenging for the feature matching approach due to significant changes of appearance or self occlusions . Furthermore, the reconstructed ND shape is usually a sparse point cloud without geometric details. Recently, a number of deep learning approaches, such as ND-RNNN, LSM, DeepMVS and RayNet have been proposed to estimate the ND dense shape from multiple images and have shown encouraging results. Both ND-RNNN and LSM formulate multi-view reconstruction as a sequence learning problem, and leverage recurrent neural networks (RNNs), particularly GRU, to fuse the multiple deep features extracted by a shared encoder from input images. However, there are three limitations. First, the recurrent network is permutation variant, \ie different permutations of the input image sequence give different reconstruction results . Therefore, inconsistent ND shapes are estimated from the same image set with different permutations. Second, it is difficult to capture long-term dependencies in the sequence because of the gradient vanishing or exploding, so the estimated ND shapes are unlikely to be refined even if more images are given during training and testing. Third, the RNN unit is inefficient as each element of the input sequence must be sequentially processed without parallelization, so is time-consuming to generate the final ND shape given a sequence of images. The recent DeepMVS applies max pooling to aggregate deep features across a set of unordered images for multi-view stereo reconstruction, while RayNet adopts average pooling to aggregate the deep features corresponding to the same voxel from multiple images to recover a dense ND model. The very recent GQN uses sum pooling to aggregate an arbitrary number of orderless images for ND scene representation. Although max, average and summation poolings do not suffer from the above limitations of RNN, they tend to be `hard attentive', since they only capture the max/mean values or the summation without learning to attentively preserve the useful information. In addition, the above pooling based neural nets are usually optimized with a specific number of input images during training, therefore being not robust and general to a dynamic number of input images during testing. This critical issue is also observed in GQN . In this paper, we introduce a simple yet efficient attentional aggregation module, named  . It can be easily included in an existing multi-view ND reconstruction network to aggregate an arbitrary number of elements of a deep feature set. Inspired by the attention mechanism which shows great success in natural language processing, image captioning, etc., we design a feed-forward neural module that can automatically learn to aggregate each element of the input deep feature set. In particular, as shown in Figure _ref_, given a variable sized deep feature set, which are usually learnt view-invariant visual representations from a shared encoder, our module firstly learns an attention activation for each latent feature through a standard neural layer (\eg a fully connected layer, a ND or ND convolutional layer), after which an attention score is computed for the corresponding feature. Subsequently, the attention scores are simply multiplied by the original elements of the deep feature set, generating a set of weighted features . At last, the weighted features are summed across different elements of the deep feature set, producing a fixed size of aggregated features which are then fed into a decoder to estimate ND shapes. Basically, this module can be seen as a natural extension of sum pooling into a ``weighted'' sum pooling with learnt feature-specific weights., . Basically, in the proposed training algorithm, the base encoder-decoder neural layers are only optimized when the number of input images is N, while the module is only optimized where there are more than N input images. Eventually, the whole optimized based neural network achieves superior performance with a large number of input images, while simultaneously being extremely robust and able to generalize to a small number of input images, even to a single image in the extreme case.,