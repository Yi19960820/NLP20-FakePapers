Estimating ND human poses from a single-view RGB image has attracted growing interest in the past few years for its wide applications in robotics, autonomous vehicles, intelligent drones etc. This is a challenging inverse task since it aims to reconstruct ND spaces from ND data and the inherent ambiguity is further amplified by other factors, e.g., clothes, occlusions, background clutters. With the availability of large-scale pose datasets, e.g., N _cite_, deep learning based methods have obtained encouraging success. These methods can be roughly divided into two categories: i) learning end-to-end networks that recover ND input images to ND poses directly, ii) extracting ND human poses from input images and then lifting ND poses to ND spaces. There are some advantages to decouple ND human pose estimation into two stages. i) For ND pose estimation, existing large-scale pose estimation datasets~ _cite_ have provided sufficient annotations; whereas pre-trained ND pose estimators~ _cite_ are also generalized and mature enough to be deployed elsewhere. ii) For ND to ND reconstruction, infinite ND-ND pose pairs can be generated by projecting each ND pose into ND poses under different camera views. Recent works~ _cite_ have shown that well-designed deep networks can achieve state-of-the-art performance on N dataset using only ND pose detections as system inputs. However, despite their promising results, few previous methods explored the problem of encoding domain-specific knowledge into current deep learning based detectors. In this paper, we develop a deep grammar network to explicitly encode a set of knowledge over human body dependencies and relations, as illustrated in Figure~ _ref_ . These knowledges explicitly express the composition process of joint-part-pose, including kinematics, symmetry and motor coordination, and serve as knowledge bases for reconstructing ND poses. We ground these knowledges in a multi-level RNN network which can be end-to-end trained with back-propagation. The composed hierarchical structure describes composition, context and high-order relations among human body parts. Additionally, we empirically find that previous methods are restricted to their poor generalization capabilities while performing cross-view pose estimation, i.e., being tested on human images from unseen camera views. Notably, on the N dataset, the largest publicly available human pose benchmark, we find that the performance of state-of-the-art methods heavily relies on the camera viewpoints. As shown in Table N, once we change the split of training and testing set, using N cameras for training and testing on the forth camera (new protocol), performance of state-of-the-art methods drops dramatically and is much worse than image-based deep learning methods. These empirical studies suggested that existing methods might over-fit to sparse camera settings and bear poor generalization capabilities. To handle the issue, we propose to augment the learning process with more camera views, which explore a generalized mapping from ND spaces to ND spaces. More specifically, we develop a pose simulator to augment training samples with virtual camera views, which can further improve system robustness. Our method is motivated by the previous works on learning by synthesis. Differently, we focus on the sampling of ND pose instance from a given ND space, following the basic geometry principles. In particular, we develop a pose simulator to effectively generate training samples from unseen camera views. These samples can greatly reduce the risk of over-fitting and thus improve generalization capabilities of the developed pose estimation system. We conduct exhaustive experiments on public human pose benchmarks, e.g., N, HumanEva, MPII, to verify the generalization issues of existing methods, and evaluate the proposed method for cross-view human pose estimation. Results show that our method can significantly reduce pose estimation errors and outperform the alternative methods to a large extend. Contributions . There are two major contributions of the proposed framework: i) a deep grammar network that incorporates both powerful encoding capabilities of deep neural networks and high-level dependencies and relations of human body; ii) a data augmentation technique that improves generalization ability of current N-step methods, allowing it to catch up with or even outperforms end-to-end image-based competitors.