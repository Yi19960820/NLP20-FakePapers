Thanks to the maturity of mid-level vision solutions such as object classification and detection~ _cite_, we are more ambitious to pursue higher-level vision-language tasks such as image captioning~ _cite_, visual Q \&A~ _cite_, and visual chatbot~ _cite_ . Unfortunately, we gradually realize that many of the state-of-the-art systems merely capture training set bias while not the underlying reasoning~ _cite_ . Recently, a promising way is to use visual compositions such as scene graph~ _cite_ and relationship context~ _cite_ for explainable visual reasoning. Therefore, visual relationship detection (VRD) ~ _cite_---the task of predicting elementary triplets such as ``person ride bike'' and ``car park on road'' in an image---is becoming an indispensable building block connecting vision with language. Despite the relatively preliminary stage of VRD compared to object detection, a major challenge of VRD is the high cost of annotating the (objN, rel, objN) triplets as shown in Fig.~ _ref_ (a) . Unlike labeling objects in images, labeling visual relationships is prohibitively expensive as it requires combinatorial checks of the three entries. Therefore, the relationships in existing VRD datasets~ _cite_ are long-tailed, and the resultant relationship models are inevitably biased to the dominant objN-objN combinations. For example, as reported in pioneering works~ _cite_, the recognition rate of unseen triplet compositions is significantly lower than the seen ones. This deficiency clearly limits the VRD potential in compositional reasoning. Though it can be alleviated by exploiting external knowledge such as language priors~ _cite_ and large-scale weak supervision~ _cite_, we still lack a principled solution in the visual modeling perspective. Unsupervised feature learning (or pre-training) is arguably the most popular remedy for training deep models with small data~ _cite_ . Therefore, we are inspired to learn object-agnostic convolutional feature maps that are less likely biased to certain objN-objN combinations. Such features should be highly responsive to object parts involved in a relationship. A plausible way is to append additional conv-layers to the original base CNN (\eg, VGGN~ _cite_ or ResNet-N~ _cite_) to remove the object-sensitive responses inherited from image classification pre-training dataset (\eg, ImageNet~ _cite_) . For example, as shown in Fig.~ _ref_ (c), compared with the base CNN's feature map, the object-agnostic one ignores object patterns but focuses on the shared patterns of interacted objects. Therefore, we raise a question: how to learn the object-agnostic feature maps without additional relationship labeling cost? In this paper, we propose a novel feature learning strategy. As shown in Fig.~ _ref_ (b), ``shuffle'' is to discard the original one-to-one paired object alignments, and thus no explicit objN-objN class information is used; ``assemble'' is to pose the relationship modeling into an unsupervised pair recover problem by transferring Region-of-Interest (ROI) features between the two unpaired domains. Our intuitive motivation is two-fold: N) if the ROI features extracted from the resultant feature maps still encode object-specific information, features are not likely to be transferred between the two domains of heterogeneous objects; N) the unsupervised fashion encourages the exploration of many more possible relationships which are usually missing in annotation. As shown in Fig.~ _ref_ (a), some simple spatial relationships such as ``chair beside bag'' are missing, and equivalent relationships are usually ignored, \ie, ``chair under person'' is missing as ``person sit on chair'' is labeled. Inspired by the recent advances in unsupervised domain transfer methods~ _cite_, we design a cycle of transformations to establish the transfer between the two domains: either transfer direction maps an RoI from domain A (or B) to B (or A), and then an adversarial network is used to confuse the mapping with RoIs in B (or A) . In particular, we use a residual structure for the transformation network, where the identity mapping encourages the feature map to capture shared but not object-specific visual patterns and the residual allows feature transformation. We demonstrate the effectiveness of the proposed strategy on two benchmarks: VRD~ _cite_ and VG~ _cite_ . We observe consistent improvement of using our pre-trained features against various ablative baselines and other state-of-the-art methods. For example, compared to feature maps without pre-training, we can boost the Recall@N of supervised, weakly supervised, and zero-shot relationship prediction by absolute N \%, N \%, N \%, respectively on VRD, and N \%, N \%, N \%, respectively on VG.