In many practical application scenarios, it is common to image a certain scene using various sensors that yield different image modalities. For example, in remote sensing domain, it is typical to have various image modalities of earth observations, such as a panchromatic band version, a multispectral bands version, and an infrared (IR) band version~ _cite_ . These different bands often exhibit similar textures, edges, corners, boundaries, or other salient features. In medical imaging domain, multi-contrast scans for the same underlying anatomy~ _cite_, such as simultaneous positron emission tomography (PET) / magnetic resonance imaging (MRI) scans, MRI TN/TN-weighted scans, also indicate strong correlation. In colorization _cite_ tasks, the output image has both chrominance channels and luminance channel which share consistent edges. These scenarios call for approaches that can capitalize on the availability of multiple image modalities of the same scene, object, or phenomenon to address interested image processing tasks. A number of multimodal image processing approaches have also been proposed in the literature to capitalize on the availability of additional or ~ _cite_, to aid the processing of target modalities, also referred as joint/collaborative image filtering~ _cite_ . Generally, the basic idea behind these approaches is that the structural details of the guidance image can be transferred to the target image. These approaches have been applied to multi-modal image denoising, super-resolution, classification, and more. However, these methods tend to introduce notable texture-copying artifacts, i.e. erroneous structure details that are not originally present in the target image because such methods typically fail to distinguish similarities from disparities between different image modalities. In this paper, we propose a new multimodal image processing framework based on coupled dictionary learning. In particular, our model captures complex correlation between different modalities using joint sparse representations over a set of adaptive coupled dictionaries. This enables us to take into account both similarities and disparities of different modalities via considering their common and unique sparse representations which are obtained in learned sparse domains. This characteristic makes our approach robust to inconsistencies between the guidance and target images, as proper guidance information can be extracted from the guidance modality and then be incorporated in a task-specific formulation to aid the processing of the target modality. Moreover, due to the sparsity prior, our model also demonstrates better robustness than other competing methods in presence of noise.