Consider Fig. _ref_: A given image (left) could give rise to many different optical flows (OF) _cite_ depending on what another image of the same scene looks like: It could show a car moving to the right (top), or the same apparently moving to the left due to camera motion to the right (middle), or it could be an artificial motion because the scene was a picture portraying the car, rather than the actual physical scene. A single image biases, but does not constrain, the set of possible flows the underlying scene can generate. We wish to leverage the information an image contains about possible compatible flows to learn better priors than those implied by generic regularizers. Note that all three flows in Fig. _ref_ are equally valid under a generic prior (piecewise smoothness), but not under a natural prior (cars moving in the scene) . A regularizer is a criterion that, when added to a data fitting term, constrains the solution of an inverse problem. These two criteria (data term and regularizer) are usually formalized as an energy function, which is minimized to, ideally, find a unique global optimum. In classical (variational) OF, the regularizer captures very rudimentary low-order statistics _cite_, for instance the high kurtosis of the gradient distribution. This does not help with the scenario in Fig. _ref_ . There has been a recent surge of (supervised) learning-based approaches to OF _cite_, that do not have explicit regularization nor do they use geometric reprojection error as a criterion for data fit. Instead, a map is learned from pairs of images to flows, where regularization is implicit in the function class _cite_, in the training procedure _cite_ (e.g. noise of stochastic gradient descent--SGD), and in the datasets used for training (e.g. Sintel _cite_, Flying Chair _cite_) . Our method does not attempt to learn geometric optics anew, even though black-box approaches are the top performers in several benchmarks. Instead, we seek to learn richer priors on the set of possible flows that are statistically compatible with an image (Fig. _ref_) . Unsupervised learning-based approaches use the same or similar loss functions as variational methods _cite_, including priors, but restrict the function class to a parametric model, for instance convolutional neural networks (CNNs) trained with SGD, thus adding implicit regularization _cite_ . Again, the priors only encode first-order statistics, which fail to capture the phenomena in Fig. _ref_ . We advocate learning a conditional prior, or regularizer, from data, but do so once and forall, and then use it in conjunction with any data fitting term, with any model and optimization one wishes. What we learn is a prior in the sense that it imposes a bias on the possible solutions, but it does not alone constraint them, which happens only in conjunction with a data term. Once the prior is learned, in a supervised fashion, one can also learn the full map to infer optical flow directly from data, without any need for (additional) supervision. In this sense, our method is {\em ``semi-unsupervised''}: Once {\em we} learn the prior, {\em anyone} can train an optical flow architecture entirely unsupervised. The key idea here is to learn a prior for the set of optical flows that are statistically compatible with {\em a single image} . Once done, we train a relatively simple network {\em in an unsupervised fashion} to map {\em pairs of images} to optical flows, where the loss function used for training includes explicit regularization in the form of the conditional prior, added to the reprojection error. Despite a relatively simple architecture and low computational complexity, our method beats all variational ones and all unsupervised learning-based ones. It is on par or slightly below a few fully supervised ones, that however are fine-tuned to a particular dataset, and are extremely onerous to train. More importantly, available fully supervised methods perform best {\em on the dataset on which they are trained.} Our method, on the other hand, performs well even when the prior is trained on one dataset and used on a different one. For instance, a fully-supervised method trained on Flying Chair beats our method on Flying Chair, but underperforms it on KITTI and vice-versa (Tab. _ref_) . Ours is consistently among the top in all datasets. More importantly, our method is complementary, and can be used in conjunction with more sophisticated networks and data terms. Let _inline_eq_ be two consecutive images and _inline_eq_ the flow, implicitly defined in the co-visible region by _inline_eq_ where _inline_eq_ is some distribution. The posterior _inline_eq_ can be decomposed as We call the first term (data) prediction error, and the second conditional prior . It is a prior in the sense that, given _inline_eq_ alone, many flows can have high likelihood for a suitable _inline_eq_ . However, it is informed by _inline_eq_ in the sense of capturing image-dependent regularities such as flow discontinuities often occurring at {\em object boundaries}, which may or may not correspond to generic image discontinuities. A special case of this model assumes a Gaussian likelihood (_inline_eq_ prediction error) and an ad-hoc prior of the form where _inline_eq_ is a scalar function that incorporates our belief in an irradiance boundary of _inline_eq_ corresponding to an object boundary. This type of conditional prior has several limitations: First, in the absence of {\em semantic context}, it is not possible to differentiate occluding boundaries (where _inline_eq_ can be discontinuous) from material boundaries (irradiance discontinuities), or illumination boundaries (cast shadows) where _inline_eq_ is smooth. Second, the image _inline_eq_ only informs the flow {\em locally}, through its gradient, and does not capture global regularities. Fig. _ref_ shows that flow fails to propagate into homogeneous region. This can be mitigated by using a fully connected CRF _cite_ but at a heavy computational cost. Our goal can be formalized as {\em learning the conditional prior} _inline_eq_ in a manner that exploits the semantic context of the scene and captures the global statistics of _inline_eq_ . We will do so by leveraging the power of deep convolutional neural networks trained end-to-end, to enable which we need to design differentiable models, which we do next.