Compelling advantages of exploiting temporal rather than merely spatial cues for video classification have been shown lately~ _cite_ . Such insights are all the more important given the surge in multimedia videos on the Internet. Even if considerable progress in exploiting temporal cues was made~ _cite_, the corresponding systems are still wanting. Recently, several variants of Convolutional Neural Networks (ConvNets) have been proposed that use ND convolutions, but they fail to exploit long-range temporal information, thus limiting the performance of these architectures. Complicating aspects include: (i) these video architectures have many more parameters than ND ConvNets; (ii) training the video architectures calls for extra large labeled datasets; and (iii) extraction and usage of optical-flow maps which is very demanding, and also difficult to obtain for large scale dataset, e.g. Sports-NM. All of these issues negatively influence their computational cost and performance. Two ways to avoid these limitations are (i) an architecture that efficiently captures both appearances and temporal information throughout videos, thus avoiding the need of optical-flow maps; and (ii) an effective supervision transfer that bridges the knowledge transfer between different architectures, such that training the networks from scratch is no longer needed. Motivated by the above observations, we introduce a novel deep spatio-temporal feature extractor network illustrated in Figure~ _ref_ . The aim of this extractor is to model variable temporal ND convolution kernel depths over shorter and longer time ranges. We name this new layer in ND ConNets configuration `Temporal Transition Layer'~ (TTL) . TTL is designed to concatenate temporal feature-maps extracted at different temporal depth ranges, rather than only considering fixed ND homogeneous kernel depths~ _cite_ . We embed this new temporal layer into the ND CNNs. In this work, we extend the DenseNet architecture-which by default has ND filters and pooling kernels-to incorporate ND filters and pooling kernels, namely DenseNetND. We used DenseNet because it is highly parameter efficient. Our TTL replaces the standard transition layer in the DenseNet architecture. We refer to our modified DenseNet architecture as `Temporal ND ConvNets'~ (TND), inspired by CND~ _cite_, Network in Network~ _cite_, and DenseNet~ _cite_ architectures. TND densely and efficiently captures the appearance and temporal information from the short, mid, and long-range terms. We show that the TTL feature representation fits action recognition well, and that it is a much simpler and more efficient representation of the temporal video structure. The TTL features are densely propagated throughout the TND architecture and are trained end-to-end. In addition to achieving high performance, we show that the TND architecture is computationally efficient and robust in both the training and inference phase. TND is evaluated on three challenging action recognition datasets, namely HMDBN, UCFN, and Kinetics. We experimentally show that TND achieves the state-of-the-art performance on HMDBN and UCFN among the other ND ConvNets and competitive results on Kinetics. It has been shown that training ND ConvNets~ _cite_ from scratch takes two months~ _cite_ for them to learn a good feature representation from a large scale dataset like Sports-NM, which is then finetuned on target datasets to improve performance. Another major contribution of our work therefore is to achieve supervision transfer across architectures, thus avoiding the need to train ND CNNs from scratch. Specifically, we show that a ND CNN pre-trained on ImageNet can act as ` a teacher ' for supervision transfer to a randomly initialized ND CNN for a stable weight initialization. In this way we avoid the excessive computational workload and training time. Through this transfer learning, we outperform the performance of generic ND CNNs (CND~ _cite_) which was trained on Sports-NM and finetuned on the target datasets, HMDBN/UCFN. The rest of the paper is organized as follows. In Section~ _ref_, we discuss related work. Section~ _ref_ describes our proposed approaches. The implementation details, experimental results and their analysis are presented in Section~ _ref_ . Finally, conclusions are drawn in Section~ _ref_ .