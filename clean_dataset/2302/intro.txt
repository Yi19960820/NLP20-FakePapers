Person re-identification is to match the persons across multiple cameras with non-overlapping views. It is challenging because the appearance of a person's surveillance images in different cameras may exhibit dramatic changes caused by illumination variation, as well as different camera views and body poses. To address it, researchers mainly focus on how to efficiently represent images that contain persons and/or how to accurately measure the similarities among them. In the former, both the robustness and distinctiveness of features are considered, while in the latter, cross-view information and the relationship among persons are analyzed. In existing image representation methods, color features have been proved to be the most important cue in person re-identification _cite_ . Among them, color histograms are commonly used color features _cite_ . However, it has the limitation of being sensitive to illumination. To increase the robustness of the color feature, a novel salient color name based color descriptor (SCNCD) is proposed in _cite_ . N pre-defined color names are successfully applied to represent person images. This kind of representation based on color names is not only compact but also has shown good robustness to illumination. The key is to develop a strategy of achieving color names descriptor (CND) to describe a pixel. Note that when it refers to a region, CND denotes a description of a region over color names (with a concatenation or pooling operation in a region) . Inspired by _cite_, we propose a novel CND to represent images for person re-identification. In this paper, we argue that it is unreliable to compare image pixels with color names straightforwardly in the original Euclidean space in view of the fact that the underlying distribution of the set of color names and that of image pixels from surveillance cameras are different. This is understandable since Euclidean distance treats three color channels as an isotropic one, and thus being unable to exactly reflect the underlying relationship between color names and image pixels. In Fig. _ref_ (a), when the Euclidean distance is regarded as a dissimilarity measure between color names and image pixels, we can see that there will be a set of image pixels (circled by a blue ellipse) being assigned to the color name . In fact, the set of image pixels visually appear totally different from color name . This observation reflects that although the image pixel stays 'close' to one color name, it does not definitely imply that it has the same semantic information with the color name. The inaccurate semantic measure between image pixels and color names will further limits the performance of CND. Motivated by Mahalanobis distance which accounts for the correlation among different dimensions, we assume a Gaussian model and propose a novel method named soft Gaussian mapping (SGM) to learn the description of an image pixel over color names. Traditional Mahalanobis distance often aggregates the color names and image pixels together and coarsely assume that they obey the same Gaussian model. However, this assumption has two deficiencies: (N) it does not take into account the difference between color names and image pixels and (N) the estimated covariance matrix may approach to reflecting the distribution of image pixels because the number of image pixels is far larger than that of color names. To overcome these problems, we model the discrepancies between color names and pixels using a Gaussian. Then, the inverse of covariance matrix are employed to bridge the gap between color names and image pixels. In Fig. _ref_ (b), we can find that after the transformation in SGM, the color name has been pushed away while the image pixels and color names are similar from a semantic perspective. To achieve the CND of a pixel, SGM further maps an image pixel to _inline_eq_ nearest (owning semantic similarity) color names. Then, an image is converted to N soft Gaussian maps. To establish stable and robust descriptors, a max pooling operation is imposed on the local region in each soft Gaussian map. With it, we can obtain a robust CND of one strip via sum pooling and sum normalization _cite_ . The image-level representation is finally obtained by concatenating them. Till now, we mainly concentrate on how to learn a robust image representation. When some labeled training data are available, we further introduce cross-view coupling learning to capture the relationship among different cameras, which can be seen as an extension of XQDA _cite_ to two coupled variables: and in _cite_ . Based on it, the dimension-reduced image representations are compact and discriminative. The results of extensive experiments on three public benchmark datasets demonstrate the effectiveness of our color name based image representation and subspace learning method.