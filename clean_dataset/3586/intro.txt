Deep neural networks (DNNs) have met with success multiple tasks, and testified a constantly increasing popularity, being able to deal with the vast heterogeneity of data and to provide state-of-the-art results across many fields and domains _cite_ . Convolutional Neural Networks (CNNs) _cite_ are one of the protagonists of this success. Starting from AlexNet _cite_, until the most recent convolutional-based architectures _cite_ CNNs have proved to be especially useful in the field of computer vision, improving the classification accuracy in many datasets _cite_ . However, a common caveat of large CNNs is that they require a lot of training data in order to work well. In the presence of classification tasks on small datasets, typically those networks are in a very large dataset like ImageNet _cite_, and then on the dataset the problem is set on. The idea is that the pre-trained network has stored a decent amount of information regarding features which are common to the majority of images, and in many cases this knowledge can be transferred to different datasets or to solve different problems (image segmentation, localization, detection, etc.) . This technique is referred as _cite_ and has been an important ingredient in the success and popularization of CNNs. Another important technique--very often paired with the previous one--is, through which small transformations are directly applied on the images. A nice characteristic of data augmentation is its agnosticism toward algorithms and datasets. _cite_ used this technique to achieve state-of-the-art results in MNIST dataset _cite_, while _cite_ used the method almost without any changes to improve the accuracy of their CNN in the ImageNet dataset _cite_ . Since then, data augmentation has been used in virtually every implementation of CNNs in the field of computer vision. Despite the practicality of the above-mentioned techniques, when the number of images per class is extremely small, the performances of CNNs rapidly degrade and leave much to be desired. The high availability of unlabeled data only solves half of the problem, since the manual labeling process is usually costly, tedious and prone to human error. Under these assumptions, we propose a new method to perform an automatic labeling, called . Starting from a very small labeled dataset, we set an automatic label propagation procedure, that relies on graph transduction techniques, to label a large unlabeled set of data. This method takes advantage of second-order similarity information among the data objects, a source of information which is not directly exploited by traditional techniques. To assess our statements, we perform a series of experiments with different CNN architectures and datasets, comparing the results with a first-order ``label propagator''. In summary, our contributions in this article are as follows: a) by using graph transductive approaches, we propose and develop the aforementioned label augmentation method and use it to improve the accuracy of state-of-the-art CNNs in datasets where the number of labels is limited; b) by gradually increasing the number of labeled objects, we give detailed results in three standard computer vision datasets and compare the results with the results of CNNs; c) we replace our transductive algorithm with linear support vector machines (SVM) _cite_ to perform label augmentation and compare the results; d) we give directions for future work and how the method can be used on other domains. Semi-supervised label propagation has a long history of usage in the field of machine learning _cite_ . Starting from an initial large dataset, with a small portion of labeled observations the traditional way of using semi-supervised learning is to train a classifier only in the labeled part, and then use the classifier to predict labels for the unlabeled part. The labels predicted in this way are called . The classifier is then trained in the entire dataset, considering the pseudo-labels as if they were real labels. Different methods with the same intent have been previously proposed. In deep learning in particular, there have been devised algorithms to use data with a small number of labeled observations. _cite_ trained the network jointly in both the labeled and unlabeled points. The final loss function is a weighted loss of both labeled and unlabeled points, where in the case of the unlabeled points, the pseudo-label is determined by the highest score proposed by the model. _cite_ optimized a CNN on such a way as to produce embeddings that have high similarities for the observations that belong to the same class. _cite_ used a totally different approach, developing a generative model that allows for effective generalization from small labeled datasets to large unlabeled ones. In all the mentioned methods, the way how the unlabeled data has been used can be considered as an intrinsic property of their engineered neural networks. Our choice of CNNs as the algorithm used for the experiments was motivated because CNNs are state-of-the-art models in computer vision, but the approach is more general than that. The method presented in this article does not even require a neural network and in principle, non-feature based observations (i.e graphs) can be considered, as long as a similarity measure can be derived for them. At the same time, the method shows good results in relatively complex image datasets, improving over the results of state-of-the-art CNNs.