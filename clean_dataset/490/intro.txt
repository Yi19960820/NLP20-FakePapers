Animals and humans learn incrementally. A child grows its vocabulary of identifiable concepts as different concepts are presented, without forgetting the concepts with which they are already familiar. Antithetically, most supervised learning systems work under the omniscience of the existence of all classes to be learned, prior to training. This is crucial for learning systems that produce an inference as a conditional probability distribution over all known categories. Incremental supervised learning though reasonably studied, lacks a formal and structured definition. One of the earliest formalization of incremental learning comes from the work of Jantke~ _cite_ . In this article the author defines incremental learning roughly as systems that . Immediately following this statement though is the relaxation of the definition:, with the terms and not being sufficiently well-defined. Subsequently, other studies made conforming or divergent assumptions and relaxations thereby adopting their own characteristic definitions. Following suit, we redefine a more fundamental and rigorous incremental learning system using two fundamental philosophies: data membrane and domain agnosticism. Consider there are two sites: the base site _inline_eq_ and the incremental site _inline_eq_ each with ample computational resources. _inline_eq_ possesses the base dataset _inline_eq_, where _inline_eq_ and _inline_eq_ . _inline_eq_ possesses the increment dataset _inline_eq_, where _inline_eq_ and _inline_eq_ and _inline_eq_ . _inline_eq_ is allowed to trai n a discriminative learner _inline_eq_ using _inline_eq_ and make _inline_eq_ available to the world. Once broadcast, _inline_eq_ does not maintain _inline_eq_ and will therefore not support queries regarding _inline_eq_ . Property~ _ref_ is referred to as the . Data membrane ensures that _inline_eq_ does not query _inline_eq_ and that no data is transferred either in original form or in any encoded fashion (say as feature vectors) . The generalization set at _inline_eq_ contains labels in the space of _inline_eq_ . This implies that though _inline_eq_, has no data for training the labels _inline_eq_, the discriminator _inline_eq_ trained at _inline_eq_ with _inline_eq_ alone is expected to generalize on the combined label space in the range _inline_eq_ . _inline_eq_ can acquire _inline_eq_ and other models from _inline_eq_ and infer the existence of the classes _inline_eq_ that _inline_eq_ can distinguish. Therefore incremental learning differs from the problem of zero-shot novel class identification. A second property of multi-class incremental learning is domain agnosticism, which can be defined as follows: Property~ _ref_ implies that we cannot presume to gain any knowledge about the label space of _inline_eq_ (_inline_eq_) by simply studying the behaviour of _inline_eq_ using _inline_eq_ . In other words, the predictions of the network _inline_eq_ does not provide us meaningful enough information regarding _inline_eq_ . This implies that the conditional probability distribution across the labels in _inline_eq_, _inline_eq_ for _inline_eq_ produced by _inline_eq_, cannot provide any meaningful inference to the conditional probability distribution across the labels _inline_eq_ when generalizing on the incremental data. For any samples _inline_eq_, the conditional probability over the labels of classes _inline_eq_ are meaningless. Property (_ref_) is called . From the above definition it is implied that sites must train independently. The training at _inline_eq_ of labels _inline_eq_ could be at any state when _inline_eq_ triggers site _inline_eq_ by publishing its models, which marks the beginning of incremental training at _inline_eq_ . To keep experiments and discussions simpler, we assume the worst case scenario where the site _inline_eq_ does not begin training by itself, but we will generalize to all chronology in the later sections. We live in a world of data abundance. Even in this environment of data affluence, we may still encounter cases of scarcity of data. Data is a valuable commodity and is often jealously guarded by those who posses it. Most large institutions and organizations that deploy trained models, do not share the data with which the models are trained. A consumer who wants to add additional capability is faced with an incremental learning problem as defined. In other cases, such as in military or medicine, data may be protected by legal, intellectual property and privacy restrictions. A medical facility that wants to add the capability of diagnosing a related-but-different pathology to an already purchased model also faces a similar problem and often has to expend large sums of money to purchase an instrument with this incrementally additional capability. All these scenarios are plausible contenders for strict incremental learning following the above definition. The data membrane property ensures that even if data could be transferred, we are restricted by means other than technological, be it legal or privacy-related that prevents the sharing of data across sites. The domain agnosticism property implies that we should be able to add the capability of predicting labels to the network, without making any assumptions that the new labels may or may not hold any tangible relationship to the old labels. \noindent A trivial baseline: Given this formalism, the most trivial incremental training protocol would be to train a machine at _inline_eq_ with _inline_eq_, transfer this machine (make it available in some fashion) to _inline_eq_ . At _inline_eq_, initialize a new machine with the parameters of the transferred machine, while alerting the new machine to the existence of classes _inline_eq_ and simply teach it to model an updated conditional probability distribution over classes _inline_eq_ . A quick experiment can demonstrate to us that such a system is afflicted by a well-studied problem called catastrophic forgetting. Figure~ _ref_ demonstrates this effect using neural networks. This demonstrates that without supplying samples from _inline_eq_, incremental training without catastrophic forgetting at _inline_eq_ is difficult without relaxing our definition. To avoid this, we propose that the use of generative models trained at _inline_eq_, be deployed at _inline_eq_ to hallucinate samples from _inline_eq_ . The one-time broadcast from _inline_eq_ could include this generator along with the initializer machine that is transferred. While this system could generate samples-on-demand, we still do not have targets for the generated samples to learn classification with. To solve this problem, we propose the generation of supervision from the initializer network itself using a temperature-raised softmax. A temperature raised softmax was previously proposed as a means of distilling knowledge in the context of neural network compression~ _cite_ . Not only does this provide supervision for generated samples, but will also serve as a regularizer while training a machine at _inline_eq_, similar to the fashion described in~ _cite_ . In summary this paper provides two major contributions: N. A novel, uncompromising and practical definition of incremental learning and N. a strategy to attack the defined paradigm through a novel sampling process called . The rest of this article is organized as follows: section~ _ref_ outlines the proposed method, section~ _ref_ discusses related works on the basis of the properties we have presented, section~ _ref_ presents the design of our experiments along with the results, section~ _ref_ extends this idea to continual learning systems, where we present an trivial extension to more than one increment and section~ _ref_ provides concluding remarks.