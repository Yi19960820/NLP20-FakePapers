Batch Normalization (BN) _cite_ has recently become a standard and crucial component for improving the training of deep neural networks (DNNs), which is successfully employed to harness several state-of-the-art architectures, such as residual networks _cite_ and Inception nets _cite_ . In the training and inference of DNNs, BN normalizes the means and variances of the internal representation of each hidden layer, as illustrated in Figure _ref_ (a) . As pointed out in _cite_, BN enables using larger learning rate in training, leading to faster convergence. Although the significance of BN has been demonstrated in many previous works, its drawback cannot be neglected, \ie its effectiveness diminishing when small mini-batch is presented in training. Consider a DNN consisting of a number of layers from bottom to top. In the traditional BN, the normalization step seeks to eliminate the change in the distributions of its internal layers, by reducing their internal covariant shifts. Prior to normalizing the distribution of a layer, BN first estimates its statistics, including the means and variances. However, it is impractical expected for the bottom layer of the input data that can be pre-estimated on the training set, as the representations of the internal layers keep changing after the network parameters have been updated in each training step. Hence, BN handles this issue by the following schemes. i) During the model training, it approximates the population statistics by using the batch sample statistics in a mini-batch. ii) It retains the moving average statistics in each training iteration, and employs them during the inference. However, BN has a limitation, which is limited by the memory capacity of computing platforms (\eg GPUs), especially when the network size and image size are large. In this case, the mini-batch size is not sufficient to approximate the statistics, making them had bias and noise. And the errors would be amplified when the network becomes deeper, degenerating the quality of the trained model. Negative effects exist also in the inference, where the normalization is applied for each testing sample. Furthermore, in the BN mechanism, the distribution of a certain layer could vary along with the training iteration, which limits the stability of the convergence of the model. Recently, an extension of BN, called Batch Renormalization (BRN) _cite_, has been proposed to improve the performance of BN when the mini-batch size is small. During training, BRN performs a correction to the mini-batch statistics by using the moving average statistics, \ie adopting a short-term memory of the mini-batch statistics in the past to make the estimate more robust. Despite BRN sheds some light on dealing with mini-batches of small sizes, its performance is still undesirable due to the following reasons. The moving averages are far away from the actual statistics in the early stage of training, making the correction of statistics in BRN unreliable. In the implementation of BRN, two extra parameters are introduced to measure whether this correction can be trusted, which need to be carefully tuned during training. Moreover, BRN may probably fail on handling the mini-batches with very few examples, \eg less than N samples. In such case, the estimates of the batch sample statistics and moving statistics by either BN or BRN are instable because the means and variances dramatically vary in different training iterations. In this paper, we present a new normalization method, Batch Kalman Normalization (BKN), for improving and accelerating training of DNNs particularly under the context of micro-batches. BKN advances the existing solutions by achieving more accurate estimation of the statistics (means and variances) of the internal representations in DNNs. Unlike BN and BRN, where the statistics were estimated by only measuring the mini-batches within a certain layer, \ie they considered each layer in the network as an isolated sub-system, BKN shows that the estimated statistics have strong correlations among the sequential layers. And the estimations can be more accurately by jointly considering its preceding layers in the network, as illustrated in Figure _ref_ (b) . By analogy, the proposed estimation method shares merits compared to the Kalman filtering process _cite_ . BKN performs two steps in an iterative way. In the first step, BKN estimates the statistics of the current layer conditioned on the estimations of the previous layer. In the second step, these estimations are combined with the observed batch sample means and variances calculated within a mini-batch. These two steps are efficient in BKN. Updating the current estimation by previous states brings negligible extra computational cost compared to the traditional BN. This paper makes the following contributions. N) We propose an intuitive yet effective normalization method, offering a promise of improving and accelerating the neural network training. N) The proposed method enables training networks with mini-batches of very small sizes (\eg~less than _inline_eq_ examples), and the resulting models perform substantially better than those using the existing batch normalization methods. This specifically makes our method advantageous in several memory-consuming problems such as training large-scale wide and deep networks, semantic image segmentation, and asynchronous SGD. N) On the classification benchmark of ImageNet, the experiments show that the recent advanced networks can be strengthened by our method, and the trained models improve the leading results by using less than N \% training steps .