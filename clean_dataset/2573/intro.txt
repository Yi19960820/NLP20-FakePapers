Deep Convolutional Neural Network has achieved great success in computer vision _cite_ . However, a complete theoretical understanding of how it works is still absent, despite efforts both numerically _cite_ and analytically _cite_ . Especially, even for simplest black-white image with _inline_eq_ pixels, _inline_eq_ parameters should be necessary to represent a general function of image. However, in practice, convolutional neural network with _inline_eq_ parameters works quite well in image classification problems. The only way to resolve the above paradox is the following. Target functions of image classification problems occupy only a small subspace of the whole function space and CNN are designed to represent functions in this subspace. Therefore, to further understand which kind of neural network architecture is better, we should first characterize this subspace. There are also other fundamental questions: why does small convolutional kernel work well? Why is increasing the depth of the CNN more efficient than increasing number of channels at each layer? In this article, we try to answer these questions using entanglement entropy, one of the most important concepts in modern theoretical physics. It's well known that functions of image form a Hilbert Space. However, it's not emphasized before that this Hilbert space has a tensor product structure because of locality of each pixel. Suppose we have a two dimensional black-white image with size _inline_eq_ . To preserve locality, we should think of an image as a two dimensional lattice, instead of a vector with dimension _inline_eq_ . In lattice representation of image, as we will shown in the main text, the Hilbert Space has a tensor product structure: _inline_eq_, where _inline_eq_ can be thought as a two dimensional local Hilbert space at each pixel. Besides, we will show an amazing mathematical relation: this Hilbert space of functions of image is exactly isomorphic to the Hilbert space of a quantum spin model _cite_ in the same lattice. Basically up to some normalization factor, any function of image can be thought of as a wavefunction and then has a one to one correspondence with a quantum state of a quantum spin model. Quantum spin model has been extensively studied over last several decades and entanglement entropy has been shown to be a powerful tool to characterize a wavefunction in the Hilbert space _cite_ . Despite that the Hilbert Space is exponentially large, tensor network with _inline_eq_ parameters is efficient to represent a general ground state wavefunction of local Hamiltonian. The reason is because that the entanglement entropy of these wavefunctions obey an area law bound (with log corrections in some cases) . It turns out that most of functions in the Hilbert space has volume law entanglement entropy and need _inline_eq_ parameters to represent. However, locality constrains interesting wavefunctions (wavefunction of ground state) to an exponentially small subspace of the whole Hilbert space. Because of this locality constraint, tensor networks are successful in approximation of these wavefunctions _cite_ . Because the Hilbert Space of image classification problem is mathematically equivalent to the Hilbert space of quantum spin model, we expect techniques in one field will also have useful applications in another field. Actually Matrix Product State, a special tensor network widely used in quantum physics numerical simulation, has already been shown to also work for MNIST handwritten-digit recognition classification problem _cite_ . Besides, restricted boltzmann machine developed in computer vision field has been proposed to be a variational ansatz of the wavefunction of a quantum spin model _cite_ . In this article, we will try to answer a more fundamental question: Can entanglement entropy also be a useful concept for image classification problems and other computer vision problems. The Boltzmann-Shannon entropy is a key tool to characterize the information of an image in information theory _cite_ . Now in the new era of artificial intelligence, we need an information theory for function of image, instead of image itself. Entanglement entropy, as a generalization of Boltzmann-Shannon entropy, can be an efficient way to characterize the information needed for representation of a function of image. First, we need to emphasize that the definition of entanglement entropy is not restricted to quantum mechanics. Actually, for any Hilbert Space with local tensor product structure, bipartite entanglement entropy is well defined mathematically. Because functions of image form such a Hilbert Space with tensor product structure, we can always define entanglement entropy. The only question is whether this concept is useful or not. In this article, we will show that entanglement entropy is a useful characterization of difficulty to represent a target function. We will show that entanglement entropy of target functions of image classification problems are bounded by a sub-volume-law (very likely to be area-law for simple problems) . Therefore one pixel only entangle locally with pixels nearby. As a result, a neural network with local connection (like convolution kernel) is efficient to represent such a function and only _inline_eq_ instead of _inline_eq_ parameters are needed. Entanglement Entropy can also be a powerful tool to study the expressive power of different network architectures. For example, we will argue that entanglement entropy of a deep convolutional neural network scales as _inline_eq_, where _inline_eq_ is the number of convolution layers and _inline_eq_ is the number of hidden channels of each layer. Therefore, to keep the entanglement entropy of CNN at the same level as the target function (thus keep the same expression power), number of channels should scale as _inline_eq_, where _inline_eq_ is the number of channels for shallow CNN with depth _inline_eq_ .