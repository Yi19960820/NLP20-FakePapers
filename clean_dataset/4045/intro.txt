Research on visual recognition is undergoing a transition from ``feature engineering'' to ``network engineering'' _cite_ . In contrast to traditional hand-designed features (\eg, SIFT _cite_ and HOG _cite_), features learned by neural networks from large-scale data _cite_ require minimal human involvement during training, and can be transferred to a variety of recognition tasks _cite_ . Nevertheless, human effort has been shifted to designing better network architectures for learning representations. Designing architectures becomes increasingly difficult with the growing number of hyper-parameters (width, filter sizes, strides, \etc), especially when there are many layers. The VGG-nets _cite_ exhibit a simple yet effective strategy of constructing very deep networks: stacking building blocks of the same shape. This strategy is inherited by ResNets _cite_ which stack modules of the same topology. This simple rule reduces the free choices of hyper-parameters, and depth is exposed as an in neural networks. Moreover, we argue that the simplicity of this rule may reduce the risk of over-adapting the hyper-parameters to a specific dataset. The robustness of VGG-nets and ResNets has been proven by various visual recognition tasks _cite_ and by non-visual tasks involving speech _cite_ and language _cite_ . Unlike VGG-nets, the family of Inception models _cite_ have demonstrated that carefully designed topologies are able to achieve compelling accuracy with low theoretical complexity. The Inception models have evolved over time _cite_, but an important common property is a strategy. In an Inception module, the input is split into a few lower-dimensional embeddings (by N _inline_eq_ N convolutions), transformed by a set of specialized filters (N _inline_eq_ N, N _inline_eq_ N, \etc), and merged by concatenation. It can be shown that the solution space of this architecture is a strict subspace of the solution space of a single large layer (\eg, N _inline_eq_ N) operating on a high-dimensional embedding. The split-transform-merge behavior of Inception modules is expected to approach the representational power of large and dense layers, but at a considerably lower computational complexity. Despite good accuracy, the realization of Inception models has been accompanied with a series of complicating factors---the filter numbers and sizes are tailored for each individual transformation, and the modules are customized stage-by-stage. Although careful combinations of these components yield excellent neural network recipes, it is in general unclear how to adapt the Inception architectures to new datasets/tasks, especially when there are many factors and hyper-parameters to be designed. In this paper, we present a simple architecture which adopts VGG/ResNets' strategy of repeating layers, while exploiting the split-transform-merge strategy in an easy, extensible way. A module in our network performs a set of transformations, each on a low-dimensional embedding, whose outputs are aggregated by summation. We pursuit a simple realization of this idea---the transformations to be aggregated are all of the same topology (\eg, Fig.~ _ref_ (right)) . This design allows us to extend to any large number of transformations without specialized designs. Interestingly, under this simplified situation we show that our model has two other equivalent forms (Fig.~ _ref_) . The reformulation in Fig.~ _ref_ (b) appears similar to the Inception-ResNet module _cite_ in that it concatenates multiple paths; but our module differs from all existing Inception modules in that all our paths share the same topology and thus the number of paths can be easily isolated as a factor to be investigated. In a more succinct reformulation, our module can be reshaped by Krizhevsky \etal's grouped convolutions _cite_ (Fig.~ _ref_ (c)), which, however, had been developed as an engineering compromise. We empirically demonstrate that our aggregated transformations outperform the original ResNet module, even under the restricted condition of maintaining computational complexity and model size---\eg, Fig.~ _ref_ (right) is designed to keep the FLOPs complexity and number of parameters of Fig.~ _ref_ (left) . We emphasize that while it is relatively easy to increase accuracy by increasing capacity (going deeper or wider), methods that increase accuracy while maintaining (or reducing) complexity are rare in the literature. Our method indicates that (the size of the set of transformations) is a concrete, measurable dimension that is of central importance, in addition to the dimensions of width and depth. Experiments demonstrate that, especially when depth and width starts to give diminishing returns for existing models. Our neural networks, named (suggesting the dimension), outperform ResNet-N/N _cite_, ResNet-N _cite_, Inception-vN _cite_, and Inception-ResNet-vN _cite_ on the ImageNet classification dataset. In particular, a N-layer ResNeXt is able to achieve better accuracy than ResNet-N _cite_ but has only N \% complexity. Moreover, ResNeXt exhibits considerably simpler designs than all Inception models. ResNeXt was the foundation of our submission to the ILSVRC N classification task, in which we secured second place. This paper further evaluates ResNeXt on a larger ImageNet-NK set and the COCO object detection dataset _cite_, showing consistently better accuracy than its ResNet counterparts. We expect that ResNeXt will also generalize well to other visual (and non-visual) recognition tasks.