Facial expressions represent one of the most important cues for recognizing non-verbal behaviour, being an important part of human social signalling _cite_ . The ability to automatically mine human intentions, attitudes or experiences has many potential applications like building socially aware systems _cite_, improving e-learning experiences _cite_, adapting game status according to player's emotional responses _cite_, detecting pain for monitoring patients _cite_ and detecting deception during police interrogations or job interviews _cite_ just to name a few. Result of the seminal research of Paul Ekman, the Facial Action Unit System (FACS) _cite_, is a descriptive coding scheme of facial expressions that focuses on what the face can do without assuming any cognitive or emotional value. Its basic components are called Action Units (AU) and they combine to form a complete representation of facial expressions. AUs are patterns of muscular activation and the way they modify facial morphology is relatively localized on the face (Fig. _ref_) . While initial methods for AU recognition (like JPML _cite_ and APL _cite_) were using shallow predefined representations, recent methods (like DRML _cite_, ROI _cite_ and GL _cite_) applied deep learning to learn richer local features that capture facial morphology. Therefore one could predict specific AUs from informative face regions adaptively selected depending on the facial geometry. For instance, contrary to non-adaptive methods like DRML _cite_ and APL _cite_, ROI _cite_ and JPML _cite_ extract features around facial landmarks which are more robust with respect to non-rigid shape changes. Patch learning is challenging as the human face is highly articulated and different face patches can contribute to either specific or groups of AUs. Learning the best patch combination together with learning specific features from each patch could be beneficial for AU recognition. Another key characteristic of AU recognition is that it is multi-label. Several AUs can be active at the same time and certain AU combinations are more probable than others (Fig. _ref_) . Taking into account such probabilistic dependencies, AU prediction performance could be improved. As in deep approaches, such correlations can be addressed implicitly in the fully connected layers (e.g. DRML _cite_, GL _cite_ and ROI _cite_) . However, structure is not learned in any explicit way and inference and sparsity are implicit by design. JPML _cite_ treats the problem by including pre-learned priors about AU correlations into their learning. Learning structured outputs has been studied by _cite_ using Graphical Models, mathematical models able to capture complex probabilistic high order inter-dependencies. However, these models are not end-to-end trainable. In this work, we claim patch and the multi-label learning are key problems in dealing with AU recognition. We propose a deep neural network that tackles those problems in an integrated way through an incremental and end-to-end trainable approach. First, the model learns local and holistic representations exhaustively from facial patches. Then it captures structure between patches by predicting specific AUs. Finally, AU correlations are captured by a structure inference network that replicates message passing inference algorithms in a connectionist fashion. Tab. _ref_ compares some of the most important features of the proposed method to the state-of-the-art (specifically JPML _cite_, APL _cite_, DRML _cite_, GL _cite_ and ROI _cite_) . We show that by separately treating problems in different parts of the network and being able to optimize them jointly, we improve state-of-the-art by N \% and N \% performance on BPND and DISFA datasets, respectively. Summarizing, our N main contributions are: N) we propose a model that learns representation, patch and output structure end-to-end, and N) we introduce a structure inference topology that replicates inference algorithm in probabilistic graphical models by using a recurrent neural network. The paper is organised as follows. Sec. _ref_ presents related work. Sec. _ref_ details the proposed model and Sec. _ref_ the results. Sec. _ref_ concludes the paper.