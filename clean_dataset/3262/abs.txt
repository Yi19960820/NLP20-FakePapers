We study the problem of recovering an underlying ND shape from a set of images. Existing learning based approaches usually resort to recurrent neural nets, \eg GRU, or intuitive pooling operations, \eg max/mean poolings, to fuse multiple deep features encoded from input images. However, GRU based approaches are unable to consistently estimate ND shapes given different permutations of the same set of input images as the recurrent unit is permutation variant. It is also unlikely to refine the ND shape given more images due to the long-term memory loss of GRU. Commonly used pooling approaches are limited to capturing partial information, \eg max/mean values, ignoring other valuable features. In this paper, we present a new feed-forward neural module, named AttSets, together with a dedicated training algorithm, named FASet, to attentively aggregate an arbitrarily sized deep feature set for multi-view ND reconstruction. The module is permutation invariant, computationally efficient and flexible to implement, while the algorithm enables the based network to be remarkably robust and generalize to an arbitrary number of input images. We thoroughly evaluate and the properties of on multiple large public datasets. Extensive experiments show that together with algorithm significantly outperforms existing aggregation approaches.