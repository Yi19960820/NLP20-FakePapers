Scene parsing, based on semantic segmentation, is a fundamental topic in computer vision. The goal is to assign each pixel in the image a category label. Scene parsing provides complete understanding of the scene. It predicts the label, location, as well as shape for each element. This topic is of broad interest for potential applications of automatic driving, robot sensing, to name a few. Difficulty of scene parsing is closely related to scene and label variety. The pioneer scene parsing task~ _cite_ is to classify N scenes for N, N images on LMO dataset~ _cite_ . More recent PASCAL VOC semantic segmentation and PASCAL context datasets~ _cite_ include more labels with similar context, such as chair and sofa, horse and cow, etc. The new ADENK dataset~ _cite_ is the most challenging one with a large and unrestricted open vocabulary and more scene classes. A few representative images are shown in Fig.~ _ref_ . To develop an effective algorithm for these datasets needs to conquer a few difficulties. State-of-the-art scene parsing frameworks are mostly based on the {\it fully convolutional network} (FCN) ~ _cite_ . The deep {\it convolutional neural network} (CNN) based methods boost dynamic object understanding, and yet still face challenges considering diverse scenes and unrestricted vocabulary. One example is shown in the first row of Fig.~ _ref_, where a {\it boat} is mistaken as a {\it car} . These errors are due to similar appearance of objects. But when viewing the image regarding the context prior that the scene is described as {\it boathouse} near a river, correct prediction should be yielded. Towards accurate scene perception, the knowledge graph relies on prior information of scene context. We found that the major issue for current FCN based models is lack of suitable strategy to utilize global scene category clues. For typical complex scene understanding, previously to get a global image-level feature, spatial pyramid pooling~ _cite_ was widely employed where spatial statistics provide a good descriptor for overall scene interpretation. Spatial pyramid pooling network~ _cite_ further enhances the ability. Different from these methods, to incorporate suitable global features, we propose {\it pyramid scene parsing network} (PSPNet) . In addition to traditional dilated FCN _cite_ for pixel prediction, we extend the pixel-level feature to the specially designed global pyramid pooling one. The local and global clues together make the final prediction more reliable. We also propose an optimization strategy with deeply supervised loss. We give all implementation details, which are key to our decent performance in this paper, and make the code and trained models publicly available . Our approach achieves state-of-the-art performance on all available datasets. It is the champion of ImageNet scene parsing challenge N~ _cite_, and arrived the Nst place on PASCAL VOC N semantic segmentation benchmark~ _cite_, and the Nst place on urban scene Cityscapes data~ _cite_ . They manifest that PSPNet gives a promising direction for pixel-level prediction tasks, which may even benefit CNN-based stereo matching, optical flow, depth estimation, etc. in follow-up work. Our main contributions are threefold.