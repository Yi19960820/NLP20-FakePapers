The revival of neural networks in the form of deep learning has been at the basis of significant breakthroughs in various application domains, including speech recognition and computer vision. In the context of robotics, however, adoption of deep learning methods seems to happen at a slower pace and is met with more skepticism. Several complicating factors may be at the basis of this phenomenon. First and foremost, robotics involves embodied physical systems. This implies that datasets cannot so easily be shared, as they tend to be robot-specific. Data collection is thus considerably more time consuming, even more so since we are dealing with active systems, which interact with their environment. This high burden in terms of data collection hampers progress, given the data-hungry nature of deep neural networks. Recently, however, has demonstrated that a control network for single-image obstacle avoidance trained solely in simulation can generalize to the real world. In this work, we experiment in a simulated environment, focusing on the basics, assuming that the step to the real world can be solved in a similar manner. On top of the difficulty of data collection, there are the traditional objections with respect to neural networks, such as the non-convexity of the parameter spaces resulting in local minima; the lack of interpretation of what the network has actually learned; and the large number of hyperparameters which need to be set. On the other hand, neural networks hold a lot of promises, also for robotics applications. In particular, they cope well with high-dimensional input data; they can learn the optimal representation for a given task, instead of relying on handcrafted features; and they are universal function approximators. Finally, they are highly non-linear, as is the world and (presumably) the control needed in such world. Most importantly, the introduction of (deep) learning in robotics holds the promise of going beyond the currently dominating model-driven, metric approach to robotics. Indeed, while such model-driven approaches work well for low-level control and/or for robot operations in a highly structured and controlled environment, they reach their limits when it comes to more flexible systems which need to adapt to their environment in a smart way, or need to interact with people. For high-level tasks, it may be easier to just show examples of how one would like the robot to behave, and learn directly from such data, rather than handcrafting features, finite-state machines, rules and algorithms implementing the intended behavior. In an ideal setting, learning a new task then boils down to collecting representative data, together with the desired outputs. In particular, our long term goal is a framework, in which one can train an unmanned aerial vehicle (UAV) to perform a wide range of high-level navigation tasks, based on {\em imitation learning} . That is, the system learns how to perform a task based on training data, in which an expert steers the drone and demonstrates the desired behavior, similar to apprenticeship learning . Note that we exclude low-level tasks such as attitude control like, for which we rely on standard algorithms which come with most commercial drones. Instead, we focus on the higher-level task of navigation, i.e. steering the drone. High-level tasks we would like our framework to learn could vary from flying a fixed route, avoiding obstacles, passing through a door or following a corridor to tracking a person, recording a high-jump or inspecting a windmill. Moreover, we want to achieve this goal using a {\em forward looking camera as the only sensor} . Indeed, experience from human pilots performing such tasks shows that the input from such camera over time contains enough information. Cameras can be made very light, both physically and power consumption wise. They are also not limited to a certain range unlike active sensors. Additional sensors might simplify some problems, yet bring extra weight which reduces the flight time. At test time, the system should then be able to steer the drone and perform the task, based on the video input stream only, under conditions similar to those seen at training time. For now, as a first step in that direction, we focus on a single, relatively simple task: traversing a room, with three known obstacles (a bump in the floor, a wall on the left or right hand side and an obstacle hanging from the ceiling)--see Figure~ _ref_ . The order in which the obstacles appear, is fixed for some experiments and variable for others; their dimensions (i.e. the height of the bump/overhang and the length of the wall) always vary. This somewhat mimics a setting where a drone flies in an unknown environment, but is given high-level instructions so knows roughly what to expect or how to cope with certain obstacles. As indicated earlier, to easily generate different rooms and for ease of experimentation, we limit ourselves to a virtual world only. Moreover, instead of manually flying the UAV in this world to generate training data, we place additional virtual sensors on the drone, based on which a behavior arbitration algorithm for this particular task can be developed relatively easily. This algorithm serves as expert in our experiments. This saves time during experimentation and ensures reproducibility of the results. Within this setting, we then explore the impact of various design choices and the effect of different training methods. In particular, some of the questions we try to answer in this paper include: We focus especially on the first question, i.e. the introduction of networks with a memory. Applications of neural networks for robot control in the real world are mainly limited to memory-free feedforward networks~ . Yet we believe that for high-level tasks, some form of memory or inner state is actually needed. In our setting, one cannot expect the forward-looking camera to always provide enough information to take the proper action without such context. The memory provided by an LSTM can help the control network to take the right decisions. For instance, the network can learn robustness to temporal distortions like delays which are common in real-time applications. Or, it can remember the drone is in the middle of a complex maneuver (e.g. overtaking or moving away from an obstacle), even if the current input is ambiguous. Besides, the state can be extracted from both temporal as well as spatial features and there is no theoretical boundary on the time-span of the memory. The main difficulty with sequential prediction problems, like navigation control, is the high correlation between the samples. This makes training a network, especially an LSTM, challenging. In this work we study how to successfully train an LSTM. In this context, we propose a new sampling scheme, which we coin window-wise truncated backpropagation through time (WW-TBPTT) . This addresses the second question. There is another issue, specific to imitation learning. In a naive approach, training data is collected offline, with the expert controlling the drone. This data is used to train a model which is then applied at test time. However, navigation control is an active system. Once the student, in our case the neural network, provides the control, it is likely to make mistakes never made by the expert. This brings the drone in situations never seen during training. Special strategies are needed to learn how to recover from these mistakes. We explore different methods to cope with this state-space distribution shift: we experiment with DAgger~, which stands for data aggregation and we test the use of recovery cameras during training, as used by _cite_ and _cite_ . This addresses the third question. It has been shown that convolutional neural networks (CNN) are capable of learning to estimate the optical flow _cite_ or depth _cite_ from an RGB image. With end-to-end learning the network can define a proper state representation combined with the proper control. Using an RNN allows to build both temporal as well as spatial representations. Yet end-to-end learning is especially data-hungry. To tackle the fourth question, we compare different networks, either trained end-to-end or starting from a pretrained network and retraining only the last control layers. For the latter, we build on a standard image classification network. The fifth and final question about guidelines and best practices is addressed throughout the entire paper and experimental setup. The main contributions of this paper can then be summarized as follows: i) the successful demonstration of UAV control based on LSTM in a navigation task using imitation learning, including a novel sampling method during training; ii) a synthetic dataset and baselines for a specific use case, namely learning to cross a virtual room containing various obstacles, with a behavior arbitration algorithm as expert; and iii) a study of how to train neural networks for such a control task, resulting in guidelines and good practices which may be helpful for other researchers. The remainder of the paper is organized as follows. First, we describe related work (Section~ _ref_) . Next, we give more details on the standard network architectures and training methods we will be building on (Section~ _ref_) . In Section~ _ref_, we first give more details on the overall setup (Section~ _ref_), the particular tasks we are addressing and the dataset used (Section~ _ref_) . Then, we explain the behavior arbitration system we will be using as expert in our experiments (Section~ _ref_) . After that, we propose an alternative sampling method for LSTMs, window-wise truncated backpropagation through time (WW-TBPTT) (Section~ _ref_) . Section _ref_ covers the implementation details. In Section~ _ref_ we describe our experimental results, and Section~ _ref_ concludes the paper.