The goal of visual texture synthesis is to infer a generating process from an example texture, which then allows to produce arbitrarily many new samples of that texture. The evaluation criterion for the quality of the synthesised texture is usually human inspection and textures are successfully synthesised if a human observer cannot tell the original texture from a synthesised one. In general, there are two main approaches to find a texture generating process. The first approach is to generate a new texture by resampling either pixels _cite_ or whole patches _cite_ of the original texture. These non-parametric resampling techniques and their numerous extensions and improvements (see _cite_ for review) are capable of producing high quality natural textures very efficiently. However, they do not define an actual model for natural textures but rather give a mechanistic procedure for how one can randomise a source texture without changing its perceptual properties. In contrast, the second approach to texture synthesis is to explicitly define a parametric texture model. The model usually consists of a set of statistical measurements that are taken over the spatial extent of the image. In the model a texture is uniquely defined by the outcome of those measurements and every image that produces the same outcome should be perceived as the same texture. Therefore new samples of a texture can be generated by finding an image that produces the same measurement outcomes as the original texture. Conceptually this idea was first proposed by Julesz _cite_ who conjectured that a visual texture can be uniquely described by the Nth-order joint histograms of its pixels. Later on, texture models were inspired by the linear response properties of the mammalian early visual system, which resemble those of oriented band-pass (Gabor) filters _cite_ . These texture models are based on statistical measurements taken on the filter responses rather than directly on the image pixels. So far the best parametric model for texture synthesis is probably that proposed by Portilla and Simoncelli _cite_, which is based on a set of carefully handcrafted summary statistics computed on the responses of a linear filter bank called _cite_ . However, although their model shows very good performance in synthesising a wide range of textures, it still fails to capture the full scope of natural textures. In this work, we propose a new parametric texture model to tackle this problem (Fig.~ _ref_) . Instead of describing textures on the basis of a model for the early visual system _cite_, we use a convolutional neural network--a functional model for the entire ventral stream--as the foundation for our texture model. We combine the conceptual framework of spatial summary statistics on feature responses with the powerful feature space of a convolutional neural network that has been trained on object recognition. In that way we obtain a texture model that is parameterised by spatially invariant representations built on the hierarchical processing architecture of the convolutional neural network.