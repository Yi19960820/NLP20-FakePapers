This work targets supervised regression problems, where the aim is to use one source of data as input to correlate and predict a different kind of information as output. The basic statistical approach to regression, i.e. maximum-likelihood estimation, assumes sufficient smoothness of the input-output mapping, meaning that nearby inputs lead to nearby outputs. In practical scenarios however, this assumption is often violated, as the mapping to be learned may vary wildly within a small input region, or even be multi-valued . In such scenarios, statistical regression fails at accurately predicting a single output value, and instead returns an average over all possible output hypotheses seen during training. While this trait is acceptable in some cases, in other applications a different behavior is preferred, which pinpoints one (or ideally all) of the modes of the output hypotheses. Consider for instance the task, for which the goal is to determine the position and orientation of a camera device given an acquired camera frame. This task can be formulated as supervised regression, however there are no general guarantees of smoothness or bijectivity concerning the image-pose mapping. In fact, an environment might well contain visually-similar landmarks, yet their images with similar appearances could map to drastically different poses. In this case a mode-averaged prediction would localize to somewhere between the similar landmarks, which generally would not be an useful piece of information. As an extreme example, consider a camera observing a N-D world with two identical hills. Given an image of a single hill, there is insufficient information to disambiguate which of the two landmarks the camera is facing, and the task itself is ill-defined. Moreover, a maximum-likelihood model trained only on single-hill images of both landmarks would predict the pose to be in between the two hills, thus exhibiting behavior, as illustrated in Figure~ _ref_ . However, we can still extract useful information from the image by reformulating the problem. For example, we can aim to predict a set of possible positions. Alternatively, given a sufficiently strong prior of the camera's position, we may be able to resolve the ambiguity. Or, even with any prior information, we can at least predict its most likely pose. This work aims to realize the last approach. For the two-hills toy instance, if we have a rough estimate of the camera's position, then we can naturally assume it to be more likely to face the hill closest to this estimate. By changing the regression goal to predict the most likely position given the estimate, the image-to-pose mapping will then be deterministically well-defined, with a sole exception when the hint is at the exact center of the two hills. Importantly, even if the estimate is completely random, this model will still produce a correct answer half the time, which might be qualitatively favorable compared to statistical regression being always . More generally, the Hinted Networks transformation reformulates a statistical regression task into a different learning problem with the same inputs and outputs, albeit with an added at the input that provides an estimate for the output. While this reformulation works best when a prior exists, our key finding is that Hinted Networks are useful even in the absence of informed priors or auxiliary information. In this work, we present a training procedure that uses synthetically-generated hints at training time, and uninformed hints at query time to improve regression performance. We devise two strategies for using the hint:, which uses the hint as a conditioning prior to resolve ambiguities in the input data, and, which builds upon Hinted Embedding by converting the learning task from absolute-value into residual regression. A key motivation and benefit of Hinted Networks is their behaviors. Also, since the hint and output have the same representation, this feed-forward model can be improved through iterative refinement, by recurrently feeding predictions as successive hints to refine accuracy. Our investigations of Hinted Networks are grounded in camera relocalization tasks. Such ability to localize camera images is invaluable for diverse applications, for example replacing Global Positioning System (GPS) and Inertial Navigation System (INS), providing spatial reference for Augmented Reality experiences, and helping robots and vehicles self-localize. In these setups, often there are auxiliary data sources that can help with localization, such as GPS and other sensors, as well as temporal odometry priors. Hinted Networks are motivated by the use of auxiliary sources as informative hints to facilitate general regression problems. Hinted Networks build upon several well-established concepts in machine learning. For instance, Hinted Embedding networks apply the hint as a conditioning variable, which is analogous to the use of prior in Bayesian inference~ _cite_ . Nevertheless, our formulation can be seen as a alternative to Maximum A Posteriori or full Bayesian inference, as we sacrifice the advantages of learning a probabilistic distribution in favor of a simpler deterministic mapping. Additionally, residual connections such as the ones used in Hinted Residual networks have been shown to often improve training speed and accuracy~ _cite_ . Finally, our use of recurrent inference is structurally similar to plain Recurrent Neural Networks (RNN) ~ _cite_, although it fulfills a different purpose of iterative refinement. Our main contribution is to demonstrate improved prediction accuracies for terrestrial localization tasks using Hinted Networks. In particular, we use uninformed hints at query time to fairly compare against an appearance-only baseline method, PoseNet~ _cite_ . We also explore the benefits of Hinted Networks for aerial-view localization tasks within simulated large-scale environments, with emphasis on repeated patterns, seasonal variations and high-density training samples. Notably, we find Hinted Networks to significantly outperform PoseNet for this task.