Learning visual representations from large unlabeled data has been an area of active research in computer vision. In this context, the goal is to learn a representation that describes the remarkable semantic features of an image. A method that can learn such representation may be adopted by a variety of supervised learning tasks such as visualization, regression, and classification. Generative models, and more specifically Generative Adversarial Networks (GANs), with no doubt, are among the most powerful techniques of unsupervised representation learning. The underlying belief of the generative frameworks is that the ability to synthesize an observed data encompasses some sort of understanding. In practice, however, GANs are not able to learn a meaningful representation of the training dataset without additional constraints. To sidestep this problem, a long line of work proposed different frameworks to learn interpretable and meaningful latent representations in an unsupervised setting, such as InfoGAN~ _cite_, BiGAN~ _cite_, and ALI~ _cite_, or a supervised setting _cite_ . Despite all the effort in this area, these approaches ignore one of the most fundamental principles of image generation, which is the disentanglement of the scene's content and style. A scene's content, here, represents its underlying geometry, while the style encodes the texture and illumination. Wang et al. _cite_ propose to decomposes the GAN latent code into two separate structure and style codes. However, their method requires the depth information and does not generalize to RGB datasets. In contrast, in this paper we propose a universal GAN framework, which does not need the depth information, to learn disentangled content and style codes. Factoring the style and content enhance the interpretability of the learned representations, compared to the counterparts, improves the performances of learning tasks which are reliant on the sole content or style representation, and enables the user to generate a specific scene with a variety of styles, or several images of a particular style. Learning disentangled style and content codes is a challenging task. Several algorithms have been developed for solving this class of problems, however, they train GAN in a supervised fashion, or in its conditional setting. In addition, these frameworks address the problem of image-to-image translation, that is an image of a source domain, e.g., edges, represents the content, and a style code is learned for the target domain. Despite all the effort in this area, the field still lacks a coherent framework for unsupervised disentangled style and content representations learning. From this consideration, in this paper, we propose an end-to-end framework to learn a pair of disentangled content and style codes for a given dataset. Note that our framework makes the assumption that each image of the training dataset can be decomposed into content and style codes. Our framework consists of a single generator, in contrast to _cite_, whose input is the content code. The generator has several upsampling, convolutional layers, and a set of residual blocks. Inspired by a recent work which showed that parameters of affine transformation in normalization layers represent the styles, we equip each residual block with an Adaptive Instance Normalization (AdaIN) layer. The parameters of the AdaIN layers then generated by a multilayer perceptron (MLP) whose input is the desired style code. We develop a training strategy and discuss the required considerations for its success, to train the MLP and generator jointly. Note that, with minimum effort, the proposed framework can be adopted by any GAN structure. Finally, by learning to invert the generator, similar to BiGAN _cite_, we can scale up the applications of our framework to transferring style or content among images of the training domain. The main contributions of our work are fourfold: