Convolutional neural networks (CNNs) have witnessed great improvement on a series of vision tasks such as object classification _cite_, scene understanding _cite_, and action recognition _cite_ . As for the face recognition task, CNNs like DeepIDN + _cite_ by Yi Sun, FaceNet _cite_, DeepFace _cite_, Deep FR _cite_, have even proven to outperform humans on some benchmarks. To train a robust deep model, abundant training data _cite_ and well-designed training strategies are indispensable. It is also worth to point out that, most of the existing training data sets like LSVRC's object detection task _cite_, which contains N basic-level categories, were carefully filtered so that the number of each object instance is kept similar to avoid the long tailed distribution. More specifically, long tail property refers to the condition where only limited number of object classes appear frequently, while most of the others remain relatively rarely. If a model was trained under such an extremely imbalanced distributed dataset (in which only limited and deficient training samples are available for most of the classes), it would be very difficult to obtain good performance. In other words, insufficient samples in poor classes/identities will result in the intra-class dispension in a relatively large and loose area, and in the same time compact the inter-classes dispension _cite_ . In _cite_, Bengio gave the terminology called ``representation sharing'': human possess the ability to recognize objects we have seen only once or even never as representation sharing. Poor classes can be beneficial for knowledge learned from semantically similar but richer classes. While in practice, other than learning the transfer feature from richer classes, previous work mainly cut or simply replicate some of the data to avoid the potential risk long tailed distribution may cause. According to _cite_ 's verification, even only N \% of positive samples are left out for feature learning, detection performance will be improved a bit if the samples are more uniform. Such disposal method's flaw is obvious: To simply abandon the data partially, information contained in these identities may also be omitted. In this paper, we propose a new loss function, namely range loss to effectively enhance the model's learning ability towards tailed data/classes/identities. Specifically, this loss identifies the maximum Euclidean distance between all sample pairs as the range of this class. During the iteration of training process, we aim to minimize the range of each class within one batch and recompute the new range of this subspace simultaneously. The main contributions of this paper can be summarized as follows: N. We extensively investigate the long tail effect in deep face recognition, and propose a new loss function called range loss to overcome this problem in deep face recognition. To the best of our knowledge, this is the first work in the literature to discuss and address this important problem. N. Extensive experiments have demonstrated the effectiveness of our new loss function in overcoming the long tail effect. We further demonstrate the excellent generalizability of our new method on two famous face recognition benchmarks (LFW and YTF) .