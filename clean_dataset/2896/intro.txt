Learning to control artificial agents in simulated environments has potential applications in numerous fields like computer graphics, human-computer interaction, etc. For instance, it can be possible to teach a control policy for artificial agents in games from expert video demonstrations of other humans without hard-coding the specific rules to achieve a certain task. Traditionally, such learning-from-demonstration algorithms learn from low-level information like joint angles and velocities, which have to be carefully acquired from the expert agent. A more natural way for imitation learning involves directly learning from visual demonstrations of the expert without access to such low-level information. In this paper, we present an imitation learning algorithm that can mimic raw videos directly and show that under certain conditions learning directly from raw videos is equivalent to learning from low-level information. Prior methods in this field have focused on learning from expert demonstration~ _cite_ or imitation learning~ _cite_ . In most conventional methods, a set of expert trajectories consisting of both state and action information is available which is used to mimic the expert behavior by maximum likelihood estimation. Attempts have also been made towards learning from observations in the absence of action information from low-level motion capture data~ _cite_ . Existing methods for learning from videos focus on extracting a high-level feature embedding of the image frames ensuring that pair of co-located frames lie close in the embedding space compared to the ones having higher temporal distance. This is then followed by a reward estimation step based on some hand-crafted distance metric between the feature embeddings of the agent and expert images. Additionally, these methods make a restrictive assumption that the agent's trajectories need to be time-synchronized to the expert for reward estimation. Our adversarial imitation learning method addresses both of the above issues faced in conventional methods. Firstly, we jointly learn the high-level feature embeddings and agent-expert distance metric removing the need for hand-crafted reward shaping. Specifically, we use a generative adversarial network (GAN, _cite_) framework with the policy network as the generator along with a discriminator, which performs binary classification between the agent and expert trajectories. The reward signal is extracted from the discriminator output indicating the discrepancy between the current performance of the agent with the expert. The overview of our method is shown in Figure _ref_ . Secondly, since the binary classifier is trained by randomly sampling the agent and expert trajectories, there is no need for time synchronization as well. Our main contribution in this paper is to establish a connection between learning from low-level state trajectories and high-dimensional video frames, which enables us to develop a visual adversarial imitation learning algorithm. Specifically, we show that if there exists an injective mapping between the state trajectories and video frames, adversarial learning from raw videos is equivalent to learning from state trajectories. Thus, adversarial methods suitable for learning from low-level state trajectories can be also applied to policy learning from raw videos. This provides the theoretical background supporting our proposed visual imitation algorithm. Empirical evaluations show that our proposed method can successfully imitate raw video demonstrations producing a performance similar to state-of-the-art imitation learning techniques even though it uses both state and action information in expert trajectories. Furthermore, we empirically show that the policies learned by our method outperforms other video imitation methods and is robust in the presence of noisy expert demonstrations. Lastly, we demonstrate that the proposed method can successfully learn policies that imitate video demonstrations available on YouTube.