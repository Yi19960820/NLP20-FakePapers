As robots will become more integrated in modern societies, the cases of interacting with humans during daily life activities and tasks are increasing _cite_ . Human-Robot Interaction (HRI) refers to the communication between robots and humans. This communication can be verbal or non-verbal, remote or proximal. A special case of HRI is CRI _cite_ . Robots enter children's lives as companions, entertainers or even educators _cite_ . Children are very adaptive, quick learners and familiarized with new technologies. They have unique communication skills, as they can easily convey or share complex information with little spoken language. In _cite_ developed and evaluated systems a proposed, which employ multiple sensors and robots, for childrens' speech, gesture and action recognition during CRI scenarios. However, a major challenge is to acquire and maintain the child's engagement and attention in a CRI task _cite_ . Robots assisting children is of particular importance in modern research, especially for ASD mediated therapy towards the development of their social skills, _cite_ . Children affected by autism spectrum disorder (ASD) can benefit from interacting with robots, since such a CRI may help them overcome the impediments posed by face-to-face interaction with humans. Moreover, it is important that the robot's behaviour can adapt to the special needs of each specific child and maintain an identical behaviour for as long as needed in the intervention process _cite_ . One key issue for social robots is the development of their ability to evaluate several aspects of interaction, such as user experience, feelings perceptions and satisfactions _cite_ . Human engagement in Human-Robot Interaction (HRI) according to _cite_ ``is a category of user experience characterized by attributes of challenge, positive affect, endurability, aesthetic and sensory appeal, attention, feedback, variety/novelty, interactivity, and perceived user control''. Poggi in _cite_ specified more by adding that engagement is the level at which a participant attributes to the goal of being together with other participants within a social interaction and how much they continue this interaction. Given this rich notion of engagement, many studies have explored human-robot engagement _cite_ . Lemaignan et. al explored the level of ``with-me-ness", by measuring to what extent the human is with the robot during an interactive task, for assessing the engagement level. Research in HRI has shown a growing interest in modeling human engagement, evaluating speech and gaze _cite_, based solely on gaze in a human-robot cooperative task _cite_, and human pose with respect to robot from static positions _cite_ . Engaging children in CRI tasks is of great importance. The social characteristics that a robot should have when performing as tutors were examined in _cite_ . Specific focus is given in estimating the engagement of children with ASD interacting with adults _cite_ or robots _cite_ . A study analyzing the engagement of children participating in a robot-assisted therapy can be found in _cite_ . A method for the automatic classification of engagement with a dynamic Bayesian network using visual and acoustic cues and support vector machine classifiers is described in _cite_ . Another approach considers the facial expressions of children with ASD to evaluate their engagement _cite_ . A robot-mediated joint attention intervention system using as input the child's gaze is presented in _cite_ . A deep learning framework for estimating the child's affective states and engagement is presented in _cite_ . These can then be used to optimize the CRI and monitor the therapy progress. In our previous work we have used reinforcement learning for adapting the robot’s behaviors and actions according to the child’s engagement, evaluated in real-time by an expert observing the child, for achieving joint attention on collaborative tasks _cite_ . In this paper we are exploring a deep learning based approach for estimating the engagement of Children in a CRI collaborative task aiming to establish joint attention between the child and the robot. The robot tries to elicit behaviors on children while interacting with them. The robot aims to achieve joint attention with the child through an experiment that tests a primitive social skill that includes attention detection from both agents, attention manipulation from the robot-agent to the child and social coordination in terms of engaging the child-agent on a handover task, resulting in the intentional understanding of the robot-agent's intentions and ultimately a successful collaboration. Our method incorporates a multi-view deep-based estimation of the child's pose, when the child is inside a specially arranged room (Fig. _ref_), interacts with the robot and has the ability to move freely, i.e. the child is not in a stationary position in front of the robot and the sensor. Thanks to the network of cameras, placed elegantly in the room, the child gets the feeling of being in its room and is left free to interact with the robot, without being restricted in front of a camera as in other works in literature. The multi-view fusion helps in confronting cases of body part occlusions and provides better pose estimations. An LSTM-based classifier which uses as input the child's pose is trained using as control targets observations by experts and classifies the engagement of the child to the task. We experimentally validate our algorithms exploiting the RGB-D data of children who participated in the experimental scenario of the interaction task. The ultimate goal is to use this framework for estimating the engagement of children in various CRI tasks. We aim to use the engagement information in a robot reinforcement learning framework which uses the engagement monitoring estimates as a reward signal during the non-verbal social interaction, for adapting the robot's motion combinations and their level of expressivity towards maximizing the child-robot joint attention _cite_ in various collaborative tasks _cite_ both for TD and ASD children.