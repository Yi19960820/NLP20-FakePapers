Unsupervised learning, where no labels are provided during training, remains one of the core challenges in machine learning. One branch of unsupervised learning that is popular in image processing/computer vision is the generative models that aim to obtain a representation of the density function, _inline_eq_ to describe the density of a given data domain, _inline_eq_ . Auto-regressive models such as Fully Visible Belief Nets _cite_, PixelRNN _cite_, explicitly model the joint distribution of pixels as a product of conditional distributions and optimize the likelihood of training data. However, due to higher dimension and the structured formation of images, modelling the long-range pixel correlation becomes challenging for these explicit tractable density models. Instead of designing an explicit tractable model, Variational Autoencoder (VAE) _cite_) approximates the likelihood by maximizing a lower bound. This is achieved with an encoder-decoder neural network architecture. The encoder's output fits a prior distribution by minimizing the KL-divergence. The decoder then transforms encoder's output to reconstruct the images. In practice, they often achieve higher likelihood values, however, fails to generate sharp images _cite_ . Generative Adversarial Networks (GANs) offer an alternative solution that draw samples from _inline_eq_ to learn an implicit density function through a minimax formulation _cite_ . However, GANs often suffer from training instability _cite_ . Although recent progresses for GANs aim to address this issue by replacing the Jensen-Shannon divergence (JSD) _cite_, the new formulations generally can not address the mode collapsing problem where the generator can only accommodate a few modes in the training domain _cite_ . Tricks such as feature matching, mini-batch discrimination _cite_ or by unrolling multiple steps during the gradient update in training _cite_, handle this issue to some extent. In this paper, we develop a deep generative model that shares the properties of both explicit density models such as PixelRNN, VAE as well as implicit density models such as GAN. We propose a Normal Similarity Network (NSN) with multiple hidden layers where every layer is constructed with learnable Gaussian-style filters. Figure _ref_ presents a schematic diagram of NSN. The proposed model downsamples the images and explicitly captures the density in a transformed domain via likelihood maximization. Further, we propose a sampling approach, NSN-Gen, to generate new images from NSN. NSN-Gen takes a noise vector as input at the final layer and performs a backward pass to generate the pixels in parallel. In our experiments, we demonstrate the applicability of NSN to a wide range of tasks including image generation, image styling and reconstruction from occluded images. Table _ref_ provides a comparison of the characteristics of our proposed NSN with existing deep generative models. In terms of model complexity, NSN requires to maintain only a single network constructed with Gaussian-like filters, whereas GANs or VAEs require two or more neural networks. Furthermore, PixelRNN, VAE and GAN all are constructed as a parametric network. In contrast, NSNs support non-parametric training that can automatically detect the required number of filters at each layer of the network. The NSNs are trained with a variant of the Expectation Maximization (EM) algorithm which has been shown to have a direct and stable density estimation process _cite_ . In addition, our generation algorithm, NSN-Gen, can be applied at any hidden layer to visualize the image patches generated at that layer, thus enabling fine tuning of the network.