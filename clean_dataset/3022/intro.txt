Human actions can be categorized by the visual appearance and motion dynamics of the involved humans and objects. The design of many popular human action recognition datasets _cite_ is based on this intrinsic property. To recognize human actions in video sequences, computer vision researchers have been developing better visual features to characterize the spatial appearance _cite_ and temporal motion _cite_ . Since video sequences can naturally be viewed as three-dimensional~ (ND) spatio-temporal signals, many existing methods seek to develop different spatio-temporal features for representing spatially and temporally coupled action patterns _cite_ . Thus far, although these methods are robust against some real-world human action conditions, when applied to more realistic, complex human actions, their performance often degrades significantly due to the large intra-category variations within action categories and inter-category ambiguities between action categories. A number of factors can cause large intra-category variations. Some major ones include large variations in visual appearance and motion dynamics of the constituent humans and objects, arbitrary illumination and imaging conditions, self-occlusion, and cluttered background. To address these challenges, some methods~ _cite_ extract trajectories of interest points from video sequences to characterize the salient spatial regions and their motion dynamics. However, in general, the challenge of recognizing complex human actions has not been well addressed. Most of the above methods use handcrafted features and relatively simple classifiers. More recently, the end-to-end approach of learning features directly from raw observations using deep architectures shows great promise in many computer vision tasks, including object detection _cite_, semantic segmentation _cite_ and so forth. Using massive training datasets, these deep architectures are able to learn a hierarchy of semantically related convolution filters (or kernels), giving highly discriminative models and hence better classification accuracy~ _cite_ . In fact, even directly applying these image-based models to individual frames of the videos has shown promising action recognition performance _cite_, because the learned features can better characterize the visual appearance in the spatial domain. However, human actions in video sequences are ND spatio-temporal signals. It is not surprising to expect that exploiting the temporal domain as well could further advance the state of the art. Some recent attempts have been made along this direction _cite_ . The ND CNN model _cite_ learns convolution kernels in both space and time based on a straightforward extension of the established ND CNN deep architectures _cite_ to the ND spatio-temporal domain. The methods in _cite_ aim at learning long-range motion features by learning a hierarchy consisting of multiple layers of ND spatio-temporal convolution kernels by early fusion, late fusion, or slow fusion. The two-stream CNN architecture _cite_ learns motion patterns using an additional CNN which takes as input the optical flow computed from successive frames of video sequences. By using the optical flow to capture motion features, the two-stream is less effective for characterizing long-range or ``slow'' motion patterns which may be more relevant to the semantic categorization of human actions _cite_ . Possibly due to the increased complexity and difficulty of training ND kernels without sufficient training video data (as compared to massive image datasets~ _cite_), ND CNN _cite_ does not perform well even on the less challenging KTH dataset~ _cite_ . For the UCF-N benchmark dataset~ _cite_, we note that the results reported in~ _cite_ are inferior to those obtained by two-stream ~ _cite_ . Indeed, spatio-temporal action patterns coupling the visual appearance and motion dynamics generally need an order of magnitude more training data than the ND spatial counterparts. Moreover, existing methods often overlook the issue of sequence alignment in which actions of different speeds and accelerations have to be handled properly for human action recognition. The above analysis motivates us to consider alternative deep architectures which can handle ND spatio-temporal signals more effectively. To this end, we propose a new deep architecture called (_inline_eq_) . A schematic diagram of _inline_eq_ is shown in Figure~ _ref_ . While details of _inline_eq_ will be presented in the next section, we summarize the key characteristics and main contributions of _inline_eq_ as follows. In summary, _inline_eq_ is a cascaded deep architecture stacking multiple lower SCLs, a T-P operator, and an upper TCL. An additional SCL is also used in parallel with the TCL, aiming at learning a more abstract feature representation of spatial appearance. With the fully-connected (FC) and classifier layers on top, the whole _inline_eq_ can be trained globally using back-propagation _cite_ . Extensive experiments on benchmark human action recognition datasets _cite_ show the efficacy of _inline_eq_ .