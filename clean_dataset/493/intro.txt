Visual attention enables the human visual system to efficiently process the flood of visual information entering the retina. This mechanism enables the visual system to focus resources on the most relevant locations in the scene. The tendency of a particular region in a scene to receive attentional focus can be represented by the visual saliency of that region. The study of computational models of this mechanism may increase our understanding of the underlying biological mechanisms, but also can lead to applications in computer vision. In these applications, saliency can be used for the same purpose as in the human visual system: to focus resources on the key parts of the scene for more efficient processing of the visual world. This has found application in object detection _cite_, scene recognition _cite_, and robotic navigation~ _cite_, among others. Visual attention is often said to be composed of top-down and bottom-up components _cite_ . Existing deep neural network based models of visual saliency can be said to encode both factors: bottom-up information can be manifested in the outputs from filters in early layers of a convolutional neural network, and top-down information (such as the location of faces) can manifest itself in later layers. However, top-down information can go beyond recognition of familiar objects to include prior experience _cite_, scene semantics and context _cite_ and task information~ _cite_ . Task and prior experience are difficult to model, so we focus our attention on scene context. Several studies show the effect of scene context on saliency in the form of contextual cueing _cite_ . In these experiments, the time to visually locate a target is reduced when the target appears in a previously seen arrangement of distractors _cite_, in consistent global colors _cite_, or more generally in natural images _cite_ . Contextual cueing tells us that humans adapt and learn contextual information to find more optimal visual search strategies. Similarly, we propose a computational model of attention that can adapt to different contextual information. We introduce a saliency model that learns a measure of global scene contextual information. In our model, global scene information corresponds to different categories of image stimuli (e.g., natural images, fractal patterns, etc.) . We posit that these categories are varied enough to induce different saliency mechanisms. Therefore we learn a different prediction for each category. These predicted saliency maps are generated using an efficient tree structure to share common bottom-up features, and diverge in later layers to compute semantic higher-level features. A context guided gating network decides, given an unknown image, weights to give to each prediction. The context gating network and the category-specific saliency predictions are implemented as convolutional neural networks that can be jointly trained. We build our model on a recent deep-learning based model _cite_, and show that the additional global scene information leads to greatly increased performance. Early models of visual saliency use biological analogs or concepts from information theory. We can think of these methods as ``unsupervised'' because they do not use any training data. One of the first computational models of visual attention was the Itti model _cite_ . This model is based on biological principles of center-surround and feature integration theory. The Itti model was extended by using random walks on a graph structure in the GBVS model~ _cite_ . Another class of models use information theoretic approaches to quantify salient regions of the image _cite_ . Saliency can also be related to the frequency response of an image _cite_ . Finally, the best performing of the unsupervised models is based on boolean map theory _cite_ . A survey of many of these unsupervised methods can be found in _cite_ . Supervised models provide an alternative to the biological or information theoretic models. Kienzle \etal propose a model that uses a SVM to learn which image patches contribute to saliency~ _cite_ . The Judd model _cite_ learns a linear combination of many hand-chosen low-level features (center surround, filter responses, etc.) and a few high-level features (face detectors, etc.) to predict saliency. Borji \etal~ _cite_ extend this further by using a boosting based model with additional features. The limitation of the aforementioned machine learning approaches is that they are largely dependent on the features used for learning. This can be termed shallow learning because the features are pre-defined. By contrast, deep learning approaches are able to learn rich hierarchies of features from the original pixel data. This type of learning has been made possible by larger scale eye-tracking datasets _cite_ . Pan \etal _cite_ train separate deep and shallow architectures from scratch to predict saliency and show that the deep architecture achieves better performance compared to the shallow architecture. It has also been shown that using deep networks pre-trained on image classification tasks can prove useful for saliency detection. The most commonly used pre-trained networks are the VGG networks~ _cite_ . DeepGaze~ _cite_ show that the deep features from VGG networks can be used without modification to predict saliency. Other works fine-tune the parameters of the VGG network. The Salicon model _cite_ incorporates a multi-scale approach in addition to fine-tuning. Deepfix~ _cite_ improves performance by adding inception modules _cite_, using dilated convolutions, and explicitly modeling the center bias. Vig \etal~ _cite_ also fine-tune a VGG-like model but compare the performance when using different cost functions in the training. ML-Net~ _cite_ draws information from the last N convolutional layers of the VGG network, instead of only the last layer as in the other networks. It could be argued that the existing deep network approaches are capable of modeling scene context and top down information. Top-down concepts such as faces are easily detected and labeled as salient by existing models. Additionally, some degree of local scene context can be modeled. However, the deep networks are limited by the receptive field of an output pixel in relation to input pixels. This receptive field often does not cover the entire input, so the global characteristics of the scene are not modeled. To achieve such large receptive fields, a network could have convolutions with very large input strides, but this may make modeling local saliency more difficult. We propose an alternative solution to incorporating global context by using a set of expert networks and a gating network to weight the experts. The most closely related work to ours is the iSEEL _cite_ model, which was made available shortly before the submission of this manuscript. For a given image, the model finds similar images in a scene-bank using ``gist'' features and ``classeme'' features from the last layer of the VGGN network. For each similar image, a separate trained Extreme Learning Machine (ELM) predicts saliency based on VGGN convolutional features. The final saliency map is computed by summing the outputs of the ELMs. Our model achieves better performance because each of the experts in our model is trained on a collection of images instead of a single image as in the iSEEL model. This allows the experts to generalize better to scene characteristics that are common across many images. Furthermore our gating network is trained along with expert networks in an end-to-end fashion, compared with the fixed features and euclidean distance used in iSEEL to retrieve similar images. In Section _ref_, we show that our model achieves better performance on the CATN dataset. Many of the aforementioned works incorporate information from bottom-up local sources. In addition to bottom-up information, several models have shown that a notion of global scene ``gist'' can be used to help predict saliency~ _cite_ . In Torralba \etal _cite_, global gist features are used to modulate low level features to compute a task-based attention map that searches for a target object. For example in a city scene, attention can be modulated to look for pedestrians. Peters and Itti _cite_ use gist features to learn task based attention for the task of playing video games. Our work is related to the gist-based models that incorporate scene context, however we make several improvements that lead to much greater performance. First, instead of using fixed gist features, we learn global scene information from labeled training data. The labels provide hard classes, but we are able to learn a soft weighting function. This weighting function encodes information similar to the ``dark-knowledge'' in supervised neural networks~ _cite_ . Secondly, we utilize recent advances in deep learning to learn a local saliency predictor. We combine the global scene information with the more local saliency information using a mixture of experts formulation _cite_ . Our model achieves better performance compared with a similar model that does not utilize the global scene information, and outperforms several other deep-learning based models.