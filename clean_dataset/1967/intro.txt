ConvNets are the de facto method for computer vision. In many computer vision tasks, a better ConvNet design usually leads to significant accuracy improvement. In previous works, accuracy improvement comes at the cost of higher computational complexity, making it more challenging to deploy ConvNets to mobile devices, on which computing capacity is limited. Instead of solely focusing on accuracy, recent work also aims to optimize for efficiency, especially latency. However, designing efficient and accurate ConvNets is difficult due to the challenges below. Intractable design space: The design space of a ConvNet is combinatorial. Using VGGN _cite_ as a motivating example: VGGN contains N layers. Assume for each layer of the network, we can choose a different kernel size from _inline_eq_ and a different filter number from _inline_eq_ . Even with such simplified design choices and shallow layers, the design space contains _inline_eq_ possible architectures. However, training a ConvNet is very time-consuming, typically taking days or even weeks. As a result, previous ConvNet design rarely explores the design space. A typical flow of manual ConvNet design is illustrated in Figure _ref_ . Designers propose initial architectures and train them on the target dataset. Based on the performance, designers evolve the architectures accordingly. Limited by the time cost of training ConvNets, the design flow has to stop after a few iterations, which is far too few to sufficiently explore the design space. Starting from _cite_, recent works adopt neural architecture search (NAS) to explore the design space automatically. Many previous works _cite_ use reinforcement learning (RL) to guide the search and a typical flow is illustrated in Figure _ref_ . A controller samples architectures from the search space to be trained. To reduce the training cost, sampled architectures are trained on a smaller proxy dataset such as CIFAR-N or trained for fewer epochs on ImageNet. The performance of the trained networks is then used to train and improve the controller. Previous works _cite_ has demonstrated the effectiveness of such methods in finding accurate and efficient ConvNet models. However, training each architecture is still time-consuming, and it usually takes thousands of architectures to train the controller. As a result, the computational cost of such methods is prohibitively high. Nontransferable optimality: the optimality of ConvNet architectures is conditioned on many factors such as input resolutions and target devices. Once these factors change, the optimal architecture is likely to be different. A common practice to reduce the FLOP count of a network is to shrink the input resolution. A smaller input resolution may require a smaller receptive field of the network and therefore shallower layers. On a different device, the same operator can have different latency, so we need to adjust the ConvNet architecture to achieve the best accuracy-efficiency trade-off. Ideally, we should design different ConvNet architectures case-by-case. In practice, however, limited by the computational cost of previous manual and automated approaches, we can only realistically design one ConvNet and use it for all conditions. Inconsistent efficiency metrics: Most of the efficiency metrics we care about are dependent on not only the ConvNet architecture but also the hardware and software configurations on the target device. Such metrics include latency, power, energy, and in this paper, we mainly focus on latency. To simplify the problem, most of the previous works adopt hardware-agnostic metrics such as FLOPs (more strictly, number of multiply-add operations) to evaluate a ConvNet's efficiency. However, a ConvNet with lower FLOP count is not necessarily faster. For example, NasNet-A _cite_ has a similar FLOP count as MobileNetVN _cite_, but its complicated and fragmented cell-level structure is not hardware friendly, so the actual latency is slower _cite_ . The inconsistency between hardware agnostic metrics and actual efficiency makes the ConvNet design more difficult. To address the above problems, we propose to use differentiable neural architecture search (DNAS) to discover hardware-aware efficient ConvNets. The flow of our algorithm is illustrated in Figure _ref_ . DNAS allows us to explore a layer-wise search space where we can choose a different block for each layer of the network. Following _cite_, DNAS represents the search space by a super net whose operators execute stochastically. We relax the problem of finding the optimal architecture to find a distribution that yields the optimal architecture. By using the Gumbel Softmax technique _cite_, we can directly train the architecture distribution using gradient-based optimization such as SGD. The search process is extremely fast compared with previous reinforcement learning (RL) based method. The loss used to train the stochastic super net consists of both the cross-entropy loss that leads to better accuracy and the latency loss that penalizes the network's latency on a target device. To estimate the latency of an architecture, we measure the latency of each operator in the search space and use a lookup table model to compute the overall latency by adding up the latency of each operator. Using this model allows us to quickly estimate the latency of architectures in this enormous search space. More importantly, it makes the latency differentiable with respect to layer-wise block choices. We name the models discovered by DNAS as FBNets (F acebook-B erkeley-Net s) . FBNets surpass the state-of-the-art efficient ConvNets designed manually and automatically. FBNet-B achieves N \% top-N accuracy with NM FLOPs and N ms latency on an Samsung SN phone, N smaller and N faster than MobileNetVN-N Being better than MnasNet, FBNet-B's search cost is N GPU-hours, Nx lower than the cost for MnasNet estimated based on _cite_ . Such low search cost enables us to re-design ConvNets case-by-case. For different resolution and channel scaling, FBNets achieve N \% to N \% absolute gain in top-N accuracy compared with MobileNetVN models. The smallest FBNet achieves N \% accuracy and N ms latency (N frames per second) with a batch size of N on Samsung SN. Using DNAS to search for device-specific ConvNet, an iPhone-x-optimized model achieves N speedup on an iPhone X compared with a Samsung-optimized model.