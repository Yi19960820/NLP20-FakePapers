One of the main challenges in mobile robotics is the development of systems capable of fully understanding their environment. This non-trivial task becomes even more complex when sensor data is noisy or missing. An intelligent system that the data generation process is much better equipped to tackle inconsistency in its sensor data. There is significant potential gain in having autonomous robots equipped with data generation capabilities which can be leveraged for reconstruction, compression, or prediction of the data stream. In autonomous driving, information from the environment is captured from sensors mounted on the vehicle, such as cameras, radars, and lidars. While a significant amount of research has been done on generating RGB images, relatively little work has focused on generating lidar data. These scans, represented as an array of three dimensional coordinates, give an explicit topography of the vehicle's surroundings, potentially leading to better obstacle avoidance, path planning, and inter-vehicle spatial awareness. To this end, we leverage recent advances in deep generative modeling, namely variational autoencoders (VAE) _cite_ and generative adversarial networks (GAN) _cite_, to produce a generative model of lidar data. While the VAE and GAN approaches have different objectives, they can be used with Convolutional Neural Networks (CNN) _cite_ to extract local information from nearby sensor points. Unlike some approaches for lidar processing, we do not convert the data to voxel grids _cite_ . Instead, .