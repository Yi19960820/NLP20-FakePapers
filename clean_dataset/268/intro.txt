Various computer vision tasks aim to decompose an image into its individual components. In image/video segmentation, the task is to decompose the image into meaningful sub-regions, such as foreground and background~ _cite_ . In transparency separation, the task is to separate the image into its superimposed reflection and transmission~ _cite_ . Such transparency can be a result of accidental physical reflections, or due to intentional transparent overlays (e.g., watermarks) . In image dehazing~ _cite_, the goal is to separate a hazy/foggy image into its underlying haze-free image and the obscuring haze/fog layers (airlight and transmission map) . Fig.~ _ref_ shows how all these very different tasks can be casted into a single unified framework of . What is common to all these decompositions is the fact that the within each separate layer is ``simpler'' (more uniform) than in the original mixed image, resulting in . Small image patches (e.g., NxN, NxN) have been shown to repeat abundantly inside a single natural image~ _cite_ . This strong internal patch recurrence was exploited for solving a large variety of computer vision tasks~ _cite_ . It was also shown that the empirical entropy of patches inside a is much smaller than the entropy in a ~ _cite_ . It was further observed by~ _cite_ that the of small image regions composing a segment, is smaller than the {\em empirical cross-entropy} of regions across different segments within the same image. This observation has been successfully used for image segmentation~ _cite_ . Finally, it was observed~ _cite_ that the distribution of patches in a tends to be more diverse (weaker internal patch similarity) than in its underlying image. This observation was exploited by~ _cite_ for blind image dehazing. In this paper we combine the power of the internal patch recurrence (its strength in solving unsupervised tasks), with the power of Deep-Learning. We propose an Deep framework for decomposing a single image into its layers, such that the distribution of ``image elements'' within each layer is ``simple''. We build on top of the ``Deep Image Prior'' (DIP) work of Ulyanov \etal~ _cite_ . They showed that the structure of a DIP generator network is sufficient to capture the low-level statistics of a natural image. The input to the DIP network is, and it trains to (which serves as its sole output training example) . This network was shown to be quite powerful for solving inverse problems like denoising, super-resolution and inpainting, in an unsupervised way. We observe that when employing to reconstruct an image, those DIPs tend to ``split'' the image, such that the patch distribution of each DIP output is ``simple''. Our approach for is thus based on {a combination of multiple (two or more) DIPs} which we coin ``Double-DIP''. We demonstrate the applicability of this approach to a wide range of computer vision tasks, including Image-Dehazing, Fg/Bg Segmentation of images and videos, Watermark Removal, and Transparency Separation in images and videos. Double-DIP is and caters many different applications. methods designed for one specific task may outperform Double-DIP on their own challenge. However, to the best of our knowledge, this is the first framework that is able to handle well such a large variety of image-decomposition tasks. Moreover, in some tasks (e.g., image dehazing), Double-DIP achieves comparable and even better results than leading methods.