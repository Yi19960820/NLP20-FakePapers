The study of marine animals’ behavior, seasonal distribution and abundance is vital for various environmental agencies, commercial fishermen and marine research institutes. To this end, extensive amount of digital imagery and video are acquired by underwater vehicles (AUVs) and remotely operated vehicles (ROVs) . Color images are more intuitive to humans compared sonar signal, yet manual labeling of images is still a daunting task considering the sheer size of the data. An automated solution is thus preferred, which consists of two major steps _inline_eq_ detection and classification. This paper will focus on the latter problem only. There are a good number of works concentrating on feature extraction for colored marine imagery. Shape (Fourier descriptors), color (normalized color histograms), and texture (Gabor filters and grey-level co-occurrence) _cite_ are among the most exploited features. Biological characteristics such as body part ratio _cite_ can distinguish between species as well. Nowadays, the rapid development in convolutional neural networks (CNN) has opened new possibility for accurate image representation, which has since benefited marine animal classification. To acquire CNN features of an image, one can either input the image to a CNN pre-trained by a large database (e.g. ImageNet _cite_) which consists of images that are visually similar to the target image _cite_, or train a new CNN with images homogenous to the target as in the example of plankton classification _cite_ . Recently, the Harbor Branch Oceanographic Institute (HBOI) at Florida Atlantic University has developed a novel system called Unobtrusive Multistatic Serial Lidar Imager (UMSLI) to perform marine hydrokinetic site monitoring and marine animal classification _cite_ _cite_ . Initial testing of the UMSLI system has been conducted inside a unique test tank facility (Figure _ref_), which is capable of extensive testing of a variety of different electro-optical system configurations under a range of environmental conditions. Compared with optical camera imaging, underwater Lidar imaging has several advantages. Firstly, red laser illuminators are beyond the visible wavelength range of marine life, thus animals being monitored will not be affected. Meanwhile, optical camera requires significant amount of white light to illuminate low light areas and is more obtrusive to marine life. Second, unlike conventional camera whose focus is governed by the lens, Lidar imagery will remain in focus throughout the entire range, which gives it superior detection range. Higher SNR is also be achieved with Lidar due to the higher photon efficiency _cite_ . Third, the transmitter in UMSLI system can operate in an adaptive mode, opting for either higher resolution or longer range of detection. A Lidar image of a fish captured by UMSLI is typically N-D grayscale integrated from the N-D point cloud Lidar return. Figure _ref_ shows example Lidar images retrieved from the test tank at Harbor Branch Oceanographic Institute (HBOI) . Using Lidar imagery for marine animal classification, however, is not without its own issues. As UMSLI is the first attempt to identify individual marine animal using Lidar imagery, there are virtually no existing database online with similar content. Given that the amount of data obtained from initial UMSLI deployment is also small, training a proper convolutional neural network becomes problematic with insufficient data. In addition, most of the valuable information revealed from the obtained Lidar imagery in the initial experimental dataset seems to be the shape of the animal. Using directly the pixels as feature is not recommended because of orientation variations among shapes, while traditional shape features such as Zernike _cite_ or Hu's moments have relatively weak description ability. There has been an attempt _cite_ to apply bag of words for quantifying a shape's feature, but the resulting "shape vocabulary" feature will be very long and highly redundant, because the common space in which all shapes reside can have very high dimensions. Instead of feature extraction, most of the existing shape classification or recognition literatures adopt a "pairwise" comparison strategy, creating directly a matrix with each entry representing the similarity/dissimilarity between a pair of shapes. As such, a "descriptor" rather than a "feature" will be enough to represent a shape, which is usually much simpler and intuitive. Commonly used descriptors include shape context (SC) _cite_, internal distance shape context (IDSC) _cite_, the triangle descriptor _cite_ and height functions _cite_ . With a proper dissimilarity measure (Chi square or earth mover distance _cite_) applied on pairs of descriptors, these methods can usually achieve satisfying classification results using k-nearest neighbor classifier, especially on data with very few training samples. This similarity-matrix based approach is not limited to shape descriptors only. For instance, Gaussian mixture models _cite_ _cite_ have been applied in hand gesture recognition, while spectral estimation _cite_ method has been used in an automobile recognition task. For both feature based and dissimilarity matrix-based marine animal classification, utilizing information from multiple sources (different features/descriptors or different view angles) will lead to more comprehensive description of objects, and thus improve classification accuracy _cite_ _cite_ . Multi-view learning _cite_ is a group of methods that introduces one function to model one particular view of the data, then jointly optimizes all the functions to improve the learning performance. The goal of this paper is to develop suitable multi-view learning algorithms for both data formats, where a "view" would be respectively a feature set or a dissimilarity matrix. There are two major considerations when designing a multi-view learning algorithm. Firstly, the algorithm should simultaneously accommodate dimensional reduction. because it is a necessary preprocessing step that makes classification faster, and also more accurate when the data is small or has a low-dimensional structure _cite_ _cite_ . CNN features usually are high dimensional (N for DeCAF _cite_) and have considerable redundancy _cite_, so it is natural to apply dimensionality reduction. With this principle in mind, one can choose the desired multi-view learning framework from various options. For features, the most notable categories of multi-view learning algorithms are co-training _cite_, multiple kernel learning (MKL) _cite_ and subspace learning _cite_ . Only subspace learning involves dimensionality reduction. Examples include multiple spectral embedding _cite_, multi-view non-negative matrix factorization _cite_ and multi-view intact space learning _cite_ . For dissimilarity matrices, there are very few well-established multi-view learning algorithms. It should be noted that MKL cannot be applied to dissimilarity matrices, as they are not guaranteed to be positive semi-definite and hence are not valid kernel matrices. Co-transduction _cite_ borrows ideas from co-training and can be viewed as a multi-view approach, yet it lacks a dimensionality reduction mechanism. It may be a good idea to firstly find a “base” method that performs dimensionality reduction for a single dissimilarity matrix, then extend it to its multi-view version. Potential candidates include principal component analysis (PCA), multidimensional scaling (MDS) and auto-encoder _cite_ . Before carrying out any of these algorithms, it is necessary to enforce the dissimilarity matrix to be a proper distance matrix. The robust Euclidean embedding (REE) _cite_ is an algorithm based on classical MDS, which also regulates the dissimilarity matrix by enforcing Euclidean distance. Meanwhile, there is actually one algorithm in the literature with the name multi-view MDS (MV-MDS) _cite_, but it is based on non-classical MDS and is compatible only with the _inline_eq_ cost function, unlike REE. Another important consideration in designing multi-view learning algorithms is the choice of cost functions, which has not been studied in detail in previous works. In a dataset, the views given may contain irregularities (noise) of different types and magnitudes. The mean square error (MSE) is the most widely-used cost function, yet its performance is suboptimal for non-Gaussian noise. The correntropy _cite_ as a non-linear, local similarity measure that is robust to outliers has attracted researchers in recent years. Notable applications of correntropy include adaptive filtering _cite_, classification _cite_, face recognition _cite_ and robust autoencoder _cite_ . More recently, the generalized correntropy _cite_ is proposed and successfully applied to adaptive filtering. It is more versatile than correntropy, as changes in the shape parameter can lead to the suppression of different types of noise. In this paper, correntropy loss based multi-view learning algorithms will be developed for both features and dissimilarity matrices. For features, the multi-view intact space learning (MISL) _cite_ is employed as the base method for two related correntropy-based multi-view learning algorithms. For dissimilarity matrices, the base method will be REE. As REE is itself single view only, its direct multi-view version will be proposed along with a correntropy-based method. The rest of this paper is organized as follows: Section II reviews the MISL and REE algorithms as well as the concept of generalized correntropy. Section III derives the algorithms. Section IV shows experiment results for both simulated data and real-world marine animal data. Section V concludes the paper.