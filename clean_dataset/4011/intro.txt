Neuromorphic event-driven time-based vision sensors operate on a very different principle than conventional frame-based cameras. Instead of acquiring static images of a scene, these sensors asynchronously record pixel intensity changes with a high temporal precision (around _inline_eq_) . The event format differs significantly from frames, and therefore conventional machine learning algorithms cannot be directly applied if one wants to fully use its potential and temporal properties in terms of power consumption, computational cost and low memory requirements. Previous notable work on object recognition using event-driven time-based vision sensors include real-time event-driven visual pattern recognition that recognizes and tracks circles of different sizes using a hierarchical spiking network running on custom hardware _cite_, a card pip recognition task on FPGAs, implementing different hierarchical spiking models inspired by Convolutional Neural Networks (CNNs) _cite_ and new methods such as HFirst _cite_ and recently HOTS _cite_, an unsupervised algorithm which fully considers the spatio-temporal aspects of event-based sensors. In this paper we introduce a compact hierarchical event-driven multi-temporal framework to learn spatiotemporal patterns of dynamic scenes extending the concept of hierarchically increasing spatiotemporal-scales time-surfaces introduced in _cite_ . A time-surface is a descriptor that provides a time context around an incoming event and describes the temporal activity in its surrounding. This descriptor, applied in a multilayer architecture requires a high number of features to correctly characterize a dynamic visual input and therefore high memory and computational costs. Ideally there must be a one to one relation between interesting spatio-temporal time-surfaces detected in the scene and the ones stored in each layer of the architecture. The aim of this work is to present a new formulation of the method in order to reduce the number of features using sparse coding to express any time-surface as a linear combination of elementary time-surfaces rather than selecting the most representative ones using clustering techniques (iterative K-means _cite_ in the original paper) .