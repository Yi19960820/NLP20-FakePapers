Recently, Symmetric Positive Definite (SPD) matrices of real numbers appear in many branches of computer vision. Examples include region covariance matrices for pedestrian detection _cite_ and texture categorization _cite_, joint covariance descriptor for action recognition _cite_, diffusion tensors for DT image segmentation _cite_ and image set based covariance matrix for video face recognition _cite_ . Due to the effectiveness of measuring data variations, such SPD features have been shown to provide powerful representations for images and videos. However, such advantages of the SPD matrices often accompany with the challenge of their non-Euclidean data structure which underlies a specific Riemannian manifold _cite_ . Applying the Euclidean geometry directly to SPD matrices often results in poor performances and undesirable effects, such as the swelling of diffusion tensors in the case of SPD matrices _cite_ . To overcome the drawbacks of the Euclidean representation, recent works _cite_ have introduced Riemannian metrics, e.g., Affine-Invariant metric _cite_, Log-Euclidean metric _cite_, to encode the Riemannian geometry of SPD manifold properly. By applying these classical Riemannian metrics, a couple of works attempt to extend Euclidean algorithms to work on manifolds of SPD matrices for learning more discriminative SPD matrices or their vector-forms. To this end, several studies exploit effective methods on one SPD manifold by either flattening it via tangent space approximation _cite_ (See Fig. _ref_ (a) _inline_eq_ (b)) or mapping it into a high dimensional Reproducing Kernel Hilbert Space (RKHS) _cite_ (See Fig. _ref_ (a) _inline_eq_ (c) _inline_eq_ (b)) . Obviously, both of the two families of methods inevitably distort the geometrical structure of the original SPD manifold due to the procedure of mapping the manifold into a flat Euclidean space or a high dimensional RKHS. Therefore, the two learning schemes would lead to sub-optimal solutions for the problem of discriminative SPD matrix learning. To more faithfully respect the original Riemannian geometry, another kind of SPD-based discriminant learning methods _cite_ aims to pursue a column full-rank transformation matrix mapping the original SPD manifold to a more discriminative SPD manifold, as shown in Fig. _ref_ (a) _inline_eq_ (d) . However, as directly learning the manifold-manifold transformation matrix is hard, the work _cite_ alternatively decomposes it to the product of an orthonormal matrix with a matrix in GL _inline_eq_, and requires the employed Riemannian metrics to be affine invariant. By doing so, optimizing the manifold-manifold transformations is equivalent to optimizing over orthonormal projections. Although the additional requirement simplifies the optimization of the transformation, this has not only reduced the original solution space but also inevitably excluded all non-affine invariant Riemannian metrics such as the well-known Log-Euclidean metric, which has proved to be much more efficient than Affine-Invariant metric _cite_ . While the work _cite_ exploited the Log-Euclidean metric under the same scheme, it actually attempts to learn a tangent map, which implicitly approximate the tangent space and hence introduces some distortions of the true geometry of SPD manifolds. In this paper, also under the last scheme (see Fig. _ref_ (a) _inline_eq_ (d)), we propose a new geometry-aware SPD similarity learning (SPDSL) framework to open a broader problem domain of learning discriminative SPD features by exploiting either affine invariant or non-affine invariant Riemannian metrics on SPD manifolds. To realize the SPDSL framework, there are three main contributions in this work: