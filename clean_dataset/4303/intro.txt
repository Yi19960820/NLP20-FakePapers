The characterization of distance preserving maps is of fundamental interest to the problem of non-linear dimensionality reduction and manifold learning. For the purpose of achieving a coherent global representation, it is often desirable to embed the high-dimensional data into a space of low dimensionality while preserving the metric structure of the data manifold. The intrinsic nature of the geodesic distance renders such a representation dependent only on the geometry of the manifold and not on how it is embedded to the ambient space. In the context of dimensionality reduction, this property makes the resulting embedding meaningful. The success of deep learning has shown that neural networks can be trained as powerful function approximators of complex attributes governing various visual and auditory phenomena. The hallmark of deep learning has been its ability to automatically learn meaningful representations from raw data without any explicit axiomatic constructions. The availability of large amounts of data and computational power, coupled with parallel streaming architectures and improved optimization techniques, have all led to computational frameworks that efficiently exploit their representational power. However, a study of their behavior under geometric constraints is an interesting question which has been relatively unexplored. In this paper, we use the computational infrastructure of neural networks to model maps that preserve geodesic distances on data manifolds. We revisit the classical geometric framework of multidimensional scaling to find a configuration of points that satisfy pairwise distance constraints. We show that instead of optimizing over the individual coordinates of the points, we can optimize over the by modeling this map as a neural network. This choice of modeling the isometric map with a parametric model provides a straightforward out-of-sample extension, which is a simple forward pass of the network. We exploit efficient sampling techniques that progressively select on the manifold by maximizing the spread of their pairwise geodesic distances. We demonstrate that a small amount of these is sufficient to train a network to generate faithful low-dimensional embeddings of manifolds. Figure _ref_ provides a visualization of our proposed approach. Our motivation to integrate deep learning into the classical isometric embedding problem is two-fold. Firstly, this approach allows us to view the concept of from a geometric viewpoint. Algorithms that exhibit poor generalizations will yield embeddings that show a clear visual depiction of suboptimal flattening as in Figures _ref_, _ref_ _ref_, _ref_ and this can be objectively measured using the stress function (Equation _ref_, which essentially measures the deviation from isometry) . We demonstrate qualitatively and quantitatively that deep learning models provide better and generalization properties as compared to the axiomatic interpolation and extrapolation formulas of their non-parametric counter-parts. Secondly, the use of geodesic sampling methods enables further analysis, by measuring global stress as a function of the number of samples used for training. Our experiments demonstrate that analogously to numerical algorithms, one can obtain an measure for each neural network architecture thereby measuring how efficient a given neural network is in modeling a function.