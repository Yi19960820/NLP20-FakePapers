the fast development of Internet and multimedia technologies, heterogeneous multimedia data including image, video, text and audio, has been growing very fast and enriching people's life. To make better use of such rich multimedia data, it is an important application to retrieve multimedia contents that users have interests in. Thus multimedia retrieval has attracted much attention over the past decades. However, with the explosive increase of multimedia data on the Internet, efficient retrieval from large scale databases has become an urgent need and a big challenge. To tackle this problem, many hashing methods~ _cite_ have been proposed to perform efficient yet effective retrieval. Generally speaking, hashing methods aim to transfer high dimensional feature into short binary codes so that similar data can have similar binary codes. Hashing methods have two major advantages when applied in multimedia retrieval: (N) Binary codes enable fast Hamming distance computation based on bit operations which can be efficiently implemented. (N) Binary codes take much less storage compared with original high dimensional feature. A large amount of hashing methods are designed for single modality retrieval~ _cite_, that is to say, data can only be retrieved by an query of the same modality, such as image retrieval~ _cite_ and video retrieval~ _cite_ . However, in real world applications, multimedia data is usually presented with different modalities. For example, an image is usually associated with a textual description, and both of them describe the same semantic. In this case, an increasing need of users is to retrieve across different modalities, such as using an image to retrieve relevant textual descriptions. Such retrieval paradigm is called cross-modal hashing. It is more useful and flexible than single modality retrieval because users can retrieve whatever they want by submitting whatever they have~ _cite_ . The key challenge of cross-modal hashing is the ``heterogeneity gap'', which means the distribution and representation of different modalities are inconsistent, and makes it hard to directly measure the similarity between different modalities. To bridge this gap, many cross-modal hashing methods~ _cite_ have been proposed. Generally speaking, cross-modal hashing methods can be divided into traditional methods and deep learning based methods. Traditional methods can be further divided into unsupervised methods and supervised methods based on whether semantic information is involved. The basic idea of unsupervised cross-modal hashing methods is to project data from different modalities into a common space where correlations between them are maximized, which is similar to Canonical Correlation Analysis (CCA) ~ _cite_ . Representative unsupervised cross-modal hashing methods including Cross-view hashing (CVH) ~ _cite_, Inter-Media Hashing (IMH) ~ _cite_, Predictable Dual-View Hashing (PDH) ~ _cite_ and Collective Matrix Factorization Hashing (CMFH) ~ _cite_ . While supervised cross-modal hashing methods try to learn hash functions to preserve the semantic correlations provided by labels. Representative supervised cross-modal hashing methods include Co-Regularized Hashing (CRH) ~ _cite_, Heterogeneous Translated Hashing (HTH) ~ _cite_, Semantic Correlation Maximization (SCM) ~ _cite_, Quantized Correlation Hashing (QCH) ~ _cite_ and Semantics-Preserving Hashing (SePH) ~ _cite_ . Recently, inspired by the successful applications of deep learning on image classification~ _cite_ and object recognition~ _cite_, some researches try to bridge the ``heterogeneity gap" by deep learning technique. Representative deep learning based methods include Deep and Bidirectional Representation Learning Model (DBRLM) ~ _cite_, Deep Visual-semantic Hashing (DVH) ~ _cite_ and Cross-Media Neural Network Hashing (CMNNH) ~ _cite_ . Among the above cross-modal hashing methods, supervised methods typically achieve better retrieval accuracy due to the utilization of semantic information. However, supervised methods are very labor intensive to obtain large scale labeled training data. It is even harder to label cross-modal hashing training data, since multiple modalities are involved. Thus it is of significant importance to improve retrieval accuracy by fully exploiting unlabeled data which is very convenient to get. Traditional semi-supervised learning methods~ _cite_ can effectively exploit distribution of unlabeled data to help supervised training. However, little efforts have been done for semi-supervised cross-modal hashing. The key challenge of semi-supervised cross-modal hashing is to exploit informative unlabeled data to promote hashing learning. With the recent progress of generative adversarial network (GAN) ~ _cite_, which has been applied in many computer vision problems, such as image synthesis~ _cite_, video prediction~ _cite_ and object detection~ _cite_ . GAN has shown its promising ability to model the data distributions. Inspired by this ability, in this paper we propose a novel semi-supervised cross-modal hashing approach by generative adversarial network (SCH-GAN) . We aim to design a generative model to fit the relevance distribution of unlabeled data near the margins between different modalities, so that it can select informative unlabeled examples close to the margin to fool the discriminative model. We also design a discriminative model to distinguish the selected data from generative model and the true positive data, so that it can better estimate the cross-modal ranking. These two models play a minimax game to iteratively optimize each other and boost cross-modal hashing accuracy. The main contributions of this paper can be summarized as follows: Extensive experiments on the widely-used Wikipedia, NUS-WIDE and MIRFlickr datasets demonstrate that our proposed SCH-GAN outperforms N state-of-the-art methods, which verify the effectiveness of SCH-GAN approach. The rest of this paper is organized as follows. We briefly review the related works in Section II. In Section III, our proposed SCH-GAN approach is presented in detail. Then Section IV reports the experimental results as well as analyses. Finally, Section V concludes this paper.