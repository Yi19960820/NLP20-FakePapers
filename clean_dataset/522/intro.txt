The success of service robotics decisively depends on a smooth robot to user interaction. Thus, a robot should be able to extract information just from the face of its user, e.g. identify the emotional state or deduce gender. Interpreting correctly any of these elements using machine learning (ML) techniques has proven to be complicated due the high variability of the samples within each task _cite_ . This leads to models with millions of parameters trained under thousands of samples _cite_ . Furthermore, the human accuracy for classifying an image of a face in one of N different emotions is N \% _inline_eq_ N \% _cite_ . One can observe the difficulty of this task by trying to manually classify the FER-N dataset images in Figure _ref_ within the following classes . In spite of these difficulties, robot platforms oriented to attend and solve household tasks require facial expressions systems that are robust and computationally efficient. Moreover, the state-of-the-art methods in image-related tasks such as image classification _cite_ and object detection are all based on Convolutional Neural Networks (CNNs) . These tasks require CNN architectures with millions of parameters; therefore, their deployment in robot platforms and real-time systems becomes unfeasible. In this paper we propose an implement a general CNN building framework for designing real-time CNNs. The implementations have been validated in a real-time facial expression system that provides face-detection, gender classification and that achieves human-level performance when classifying emotions. This system has been deployed in a care-O-bot N robot, and has been extended for general robot platforms and the RoboCup@Home competition challenges. Furthermore, CNNs are used as black-boxes and often their learned features remain hidden, making it complicated to establish a balance between their classification accuracy and unnecessary parameters. Therefore, we implemented a real-time visualization of the guided-gradient back-propagation proposed by Springenberg _cite_ in order to validate the features learned by the CNN.