Network of surveillance cameras are everywhere nowadays. A major problem is to figure out how to extract useful information from the videos captured by these cameras. Fig.~ _ref_ depicts an illustrative example where a network of cameras, with both overlapping and non-overlapping fields of view (fovs) are capturing videos from a region. The basic question that we want to explore in such scenario is: can we get an idea of the video content without watching all the videos entirely? Much progress has been made in developing a variety of ways to summarize a single video, by exploring different design criteria (representativeness~ _cite_, interestingness~ _cite_) in an unsupervised manner, or developing supervised algorithms~ _cite_) . However, with some exceptions of~ _cite_, summarizing multi-view videos still remains as a challenging problem because of large amount of inter-view content correlations along with intra-view correlations present in such videos. In this paper, we focus on the task of summarizing multi-view videos, and illustrate how a new representation that exploits multi-view correlations can effectively generate a more informative summary while comparing with the prior multi-view works. Our work builds upon the idea of subspace learning, which typically aim to obtain a latent subspace shared by multiple views by assuming that the multiple views are generated from this latent subspace. Specifically, our key idea is the following: by viewing two or more multi-view videos as actually being one large video, making inference about multi-view videos reduces to making inference about a single video in the latent subspace. Our approach works as follows. First, we embed all the frames in an unified low dimensional latent space such that the locations of the frames preserve both intra-and inter-view correlations (Section~ _ref_) . This is achieved by minimizing an objective function that has two terms: one due to intra-view correlations and another due to inter-view correlations across the multiple views. The solution can be obtained by solving an eigen-value problem that is linear in size of the multi-view videos. Then, we employ a sparse representative selection approach over the embedding to produce multi-view summaries (Section~ _ref_) . Specifically, we formulate the task of finding summaries as a sparse coding problem where the dictionary is constrained to have a fixed basis (dictionary to be the matrix of same data points) and the nonzero rows of sparse coefficient matrix represent the multi-view summaries. {\bf Contributions.} The contributions of our work can be summarized as follows. \newline (N) We propose a multi-view frame embedding which is able to preserve both intra and inter-view correlations without assuming any prior correspondences/alignment between the multi-view videos. \newline (N) We propose a sparse representative selection method over the learned embedding to summarize the multi-view videos, which provides scalability in generating summaries. In particular, this allows us to generate summaries of different lengths as per the user request (analyze once, generate many) . \newline (N) The proposed method is a generalized framework which makes sparse coding feasible in summarizing both single and multi-view videos. We demonstrate the generalizability of our framework with extensive experiments on three publicly available multi-view datasets (N videos) and two single view datasets (N videos) . Most of the previous summarization techniques are designed for single-view videos. Various strategies have been studied, including clustering~ _cite_, attention modeling~ _cite_, saliency based regression model~ _cite_, super frame segmentation~ _cite_, kernel temporal segmentation~ _cite_, crowd-sourcing~ _cite_, submodular maximization~ _cite_, and point process~ _cite_ . Interested readers can check~ _cite_ for a more comprehensive summary. However, they usually do not perform well for summarizing multi-view videos since they cannot exploit the large inter-view correlations. To address the challenges encountered in a multi-view camera network, some state-of-the-art approaches use random walk over spatio-temporal shot graphs~ _cite_ and rough sets~ _cite_ to summarize multi-view videos. A very recent work in~ _cite_ uses bipartite matching constrained optimum path forest clustering to solve the problem of summarizing multi-view videos. In~ _cite_, stochastic neighbor embedding with sparse coding is employed to summarize multi-view videos. An online method for summarization can also be found in~ _cite_ . The work in~ _cite_ and~ _cite_ also addresses a similar problem of summarization in multi-camera settings with non-overlapping field of views. By contrast, the approach that we describe here seeks to find summary from a multi-view network as shown in Fig.~ _ref_ . Moreover, our approach does not require a priori knowledge of field of view.