{\bf Deep Learning} has emerged as a promising new area of research in statistical machine learning (see~ for a review) . Learning algorithms for deep architectures are centered on the learning of useful representations of data, which are better suited to the task at hand, and are organized in a hierarchy with multiple levels. This is in part inspired by observations of the mammalian visual cortex, which consists of a chain of processing elements, each of which is associated with a different representation of the raw visual input. In fact, it was found recently that the features learnt in deep architectures resemble those observed in the first two of these stages (in areas VN and VN of visual cortex) ~, and that they become more and more invariant to factors of variation (such as camera movement) in higher layers~ . Learning a hierarchy of features increases the ease and practicality of developing representations that are at once tailored to specific tasks, yet are able to borrow statistical strength from other related tasks (e.g., modeling different kinds of objects) . Finally, learning the feature representation can lead to higher-level (more abstract, more general) features that are more robust to unanticipated sources of variance extant in real data. {\bf Self-taught learning} ~ is a paradigm that combines principles of semi-supervised and multi-task learning: the learner can exploit examples that are unlabeled and possibly come from a distribution different from the target distribution, e.g., from other classes than those of interest. It has already been shown that deep learners can clearly take advantage of unsupervised learning and unlabeled examples~, but more needs to be done to explore the impact of {\em out-of-distribution} examples and of the multi-task setting (one exception is~, which uses a different kind of learning algorithm) . In particular the {\em relative advantage} of deep learning for these settings has not been evaluated. The hypothesis discussed in the conclusion is that a deep hierarchy of features may be better able to provide sharing of statistical strength between different regions in input space or different tasks. Whereas a deep architecture can in principle be more powerful than a shallow one in terms of representation, depth appears to render the training problem more difficult in terms of optimization and local minima. It is also only recently that successful algorithms were proposed to overcome some of these difficulties. All are based on unsupervised learning, often in an greedy layer-wise ``unsupervised pre-training'' stage~ . One of these layer initialization techniques, applied here, is the Denoising Auto-encoder~ (DA) ~ (see Figure~ _ref_), which performed similarly or better than previously proposed Restricted Boltzmann Machines in terms of unsupervised extraction of a hierarchy of features useful for classification. Each layer is trained to denoise its input, creating a layer of features that can be used as input for the next layer. In this paper we ask the following questions: _inline_eq_ Do the good results previously obtained with deep architectures on the MNIST digit images generalize to the setting of a much larger and richer (but similar) dataset, the NIST special database N, with N classes and around Nk examples? _inline_eq_ To what extent does the perturbation of input images (e.g. adding noise, affine transformations, background images) make the resulting classifiers better not only on similarly perturbed images but also on the {\em original clean examples} ? We study this question in the context of the N-class and N-class tasks of the NIST special database N. _inline_eq_ Do deep architectures {\em benefit {\bf more} from such out-of-distribution} examples, i.e. do they benefit more from the self-taught learning~ framework? We use highly perturbed examples to generate out-of-distribution examples. _inline_eq_ Similarly, does the feature learning step in deep learning algorithms benefit {\bf more} from training with moderately {\em different classes} (i.e. a multi-task learning scenario) than a corresponding shallow and purely supervised architecture? We train on N classes and test on N (digits) or N (upper case or lower case) to answer this question. Our experimental results provide positive evidence towards all of these questions, as well as classifiers that reach human-level performance on N-class isolated character recognition and beat previously published results on the NIST dataset (special database N) . To achieve these results, we introduce in the next section a sophisticated system for stochastically transforming character images and then explain the methodology, which is based on training with or without these transformed images and testing on clean ones. We measure the relative advantage of out-of-distribution examples (perturbed or out-of-class) for a deep learner vs a supervised shallow one. Code for generating these transformations as well as for the deep learning algorithms are made available at {\tt http: //hg.assembla.com/iftN} . We estimate the relative advantage for deep learners of training with other classes than those of interest, by comparing learners trained with N classes with learners trained with only a subset (on which they are then tested) . The conclusion discusses the more general question of why deep learners may benefit so much from the self-taught learning framework. Since out-of-distribution data (perturbed or from other related classes) is very common, this conclusion is of practical importance.