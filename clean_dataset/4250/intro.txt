other abilities the human visual system~ (HVS) can recognize colors of scene objects even under various illumination. This ability is known as color constancy~ _cite_ and most digital cameras have computational color constancy implemented in their image processing pipelines~ _cite_ . The task of computational color constancy is to get an accurate illumination estimation and then use it to chromatically adapt the image in order to remove the influence of the illumination on colors. The most commonly used image _inline_eq_ formation model for this problem with included Lambertian assumption is~ _cite_ where _inline_eq_ is a color channel, _inline_eq_ is a given image pixel, _inline_eq_ is the wavelength of the light, _inline_eq_ is the visible spectrum, _inline_eq_ is the spectral distribution of the light source, _inline_eq_ is the surface reflectance, and _inline_eq_ is the camera sensitivity of color channel _inline_eq_ . To make the problem simpler, uniform illumination is usually assumed and by removing _inline_eq_ from _inline_eq_, the observed light source color is given as By knowing only the direction of _inline_eq_, an image can be successfully chromatically adapted~ _cite_ . With only image pixel values _inline_eq_ given and both _inline_eq_ and _inline_eq_ unknown, calculating _inline_eq_ is an ill-posed problem, which needs additional assumptions to be solved. Many illumination estimation methods with different assumptions have been proposed. In the first of two main groups of illumination estimation methods are low-level statistics-based methods such as White-patch~ _cite_ and its improvements _cite_, Gray-world~ _cite_, Shades-of-Gray~ _cite_, Grey-Edge~ (and order) ~ _cite_, Weighted Gray-Edge~ _cite_, using bright pixels~ _cite_, using bright and dark colors~ _cite_, exploiting illumination color statistics perception~ _cite_, using gray pixels~ _cite_, exploiting expected illumination statistics~ _cite_ . The second main group consists of learning-based methods, all of which are supervised, like gamut mapping~ (pixel, edge, and intersection based) ~ _cite_, using neural networks~ _cite_, using high-level visual information~ _cite_, natural image statistics~ _cite_, Bayesian learning~ _cite_, spatio-spectral learning~ (maximum likelihood estimate, and with gen. prior) ~ _cite_, simplifying the illumination solution space~ _cite_, using color/edge moments~ _cite_, using regression trees with simple features from color distribution statistics~ _cite_, performing various kinds of spatial localizations~ _cite_, using convolutional neural networks~ _cite_, using genetic algorithms~ _cite_, modelling colour constancy by using the overlapping asymmetric Gaussin kernels with surround pixel contrast based sizes~ _cite_, finding paths for the longest dichromatic line produced by specular pixels~ _cite_, detecting grey pixels with specific illuminant-invariant measuse in logarithmic space~ _cite_, channel-wise pooling the responses double-oponnecy cells in LMS color space~ _cite_ . Statistics-based methods are characterized by a relatively high speed, simplicity, and usually lower accuracy, while learning-based methods are slower, but have higher accuracy. However, several recently proposed learning-based methods are not only highly accurate, but also as fast as statistics-based methods to the level of outperforming some of them~ _cite_ . This trend will likely continue and it will bring more accurate real-time color constancy to digital cameras. Nevertheless, since all well-known learning-based methods are supervised, a major obstacle for their application is that for a given sensor, despite proposed workarounds~ _cite_, supervised learning-based methods have to be trained on calibrated images taken by preferably the same sensor~ _cite_ . To calibrate the images, a calibration object has to be placed in the scenes of these images and later segmented to extract the ground-truth illumination. Careful image acquisition and the amount of manual work required for calibration is the main bottleneck in enabling highly accurate color constancy for a given sensor. To try to avoid such calibration, in this paper an unsupervised learning-based method is proposed that learns its parameter values from non-calibrated images with unknown ground-truth illumination. Such learning is possible by clustering the approximated ground-truth illuminations of images from the training set and then extracting information useful for illumination estimation on future new images. The method is fast, hardware-friendly, and it outperforms many state-of-the-art methods in terms of accuracy. To the best of the authors' knowledge this is the first unsupervised learning-based color constancy method with high accuracy on well-known and widely used benchmark datasets and therefore it represents a potential contribution to the color constancy philosophy. Besides being a clear proof-of-concept that using unsupervised learning for highly accurate color constancy is possible, the proposed method opens another valuable and useful possibility, namely that of automatic online learning and adjustment of parameter values when a camera is used in special illumination conditions for a prolonged period of time. An extension of the method is also proposed, which learns the needed parameters from non-calibrated images taken with one sensor and which can then be successfully applied to images taken with another sensor. This effectively enables inter-camera unsupervised learning for color constancy. Additionally, a new high quality color constancy benchmark dataset with _inline_eq_ calibrated high-quality images is created, used to test the proposed method, and made publicly available. In short, the contributions of the paper are as follows: The paper is structured as follows: Section~ _ref_ lays out the motivation for the proposed method, Section~ _ref_ describes the method, Section~ _ref_ extends the proposed method to perform inter-camera learning, the general applicability of the proposed method is shown in Section~ _ref_, in Section~ _ref_ the newly created dataset and the experimental results are presented and discussed, and, finally, Section~ _ref_ concludes the paper.