Recent publications suggest that unsupervised pre-training of deep, hierarchical neural networks improves supervised pattern classification _cite_ . Here we train such nets by simple online back-propagation, setting new, greatly improved records on MNIST _cite_, Latin letters _cite_, Chinese characters _cite_, traffic signs _cite_, NORB (jittered, cluttered) _cite_ and CIFARN _cite_ benchmarks. We focus on deep convolutional neural networks (DNN), introduced by _cite_, improved by _cite_, refined and simplified by _cite_ . Lately, DNN proved their mettle on data sets ranging from handwritten digits (MNIST) _cite_, handwritten characters _cite_ to ND toys (NORB) and faces _cite_ . DNNs fully unfold their potential when they are big and deep _cite_ . But training them requires weeks, months, even years on CPUs. High data transfer latency prevents multi-threading and multi-CPU code from saving the situation. In recent years, however, fast parallel neural net code for graphics cards (GPUs) has overcome this problem. Carefully designed GPU code for image classification can be up to two orders of magnitude faster than its CPU counterpart _cite_ . Hence, to train huge DNN in hours or days, we implement them on GPU, building upon the work of _cite_ . The training algorithm is fully online, i.e. weight updates occur after each error back-propagation step. We will show that properly trained big and deep DNNs can outperform all previous methods, and demonstrate that unsupervised initialization/pretraining is not necessary (although we don't deny that it might help sometimes, especially for small datasets) . We also show how combining several DNN columns into a Multi-column DNN (MCDNN) further decreases the error rate by N-N