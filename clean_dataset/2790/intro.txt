We have recently seen a wide and steady release of state-of-the-art feed-forward deep convolutional neural networks for vision related tasks . These models have reached, and then also surpassed the human-level performance of object recognition in the ImageNet classification challenge . Currently, these models are trained end-to-end, using strong supervision. This means that large collections of annotated still-images are fed to the networks, the gradient of a cross entropy loss function with respect to the model parameters is computed with back-propagation, and gradient descent is used to minimise the error between the prediction and the ground truth. We then want to utilise these models for real life applications, feeding them with a stream of video frames, and expecting them to behave similarly well on live data, but this is not often the case. Furthermore, these models are highly susceptible to inputs corrupted by adversarial noise . Such inputs are made up of small carefully designed perturbations, which are invisible to the normal human vision. For some extent, we can attribute the temporal prediction instability of the feed-forward models to the natural occurrence of adversarial noise. Arguably, our visual system is immune to such temporal perturbations, because in the early years of an individual it has been ``trained to see'' by performing tracking on specific objects with sporadic parental weak supervision, and not from a large collection of static annotated flash cards. Therefore, we propose CortexNet, a neural network family which not only models the bottom-up feed-forward connections in our visual system, but also employs delayed modulatory feedback with lateral connections, in order to learn end-to-end a more robust representation of natural temporal visual inputs. We train our models either unsupervisedly or with weak sparse annotations, through leveraging of the temporal coherence which is present among the frames of a natural video clip. Our models show short-term reliable next frames prediction by (N) compensating for the camera egomotion, (N) learning the trajectory of the object present in the current scene, and (N) focussing on one object at the time. Our preliminary results indicate that the network develops an internal salient and attention mechanism. This leads to an effective internal representation of our reality, which demonstrates a superior and more robust network class.