Speech is considered as a form of biometric verification, since everybody has his or her unique voice. Speaker verification aims to extract features from a speaker's speech samples and use them to recognize or verify speaker identity through modelings of the speaker's speech samples~ _cite_ . The speaker-verification literature focuses on designing a setup in which the claimed identity of a speaker is either accepted or rejected, which can be conducted as text-dependent~ _cite_ or text-independent~ _cite_ . During text-dependent speaker verification the speech content is a predefined, fixed text, such as a passphrase, while text-independent speaker verification aims to verify the speaker using freeform spoken words, independent of the text or language or other prior constraints. The possible unconstrained variations in text-independent speaker verification make it much more challenging compared to text-dependent models~ _cite_ . Voice samples can be acquired through different recording devices and are subject to device and quality mismatch. In addition, the samples can be recorded at different sampling rates and distances, which result in bit-rate mismatch and channel noise. The samples are also subject to background noise problem due to environmental noise and distortion. Channel-independent speaker verification frameworks~ _cite_ try to address this problem. Channel-independent text-independent frameworks are considered to be the ultimate test in the speaker verification domain~ _cite_ . Deep learning algorithms are the state-of-the-art frameworks for many biometric applications such as face~ _cite_, fingerprint~ _cite_, and iris~ _cite_ classification, as well as multimodal classification~ _cite_, attribute-enhanced classification~ _cite_, and domain adaptation~ _cite_ . Deep learning architectures have recently proven to be able to provide superior performance compared to traditional speaker verification algorithms, showing significant gains over the state-of-the-art Gaussian Mixture Models and Hidden Markov Models~ _cite_ . The majority of the deep learning architectures proposed for speaker recognition task are multilayer perceptron (MLP)-based models using Mel-frequency cepstral coefficients (MFCCs) ~ _cite_ . However, MLP-MFCC architectures fail to preserve the correlation between the adjacent features. To address this issue convolutional neural networks (CNNs) are used in speaker recognition~ _cite_ . Additionally, compared to architectures requiring hand-crafted features, convolutional neural networks (CNNs) extract and classify features simultaneously, and, therefore, avoid losing valuable information~ _cite_ . State-of-the-art deep speaker recognition systems use spectro-temporal voice features~ _cite_ . The most well-known these short-term features used in the literature are spectrogram, MFCCs, and Mel-frequency spectrogram coefficients (MFSCs) . Inheriting from the short-term nature of these features, most of the models proposed only explore the acoustic level of the signal, such as spectral magnitudes and formant frequencies~ _cite_ . However, several important linguistic levels such as lexicon, prosody or phonetics cannot be recognized from short-term features. These levels of information are learned habits by the speaker. These features do not perform as well as the short-term features in the identification and verification scenarios when the utterances are significantly short. However, when the length of the utterance increases, it is shown that the identification and verification performance of the prosodic features increases drastically~ _cite_ . These features also significantly improve the model when fused with short-term features~ _cite_ . In the speaker-verification literature a three-phase procedure is defined. Initially, in the {\it development} phase, background models are developed from a large collection of data. New speakers are added to the model during the {\it enrollment} phase to construct speaker-dependent models. In the {\it evaluation} phase test utterances are compared to the enrolled speaker models and the background model to verify the identity of the speaker~ _cite_ . In this setup, the difference between low-dimensional representations of enrollment and test utterances is considered to accept or reject the hypothesis~ _cite_ . However, in the proposed algorithm, the enrollment phase is excluded. The proposed Siamese model is trained using the utterances from the training set in the {\it training} phase. In the {\it test} phase, the trained model is deployed to compute the distance between two utterances. The computed inter sub-network distance is used to determine whether or not the utterances belong to the same speaker. In this paper, we make the following contributions: (i) prosodic, jitter, and shimmer features are deployed to enhance the performance of the proposed CNN Siamese network; (ii) a text-independent embedding space is constructed considering short-term and prosodic features; (iii) rather than extracting the features using hand-crafted methods, a fully data-driven architecture using fused CNN and MLP networks has been optimized for joint domain-specific feature extraction and representation with the application of speaker verification, finally (iv) the proposed algorithm can be used for real-time applications since it does not require the enrollment phase.