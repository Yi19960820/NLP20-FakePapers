Image captioning, the task of automatically describing the content of an image with natural language, has attracted increasingly interests in computer vision. It is interesting because it aims at endowing machines with one of the core human intelligence to understand the huge amount of visual information and to express it in natural language. Recent state-of-the-art approaches~ _cite_ follow an encoder-decoder framework to generate captions for the images. They generally employ convolutional neural networks to encode the visual information and utilize recurrent neural networks to decode that information to coherent sentences. During training and inference, they try to maximize the probability of the next word based on recurrent hidden state. In this paper, we introduce a novel decision-making framework for image captioning. Instead of learning a sequential recurrent model to greedily look for the next correct word, we utilize a ``policy network" and a ``value network" to jointly determine the next best word at each time step. The policy network, which provides the confidence of predicting the next word according to current state, serves as a . The value network, that evaluates the reward value of all possible extensions of the current state, serves as a . Such value network adjusts the goal of predicting the correct words towards the goal of generating captions that are similar to ground truth captions. Our framework is able to include the good words that are with low probability to be drawn by using the policy network alone. Figure~ _ref_ shows an example to illustrate the proposed framework. The word is not among the top choices of our policy network at current step. But our value network goes forward for one step to the state supposing is generated and evaluates how good such state is for the goal of generating a good caption in the end. The two networks complement each other and are able to choose the word . To learn the policy and value networks, we use deep reinforcement learning with embedding reward. We begin by pretraining a policy network using standard supervised learning with cross entropy loss, and by pretraining a value network with mean squared loss. Then, we improve the policy and value networks by deep reinforcement learning. Reinforcement learning has been widely used in gaming~ _cite_, control theory~ _cite_, . The problems in control or gaming have concrete targets to optimize by nature, whereas defining an appropriate optimization goal is nontrivial for image captioning. In this paper, we propose to train using an actor-critic model~ _cite_ with reward driven by visual-semantic embedding~ _cite_ . Visual-semantic embedding, which provides a measure of similarity between images and sentences, can measure the correctness of generated captions and serve as a reasonable global target to optimize for image captioning in reinforcement learning. We conduct detailed analyses on our framework to understand its merits and properties. Extensive experiments on the Microsoft COCO dataset~ _cite_ show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU _cite_, Meteor~ _cite_, Rouge~ _cite_ and CIDEr~ _cite_ . The contributions of this paper are summarized as follows: