Prostate cancer is among the main causes of cancer death in men in the United States _cite_ . Although transrectal ultrasound (TRUS) usually has low sensitivity with respect to prostate cancer, it is still the most commonly used imaging modality for guiding prostate biopsy. On the other hand, multi-parametric magnetic resonance (MR) imaging has been shown to have good sensitivity and specificity for identifying a prostate cancer lesion. This is an expensive and time-consuming procedure. Over the past decade, studies have shown that the fusion of TRUS and MR images for guiding prostate biopsies for cancer diagnosis provides clinical benefit by limiting the rate of false negative prostate cancer diagnoses _cite_ . Image registration is a key component for multimodal image fusion, which generally refers to the process by which two or more image volumes and their corresponding features (acquired from different sensors, points of view, imaging modalities, etc.) are aligned into the same coordinate space. Medical images that are acquired from different imaging modalities use different imaging physics, which creates unique advantages and disadvantages. Relatively unique information about the imaged volume is provided by each modality. Image fusion through registration can integrate the complementary information from multimodal images to help achieve more accurate diagnosis and treatment _cite_ . In the case of MR-TRUS fusion, the real-time imaging and cost-effective properties of TRUS can be well complemented by the high prostate cancer identification accuracy of MR imaging for image-guided prostate interventions _cite_ . Mutual information is the most common pixel-based similarity metric for multi-modality image registration and utilizes the statistical information associated with the image volumes obtained from different modalities _cite_ . However, the correspondence between the alignment with maximum mutual information and the expert alignment for difficult multimodal registration tasks is typically poor because of the inadequate description of the image alignment using pixel intensity mapping. Due to the difficulties associated with directly registering TRUS and MR images, this registration is commonly performed using surface-based methods through shape modeling and the use of feature descriptors _cite_ . For example, Sun et~al. _cite_ used the modality independent neighborhood descriptor (MIND) _cite_ to map the voxels that constitute the MR and TRUS volumes to a descriptor value for comparison. In their image registration framework, the sum of squared differences between the MIND descriptors at the corresponding locations in MR and TRUS images is used as the similarity metric. Although they were able to obtain good registration results in many cases, the quantification of similarity was done using manually crafted feature mapping and can limit the registration performance when the initialization is far from the underlying registration. Sparks et~at. _cite_ developed a fully automatic registration approach utilizing image segmentations to address the appearance difference between the two modalities _cite_ . However, such registration techniques cannot guarantee adequate voxel-to-voxel correspondence of internal structures because these approaches are primarily influenced by the information that is extracted from voxels proximal to the boundary of the prostate. Unlike the approaches described in the works discussed above, our method uses raw pixel data as its input and uses learned features for estimating the image similarity. Recently, several works have used deep neural networks to learn application-specific similarity metrics for image registration tasks _cite_ . Although these existing similarity metric learning methods outperform mutual information and other manually defined metrics for their applications, the existing deep learning based methods deal with multimodal images that share largely similar views or relatively simple intensity mappings (for example MR-CT or TN-TN weighted MR images) . In our application, the prostate in MR and TRUS looks very different in terms of not only image intensities but also fields of view, which is a much more challenging problem. In this paper, we propose a deep learning based approach directly registering the two imaging modalities using image pixel intensities. Our primary contributions are two-fold. First, we propose designing a CNN with a skip connection to learn the target registration error (TRE) between ND MR and TRUS images to act as image similarity metric for registration. Second, we propose a differential evolution initialized Newton-based optimization (DINO) method to perform the optimization and expand the capture range of the registration. To efficiently explore the solution space and also enhance the capture range, the proposed approach uses differential evolution, followed by the local second order algorithm-BFGS. The application of the developed approach shows promising performance and also advantages over the classical mutual information and MIND based approaches _cite_ . The rest of the paper is organized as follows. Section~ _ref_ presents the details of the proposed method. Section~ _ref_ describes how the convolutional neural network used for learning the similarity metric is trained. The experimental results are given in Section~ _ref_ . Finally, Section~ _ref_ draws the conclusions and briefly discusses our future work.