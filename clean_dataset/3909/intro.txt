Most of the existing methods of neural networks use one-hot vector representations for labels. The one-hot vector has two main restrictions. The first restriction is the ``discrete distribution'', where each label is distributed at a completely different dimension from the others. The second restriction is the ``extreme value'' based representation, where the value at each dimension is either N or N, and there is no ``soft value'' allowed. Those deficiencies may cause the following two potential problems. First, it is not easy to measure the correlation among the labels due to the ``discrete distribution''. Not being able to measure the label correlation is potentially harmful to the learned models, e.g., causing the data sparseness problem. Given an image recognition task, the image of the is often similar to the image of the . Naturally, we expect the two labels to be ``similar''. Suppose that we have a lot of training examples for, and very few training examples for . If the label and the label have similar representations, the prediction for the label will suffer less from the data sparsity problem. Second, the N/N value encoding is easy to cause the overfitting problem. Suppose and are labels of two similar types of fishes. One-hot label representation prefers the ultimate separation of those two labels. For example, if currently the system output probability for is N and the probability for is N, it is good enough to make a correct prediction of . However, with the one-hot label representation, it suggests that further modification to the parameters is still required, until the probability of becomes N and the probability of becomes N. Because the fish and the fish are very similar in appearance, it is probably more reasonable to have the probability N for and N for, rather than completely N for and N for, which could lead to the overfitting problem. We aim to address those problems. We propose a method that can automatically learn label representation for deep neural networks. As the training proceeds, the label embedding is iteratively learned and optimized based on the proposed label embedding network through back propagation. The original one-hot represented loss function is softly converted to a new loss function with soft distributions, such that those originally unrelated labels have continuous interactions with each other during the training process. As a result, the trained model can achieve substantially higher accuracy, faster convergence speed, and more stable performance. The related prior studies include the traditional label representation methods, the ``soft label'' methods, and the model distillation methods . Our method is substantially different from those existing work, and the detailed comparisons are summarized in Appendix _ref_ . The contributions of this work are as follows: