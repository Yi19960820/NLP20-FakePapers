Multimedia applications involve the presentation of different audiovisual objects organized in time and space _cite_ . Given the nature of the content they present, the majority of current multimedia applications stimulate only two human senses: sight and hearing. As discussed in _cite_, aiming at increasing the user quality of experience (QoE) and immersion with multimedia applications, the literature present works _cite_ that propose the use of other sensory effects in multimedia applications in order to provide users with new sensations during a multimedia presentation. In _cite_, the term mulsemedia () is put forward to denote multimedia applications in which traditional media content (text, image, audio, video, etc.) can be related to media objects that target other human senses (e.g., smell, haptics, etc.) . To clarify, let us describe a simple yet powerful example . Consider a non-linear show, i.e., a show whose narrative line is not known a priori and is constructed based on user interaction. In this show the user actively participates in the construction of the narrative line by choosing the next sight in a city tour from a list of available options provided by the application. At each sight, a video and complementary information about it are presented to the user. At the beginning of the show, the user may choose whether he/she wants to interact with the application. If not, a default tour is presented. Let us now consider an evolution of such application which includes sensory effects. In this new application, several sensory effects are presented along the narrative line in a synchronized way, with the purpose of increasing the immersion of the user in the audiovisual content. In this scenario, if the user chooses to visit a particular sight, e.g., the beach at Rio de Janeiro, the sensory effects to be presented by the application would mimic the sight's environmental conditions (e.g., hot wind blowing, smell of the sea breeze, etc) . Synchronization plays an important role for multimedia applications. Multimedia authoring languages are domain specific languages that provide constructions for defining how media objects shall be presented during the execution of a multimedia application, i.e., their temporal synchronization. Moreover, those languages also manage user interaction as a special case of temporal synchronization. Examples of such languages are SMIL () ~ _cite_ and NCL () ~ _cite_ . When considering the use of sensory effects in multimedia applications, the usual approach is to use audiovisual media objects as the base for synchronization, such that the timestamps in which sensory effects are to be executed are defined in relation to certain media objects. For example, light and heat effects may be presented when an explosion occurs in the main video. One should notice, however, that although multimedia languages easy the specification of the synchronization aspect in an application, the task of defining all the moments in which a given sensory effect shall be executed is still carried manually by the author. In general, authors relate sensory effects to the content being presented by the application. In _cite_, we define a as, a given element (rock, tree, dog, person, etc.) or concept (happy, crowded, dark, etc.) that appears in the content of a media object. In the application example we presented earlier, scene components may refer to the sun, the beach, trees, flowers, and other elements that may appear in the Rio de Janeiro's sights presented in the touristic program. In previous work _cite_, we tackle the problem of automatically recognizing scene components in audiovisual objects, in order to assist the realization of the synchronization task in mulsemedia applications. We proposed an architecture capable of identifying the presence of scene components in video and audio objects and defining, in a semi-supervised manner, the synchronization among sensory effects and an application main video and/or audio. In this paper, we focus on the recognition of scene components specifically related to sensory effects. Examples of such kind of scene components are wind, explosion, rain, lightning, etc. Some of these components may be predominantly found either in the video (e.g., lightning) or in the audio (e.g., wind) . Other components (e.g, explosion) can be found in both modalities of the audiovisual object. In particular, we propose the use of a bimodal neural network architecture for increasing the accuracy of recognition of scene components specifically for the task of synchronizing sensory effects in mulsemedia applications. Our premise is that trying to combine both modalities in the identification can produce a better accuracy when compared to the separate identification. We perform computational experiments with different network architectures, both unimodal (either audio or video) and bimodal (audio and video) . Through experimental verification, we show that the proposed bimodal fusion approach is superior in accuracy when compared with any single modal recognition system. The contributions of this paper are twofold: (i) we propose a bimodal learning architecture for predicting scene components in audiovisual content, and (ii) we present the first version of a dataset tailored for the prediction of scene components related to sensory effects. The remaining of this paper is structured as follows. Section~ _ref_ presents related work regarding the use of bimodal neural networks to combine audio and video information in several application domains. Section~ _ref_ provides a brief description of AudioSet, the collection of video clips we use on our learning architecture. This Section also describes how we built our own training set based on AudioSet. Section~ _ref_ discusses the network architectures used in this work and how both modalities can be used to improve accuracy. Section~ _ref_ presents experimental results along with a corresponding analysis. Section~ _ref_ concludes and discusses future work.