Deep neural networks have been successfully applied in many artificial intelligence tasks and provide state-of-the-art performance. However, the theoretical understandings of learning in deep neural networks has lagged the practical success. The reason why deep learning algorithms perform so well and how to predict their generalization performance are still unclear. Most of well-established approaches, such as hypothesis capacity or sparseness, have not provided complete explanations, due to the high complexity of the deep learning algorithms and their inherent randomness. Recently, Xu \etal~ established the connection between robustness of an algorithm and its generalization performance. In particular, they point out that if one algorithm is robust (\ie, its empirical loss does not change dramatically for perturbed samples), its generalization performance can also be guaranteed in a positive way. In parallel with that work, several methods based on robustly optimizing the empirical loss are developed for training deep neural network models~ _cite_ . However, deep neural networks were shown to be fragile to adversarial perturbation on the inputs~ _cite_ . Even an imperceptible perturbation over training samples can corrupt performance of a neural network. Thus the robustness argument in~ _cite_ cannot be applied here for explaining the performance of deep learning algorithms. Several researchers also propose to understand deep learning performance through regularization~ _cite_, stability~ _cite_ and non-convex optimization~ _cite_ . Although those works and others are pursuing explanation of the performance of deep neural networks, the reason is still far away from being understood well. In this work, we present a new approach,, to characterize the generalization performance of deep learning algorithms. Our proposed approach is not intended to give tight performance guarantees for general deep learning algorithms, but rather to pave a way for addressing the question: why deep learning performs so well? Answering this question is difficult, yet we present evidence in both theory and simulations strongly suggesting that is crucial to the generalization performance of deep learning algorithms. Ensemble robustness concerns the fact that a randomized algorithm (\eg, SGD and dropout) produces a distribution of hypotheses instead of a deterministic one. Therefore, ensemble robustness takes into consideration robustness of the of the hypotheses: even though some hypotheses may be sensitive to perturbation on inputs, an algorithm can still generalize well as long as the most of the hypotheses sampled from the distribution are robust. Ensemble robustness gives a rigorous description to this feature of randomized algorithms. Through ensemble robustness, we prove that the following holds with a high probability: randomized learning algorithms can generalize well as long as its output hypothesis has bounded sensitiveness to perturbation in average (see Theorem _ref_) . Specified for deep learning algorithms, we reveal that if the hypothesis from different runs of a deep learning method performs consistently well in terms of robustness, the performance of such a deep learning method can be confidently expected. Furthermore, we provide an explanation on the effectiveness of dropout through ensemble robustness. Although ensemble robustness may be difficult to compute analytically, we demonstrate an empirical estimate of ensemble robustness also works well. We empirically investigate the role of ensemble robustness via extensive simulations on seven common deep learning algorithms. The results provide supporting evidence for our claim: ensemble robustness consistently explain the performance well for deep networks with different architectures and deep learning algorithms. We believe this work could pave a way to understand why deep learning methods perform well in practice.