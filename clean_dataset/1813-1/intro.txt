In N, deep residual networks won Nst place on the ILSVRC classification task _cite_ . We seek to understand the qualitative characteristics underlying the shortcut connections and identity mappings that motivated He et al. towards this network architecture. To this end, we implement two visualizations of the feature maps after each residual building block: the top-N images that maximally activate a unit in a given channel, and a guided backpropagation _cite_ visualization that unit's activation. From these visualizations, we can visually confirm He et al.'s intuition that preconditioning layers to the identity mapping helps, and that it is easier to learn functions relative to the identity mapping. Specifically, we observe that residual layers of the same dimensionality learn features that get refined and sharpened. Zeiler et al. performed similar visualizations of AlexNet features _cite_, introducing a deconvolutional transformation that consists of taking the desired unit activation that one wishes to visualize and moving backwards through a series of deconvolutional steps. Instead of mapping from pixel space to feature space, deconvolution maps from feature space to pixel space. To move backwards through max-pooling, an unpooling step is performed, where the units that were selected as the maximal units in the forward pass are assigned the values being propagated backwards. To reverse convolution, a transposed convolution (also known as a fractionally strided convolution _cite_) is performed, using the same parameters as the network's learned convolutional layer. Finally, moving backwards through rectification was performed with rectification of the backwards inputs. This approach constructs in pixel space a visualization of the parts of an image contributing most to the activation of a given unit. Springenberg et al. built upon the deconvolutional approach, developing guided backpropagation _cite_ . Guided backpropagation is a tweak of the deconvolutional approach, where units that are rectified to zero in the forward pass (because they have a negative value) are also set to zero in the deconvolutional pass. This was demonstrated to be visually superior to visualization based on deconvolution, for the networks that Springenberg et al. examined. Finally, Yosinski et al. visualized AlexNet using a variety of approaches, including the previous approaches and optimization to synthetically generate maximally-activating images _cite_ .