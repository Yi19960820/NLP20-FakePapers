Since the resurgence of neural networks, deep learning methods have been gaining huge success in diverse fields of applications, amongst which, semantic segmentation, classification, object detection, image annotation and natural language processing are few to mention _cite_ . What has made this enormous success possible is the ability of deep architectures to do feature learning automatically, eliminating the need for a feature engineering stage. In this stage which is the most important one amongst others, the preprocessing pipelines and data transformation are designed using human ingenuity and prior knowledge _cite_ and has a profound effect on the end result. It is highly dependent on the level of engineers experience and expertise and if done poorly the result would be disappointing. It however, cannot scale or be generalized for other tasks well. Furthermore, in deep learning methods, instead of manual and troublesome feature engineering, feature learning is carried out automatically in an efficient way. Deep methods also scale very well to different tasks of different essence. This proved extremely successful which one can say by looking at the diverse fields it has been being used. CNNs, have been one of the most popular deep learning methods and also a major winner in many computer vision and natural language processing related tasks lately _cite_ . Since CNNs take into account the locality of the input, they can find different levels of correlation through a hierarchy of consecutive application of convolution filters. This way they are able to find and exploit different levels of abstractions in the input data and using this perform very well on both coarse and fine level details. Therefore the depth of a CNN plays an important role in the discriminability power the network offers. The deeper the better. What all of the recent architectures have in common is the increasing depth and complexity of the network that provides better accuracy for the aforementioned tasks. The winner of the ImageNet Large Scale Visual Recognition Competition N (ILSVRC) _cite_ has achieved its success using a very deep architecture of N layers _cite_ . The runner up also deploys a deep architecture of N layers _cite_ . This trend has proved useful in the natural language processing benchmarks as well _cite_ . While this approach has been useful, there are some inevitable issues that arise when the network gets more complex. Computation and memory usage cost and overhead is one of the critical issues that is caused by the excessive effort put on making networks deeper and more complex in order to make them perform better. This has a negative effect on the expansion of methods and applications utilizing deep architectures. Despite the existence of various techniques for improving the learning algorithm, such as different initialization algorithms _cite_, normalization and regularization method and techniques _cite_, non-linearities _cite_ and data-augmentation tricks _cite_, they are most beneficial when used on an already well performing architecture. In addition, some of these techniques may even impose more computational and memory usage overhead _cite_ . Therefore, it would be highly desirable to propose efficient architectures with smaller number of layers and parameters that are as good as their deeper versions. Such architectures can then be further tweaked using novel advancements in the literature. The main contribution of our work is the proposal of a simple architecture, with minimum reliance on new features that outperforms almost all deeper architectures with N to N times fewer parameters. Our architecture, SimpleNet, can be a very good candidate for many scenarios, especially for deploying in the embedded devices. It can be further compressed using methods such as DeepCompression _cite_ and thus its memory consumption can be decreased drastically. We intentionally imposed some limitation on ourselves when designing the architecture and tried to create a mother architecture with minimum reliance on new features proposed recently, to show the effectiveness of a well-crafted yet simple convolutional architecture. It is clear when the model performs well in spite of all limitations, relaxing those limitations can further boost the performance with little to no effort which is very desirable. This performance boost however has direct correlation with how well an architecture is designed. However a fundamentally badly designed architecture would not be able to harness the advantages because of its inherent [flawed] design, therefore we also provide the intuitions behind the overall design choices. The rest of the paper is organized as follows: Section _ref_ presents the most relevant works. In Section _ref_ we present our architecture and the set of designing principles used in the design of the architecture. In Section _ref_ the experimental results are presented conducted on N major datasets (CIFARN, CIFARN, SVHN and MNIST) and more details about the architecture and different changes pertaining to each dataset are explained. Finally, conclusions and future work are summarized in Section _ref_ and acknowledgment is covered in section _ref_ .