Segmentation is one of the main medical image analysis tasks that when automated substantially facilitates morphological measurements and increase efficiency in treatment planning~ _cite_ . With the introduction of machine learning and especially convolutional neural networks (CNNs) the performance of automatic segmentation approaches improved greatly~ _cite_ . Recent studies showed that CNN-based approaches were able to achieve inter-and intra-expert performance in certain segmentation tasks, for example prostate segmentation in Magnetic Resonance Images (MRIs) as shown in~ _cite_ . Although these approaches achieve impressive performance on average, when considering an individual image, there are often parts of the segmentation users would like to change and improve to fit their needs. The need for edits and improvement is even larger when the test image differs slightly from the training dataset, for example due to scanner differences, and more errors are expected. To address the need for editing, interactive segmentation algorithms have been proposed such as GrabCut, GeoS or Random Walker~ _cite_ that allow operators to modify segmentations. Even though accurate results have been shown with these methods, the interaction can be time consuming as large number of interactions might be necessary. In particular, updates aiming to correct segmentation in one region can lead to inaccuracies in another region, consequently requiring further interactions. In recent years, studies such as~ _cite_ proposed CNNs for interactive segmentations and showed better results compared to traditional methods. These initial works focused on segmenting objects in medical images from scratch using simple user interactions, and mostly in the form of binary segmentations. More recently, authors in~ _cite_ proposed a CNN-based method for editing segmentations predicted by an automatic algorithm, one of the most important steps in translating automatic segmentations in practice, and showed the benefits for binary segmentations. In the same work, authors assumed multiple scribbles to be made at a single time and the editing network was trained to take into account all the edits, initial prediction and the image to generate an updated segmentation. This training strategy may not be ideal since it does not take into account the fact that a user may be interacting with the tool over several iterations, each time providing scribbles based on the result of the last update. In this work, we present a different strategy for training a CNN that interactively edits segmentations. As in _cite_, we assume the editing CNN is an auxiliary tool that supports a base segmentation algorithm and is optimized to take into account user edits and improve segmentation accuracy. Different than _cite_, we investigate training in an iterative interaction fashion on simulated user inputs and we also focus on multi-label segmentation problems as well as binary ones. We assess the potential of the proposed training strategy on the prostate data of the NCI-ISBI N challenge and show the value of iterative interaction training. Moreover, we empirically compare networks for editing segmentations with a state-of-the-art fully interactive segmentation algorithm that segments the image from scratch using user-made scribbles.