Normalization techniques are effective components in deep learning, advancing many research fields such as natural language processing, computer vision, and machine learning. In recent years, many normalization methods such as Batch Normalization (BN), Instance Normalization (IN), and Layer Normalization (LN) have been developed. Despite their great successes, existing practices often employed the same normalizer in all normalization layers of an entire network, rendering suboptimal performance. Also, different normalizers are used to solve different tasks, making model design cumbersome. To address the above issues, we propose, which combines three types of statistics estimated channel-wise, layer-wise, and minibatch-wise by using IN, LN, and BN respectively. SN switches among them by learning their importance weights. By design, . For example, the ratios of IN, LN, and BN in SN are compared in multiple tasks as shown in Fig. _ref_ (a) . We see that using one normalization method uniformly is not optimal for these tasks. For instance, image classification and object detection prefer the combination of three normalizers. In particular, SN chooses BN more than IN and LN in image classification and the backbone network of object detection, while LN has larger weights in the box and mask heads. For artistic image style transfer, SN selects IN. For neural architecture search, SN is applied to LSTM where LN is preferable than group normalization (GN), which is a variant of IN by dividing channels into groups. The selectivity of normalizers makes . As shown in Fig. _ref_ (b), when training ResNetN on ImageNet with different batch sizes, SN is close to the ``ideal case'' more than BN and GN. For _inline_eq_ as an example, ResNetN trained with SN is able to achieve N \% top-N accuracy, surpassing BN and GN by N \% and N \% respectively. In general, SN obtains better or comparable results than both BN and GN in all batch settings. Overall, this work has three key contributions . {(N)} We introduce Switchable Normalization (SN), which is applicable in both CNNs and RNNs/LSTMs, and improves the other normalization techniques on many challenging benchmarks and tasks including image recognition in ImageNet, object detection in COCO, scene parsing in Cityscapes and ADENK, artistic image stylization, neural architecture search, and video recognition in Kinetics . (N) The analyses of SN are presented where multiple normalizers can be compared and understood with geometric interpretation. {(N)} By enabling, SN helps ease the usage of normalizers, pushes the frontier of normalization in deep learning, as well as opens up new research direction. We believe that all existing models could be reexamined with this new perspective. We'll make the code of SN available and recommend it as an alternative of existing handcrafted approaches. In the following sections, we first present SN in Sec. _ref_ and then discuss its relationships with previous work in Sec. _ref_ . SN is evaluated extensively in Sec. _ref_ .