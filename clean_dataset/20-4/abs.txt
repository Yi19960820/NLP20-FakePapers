In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a \las \based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-N achieves the state-of-the-art results by speed-up along with only N \% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only \resft \%, \xceptionft \% accuracy loss under speed-up respectively, which is significant. Code has been made publicly available .