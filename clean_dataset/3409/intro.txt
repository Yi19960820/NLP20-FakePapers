We consider the problem of novel ND view synthesis---given a single view of an object in an arbitrary pose, the goal is to synthesize an image of the object after a specified transformation of viewpoint. It has a variety of practical applications in computer vision, graphics, and robotics. As an image-based rendering technique~ _cite_, it allows placing a virtual object on a background with a desired pose or manipulating virtual objects in the scene~ _cite_ . Also, multiple generated ND views form an efficient representation for ND reconstruction~ _cite_ . In robotics, synthesized novel views give the robot a better understanding of unseen parts of the object through ND reconstruction, which will be helpful for grasp planning~ _cite_ . This problem is generally challenging due to unspecified input viewing angle and the ambiguities of ND shape observed in only a single view. In particular inferring the appearances of unobserved parts of the object that are not visible in the input view is necessary for novel view synthesis. Our approach attacks all of these challenges, but our contributions focus on the later aspect, dealing with disoccluded appearance in novel views and outputting highly-detailed synthetic images. Given the eventual approach we will take, using a carefully constructed deep network, we can consider related work on dense prediction with encoder-decoder methods to see what makes the structure of the novel ND view synthesis problem different. In particular, there is a lack of pixel-to-pixel correspondences between the input and output view. This, combined with large chunks of missing data due to occlusion, makes novel view synthesis fundamentally different than other dense prediction or generation tasks that have shown promising results with deep networks~ _cite_ . Although the input and desired output views may have similar low-level image statistics, enforcing such constraints directly is difficult. For example, skip or residual connections, are not immediately applicable as the input and output have significantly different global shapes. Hence, previous ND novel view synthesis approaches~ _cite_ have not been able to match the visual quality of geometry-based methods that exploit strong correspondence. The geometry-based methods are an alternative to pure generation, and have been demonstrated in~ _cite_ . Such approaches estimate the underlying ND structure of the object and apply geometric transformation to pixels in the input (e.g. performing depth-estimation followed by ND transformation of each pixel~ _cite_) . When successful, geometric transformation approaches can very accurately transfer original colors, textures, and local features to corresponding new locations in the target view. However, such approaches are fundamentally unable to hallucinate where new parts are revealed due to disocclusion. Furthermore, even for the visible geometry precisely estimating the ND shape or equivalently the precise pixel-to-pixel correspondence between input and synthesized view is still challenging and failures can result in distorted output images. In order to bring some of the power of explicit correspondence to deep-learning-based generation of novel views, the recent appearance flow network (AFN) ~ _cite_ trains a convolutional encoder-decoder to learn how to move pixels without requiring explicit access to the underlying ND geometry. Our work goes further in order to integrate more explicit reasoning about ND transformation, hallucinate missing sections, and clean-up the final generated image producing significant improvements of realism, accuracy, and detail for synthesized views. To achieve this we present . Our approach first predicts the transformation of existing pixels from the input view to the view to be synthesized, as well as a visibility map, exploiting the learned view dependency. We use the transformation result matted with the predicted visibility map to condition the generation process. The image generator not only hallucinates the missing parts but also refines regions that suffer from distortion or unrealistic details due to the imperfect transformation prediction. This holistic pipeline alleviates some difficulties in novel view synthesis by explicitly using transformation for the parts where there are strong cues. We propose an architecture composed of two consecutive convolutional encoder-decoder networks. First, we introduce a disocclusion aware appearance flow network (DOAFN) to predict the visibility map and the intermediate transformation result. Our second encoder-decoder network is an image completion network which takes the matted transformation as an input and completes and refines the novel view with a combined adversarial and feature-reconstruction loss. A wide range of experiments on synthetic and real images show that the proposed technique achieves significant improvement compared to existing methods. Our main contributions are: