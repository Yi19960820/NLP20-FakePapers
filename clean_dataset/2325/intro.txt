Obtaining sentence level descriptions for images is becoming an important task and it has many applications, such as early childhood education, image retrieval, and navigation for the blind. Thanks to the rapid development of computer vision and natural language processing technologies, recent work has made significant progress on this task (see a brief review in Section _ref_) . Many previous methods treat it as a retrieval task. They learn a joint embedding to map the features of both sentences and images to the same semantic space. These methods generate image captions by retrieving them from a sentence database. Thus, they lack the ability of generating novel sentences or describing images that contain novel combinations of objects and scenes. In this work, we propose a multimodal Recurrent Neural Networks (m-RNN) model to address both the task of generating novel sentences descriptions for images, and the task of image and sentence retrieval. The whole m-RNN model contains a language model part, a vision part and a multimodal part. The language model part learns a dense feature embedding for each word in the dictionary and stores the semantic temporal context in recurrent layers. The vision part contains a deep Convolutional Neural Network (CNN) which generates the image representation. The multimodal part connects the language model and the deep CNN together by a one-layer representation. Our m-RNN model is learned using a log-likelihood cost function (see details in Section _ref_) . The errors can be backpropagated to the three parts of the m-RNN model to update the model parameters simultaneously. In the experiments, we validate our model on four benchmark datasets: IAPR TC-N (_cite_), Flickr NK (_cite_), Flickr NK (_cite_) and MS COCO (_cite_) . We show that our method achieves state-of-the-art performance, significantly outperforming all the other methods for the three tasks: generating novel sentences, retrieving images given a sentence and retrieving sentences given an image. Our framework is general and can be further improved by incorporating more powerful deep representations for images and sentences.