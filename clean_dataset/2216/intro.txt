Deep Convolutional Neural Network (CNN) models have seen great success in solving general object recognition problems~ . However, due to their extremely huge parameter space, the performance of deep models relies heavily on the availability of a sufficiently large number of training examples. In practice, collecting images as well as their accurate annotations at a large scale is usually tedious and expensive. On the other hand, there are millions of freely available images with user-supplied tags that can be easily collected from the web. Being able to exploit this rich resource seems promising for learning a deep classification model. The labels of these web images, however, tend to be much more noisy, and hence challenging to learn from. In this work, we consider the problem of image classification in this challenging scenario where annotations of the training examples are noisy. In particular, we are interested in how to learn a deep CNN model that is able to produce robust image representations and classification results in presence of noisy supervision. Deep CNN models have been recognized to be sensitive to sample and label noise in recent works . Thus, several methods have been developed to alleviate the negative effect of noisy labels in learning the deep models. However, most of the existing methods only consider modeling a fixed category-level label confusion distribution and cannot alleviate the effect of noise in representation learning for each sample. We propose a novel auxiliary image regularizer (AIR) to address this issue of deception of training annotations. Intuitively, the proposed regularizer exploits the structure of the data and automatically retrieves useful auxiliary examples to collaboratively facilitate training of the classification model. Here, structure of the data means the nonlinear manifold structure underlying images from multiple categories learned from a well-trained deep model on another data set _inline_eq_ . To some extent, the AIR regularizer can be deemed as seeking some ``nearest neighbors'' within the training examples to regularize the fitting of a deep CNN model to noisy samples and improve its classification performance in presence of noise. Robustifying classification models via regularization is a common practice to enhance robustness of the models. Popular regularizers include Tikhonov regularization (_inline_eq_-norm) and the _inline_eq_-norm on the model parameters . However, an effective regularizer for training a deep CNN model is still absent, especially for handling the above learning problem with faulty labels. To the best of our knowledge, this work is among the first to introduce an effective regularizer for deep CNN models to handle label noise. Inspired by related work in natural language processing, we use a group sparse norm to automatically select auxiliary images. Figure~ _ref_ shows an exemplar overview of our model. In contrast to previous works imposing the regularization on the model parameter, we propose to construct groups of input image features and apply the group sparse regularizer on the response maps. Imposing such group sparsity regularization on the classifier response enables it to actively select the relevant and useful features, which gives higher learning weights to the informative groups in the classification task and forces the weights of irrelevant or noisy groups toward zero. The activated auxiliary images implicitly provide guiding information for training the deep models. We solve the associated optimization problem via ADMM~, recently popularized by~ _cite_ . In particular, we use the stochastic ADMM method of~ _cite_ and~ _cite_ on this large-scale problem. We demonstrate the effect of AIR on image classification via deep CNNs, where we synthetically corrupt the training annotations. We investigate how the proposed method identifies informative images and filters out noisy ones among the candidate auxiliary images. Going one step further, we then explore how the proposed method improves learning of image classification from user-supplied tags and handles the inherent noise in these tags. Comprehensive experiments on benchmark data sets, shown in Section~ _ref_, clearly demonstrate the effectiveness of our proposed method for the large-scale image classification task. A large body of existing work proposes to employ sparsity-inducing or group-sparsity inducing norms for effective model regularization and better model selection, with applications to dictionary learning and image representation learning, to name a few examples from the field of computer vision. However, most focus on imposing a structured prior on the model parameters. The idea of exploiting the group sparsity structure within raw data was recently proposed in, to solve text recognition problems, by exploiting the intrinsic structure among sentences in a document. However, as far as we know, none of existing works have investigated how to exploit the structural information among data for a deep model, as we address here, and we are among the first to propose such a regularized deep model based on auxiliary data. In this work, we are particularly interested in learning from noisy labeled image data, where a limited number of training examples are supplied with clean labels. Among the most recent contributions, solve this problem by learning the noise distribution through an extra noise layer added to the deep model, while explore this problem from a probabilistic graphical model point of view and train a classifier in an end-to-end learning procedure. introduce a robust logistic regression method for classification with user-supplied tags. deal with arbitrary outliers in the data through a robust logistic regression method by estimating the parameters in a linear programming scheme. Different from those existing works, we automatically exploit the contextual information from useful auxiliary images via a new regularization on deep CNN models.