Generalizability is crucial to deep learning, and many efforts have been made to improve the training and generalization accuracy of deep neural nets (DNNs) _cite_ . Advances in network architectures such as VGG networks _cite_, deep residual networks (ResNets) _cite_ and more recently DenseNets _cite_ and many others _cite_, together with powerful hardware make the training of very deep networks with good generalization capabilities possible. Effective regularization techniques such as dropout and maxout _cite_, as well as data augmentation methods _cite_ have also explicitly improved generalization for DNNs. A key component of neural nets is the activation function. Improvements in designing of activation functions such as the rectified linear unit (ReLU) _cite_, have led to huge improvements in performance in computer vision tasks _cite_ . More recently, activation functions adaptively trained to the data such as the adaptive piecewise linear unit (APLU) _cite_ and parametric rectified linear unit (PReLU) _cite_ have lead to further improvements in performance of DNNs. For output activation, support vector machine (SVM) has also been successfuly applied in place of softmax _cite_ . Though training DNNs with softmax or SVM as output activation is effective in many tasks, it is possible that alternative activations that consider manifold structure of data by interpolating the output based on both training and testing data can boost performance of the network. In particular, ResNets can be reformulated as solving control problems of a class of transport equations in the continuum limit _cite_ . Transport theory suggests that by using an interpolating function that interpolates terminal values from initial values can dramatically simplify the control problem compared to an ad-hoc choice. This further suggests that a fixed and data-agnostic activation for the output layer may be suboptimal. To this end, based on the ideas from manifold learning, we propose a novel output layer named weighted nonlocal Laplacian (WNLL) layer for DNNs. The resulted DNNs achieve better generalization and are more robust for problems with a small number of training examples. On CIFARN/CIFARN, we achieve on average a N \%/N \% reduction in terms of test error on a wide variety of networks. These include VGGs, ResNets, and pre-activated ResNets. The performance boost is even more pronounced when the model is trained on a random subset of CIFAR with a low number of training examples. We also present an efficient algorithm to train the WNLL layer via an auxiliary network. Theoretical motivation for the WNLL layer is also given from the viewpoint of both game theory and terminal value problems for transport equations. This paper is structured as follows: In Section _ref_, we introduce the motivation and practice of using the WNLL interpolating function in DNNs. In Section _ref_, we explain in detail the algorithms for training and testing DNNs with WNLL as output layer. Section _ref_ provides insight of using an interpolating function as output layer from the angle of terminal value problems of transport equations and game theory. Section _ref_ demonstrates the effectiveness of our method on a variety of numerical examples.