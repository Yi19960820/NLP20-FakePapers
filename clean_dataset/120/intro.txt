Over the last few years, generic deep convolutional neural network (DCNN) architectures such as variants of VGG~ _cite_ and ResNet~ _cite_ have been immensely successful in tackling a diverse range of classification problems and achieve state-of-the-art performance on most benchmarks when used out of the box. The key feature of these architectures is an extremely high model capacity along with a robustness to minor unwanted (\eg translational/rotational/illumination) variations. Given suitable training data, such models can be discriminatively trained in a reliable end-to-end fashion. However, since classification tasks only require a single (potentially multi-variate) class label corresponding to the entire image, early architectures focused solely on developing strong global image features. Semantic Segmentation was one of the first applications to witness the extension of DCNNs to output dense pixel wise predictions~ _cite_ . These approaches used either VGG or ResNet (without the fully connected layers) as their backbone and introduced architectural changes such as skip layers~ _cite_, deconvolutional networks~ _cite_, hypercolumns~ _cite_ or laplacian pyramids~ _cite_ to facilitate the retention/reconstruction of local input-output correspondences. While these approaches performed very well on segmentation benchmarks, they introduced a trade-off between locality and context. Since the task still remained one of classification (albeit at a pixel level), the trade-off was skewed in favor of incorporating more context and subsequently reconstructing local correspondences from global activations. This is perhaps why some of these approaches had to rely on ancillary methods such as Conditional Random Fields (CRFs) ~ _cite_ to enhance the granularity of their predictions. Image-to-Image (ImNIm) regression entails the generation of dense ``continuous" pixel-wise predictions, where the locality-context trade-off is highly task-dependent (typically skewed more in favor of locality) . Several DCNN based approaches have been proposed for specific ImNIm regression tasks such as denoising, relighting, colorization, \etc. These approaches typically involve highly task-specific architectures coupled with fine-tuned ancillary post processing methods. However, unlike classification DCNNs, no truly generic architecture for ImNIm regression has yet been proposed which performs consistently well on a diverse range of tasks. It is perhaps the task-dependent locality-context trade-off coupled with the habitual trend of incorporating VGG/ResNet architectures for non-classification tasks, that have impeded progress in this regard. We propose a generic ImNIm DCNN architecture: {\bf RBDN} which eliminates this trade-off and automatically learns how much locality/context is needed based on the task at hand, through the of a cheaply computed rich multi-scale image representation using recursive multi-scale branches, learnable upsampling and extensive parameter sharing.