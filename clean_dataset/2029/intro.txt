Deep learning powers many research areas and impacts various aspects of society _cite_ from computer vision _cite_, natural language processing _cite_ to biology _cite_ and e-commerce. Recent progress in designing architectures for deep networks has further accelerated this trend _cite_ . Among the most successful architectures are deep residual network (ResNet) and its variants, which are widely used in many computer vision applications _cite_ and natural language processing tasks _cite_ . However, there still are few theoretical analyses and guidelines for designing and training ResNet. In contrast to the recent interest in deep residual networks, system of Ordinary Differential Equations (ODEs), special kinds of dynamical systems, have long been studied in mathematics and physics with rich theoretical and empirical success _cite_ . The connection between nonlinear ODEs and deep ResNets has been established in the recent works of _cite_ . The continuous interpretation of ResNets as dynamical systems allows the adaption of existing theory and numerical techniques for ODEs to deep learning. For example, the paper _cite_ introduces the concept of stable networks that can be arbitrarily long. However, only deep networks with simple single-layer convolution building blocks are proposed, and the architectures are not reversible (and thus the length of the network is limited by the amount of available memory), and only simple numerical examples are provided. Our work aims at overcoming these drawbacks and further investigates the efficacy and practicability of stable architectures derived from the dynamical systems perspective. In this work, we connect deep ResNets and ODEs more closely and propose three stable and reversible architectures. We show that the three architectures are governed by stable and well-posed ODEs. In particular, our approach allows to train arbitrarily long networks using only minimal memory storage. We illustrate the intrinsic reversibility of these architectures with both theoretical analysis and empirical results. The reversibility property easily leads to a memory-efficient implementation, which does not need to store the activations at most hidden layers. Together with the stability, this allows one to train almost arbitrarily deep architectures using modest computational resources. The remainder of our paper is organized as follows. We discuss related work in Sec.~ _ref_ . In Sec.~ _ref_ we review the notion of reversibility and stability in ResNets, present three new architectures, and a regularization functional. In Sec.~ _ref_ we show the efficacy of our networks using three common classification benchmarks (CIFAR-N, CIFAR-N, STL-N) . Our new architectures achieve comparable or even superior accuracy and, in particular, generalize better when a limited number of labeled training data is used. In Sec.~ _ref_ we conclude the paper.