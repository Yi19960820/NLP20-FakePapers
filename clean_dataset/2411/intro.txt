Feature extractors based on so-called deep convolutional neural networks have been applied with tremendous success in a wide range of practical signal classification tasks _cite_ . These networks are composed of multiple layers, each of which computes convolutional transforms, followed by the application of non-linearities and pooling operations. The mathematical ana \-lysis of feature extractors gene \-rated by deep convolutional neural networks was initiated in a se \-minal paper by Mallat _cite_ . Specifically, Mallat analyzes so-called scattering networks, where signals are propagated through layers that compute semi-discrete wavelet transforms (i.e., convolutional transforms with pre-specified filters obtained from a mother wavelet through scaling operations), followed by modulus non-linearities. It was shown in _cite_ that the resulting wavelet-modulus feature extractor is horizontally translation-invariant _cite_ and deformation-stable, with the stability result applying to a function space that depends on the underlying mother wavelet. Recently, Wiatowski and B \"olcskei _cite_ extended Mallat's theory to incorporate convolutional transforms with filters that are (i) pre-specified and potentially structured such as Weyl-Heisenberg (Gabor) functions _cite_, wavelets _cite_, cur \-ve \-lets _cite_, shearlets _cite_, and ridge \-lets _cite_, (ii) pre-specified and unstructured such as random filters _cite_, and (iii) learned in a supervised _cite_ or unsupervised _cite_ fashion. Furthermore, the networks in _cite_ may employ gene \-ral Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and the modulus function) and pooling through sub-sampling. The essence of the results in _cite_ is that vertical translation invariance and deformation stability are induced by the network structure per se rather than the specific choice of filters and non-linearities. While the vertical translation invariance result in _cite_ is general in the sense of applying to the function space _inline_eq_, the deformation stability result in _cite_ pertains to square-integrable band-limited functions. Moreover, the corresponding deformation stability bound depends linearly on the bandwidth. Many signals of practical relevance (such as natural ima \-ges) can be modeled as square-integrable functions that are, however, not band-limited or have large bandwidth. Large bandwidths render the deformation stability bound in _cite_ void as a consequence of its linear dependence on bandwidth. \paragraph* {Contributions} The question considered in this paper is whether taking structural properties of natural images into account can lead to stronger deformation stability bounds. We show that the answer is in the affirmative by analyzing the class of cartoon functions introduced in _cite_ . Cartoon functions satisfy mild decay properties and are piecewise continuously differentiable apart from curved discontinuities along _inline_eq_-hypersurfaces. Moreover, they provide a good model for natural images such as those in the MNIST _cite_, Caltech-N _cite_, and CIFAR-N _cite_ datasets as well as for images of geometric objects of different shapes, sizes, and colors _cite_ . The proof of our main result is based on the decoupling technique introduced in _cite_ . The essence of decoupling is that contractivity of the feature extractor combined with deformation stability of the signal class under consideration---under smoothness conditions on the deformation---establishes deformation stability for the feature extractor. Our main technical contribution here is to prove deformation stability for the class of cartoon functions. Moreover, we show that the decay rate of the resulting deformation stability bound is best possible. The results we obtain further underpin the observation made in _cite_ of deformation stability and vertical translation invariance being induced by the network structure per se. \paragraph* {Notation} We refer the reader to _cite_ for the general notation employed in this paper. In addition, we will need the following notation. For _inline_eq_, we set _inline_eq_ . The Minkowski sum of sets _inline_eq_ is _inline_eq_ . The indicator function of a set _inline_eq_ is defined as _inline_eq_, for _inline_eq_, and _inline_eq_, for _inline_eq_ . For a measurable set _inline_eq_, we let _inline_eq_ .