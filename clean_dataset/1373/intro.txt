Image search by text is widely used in everyday life (, search engines, security surveillance, mobile phones) . Given a textual query, image search systems retrieve a set of related images by the rank of their relevance. Learning this relevance,, correlation between query and image, is key to the system's utility. To measure the correlation between query and image, typically a shared latent subspace is learned for query's text modality and a single image-related modality (, visual contents, surrounding text) . Traditional image search engines~ _cite_ match queries with text or tags associated with images. DSSM~ _cite_ learns an embedding subspace to measure the correlation between document-related text modality and query's text modality using deep learning. On the other hand, cross-modal methods _cite_ learn a subspace to better measure correlation between query's text modality and image's visual modality. In recent years, multiple image-related modalities are becoming widely available online (, images on social networks are typically posted with captions and tags, followed by friends' comments) . Text matching and cross-modal methods are suboptimal due to their focus on only single image-related modality. As shown in Fig~ _ref_, image content can provide detailed visual information (, color, texture) of objects while keywords can offer abstract concepts (, scene description) or external background information (, people's identities) . Different modalities describe images from different views, which together provide information in a more comprehensive way. It benefits to learn a subspace to measure the correlation between query's text modality and image-related modalities,, multi-modal correlation. There is a major challenge in learning this subspace: not all modalities are equally informative due to the variation in query's intent. To overcome this problem, we introduce an attention mechanism to adaptively evaluate the relevance between a modality and query's intent. For the image search task, we consider two kinds of attention mechanisms. First, there is query-unrelated information within each modality (, background regions in images, keyword ``Ice-cream'' for queryN ``Christmas'' in Fig~ _ref_) ; an image search system should attend on the most informative parts for each modality (, intra-attention) . Second, different modalities' contributions vary for different queries; an image search system should carefully balance the importance of each modality according to query's intent (, inter-attention) . To address the aforementioned issues, we propose a novel Attention guided Multi-modal Correlation (AMC) learning method. AMC framework contains three parts: visual intra-attention network (VAN), language intra-attention network (LAN) and multi-modal inter-attention network (MTN) . VAN focuses on informative image regions according to query's intent by generating a query-guided attention map. LAN learns to attend on related words by learning a bilinear similarity between each word in language modality and query. MTN is built to attend between different modalities. Finally, the correlation between query and image-related modalities is calculated as the distance between query embedding vector and a multi-modal embedding vector in the learned AMC space. To validate the AMC framework, we choose image-related keywords as the language modality and image contents as the visual modality. AMC models are evaluated on two datasets: Clickture dataset~ _cite_ and Adobe Stock dataset (ASD) . ASD is collected from Adobe Stock search engine, including queries, images, manually curated keywords and user clickthrough data. For Clickture, we curated keywords for all images by an auto-tagging program developed internally at Adobe. Experiments show that AMC achieves significant improvement on both datasets. More importantly, this finding indicates that AMC can benefit from not only human curated data, but also information generated by machines, which could be noisy and biased. Moreover, since AMC can scale to any number of modalities, it has the ability to integrate and benefit from the output of any intelligent visual analysis system. We further evaluate AMC for caption ranking task on COCO image caption data~ _cite_ with keyword set curated in the same way for Clickture~ _cite_ . AMC models achieve very competitive performance, even surpass the state-of-the-art method in Recall@N metric. Our contributions are as follows: we propose a novel AMC learning framework to select query-dependent information within and cross different modalities. AMC model achieves significant improvement in image search task. We plan to release the auto-tagged Clickture and COCO dataset upon publication.