One of the key factors that drive recent advances in large-scale image recognition is massive collections of labeled images like ImageNet _cite_ and COCO _cite_ . However, it is normally expensive and time-consuming to collect large-scale manually labeled datasets. In practice, for fast development of new image recognition tasks, a widely used surrogate is to automatically collect noisy labeled data from Internet _cite_ . Yet many studies have shown that label noise can affect accuracy of the induced classifiers significantly _cite_, making it desirable to develop algorithms for learning in presence of label noise. Learning with label noise can be categorized by type of supervision: methods that rely on human supervision and methods that do not. For instance, some of the large-scale training data were constructed using classifiers trained on manually verified seed images to remove label noise (e.g. LSUN _cite_ and Places _cite_) . Some studies for learning convolutional neural networks (CNNs) with noise also rely on manual labeling to estimate label confusion _cite_ . The methods using human supervision exhibit a disadvantage in scalability as they require labeling effort for every class. For classification tasks with millions of classes _cite_, it is infeasible to have even one manual annotation per class. In contrast, methods without human supervision (e.g. model predictions-based filtering _cite_ and unsupervised outliers removal _cite_) are scalable but often less effective and more heuristic. Going with any of the existing approaches, either all the classes or none need to be manually verified. It is difficult to have both scalability and effectiveness. In this work, we strive to reconcile this gap. We observe that one of the key ideas for learning from noisy data is finding ``class prototypes'' to effectively represent classes. Methods learn from manually verified seed images like _cite_ and methods assume majority correctness like _cite_ belong to this category. Inspired by this observation, we develop an attention mechanism that learns how to select representative seed images in a reference image set collected for each class with supervised information, and transfer the learned knowledge to other classes without explicit human supervision through transfer learning. This effectively addresses the scalability problem of the methods that rely on human supervision. Thus, we introduce ``label cleaning network'' (CleanNet), a novel neural architecture designed for this setting. First, we develop a reference set encoder with the attention mechanism to encode a set of reference images of a class to an embedding vector that represents that class. Second, in parallel to reference set embedding, we also build a query embedding vector for each individual image and impose a matching constraint in training to require a query embedding to be similar to its class embedding if the query is relevant to its class. In other words, the model can tell whether an image is mislabeled by comparing its query embedding with its class embedding. Since class embeddings generated from different reference sets represents different classes where we wish the model to adapt to, CleanNet can generalize to classes without explicit human supervision. Fig. _ref_ illustrates the end-to-end differentiable model. As the first step of this work, we demonstrate that CleanNet is an effective tool for label noise detection. Simple thresholding based on the similarity between the reference set and the query image lead to good results compared with existing methods. Label noise detection not only is useful for training image classifiers with noisy data, but also has important values in applications like image search result filtering and linking images to knowledge graph entities. CleanNet predicts the relevance of an image to its noisy class label. Therefore, we propose to use CleanNet to assign weights to image samples according to the image-to-label relevance to guide training of the image classifier. On the other hand, as a better classifier provides more discriminative convolutional image features for learning CleanNet, we refresh the CleanNet using the newly trained classifier. We introduce a unified learning scheme to train the CleanNet and image classifier jointly. To summarize, our contributions include a novel neural architecture CleanNet that is designed to make label noise detection and learning from noisy data with human supervision scalable through transfer learning. We also propose a unified scheme for training CleanNet and the image classifier with noisy data. We carried out comprehensive experimentation to evaluate our method for label noise detection and image classification on three large datasets with real-world label noise: ClothingNM _cite_, WebVision _cite_, and Food-NN. Food-NN contains NK images we collected from Internet with the Food-N taxonomy _cite_, and we added ``verification label'' that verifies whether a noisy class label is correct for an image . Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by N \% compared to current weakly supervised methods. It also achieves N \% of the performance gain of verifying all images with only N \% images verified on an image classification task.