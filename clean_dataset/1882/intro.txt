Human parsing aims to segment a human image into multiple parts with fine-grained semantics and provide more detailed understanding of image contents. It can stimulate many higher-level computer vision applications~ _cite_, such as person re-identification~ _cite_ and human behavior analysis~ _cite_ . Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing~ _cite_ . Nevertheless, as demonstrated in many other problems such as object detection~ _cite_ and semantic segmentation~ _cite_, the performance of those CNN-based approaches heavily rely on the availability of annotated images for training. In order to train a human parsing network with potentially practical value in real-word applications, it is highly desired to have a large-scale dataset composed of representative instances with varied clothing appearances, strong articulation, partial (self-) occlusions, truncation at image borders, diverse viewpoints and background clutters. Although there exist training sets for special scenarios such as fashion pictures~ _cite_ and people in constrained situations (e.g., upright) ~ _cite_, these datasets are limited in their coverage and scalability, as shown in Fig.~ _ref_ . The largest public human parsing dataset~ _cite_ so far only contains N, N fashion images while others only include thousands of images. Moreover, to the best of our knowledge, no attempt has been made to establish a standard representative benchmark aiming to cover a wide pallet of challenges for the human parsing task. The existing datasets did not provide an evaluation server with a secret test set to avoid potential dataset over-fitting, which hinders further development on this topic. Therefore we propose a new benchmark ``Look into Person (LIP) " and a public server for automatically reporting evaluation results. Our benchmark significantly advances the state-of-the-arts in terms of appearance variability and complexity, which includes N, N human images with pixel-wise annotations of N semantic parts. The recent progress on human parsing~ _cite_ has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks. To capture rich structure information, they combine CNNs and the graphical models (e.g., Conditional Random Fields (CRFs)), similar to the general object segmentation approaches~ _cite_ . However, evaluated on the new LIP dataset, the results of some existing methods~ _cite_ are unsatisfactory. Without imposing human body structure priors, these general approaches based on bottom-up appearance information sometimes tend to produce unreasonable results (e.g., right arm connected with left shoulder), as shown in Fig.~ _ref_ . The human body structural information has been previously well-explored in the human pose estimation~ _cite_ where dense joint annotations are provided. However, since human parsing requires more extensive and detailed prediction than pose estimation, it is difficult to directly utilize joint-based pose estimation models in pixel-wise prediction to incorporate the complex structure constraints. In order to explicitly enforce the produced parsing results to be semantically consistent with the human pose / joint structures, we propose a novel structure-sensitive learning approach for human parsing. In addition to using the traditional pixel-wise part annotations as the supervision, we introduce a structure-sensitive loss to evaluate the quality of predicted parsing results from a joint structure perspective. That means a satisfactory parsing result should be able to preserve a reasonable joint structure (e.g., the spatial layouts of human parts) . Note that annotating both pixel-wise labeling map and pose joints is expensive and may cause ambiguities. Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a ``self-supervised" strategy, noted as Self-supervised Structure-sensitive Learning (SSL) . Our contributions are summarized in the following three aspects. N) We propose a new large-scale benchmark and an evaluation server to advance the human parsing research, in which N, N images with pixel-wise annotations on N semantic part labels are provided. N) By experimenting on our benchmark, we present the detailed analyses about the existing human parsing approaches to gain some insights into the success and failures of these approaches. N) We propose a novel self-supervised structure-sensitive learning framework for human parsing, which is capable of explicitly enforcing the consistency between the parsing results and the human joint structures. Our proposed framework significantly surpasses the previous methods on both the existing PASCAL-Person-Part dataset~ _cite_ and our new LIP dataset. Human parsing datasets: The commonly used publicly available datasets for human parsing are summarized in Table.~ _ref_ . The previous datasets were labeled with limited number of images or categories. \iffalse The largest one~ _cite_ so far only contains N, N fashion images with mostly upright fashion models. \fi Containing N, N images annotated with N categories, our LIP dataset is the largest and most comprehensive human parsing dataset to date. Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval~ _cite_ and human pose estimation~ _cite_, while our LIP dataset only focuses on human parsing. Human parsing approaches: Recently many research efforts have been devoted to human parsing~ _cite_ . For example, Liang \etal~ _cite_ proposed a novel Co-CNN architecture which integrates multiple levels of image contexts into a unified nerwork. Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars~ _cite_ . To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs~ _cite_ and the adoptions of multi-scale feature representations~ _cite_ . Chen \etal~ _cite_ proposed an attention mechanism that learns to weight the multi-scale features at each pixel location. Some previous works~ _cite_ explored human pose information to guide human parsing by generating ``pose-guided'' part segment proposals. To leverage human joint structure more effortlessly and efficiently, the focus in our approach is nevertheless a new self-supervised structure-sensitive learning approach, which actually can be embedded in any networks.