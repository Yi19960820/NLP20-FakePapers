As a natural extension of image style transfer, video style transfer has recently gained interests among researchers~ _cite_ . Although some image style transfer methods~ _cite_ have achieved real-time processing speed, i.e. around or above N frames per second (FPS), one of the most critical issues in their stylization results is high temporal inconsistency. Temporal inconsistency, or sometimes called incoherence, can be observed visually as flickering between consecutive stylized frames and inconsistent stylization of moving objects~ _cite_ . Figure _ref_ (a) (b) demonstrate temporal inconsistency in video style transfer. To mitigate this effect, optimization methods guided by optical flows and occlusion masks were proposed~ _cite_ . Although these methods can generate smooth and coherent stylized videos, it generally takes several minutes to process each video frame due to optimization on the fly. Some recent models~ _cite_ improved the speed of video style transfer using optical flows and occlusion masks explicitly or implicitly, yet they failed to guarantee real-time processing speed, nice perceptual style quality, and coherent stylization at the same time. In this paper, we propose ReCoNet, a real-time coherent video style transfer network as a solution to the aforementioned problem. ReCoNet is a feed-forward neural network which can generate coherent stylized videos with rich artistic strokes and textures in real-time speed. It stylizes videos frame by frame through an encoder and a decoder, and uses a VGG loss network~ _cite_ to capture the perceptual style of the transfer target. It also incorporates optical flows and occlusion masks as guidance in its temporal loss to maintain temporal consistency between consecutive frames, and the effects can be observed in Figure _ref_ (c) . In the inference stage, ReCoNet can run far above the real-time standard on modern GPUs due to its lightweight and feed-forward network design. We find that the brightness constancy assumption~ _cite_ in optical flow estimation may not strictly hold in real-world videos and animations, and there exist luminance differences on traceable pixels between consecutive image frames. Such luminance differences cannot be captured by temporal losses purely based on optical flows. To consider the luminance difference, we further introduce a luminance warping constraint in our temporal loss. From stylization results of previous methods~ _cite_, we have also observed instability such as different color appearances of the same moving object in consecutive frames. With the intuition that the same object should possess the same features in high-level feature maps, we apply a feature-map-level temporal loss to our encoder. This further improves temporal consistency of our model. In summary, there exist the following contributions in our paper: In this paper, related work for image and video style transfer will be first reviewed in Section N. Detailed motivations, network architecture, and loss functions will be presented in Section N. In Section N, the experiment results will be reported and analyzed, where our model shows outstanding performance.