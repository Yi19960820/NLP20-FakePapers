The success of voice assistant products including Siri, Google Assistant, and Cortana has made the use of speech technology more widespread in our life. These interfaces rely on several speech processing tasks, including (SAD) . SAD is a very important pre-processing step, especially for interfaces without push-to-talk button. The accuracy of a SAD system directly affects the performance of other speech processing technologies including (ASR), speaker verification and identification, speech enhancement and speech emotion recognition _cite_ . A key challenge for SAD is the environmental noise observed in real world applications, which can greatly affect the performance of the speech interface, especially if the SAD models are built with energy-based features _cite_ . An appealing way to increase the robustness of a SAD system against acoustic noise is to include visual features _cite_, mimicking the multimodal nature of speech perception used during daily human interactions _cite_ . While this solution is not always practical for all applications, the use of portable devices with camera and the advances of (HRI) make an (AV-SAD) system a suitable solution. Noisy environment leads speakers to affect their articulatory production to increase their speech intelligibility, a phenomenon known as Lombard effect. While studies have reported differences in visual features between clean and noisy environments, these variations are not as pronounced as the differences in acoustic features _cite_ . Therefore, visual feature are more robust to acoustic noise. For example, Tao \etal _cite_ showed that a (V-SAD) system can achieve robust performance under clean and noisy conditions using the camera of a portable device. Conventional approaches to integrate acoustic and visual information in SAD tasks have relied on ad-hoc fusion schemes such as logic operation, feature concatenation or pre-defined rules _cite_ . These approaches oversimplify the relationship between audio and visual modalities, which may lead to rigid models that cannot capture the temporal dynamic between these modalities. Recent advances on (DNN) have provided new data-driven frameworks to appropriately model sequential data _cite_ . These models avoid defining predefined rules or making unnecessary assumptions by directly learning relationships and distributions from the data _cite_ . Recent studies on audiovisual speech processing have demonstrated the potential of (DL) in this area _cite_ . A straight forward extension from conventional approaches is concatenating audiovisual features as the input for a DNN _cite_ . Another way is to rely on auto-encoder to extract bottleneck audiovisual representations _cite_ . However, these methods do not directly capture the temporal relationship between acoustic and visual features. Furthermore, the systems still rely on hand-crafted features, which may not lead to optimal systems. This study proposes an end-to-end framework for AV-SAD that explicitly captures the temporal dynamic between acoustic and visual features. The approach builds upon the framework presented in our preliminary work _cite_, which relies on (RNNs) . Our approach, referred to as (BRNN), consists of three subsystems. The first two subsystems independently process the modalities using RNNs, creating an acoustic RNN and a visual RNN. These subsystems are implemented with (LSTM) layers, and their objective is to capture the temporal relationship within each modality that are discriminative for speech activity. These subsystems provide high level representations for the modalities, which are concatenated and fed as an input vector to a third subsystem. This system, also implemented with LSTMs, predicts the speech/non-speech label for each frame, capturing the temporal information across the modalities. An important contribution of this study is that the acoustic and visual features are directly learned from the data. Recent advances in DNN for speech processing tasks have shown the benefits of learning discriminative features as part of the training process, using (CNN) and sequence modeling with RNN _cite_ . We can learn end-to-end system with this approach, which has led to performance improvements over hand-crafted features in many cases _cite_ . Furthermore, we can capture the characteristics of the raw input data and extract discriminative representation for a target task _cite_ . These observations motivate us to learn discriminate features from the data. The inputs of the BRNN framework are the raw image around the orofacial area as visual features, and the Mel-filterbank as acoustic features. For the visual input, we use three ND convolutional layers to extract high-level representation from the raw image around the mouth area. On top of the convolutional layers, we use LSTM layers to model temporal information. For the acoustic input, we use (FC) layers that are connected to LSTM layers to model the temporal evolution of the data, similar to the visual part. The proposed approach is jointly trained learning discriminative features from the data, creating an effective and robust end-to-end AV-SAD system. We evaluate our framework on a subset of the CRSS-NEnglish-N corpus consisting of over Nh of recordings from N speakers. The corpus includes multiple sensors, which allows us to evaluate the proposed approach under ideal channels (i.e., close-taking microphone, high definition camera) or under more practical channels (i.e., camera and microphone from a portable tablet) . The corpus also has noisy sessions where different types of noise were played during the recordings. The various conditions can mimic practical scenarios for speech-based interfaces. We replicate state-of-the-art supervised SAD approaches proposed in previous studies to demonstrate the superior performance of the proposed approach. The experimental evaluation shows that our end-to-end BRNN approach achieves the best performance under all conditions. The proposed approach can achieve at least N \% absolute improvement compared to the state-of-the-art A-VAD system. Among the AV-SAD systems, the proposed approach outperforms the best baseline by N \% in the most challenging scenario corresponding to sensors from a portable device under noisy environment. This result for this condition is N \% higher than an A-SAD system, providing clear benefits of the proposed audiovisual solution for SAD. The paper is organized as following. Section _ref_ reviews previous studies on AV-SAD, describing the differences with our approach, and highlighting our contributions. Section _ref_ describes the CRSS-NEnglish-N corpus and the post-processing steps to use the recordings. Section _ref_ introduces our proposed end-to-end BRNN framework. Section _ref_ presents the experimental evaluations that demonstrate the benefits of our approach. The paper concludes with Section _ref_, which summarizes our study and discusses potential future directions in this area.