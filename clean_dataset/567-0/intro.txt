Understanding indoor scene in ND space is critically useful in many applications, such as indoor robotics, augmented reality. To support this task, the goal of this paper is to recognize the category and the ND location of furniture from a single depth image. Context has been successfully used to handle this challenging problem in many previous works. Particularly, holistic scene context models, which integrate both the bottom up local evidence and the top down scene context, have achieved superior performance _cite_ . However, they suffer from a severe drawback that the bottom up and top down stages are run separately. The bottom up stage using only the local evidence needs to generate a large quantity of noisy hypotheses to ensure a high recall, and the top down inference usually requires combinatorial algorithms, such as belief propagation or MCMC, which are computationally expensive in a noisy solution space. Therefore, the whole combined system can hardly achieve a reasonably optimal solution efficiently and robustly. Inspired by the success of deep learning, we propose a ND deep convolutional neural network architecture that jointly leverages local appearance and global scene context efficiently for ND scene understanding. Designing a deep learning architecture to encode context for scene understanding is challenging. Unlike an object whose location and size can be represented with a fixed number of parameters, a scene could involve unknown number of objects and thus requires variable dimensionality to represent, which is hard to incorporate with convolutional neural network with a fixed architecture. Also, although holistic scene models allow flexible context, they require common knowledge to manually predefine relationship between objects, e.g. the relative distance between bed and nightstands. As a result, the model may unnecessarily encode weak context, ignore important context, or measure context in an over simplified way. To solve these issues, we propose and learn a scene representation encoded in scene templates. A scene template contains a super set of objects with strong contextual correlation that could possibly appear in a scene with relatively constrained furniture arrangements. It allows a prediction of ``not present'' for the involved objects so that a variety of scenes can be represented with a fixed dimensionality. A scene can be considered as a scene template with a subset of objects activated. Scene template also learns to only consider objects with strong context, and we argue that context-less objects, such as a chair can be arbitrarily placed, should be detected by a local appearance based object detector. Each template represents a functional sub-region of an indoor scene, predefined with canonical furniture arrangements and estimated ND anchor positions of possible objects with respect to the reference frame of the template. We incorporate these template anchors as priors in the neural architecture by designing a transformation network that aligns the input ND scene (corresponding to the observed depth image) with the template (i.e. the canonical furniture arrangement in ND space) . The aligned ND scene is then fed into a ND context neural network that determines the existence and location of each object in the scene template. This ND context neural network contains a holistic scene pathway and an object pathway using ND Region Of Interest (ROI) pooling in order to classify object existence and regress object location respectively. Our model learns to leverage both global and local information from two pathways, and can recognize multiple objects in a single forward pass of a ND neural network. It is noted that we do not manually define the contextual relationships between objects, but allow the network to automatically learn context in arbitrary format across all objects. Data is yet another challenging problem for training our network. Holistic scene understanding requires the ND ConvNet to have sufficient model capacity, which needs to be trained with a massive amount of data. However, existing RGB-D datasets for scene understanding are all small. To overcome this limitation, we synthesize training data from existing RGB-D datasets by replacing objects in a scene with those from a repository of CAD models from the same object category, and render them in place to generate partially synthesized depth images. Our synthetic data exhibits a variety of different local object appearances, while still keeping the indoor furniture arrangements and clutter as shown in the real scenes. In experiments, we use these synthetic data to pretrain and then finetune our network on a small amount of real data, whereas the same network directly trained on real data can not converge. The contributions of this paper are mainly three aspects. N) We propose a scene template representation that enables the use of a deep learning approach for scene understanding and learning context. The scene template only encodes objects with strong context, and provides a fixed dimension of representation for a family of scenes. N) We propose a ND context neural network that learns scene context automatically. It leverages both global context and local appearance, and detects all objects in context efficiently in a single forward pass of the network. N) We propose a hybrid data augmentation method, which generates depth images keeping indoor furniture arrangements from real scenes but containing synthetic objects with different appearance. The role of context has been studied extensively in computer vision~ _cite_ . While most existing research is limited to ND, there are some works on modeling context for total scene understanding from RGB-D images~ _cite_ . In term of methodology, most of such approaches take object detection as the input and incorporate context models during a post-processing. We aim to integrate context more tightly with deep neural network for object detection. There are some efforts incorporating holistic context model for scene understanding, which is closely related to our work. Scene context is usually manually defined as a unary term on a single object, pairwise term between a pair of objects to satisfy certain functionality _cite_, or a more complicated hierarchy architecture _cite_ . The learned context models are usually applied on a large set of object hypotheses generated using local evidence, e.g. line segments _cite_ or cuboid _cite_, by energy minimization. Therefore high order context might be ignored or infeasible to optimize. Context can be also represented in a non-parametric way _cite_, which potentially enables high order context but is more computationally expensive to infer during the testing time. In contrast, our ND context network does not require any heuristic intervene on the context and learns context automatically. We also require no object hypothesis generation, which is essential in making our method more computationally efficient. Deep learning has been applied to ND data, but most of these works focus on modeling objects~ _cite_ and object detection~ _cite_ . Recently, some successes have been made on applying deep learning for inverse graphics _cite_ . Our approach goes one step further to embrace the full complexity of real-world scenes to perform holistic scene understanding. Related to our transformation network, Spatial Transformation Networks _cite_ can learn the transformation of an input data to a canonical alignment in an unsupervised fashion. However, unlike MNIST digits (which were considered in _cite_) or an individual object where an alignment to a canonical viewpoint is quite natural, it is not clear what transforms are needed to reach a canonical configuration for a ND scene. We define the desired alignment in template coordinates and use supervised training by employing the ground truth alignments available from our training data. While many works have considered rendering synthetic data for training (a.k.a, graphics for vision, or synthesis for analysis), these efforts mostly focus on object rendering, either in color _cite_ or depth _cite_ . There is also work rendering synthetic data from CAD model of complicated scenes for scene understanding _cite_ . However, the generated depth is overly clean, and the scene layouts generated by either by algorithm or human artists are not guaranteed to be correct. In contrast, we utilize both the CAD models and real depth maps to generate more natural data with appropriate context and real-world clutter.