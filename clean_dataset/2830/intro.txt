As a classical and challenge task in computer vision, visual tracking has been widely used in various applications, such as intelligent surveillance and automatic driving. Although appealing results have been achieved, visual tracking is still challenging partly due to the existences of the extreme factors including heavy occlusion, abruptly changing, large deformation and out of view. Most successful visual trackers follow the tracking-by-detection framework, in which a set of positive and negative samples are used to train the parameters of classifiers and deep representations. However, existing tracking models mostly treat different training samples independently while ignores the relationship information among them, which is crucial to the robustness of feature representation and classifier learning. For instance, when we attempt to estimate the response score of one sample, existing approaches only consider the relationship between positive and negative samples. Other relations among different sample pairs are ignored. As a result, some hard positive or negative samples are difficult to obtain proper response score since only limited relationship information among samples is utilized for the overall estimation. Recently, natural language is introduced in visual tracking, which achieves improved tracking performance~ _cite_ . For example, Li ~ _cite_ propose three models including natural language only, visual target specification based on language, and leveraging their joint capacity, to help visual trackers against model drift. However, they use the Recurrent Neural Network (RNN) model to encode the input sentences to generate a dynamic filter, in which the RNN module would increase heavy computational burden on the tracking speed. Moreover, how to use natural language to guide the learning of graph-based structural feature representations remains not studied yet. To handle above problems, we propose a novel structure-aware deep neural network that is end-to-end trained for visual tracking. On the one hand, we utilize the graph convolutional network (GCN) to model the relations among training samples. Specifically, we first take them as graph nodes and use the standard convolutional network to extract their features. To fully utilize the spatial and temporal relations among samples (, graph nodes), the deeply learned messages are then propagated among nodes via GCN to update and refine the pairwise relation feature for each node. After that, we form the final feature representation for each proposal by concatenating the enhanced and the original feature. On the other hand, we treat natural language embedding as high-level semantic information to guide the structural feature learning in the training phrase with triplet loss function, as shown in Figure _ref_ (b) . In the visual tracking task, the targets are easily lost when heavy occlusion occurs and they are out of view. It is difficult to re-track the targets when they are back to view as online update scheme adopted in most tracking methods will contaminate tracking models and the used local search strategy is also limited to recover the targets. Although some trackers employ the strategy of target re-detection~ _cite_, how to judge whether tracking failures occur or not is a challenging problem, and the re-detection models are too weak to recover the targets effectively when they reappear. To handle this problem, we elaborately design a novel subnetwork to learn the target-driven visual attentions from the guidance of both visual and natural language cues. Specifically, we use convolutional network to encode all the input data, i.e., the whole video frame, target object patch and natural language specification, for more efficient computation. The features are concatenated and input to an upsample module to generate the target-driven attention maps. The global proposals can be extracted from the attention regions and then input to the classifier together with local proposals. Therefore, in addition to providing complementary proposals to local ones, the global proposals could cover the targets well when they are back to view from heavy occlusion and out of view. Generally speaking, our proposed algorithm is more intelligent by mining more structure information and exploring high-quality global proposal generation via target-driven visual attention. The contributions of this paper can be summarized as the following three aspects: