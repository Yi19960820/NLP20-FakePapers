Collecting the right data for supervised deep learning is an important and challenging task that can improve performance on most modern computer vision problems. Active learning aims to select, from a large unlabeled dataset, the smallest possible training set to solve a specific task _cite_ . To this end, the uncertainty of a model is used as a means of quantifying what the model does not know, to select data to be annotated. In deterministic neural networks, uncertainty is typically measured as the confidence in the output of the last softmax-normalized layer. However, these estimates do not always provide reliable information to select appropriate training data, as neural networks tend to be overconfident _cite_ . On the other hand, Bayesian methods provide a principled approach to estimate the uncertainty of a model. Though they have recently gained momentum, they have not found widespread use in practice _cite_ . The formulation of a Bayesian Neural Network (BNN) involves placing a prior distribution over all the parameters of the network, and obtaining the posterior given the observed data _cite_ . The spread in the {distribution of predictions} provided by a trained BNN helps to capture the model's uncertainty. However, training a BNN involves marginalization over all possible assignments of weights, which is intractable for deep BNNs without approximations _cite_ . Existing approximation algorithms limit their applicability, since they do not specifically address the fact that deep BNNs on large datasets are more difficult to optimize than deterministic networks, and require extensive parameter tuning to provide good performance and uncertainty estimates _cite_ . Furthermore, estimating uncertainty in BNNs requires drawing a large number of samples at test time, which can be extremely computationally demanding _cite_ . In practice, a common approach to estimate uncertainty is based on ensembles _cite_ . Different models in an ensemble of networks are treated as if they were samples drawn directly from a BNN posterior. Ensembles are easy to optimize and fast to execute. However, they do not approximate uncertainty in the same manner as a BNN. For example, the parameters in the first kernel of the first layer of a convolutional neural network may serve a completely different purpose in different members of the ensemble. Therefore, the variance of the values of these parameters after training cannot be compared to the variance that would have been obtained in the first kernel of the first layer of a trained BNN with the same architecture. In this paper, we propose Deep Probabilistic Ensembles (DPEs), a novel approach to regularize ensembles based on BNNs. Specifically, we use variational inference _cite_, a popular technique for training BNNs, to derive a generic regularization approach for ensembles. Our formulation focuses on a specific form of this regularization designed for active learning, leading to promising results on large-scale visual classification benchmarks with up to millions of samples. Our technique has no computational overhead compared to ensembles at inference time, and a negligible overhead during training. When applied to CIFAR-N, CIFAR-N and ImageNet, DPEs consistently outperform strong baselines. To further show the applicability of our method, we propose a framework to actively annotate data for semantic segmentation on the challenging BDDNk dataset, which on certain classes that are underrepresented in the training distribution yields IoU improvements of up to N \%. Our contributions therefore are (i) we propose KL regularization for training DPEs, which combine the advantages of ensembles and BNNs; (ii) we apply DPEs to active image classification on large-scale datasets; and (iii) we propose a framework for active semantic segmentation using DPEs. Our experiments on both classification and segmentation demonstrate the benefits and scalability of our method compared to existing methods. DPEs are parallelizable, easy to implement, yield high performance, and provide good uncertainty estimates when used for active learning.