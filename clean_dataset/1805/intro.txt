Semantic image segmentation amounts to predicting the category of individual pixels in an image, which has been one of the most active topics in the field of image understanding and computer vision for a long time. Most of the recently proposed approaches to this task are based on deep convolutional networks. Particularly, the fully convolutional network (FCN) ~ _cite_ is efficient and at the same time has achieved the state-of-the-art performance. By reusing the computed feature maps for an image, FCN avoids redundant re-computation for classifying individual pixels in the image. FCN becomes the defacto approach to dense prediction and methods were proposed to further improve this framework, e.g., the DeepLab~ _cite_, and the Adelaide-Context model _cite_ . One key reason for the success of these methods is that they are based on rich features learned from the very large ImageNet~ _cite_ dataset, often in the form of a N-layer VGGNet~ _cite_ . However, currently, there exist much improved models for image classification, e.g., the ResNet~ _cite_ . To the best of our knowledge, building FCNs using ResNets is still an open topic to study on. These networks are so deep that we would inevitably be faced with the limitation of GPU memories. Besides, there are some aspects of the framework of FCN, which need to be explored carefully, such as the size of field-of-view~ _cite_, the resolution of feature maps, and the sampling strategies during training. Based on the above consideration, here we attempt to fulfill the missing part of this topic. In summary, we highlight the main contributions of this work as follows: