Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition~ which has become possible due to the large public image repositories, such as ImageNet~, and high-performance computing systems, such as GPUs or large-scale distributed clusters~ . In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) ~, which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings~ (the winner of ILSVRC-N) to deep ConvNets~ (the winner of ILSVRC-N) . With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of~ in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC-N~ utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales~ . In this paper, we address another important aspect of ConvNet architecture design--its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (_inline_eq_) convolution filters in all layers. As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (\eg deep features classified by a linear SVM without fine-tuning) . We have released our two best-performing models to facilitate further research. The rest of the paper is organised as follows. In~, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in~, and the configurations are compared on the ILSVRC classification task in~ . concludes the paper. For completeness, we also describe and assess our ILSVRC-N object localisation system in~, and discuss the generalisation of very deep features to other datasets in~ . Finally, ~ contains the list of major paper revisions.