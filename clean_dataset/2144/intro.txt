Single image super-resolution (SISR) is to reconstruct a high-resolution (HR) image from a single low-resolution (LR) image, which is an ill-posed inverse problem. SISR has gained increasing research interest for decades. Recently, convolutional neural networks (CNNs) _cite_ significantly improve the peak signal-to noise ratio (PSNR) in SISR. These networks commonly use an extraction module to extract a series of feature maps from the LR image, cascaded with an up-sampling module to increase resolution and reconstruct the HR image. The quality of extracting features will seriously affect the performance of the HR image reconstruction. The main part of extraction module used in modern SR networks can be primarily divided into three types: conventional convolution layers _cite_, residual blocks _cite_ and dense blocks _cite_ . Conventional convolution has been widely considered by scholars since AlexNet _cite_ won the first prize of ILSVRC in N. The first model using conventional convolution to solve the SR problem is SRCNN _cite_ . After that, many improved networks such as FSRCNN _cite_, SCN _cite_, ESPCN _cite_ and DRCN _cite_ also use conventional convolution and achieve great results. Residual block _cite_ is an improved version of the convolutional layer, which exhibits excellent performance in computer vision problems. Since it can enhance the feature propagation in networks and alleviate the vanishing-gradient problem, many SR networks such as VDSR _cite_, LapSRN _cite_, EDSR _cite_ and SRResNet _cite_ import residual blocks and exhibit improved performances. To make use of the skip connections used in residual blocks, Huang et al. proposed the dense block _cite_ further. A dense block builds more connections among layers to enlarge the information flow. Tong et al. _cite_ proposed SRDenseNet using dense blocks, which boosts the performance further more. Recently, Yang et al proposed a novel block called the clique block _cite_, where the layers in a block are constructed as a clique and are updated alternately in a loop manner. Any layer is both the input and the output of another one in the same block so that the information flow is maximized. The propagation of a clique block contains two stages. The first stage does the same thing as a dense block. The second stage distills the feature maps by using the skip connections between any layers, including connections between subsequent layers. A suitable up-sampling module can further improve image reconstruction performance. The up-sampling modules used in modern SR networks to increase the resolution can also be primarily divided into three types: interpolation up-sampling, deconvolution up-sampling and sub-pixel convolution up-sampling. Interpolation up-sampling was first used in SRCNN _cite_ . At that time, there were no effective implementations of module that can make the output size larger than the input size. So SRCNN used pre-defined bicubic interpolation on input images to get the desired size first. Following SRCNN using pre-interpolation, VDSR _cite_, IRCNN _cite_, DRRN _cite_ and MemNet _cite_ used different extraction modules. However, this pre-processing step increases computation complexity because the size of feature maps is multiple. Deconvolution proposed in _cite_ can be seen as multiplication of each input pixel by a filter, which could increase the input size if the stride _inline_eq_ . Many modern SR networks such as FSRCNN _cite_, LapCNN _cite_, DBPN _cite_ and IDN _cite_ got better results by using deconvolution as the up-sampling module. However, the computation complexity of forward and back propagation of deconvolution is still a major concern. Sub-pixel convolution proposed in _cite_ aims at accelerating the up-sampling operation. Unlike previous up-sampling methods that change the height and width of the input feature maps, sub-pixel convolution implements up-sampling by increasing the number of channels. After that sub-pixel convolution uses a periodic shuffling operation to reshape the output feature map to the desired height and width. ESPCN _cite_, EDSR _cite_ and SRMD _cite_ used sub-pixel convolution to achieve good performances on benchmark datasets. These above-mentioned networks tend to produce blurry and overly-smoothed HR images, lacking some texture details. Wavelet transform (WT) has been shown to be an efficient and highly intuitive tool to represent and store images in a multi-resolution way _cite_ . WT can describe the contextual and textural information of an image at different scales. WT for super-resolution has been applied successfully to the multi-frame SR problem _cite_ . Motivated by the remarkable properties of clique block and WT, we propose a novel network for SR called SRCliqueNet to address the above-mentioned challenges. We design the res-clique block as the main part of the extraction module to improve the network's performance. We also design a novel up-sampling module called clique up-sampling. It consists of four sub-nets which use to predict the high resolution wavelet coefficients of four sub-bands. Since we consider the edge feature properties of four sub-bands, four sub-nets can learn the coefficients of four sub-bands jointly. For magnification factors greater than N, we design a progressive SRCliqueNet upon image pyramids _cite_ . Our proposed network achieves superior performance over the state-of-the-art methods on benchmark datasets.