In breast examinations, such as mammography, detected actionable tumors are further examined through invasive histology. Objective interpretation of these modalities is fraught with high inter-observer variability and limited reproducibility~ _cite_ . In this context, a reference based assessment, such as presenting prior cases with similar disease manifestations (termed Content Based Image Retrieval (CBIR)) could be used to circumvent discrepancies in cancer grading. With growing sizes of clinical databases, such a CBIR system ought to be both scalable and accurate. Towards this, hashing approaches for CBIR are being actively investigated for representing images as compact binary codes that can be used for fast and accurate retrieval~ _cite_ . Malignant carcinomas are often co-located with benign looking manifestations and suspect normal tissues. In such cases, describing the whole image with a single label is often inadequate for objective machine learning and alternatively requires expert annotations delineating the exact location of the region of interest. This argument extends to screening modalities like mammograms, where multiple anatomical views are acquired. In such scenarios, the status of the tumor is best represented to a CBIR system by constituting a bag of all associated images, thus veritably becoming multiple instance (MI) in nature. This is illustrated in Fig.~ _ref_ . With this as our premise we present, for the first time, a novel deep learning based MI hashing method, termed as Robust Multiple Instance Hashing (RMIH) . Seminal works on shallow learning-based hashing include Iterative Quantization (ITQ) ~ _cite_, Kernel Sensitive Hashing (KSH) ~ _cite_ etc. that propose a two-stage framework involving extraction of hand-crafted features followed by binarization. Yang et al. extend these methods to MI learning scenarios with two variants: Instance Level MI Hashing (IMIH) and Bag Level MI Hashing (BMIH) ~ _cite_ . However, these approaches are not end-to-end and are susceptible to semantic gap between features and associated concepts. Alternatively, deep hashing methods such as simultaneous feature learning and hashing (SFLH) ~ _cite_, deep hashing networks (DHN) ~ _cite_ and deep residual hashing (DRH) ~ _cite_ to name a few, propose the learning of representations and hash codes in an end-to-end fashion, in effect bridging this semantic gap. It must be noted that all the above deep hashing works targeted single instance (SI) hashing scenarios and an extension to MI hashing was not investigated. Earlier works on MI deep learning in computer vision include work by Wu et al. ~ _cite_, where the concept of an MI pooling (MIPool) layer is introduced to aggregate representations for multi-label classification. Yan et al. leveraged MI deep learning for efficient body part recognition~ _cite_ . Unlike MI classification that potentially substitutes the decision of the clinician, retrieval aims at presenting them with richer contextual information similar to the case at hand to facilitate decision-making. RMIH effectively bridges the two concepts for CBIR systems by combining the representation learning strength of deep MI learning with the potential for scalability arising from hashing. Within CBIR for breast cancer, notable prior art includes work on mammogram image retrieval by Jiang et al. ~ _cite_ and large-scale histology retrieval by Zhang et al. ~ _cite_ . Both these works pose CBIR as an SI retrieval problem. Contrasting with~ _cite_ and~ _cite_, within RMIH we create a bag of images to represent a particular pathological case and generate a bag-level hash code, as shown in Fig.~ _ref_ . Our contributions in this paper include: N) introduction of a robust supervised retrieval loss for learning in presence of weak labels and potential outliers; N) propose training with an auxiliary SI arm with gradual loss trade-off for improved trainability; and N) incorporation of the MIPool layer to aggregate representations across variable number of instances within a bag, generating bag-level discriminative hash codes.