Since the beginning of research in artificial intelligence, there has been a constant shift from engineering of solutions for special scenarios towards methodologies that are more generally applicable. The increased popularity of learning methods, which already started before the success of deep learning, is another chapter in this progress towards more generic approaches. These methods allow restricting the engineering on the algorithmic side, but at the same time much more influence is given to the data. While classical computer vision methods have not required any training data, in which sense they are unsupervised methods, the final performance of approaches following the dominant supervised learning paradigm depends very much on the size and quality of training datasets. Especially the large potential of deep learning can only be fully exploited with large datasets. Significant research efforts have been made to create such datasets, \eg ImageNet~, MS COCO~, CityScapes~, or the NYU dataset~ . These datasets have enabled most of the progress in computer vision in recent years. Notably, all these datasets cover the field of visual recognition such as object detection and classification. This has been one of the reasons why deep learning in computer vision has mainly focused on visual recognition for a long time. A notable exception is the NYU dataset. It contains a large collection of RGB-D images and enabled the seminal work of~ on depth prediction from a single image. In this paper, we present and analyze one of the very first approaches to create training data for learning optical flow and disparity estimation---two classical computer vision tasks---on a large scale. Creating data for such tasks requires a different approach than in visual recognition, where web data in conjunction with manual annotation by mostly untrained persons can yield large datasets with reasonable time and monetary effort. In case of optical flow estimation, for instance, it is not obvious how to derive ground truth for large sets of real videos. Notable works include the Middlebury dataset~ and the KITTI datasets~, but due to the difficulty of computing accurate ground truth, these have been restricted to _inline_eq_ and _inline_eq_ pairs of frames, respectively (the latter are also limited by the special driving setting) . This is far from what is needed to train a powerful deep network. Therefore, these datasets have been used mainly as test sets for classical, non-learning-based methodology. The situation is similar for disparity estimation from stereo pairs, or depth estimation and camera tracking from monocular videos. The MPI-Sintel dataset demonstrated the feasibility of using existing data from an open source movie to render videos together with the desired ground truth~ . Sintel provides ground truth for optical flow, disparities and occlusion areas, and has been largely adopted as a serious benchmark dataset despite its synthetic nature. Still, the dataset is small by deep learning standards: The training set consists of _inline_eq_ image pairs--which is sufficient to train a network, but not a very powerful one, as we show in this paper. Moreover, the approach does not scale arbitrarily due to the limited availability of open source movies. Inspired by the Sintel dataset, we take the approach of rendering images together with various ground truth outputs to a new level. Embracing procedural generation and abstract instead of naturalistic data, we focus on training rather than testing and aim for a much larger scale. In contrast to the Sintel dataset, which was intended for benchmarking, this allows us to ignore many subtle problems that appear in the rendering of synthetic data~, and rather focus on the size and diversity of the dataset. There are many ways to generate training data in a synthetic manner: using existing scene data as in Sintel, manually designing new scenes, or creating randomized scenes in a procedural manner. In this paper, we investigate which of these options are most powerful and which properties of a dataset are most important for training a network that will generalize also to other data, particularly real data. We focus this investigation on networks that are trained for optical flow estimation, specifically FlowNet~, and disparity estimation, specifically DispNet~ . The study leads to many interesting findings. For instance, the data does not have to be realistic to make for a good training dataset. In fact, our simplistic ND FlyingChairs dataset yields good data to start training a network for optical flow estimation. Moreover, we find that: {white} {white}