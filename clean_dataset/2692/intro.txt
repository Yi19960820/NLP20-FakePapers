When we humans look at an image, we always perform a sequential extraction of information in order to understand its content. First, we fix our gaze to the most salient part of the image and, from the extracted information, we guide our look towards another image point, until we have analyzed all its relevant information. This is our natural and instinctive behavior to gather information from our surroundings. Traditionally in computer vision, images have been analyzed at the local scale following a sliding window scanning, often at different scales. This approach analyses the different image parts independently, without relating them. Just by introducing a hierarchical representation of the image, we can more easily exploit the relationship between regions. We propose to use a top-down scanning which firstly takes a global view of the image to sequentially focus on the local parts that contain the relevant information (e.g. objects or faces) . Our algorithm is based on an intelligent agent trained by reinforcement learning that is capable of making decisions to detect an object in a still image, similarly to _cite_ . The agent first analyzes the whole image, and decides in which region of the image to focus among a set of predefined ones. Inspired by _cite_, our agent can top-down explore a set of five different predefined region candidates: four regions representing the four quadrants plus a central region. Two different strategies have been studied: proposing overlapping or non-overlapping candidates. The agent stops its search when it finds an object. Reinforcement learning is useful for our task because there is no single way of completing it. The agent can explore the hierarchical representation in different ways and still achieve its goal. Then, instead of programming every step that the agent should do, we train it so that it makes decisions under uncertainty to reach its target. Notice that the final goal of object detection is to define a bounding box around the object and that, in our work, these bounding boxes are limited to the predefined regions in the hierarchy. Most state of the art solutions for object detection analyze large amounts of region proposals. These algorithms need to leverage the bottleneck of describing all these proposals by reusing convolutional feature maps of the whole image. In our work though, as the reinforcement learning agent and the hierarchy allow us to analyze a very reduced number of regions, we can feed each region visited by the agent through a convolutional network to extract its features, allowing us to work with region representations of higher spatial resolution, which are also more informative than those cropped from a feature map of the whole image. To study this trade-off, we have trained and compared two different models based on each of these two principles: the Image-Zooms model, which extracts descriptors at each region, and the PoolN-Crops model, which reuses feature maps for different regions of the same image. The first contribution of our work is the introduction of a hierarchical representation to top-down (zoom-in) guide our agent through the image. We explore how the design of the hierarchy affects the detection performance and the amount of visited regions. The second contribution is the study between extracting features for each region instead of reusing feature maps for several locations. We show the gain of the region-specific features for our scheme, and argue that the computational overhead is minor thanks to the very reduced amount of regions considered by the agent.