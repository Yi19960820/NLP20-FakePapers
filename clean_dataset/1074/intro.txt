In many AI challenges, including perception and planning, one specific problem requires multiple strategies. In the computer vision literature, this topic has gained little attention. Since a single model is typically trained, the conventional view is that of a unified, albeit complex, solution that captures all scenarios. Our work shows that careful consideration of the multifaceted nature of optical flow leads to a clear improvement in performing this task. In optical flow, one can roughly separate between the small-and the large-displacement scenarios, and train model to apply different strategies to these different cases. The small displacement scenarios are characterized by relatively small appearance changes and require patch descriptors that can capture minute differences in appearance. The large displacement scenarios, on the other hand, require much more invariance in the matching process. State of the art methods in optical flow employ metric learning in order to learn the patch descriptors. We focus on the process of selecting negative samples during training and suggest two modifications. First, rather than selecting all negative samples close to the ground truth, we propose an interleaving learning method that selects negative samples at a distance that match the amount of displacement that the true match (the positive sample) undergoes, as is illustrated in Fig.~ _ref_ . Second, we suggest gradually increasing the difficulty of the negative samples during training. In the implementation of the second component, scheduling samples by difficulty, we combine two methods well known in the literature. The curriculum learning method~ _cite_ selects samples, stratified by difficulty, using a predefined order. The method of self-paced learning ~ _cite_ identifies a set of easy samples by their loss, and learns using only those samples. The amount of samples defined as easy is increased over time. The Self-Paced-Curriculum-Interleaving method we propose here combines in the selection process both the difficulty of a sample and its loss. However, in difference from the self-paced method, no samples are excluded during training. Instead, we control the level of the difficulty of instances used for training by selecting negative samples of appropriate distances. The pipeline employed for computing optical flow is similar to the PatchBatch method~ _cite_ . We slightly modify it by replacing the DrLIM loss with a Hinge loss. \noindent Our main contributions in this work are: