Video object segmentation is a task of separating the moving foreground object consistently from the complex background in unconstrained video sequences. Although much progress has been made in the past decades, it remains a challenging task due to the factors such as fast motion, cluttered backgrounds, arbitrary object appearance variation and shape deformation, to name a few. One key step to deal with this problem is to maintain both spatial and temporal consistency across the whole video, based on which numerous methods have been proposed, which can be generally categorized into two categories: supervised segmentation and unsupervised segmentation. The supervised video segmentation requires a user to manually annotate some frames, which guide the segmentation of other frames across all frames. Most supervised methods are graph-based~ _cite_, which usually include a unary term comprised of foreground appearance, motions or locations and a pairwise term that encodes spatial and temporal smoothness to propagate the user _inline_eq_ s annotations to all other frames. Moreover, the optical flows are usually adopted to deliver information among frames, but it is prone to failure because of the inaccurately estimated optical flows. To address this issue, some methods based on tracking~ _cite_ have been proposed, which first label the position of the object in the first frame, and then enforce the temporal consistency in the video by tracking pixels, superpixels or object proposals. However, most of those approaches only consider the pixels or superpixels that are generated independently in each frame without exploiting the ones from the long-term spatio-temporal regions, which are helpful to learn a robust appearance model. In contrast to the supervised segmentation, a variety of unsupervised video segmentation algorithms have been proposed in recent years~ _cite_, which are fully automatic without any manual interventions. _cite_ are based on clustering point trackers, which can integrate information of a whole video shot to detect a separately moving object, among which~ _cite_ explores point trajectories as well as motion cues over a large time window, which is less susceptible to the short-term variations that may hinder separating different objects.~ _cite_ are based on object proposals, which utilize appearance features to calculate the foreground likelihood and match partial shapes by a localization prior. Besides, some other segmentation methods have been proposed, which consider occlusion cues~ _cite_ and motion characteristics~ _cite_ to hypothesize the foreground locations. In this paper, we propose a fully automatic video object segmentation algorithm without the help of any extra knowledge about the object position, appearance or scale. Figure~ _ref_ shows the flow chart of our method. First, we utilize the optical flow information to obtain a rough object position that ensures the frame-to-frame segmentation consistency. Specifically, we employ the method of~ _cite_, which can produce a rough motion boundary in pairs of adjacent frames and then get an efficiently initial foreground estimation. Here, the only requirement for the object is to move differently from its surrounding background in some frames of the video. Moreover, in order to reduce the noises introduced by appearance learning, we explore the information of superpixels from the long-term spatio-temporally nonlocal regions to learn a robust appearance model, which is integrated into a spatio-temporal graphical model. Finally, as GrabCut~ _cite_, the graphical model is iteratively solved by refining the foreground-background labeling and updating the foreground-background appearance models. We evaluate the proposed algorithm on two challenging datasets, i.e. SegTrack~ _cite_ and Youtube-Objects~ _cite_, and show favorable results against some state-of-art methods.