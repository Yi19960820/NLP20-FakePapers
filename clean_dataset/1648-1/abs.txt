Convolutional neural networks have many hyperparameters such as the filter size, number of filters, and pooling size, which require manual tuning. Though deep stacked structures are able to create multi-scale and hierarchical representations, manually fixed filter sizes put a limit on the scale of represented objects that can be learned in a single convolutional layer. This paper introduces a new adaptive filter model that allows variable scale and orientation. The scale and orientation parameters of filters can be learned using classical back-propagation. Therefore, in a single convolution layer, we can create filters of different scale and orientation that can adapt to small or large objects and features. The proposed model uses a relatively large base size (grid) for filters. In the grid, a differentiable function acts as an envelope for filters. Masking of convolution filter weights with the envelope function limits effective filter scale and shape. Only the weights in the envelope are updated during training. In this work, we employed a multivariate (ND) Gaussian as the envelope function, which can grow, shrink, or rotate during training using its covariance matrix. We tested the new filter model on MNIST, MNIST-cluttered, and CIFAR-N and compared the results to the conventional convolution layers. The results demonstrate that the new model can effectively learn and produce filters of different scales and orientations. Moreover, the experiments show that the adaptive convolution layers perform equally; or better, especially when data includes multi-scale objects.