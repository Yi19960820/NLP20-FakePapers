Understanding detailed audience composition is important for movie studios that invest in stories of uncertain commercial outcome. One source of uncertainty is that studios do now know how the movie is going to be like--what is it going to feel like--until the last few months before release. The other related source of uncertainty is audience and market fluidity. Studios don't know with certainty 'how', and especially 'which' audiences are going to respond because the movie isn't finished and because the strength and nature of competitive movies is also unknown. An important function at movie studios is understanding the micro-segmentation of the customer base. E.g., not all super hero movies bring the same audience, etc. Over the last years, studios have invested in data tools to learn and to map out the customer segments, and to make predictions for future films. Granular predictions at the micro-segment level, and even at the customer level, have became routine inputs into important business decisions, and provide a trusted barometer of the potential financial performance of the movie. Recommendation systems for movie theatrical releases have emerged as valuable tools that are especially well suited to provide granular forward looking audience projections to support greenlighting decisions, movie positioning studies, and marketing and distribution. MERLIN, the recommendation system for theatrical releases built at Nth Century Fox, is used to predict user attendance and segment indexes a year in advance, and to refine the prediction with anonymized user behavior signals. Predicting user behavior far in advance of the movie release is an example of pure cold-start prediction and is challenging for movies that are novel, movies that are non-sequels, and movies that cross traditional genres. Recent research has explored using movie synopses _cite_ and movie trailers (_cite_, _cite_), combined with collaborative filter models, to predict which customers consume which movies. In our analysis, Campo et al. showed that recommendations made based on video data are qualitatively different from those based on the synopsis data _cite_ . This finding can be explained by the different information content of the two media. An open question when training video-based models is the choice of the feature space. A popular approach due to its simplicity is to analyze, individually, the different frames of a video using a trained image classification deep architecture. With this approach, the dense feature representations of the different frames can be pooled together in a single dense representation through an averaging or max element-wise operation. For example, Campo et al. _cite_ use the average pooling of image features of individual video frames and use it as the video features. Video analysis using pooling schemes to collapse an entire video or part of a video into a unique dense feature vector can miss important semantic aspects of the video. Although simple to implement, the approach neglects the sequential and temporal aspects of the story, which, we argue, can be useful for characterization of a motion picture. For example, a trailer with a long close-up shot of a character is more likely for a drama movie than for an action movie, whereas a trailer with quick but frequent shots is more likely for an action movie. In both cases, though, average pooling would result in the same feature vector. A second question when training video-based models is the level of semantic expressiveness of the model. One could use individual frames as unit of analysis, and apply image classification models to create a dense feature vector for each frame. Those vectors represent the 'objects' depicted in the frames (car, face, etc), and can then be pooled together to create a video-level feature vector. A collaborative filter can use the presence of the objects in the trailer to predict customer behavior (e.g., maybe because the customer has seen movies in the past that also depicted some of those objects in the trailer) . Alternatively, one could try to use the sequences of ordered frames to identify patterns in the presence of objects that repeat themselves multiple times, perhaps at some regular time intervals, and perhaps across different trailers. Some of those sequences could indicate actual actions. For example, intermittent close-up shots of actors faces could be indicative of dialog, whereas intermittent shots of cars could be indicative of a car chase. One could feed those identified sequences, represented in a suitable dense vectorization, to a collaborative filter to determine whether the presence of some sequence in a trailer is predictive of customer behavior (e.g., maybe because the customer has seen movies in the past that also contained such sequence) . In this paper, we explore a temporal-aware model that takes temporal dynamics of movie trailer imagery into account to more faithfully capture salient elements that are conveyed not in individual frames, but over sequences of frames, and that hopefully create a better representation of the elements of the actual story. Our model is based on the idea of convolution over time _cite_ . As in previous work, for each video frame, we extract dense image features using a pre-trained image feature extractor. Then, we apply multi-layered convolution filters over the dense image features of multiple consecutive video frames. Convolutional filters are capable of capturing signals from not specific to individual frames, but that result from the combination of a sequence of frames that are in the filter receptive field. A longer receptive field can capture actions that unfold over longer periods of time. The convolution layer is followed by a temporal pooling layer to summarize the signals throughout the video before fed into a hybrid-collaborative filtering (CF) pipeline. The convolutional filters and collaborative filters are trained end-to-end and against millions of moviegoers' attendance records. This allows them to learn video actions (or non-actions) that are most predictive of users' movie preferences.