Deep convolutional neural networks (CNNs) have been successful in many computer vision tasks including image classification~ _cite_, object detection~ _cite_, semantic segmentation~ _cite_, action recognition~ _cite_, and facial landmark localization~ _cite_ . However, the common prerequisite for all these successes is the availability of large training corpora of labeled images. Of these tasks, semantic image segmentation is one of the most costly tasks in terms of data annotation. For example, drawing a segmentation annotation on an object is on average _inline_eq_ Nx slower than drawing a bounding box and _inline_eq_ Nx slower than labeling the presence of objects in images~ _cite_ . As a result, most image segmentation datasets are orders of magnitude smaller than image-classification datasets. In this paper, we mitigate the data demands of semantic segmentation with a semi-supervised method that leverages cheap object bounding box labels in training. This approach reduces the data annotation requirements at the cost of requiring inference of the mask label for an object within a bounding box. Current state-of-the-art semi-supervised methods typically rely on hand-crafted heuristics to infer an object mask inside a bounding box~ _cite_ . In contrast, we propose a principled framework that trains semantic segmentation models in a semi-supervised setting using a small set of fully supervised images (with semantic object masks and bounding boxes) and a weak set of images (with only bounding box annotations) . The fully supervised set is first used to train an ancillary segmentation model that predicts object masks on the weak set. Using this augmented data a primary segmentation model is trained. This primary segmentation model is probabilistic to accommodate the uncertainty of the mask labels generated by the ancillary model. Training is formulated so that the labels supplied to the primary model are refined during training from the initial ancillary mask labels to more accurate labels obtained from the primary model itself as it improves. Hence, we call our framework a self-correcting segmentation model as it improves the weakly supervised labels based on its current probabilistic model of object masks. We propose two approaches to the self-correction mechanism. Firstly, inspired by Vahdat~ _cite_, we use a function that linearly combines the ancillary and model predictions. We show that this simple and effective approach is the natural result of minimizing a weighted Kullback-Leibler (KL) divergence from a distribution over segmentation labels to both the ancillary and primary models. However, this approach requires defining a weight whose optimal value should change during training. With this motivation, we develop a second adaptive self-correction mechanism. We use CNNs to learn how to combine the ancillary and primary models to predict a segmentation on a weak set of images. This approach eliminates the need for a weighting schedule. Experiments on the PASCAL VOC and Cityscapes datasets show that our models trained with a small portion of fully supervised set achieve a performance comparable to (and in some cases better than) the models trained with all the fully supervised images.