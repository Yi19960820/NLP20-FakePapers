Conversational group detection from images has applications in video surveillance and social robotics. Effective detection of groups allows a mobile robot to navigate around people without interfering inappropriately with social interactions. Sociologists define a facing formation, or f-formation, as a socio-spatial formation in which people maintain a convex space termed an o-space _inline_eq_ . Detection of o-spaces and assigning people to these o-spaces is a standard and sociologically consistent method for detection of conversational groups. Robots which operate in public spaces, particularly those which interact with and serve humans, can perform their functions more naturally and seamlessly if able to recognize these social formations and spaces. There is ample literature on group detection which sets standards for error analysis, group definition, and a baseline for performance. The best performance to date has come not from a learning algorithm but rather a graph-cuts clustering algorithm, which attained precisions from approximately .N to .N on the Cocktail Party dataset depending on the threshold for accuracy used _inline_eq_ (see Evaluation Metrics for explanation of error analysis) . We will attempt to improve upon these results. Other attempts have included edge-weighted graph algorithms, for instance one in which each node is a person and the edge measures affinity between pairs _inline_eq_ . There is a similar method with weightings based on an attention metric, differing from the previous method by exploiting social cues to determine edge weightings _inline_eq_ . There is also a game theoretic framework with probabilistic overlapping regions determined by an attention metric which uses temporal data _inline_eq_ . Finally, another successful algorithm has been a voting-based algorithm where each individual gets a vote for the o-space given by a Gaussian function for that individual _inline_eq_ . All attempts have performed worse or similar to the graph-cuts algorithm, so we will focus on improving upon those results. Additionally, it is promising that we are the first paper to apply deep learning to this task and the first paper to use features from a layout image of the room. Our data comes from standard datasets for f-formation detection, which are videos of social interactions in confined spaces. These are annotated by sociologists with position, orientation, and group assignment for every person in various frames taken throughout the video. We use ND coordinates and yaw orientations of the people as features. Additionally, we manually transform the camera images into a ND map of the room, and extract features from this map through deep convolutions. Using the annotated features and image map, we run a deep network to output a ND image of the room, with each pixel of the output image representing the likelihood of that location containing an o-space. We then perform non-maximal suppression and thresholding to arrive at our final predictions for the o-space locations and greedily assign people to the nearest o-space to determine our conversational groups.