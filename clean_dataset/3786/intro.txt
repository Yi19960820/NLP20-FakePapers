not receiving as much attention by the scientific community as speech processing tasks, environmental sound recognition nonetheless contributes to important applications in surveillance _cite_, robotics _cite_ and home automation _cite_ among others. In comparison to standard speech, environmental sounds are often more chaotic and noise-like, without the underlying phonetic structure that has been successfully modeled by traditional machine learning methods like the hidden Markov model (HMM) . Recent work in this field has shown two distinctive developments: the utilization of deep neural networks (DNNs) in particular the convolutional neural network (CNN) as a classifier and feature extractor, and the use of the time-frequency representation of an audio signal, known as the spectrogram, as input. CNN-based models were first adopted for speech recognition systems by Abdel-Hamid et al. _cite_ for the TIMIT phone recognition task. This model was later improved architecturally in _cite_, with added consideration to kernel size, pooling, network size and regularization, while other large-scale speech tasks were also carried out in _cite_ _cite_ _cite_ using CNNs. More recently, Piczak _cite_ and Salamon _cite_ showed that a basic CNN could generally outperform existing methods for environmental sound classification provided sufficient data. To achieve desirable results, the classifier has to be paired with an appropriate input representation. Conventional choices were largely hand-crafted features such as Mel-frequency cepstral coefficients (MFCCs) or Perceptual Linear Prediction (PLP) coefficients that were previously state-of-the-art when used with Gaussian mixture model (GMM)-based HMMs. However, such cepstral features became less popular with deep learning algorithms as it was no longer essential for feature maps to be sufficiently de-correlated _cite_ _cite_ . Conversely, the strength of CNNs lie with its ability to learn localized patterns through weight-sharing and pooling _cite_-patterns present in the spectro-temporal features of spectrograms. In the domain of environmental sound it has been noted that time-frequency representations are especially useful as learning features _cite_ _cite_ _cite_ _cite_ _cite_ due to the non-stationary and dynamic nature of the sounds. To extract these spectro-temporal features, a range of signal processing techniques have been proposed. A survey on environmental sound recognition by Chachada and Kuo _cite_ covers several methods, including sparse-representation-based techniques such as matching pursuit, power-spectrum-based techniques to obtain variants of the spectrogram, and several wavelet-based approaches. Another comparative study _cite_ investigated the performance of methods such as short-time Fourier transform (STFT), fast Wavelet transform (FWT) and continuous Wavelet transform (CWT) against stationary features like the aforementioned MFCC and PLP. The authors classified the extracted features using conventional machine learning techniques, including GMM-HMM, support vector machines (SVMs) and shallow artificial neural nets. This letter builds upon previous comparative studies by focusing on the specifics of a CNN model as opposed to more traditional classifiers. We investigate four common approaches to obtain the time-frequency representation, namely the short-time Fourier transform (STFT) with both linear and Mel scales, the constant-Q transform (CQT) and the continuous Wavelet transform (CWT), while addressing additional considerations like window size. The impact of the different approaches is evaluated in comparison to baseline MFCC features on two publicly available environmental sound datasets (ESC-N, UrbanSoundNK) through the classification performance of several CNN variants.