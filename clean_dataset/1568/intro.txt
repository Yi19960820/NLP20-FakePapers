Deep neural networks (DNNs) are among the most powerful and versatile machine learning techniques, achieving state-of-the-art accuracy in a variety of important applications, such as visual object recognition _cite_, speech recognition _cite_, and machine translation _cite_ . However, the power of DNNs comes at a considerable cost, namely, the computational cost of applying them to new examples. This cost, often called the, has increased rapidly for many tasks (see Fig.~ _ref_) with ever-growing demands for improved performance in state-of-the-art systems. As a point of fact, the ResnetN _cite_ architecture with N layers, realizes a substantial N \% accuracy gain in top-N performance over GoogLeNet _cite_ on the large-scale ImageNet dataset _cite_ but is about NX slower at test-time. The high test-time cost of state-of-the-art DNNs means that they can only be deployed on powerful computers, equipped with massive GPU accelerators. As a result, technology companies spend billions of dollars a year on expensive and power-hungry computer hardware. Moreover, high test-time cost prevents DNNs from being deployed on resource constrained platforms, such as those found in Internet of Things (IoT) devices, smart phones, and wearables. This problem has given rise to a concentrated research effort to reduce the test-time cost of DNNs. Most of the work on this topic focuses on designing more efficient network topologies and on compressing pre-trained models using various techniques (see related work below) . We propose a different approach, which leaves the original DNN intact and instead changes the way in which we apply the DNN to new examples. We exploit the fact that natural data is typically a mix of easy examples and difficult examples, and we posit that the easy examples do not require the full power and complexity of a massive DNN. We pursue two concrete variants of this idea. First, we propose an adaptive early-exit strategy that allows easy examples to bypass some of the network's layers. Before each expensive neural network layer (e.g., convolutional layers), we train a policy that determines whether the current example should proceed to the next layer, or be diverted to a simple classifier for immediate classification. Our second approach, an adaptive network selection method, takes a set of pre-trained DNNs, each with a different cost/accuracy trade-off, and arranges them in a directed acyclic graph _cite_, with the the cheapest model first and the most expensive one last. We then train an exit policy at each node in the graph, which determines whether we should rely on the current model's predictions or predict the most beneficial next branch to forward the example to. In this context we pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. We demonstrate the merits of our techniques on the ImageNet object recognition task, using a number of popular pretrained DNNs. The early exit technique speeds up the average test-time evaluation of GoogLeNet _cite_, and ResnetN _cite_ by N-N \% within reasonable accuracy margins. The network cascade achieves N speed-up compared to pure ResnetN model at N \% top-N accuracy loss and N speed-up with no change in model accuracy. We also show that our method can approximate a oracle policy that can see true errors suffered for each instance. In addition to reducing the average test-time cost of DNNs, it is worth noting that our techniques are compatible with the common design of large systems of mobile devices, such as smart phone networks or smart surveillance-camera networks. These systems typically include a large number of resource-constrained edge devices that are connected to a central and resource-rich cloud. One of the main challenges involved in designing these systems is determining whether the machine-learned models will run in the devices or in the cloud. Offloading all of the work to the cloud can be problematic due to network latency, limited cloud ingress bandwidth, cloud availability and reliability issues, and privacy concerns. Our approach can be used to design such a system, by deploying a small inaccurate model and an exit policy on each device and a large accurate model in the cloud. Easy examples would be handled by the devices, while difficult ones would be forwarded to the cloud. Our approach naturally generalizes to a fog computing topology (where resource constrained edge devices are connected to a more powerful local gateway computer, which in turn is connected to a sequence of increasingly powerful computers along the path to the data-center) . Such designs allow our method to be used in memory constrained settings as well due to offloading of complex models from the device.