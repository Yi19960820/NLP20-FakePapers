Video summarisation has traditionally been formulated as an unsupervised learning problem _cite_, with criteria to identify keyframes (or key-segments) hand-crafted based on generic rules, such as diversity and representativeness. However, different types of video content may require different criteria or different combinations of them: for instance, summaries of {\em Eiffel Tower} videos should contain scenes with the tower, whereas summaries of {\em Making Sandwich} videos should focus on the key temporal stages of the task. How humans deploy these criteria based on the video content can be reflected through their summary annotations, which indicate whether each video frame or segment should be included in the summary. With the annotations, a supervised video summarisation model can be developed _cite_, capturing implicitly the content-specific frame/segment selection criteria. However, its use for large-scale summarisation tasks is limited because summary annotations are expensive to collect and prone to bias due to the subjective nature of video summaries. In this paper, a novel weakly-supervised video summarisation approach is proposed, which is content-specific but only requires video-level annotations in the form of video category labels. These video-level labels are easy to obtain, making the approach much more scalable than the supervised alternatives. Our approach is motivated by the fact that category labels typically encapsulate strong semantic information about the video content. Maintaining the recognisability of the video after removing frames/segments to produce a summary can thus be considered as a top-level selection criterion. Such a criterion encapsulates various fine-grained, content-specific criteria deployed by humans. For example, to summarise videos labelled as {\em Groom Animal}, humans would select segments containing one or more people who are working on an animal to support the semantic meaning conveyed by the category label. We therefore propose to learn a video summarisation model that selects video frames/segments based on whether collectively they contribute the most to recognising the summarised video into its category label. More specifically, we propose to utilise the video classification criterion elaborated above to guide the learning of a deep video summarisation network. We train the video summarisation network using reinforcement learning (RL) due to the following reasons. First, the classification of summaries can only be made at the end of videos whilst a decision/action needs to be made at every single frame on whether it should be included in the summary. This problem is thus naturally suited for RL. Second, the frame selections are inter-dependent in that the selection of one frame would have implications on the selection of others. The exploration-exploitation strategy of RL can better guide the summarisation network to capture the interdependencies among frames as different combinations of frames are explored. Figure _ref_ shows the proposed framework. In order to provide rewards during the reinforcement learning of the summarisation network so that the video category recognisability is maintained, our framework includes a companion classification network. This network is a recurrent network learned with supervised classification loss. This classification network can judge whether a given video sequence contains sufficient information to be classified to a certain category. This judgement is then used as a supervision signal/reward to guide the learning of the summarisation network. Concretely, we formulate the judgement made by the classifier as global recognisability reward and train the summarisation network with deep Q-learning _cite_ . The summarisation network is thus termed as Deep Q-learning Summarisation Network (DQSN) . Given a video, DQSN generates a summary by sequentially removing frames based on the prediction on future rewards. The classifier then classifies the summary and returns the global recognisability reward to DQSN, which is explicitly encouraged to produce summaries containing category-related information. A well-known challenge in RL is the credit assignment problem, \ie rewards are sparse or temporally delayed thus making it difficult to associate each action with a reward. With only the global recognisability reward, our DQSN also suffers from this problem as the single global reward can only be generated after a complete sequence of actions, which inevitably slows down the model convergence. The problem is particularly acute in our case due to the length of the sequences we are dealing with. We mitigate this problem by proposing a novel {\it dense} reward, which we call local relative importance reward. This reward gives each action a feedback by checking if the action changes the recognisability of the partial summary generated so far. Importantly, this reward is also obtained by the classification network, without requiring additional modules. {\bf Contributions} . (N) For the first time, a RL-based weakly supervised video summarisation framework is proposed, which requires only video-level category labels. (N) We overcome the notorious credit assignment problem in RL by introducing a novel dense reward. (N) To show the flexibility of our framework, we combine our weakly supervised rewards with those deployed in existing unsupervised approaches and demonstrate their complementarity. (N) We show that, on two widely-used benchmark datasets, namely TVSum _cite_ and CoSum _cite_, our approach not only outperforms unsupervised/weakly supervised alternatives but is also highly competitive against supervised approaches.