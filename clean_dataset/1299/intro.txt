Deep neural networks have recently led to significant improvement in countless areas of machine learning, from speech recognition to computer vision~ _cite_ . DNNs achieve high performance because deep cascades of nonlinear units allow to generalize non-locally, in data-specific manifolds~ _cite_ . While this ability to automatically learn non-local generalization priors from data is a strength of DNNs, it also creates counter-intuitive properties. In particular ~ _cite_ showed in their seminal paper that one can engineer small perturbations to the input data, called adversarial examples, that make an otherwise high-performing DNN misclassify every example. For image datasets, such perturbations are often imperceptible to the human eye, thus creating potential vulnerabilities when deploying neural networks in real environments. As an example, one could envision situations where an attacker having knowledge of the DNN parameters could use adversarial examples to attack the system and make it fail consistently. Even worse, due to the cross-model, cross-dataset generalization properties of the adversarial examples~ _cite_, the attacker might generate adversarial examples from independent models without full knowledge of the system and still be able to conduct a highly successful attack. This indicates there is still a significant robustness gap between machine and human perception, despite recent results showing machine vision performance closing in on human performance~ _cite_ . More formally, the challenge is: can we design and train a deep network that not only generalizes in abstract manifold space to achieve good recognition accuracy, but also retains local generalization in the input space? A main result from ~ _cite_ is that the smoothness assumption that underlies many kernel methods such as Support Vector Machines (SVMs) does not hold for deep neural networks trained through backpropagation. This points to a possible inherent instability in all deterministic, feed-forward neural network architectures. In practice, SVMs can be used to replace the final softmax layer in classifier neural networks leading to better generalization~ _cite_, but applying SVM in the manifold space does not guarantee local generalization in the input space. Recently, ~ _cite_ categorize distributions of deep neural networks through deep Gaussian Process (GP) and show that in stacked architectures, the capacity of the network captures fewer degrees of freedom as the layers increase. They propose to circumvent this by connecting inputs to every layer of the network. Without this trick, the input locality is hardly preserved in higher layers due to the complexity of nonlinear mapping cascades. A framework leveraging both approaches is Random Recursive SVM (R _inline_eq_ SVM) ~ _cite_, which recursively solves a SVM whose input combines input data and outputs from the previous SVM layer, randomly projected to the same dimension as the input data. R _inline_eq_ SVM avoids solving nonconvex optimization by recursively solving a SVM and demonstrates generalization on small datasets. However, performance is suboptimal compared to state-of-the-art DNNs, possibly due to lack of end-to-end training~ _cite_ . Another work inspired by the recursive nature of the human perceptual system is Deep Attention Selective Network (dasNet) ~ _cite_, which dynamically fine-tunes the weight of each convolutional filter at recognition time. We speculate that the robustness of human perception is due to complex hierarchies and recursions in the wirings of the human brain~ _cite_, since recursions provide multiple paths to input data and could retain locality information at multiple levels of representation. Such an intuition is also partially supported by the recent state-of-the-art models for object classification and detection involving multi-scale processing~ _cite_ . Since modeling such recursions in DNNs is notoriously hard and often relies on additional techniques such as reinforcement learning~ _cite_, we will at first investigate explicit inclusion of input generalization as an additional objective for the standard DNN training process. It is important to note that the adversarial examples are universal and unavoidable by their definition: one could always engineer an additive noise at input to make the model misclassify an example, and it is also a problem in shallow models such as logistic regression~ _cite_ . The question is how much noise is needed to make the model misclassify an otherwise correct example. Thus, solving the adversarial examples problem is equivalent to increasing the noticeability of the smallest adversarial noise for each example. In this paper we investigate new training procedures such that the adversarial examples generated based on ~ _cite_ have higher distortion, where distortion is measured by _inline_eq_ where _inline_eq_ are the adversarial data and original data respectively. First, we investigate the structure of the adversarial examples, and show that contrary to their small distortion it is difficult to recover classification performance through additional perturbations, such as Gaussian additive noises and Gaussian blur. This suggests the size of ``blind-spots'' are in fact relatively large, in input space volume, and locally continuous. We also show that adversarial examples are quite similar~ _cite_, and an autoencoder (AE) trained to denoise adversarial examples from one network generalizes well to denoise adversarials generated from different architectures. However, we also found that the AE and the classifier DNN can be stacked and the resulting network can again be attacked by creating new, adversarial examples of even smaller distortion. Because of this, we conclude that ideal architectures should be trained end-to-end and incorporate input invariance with respect to the final network output. We find that ideas from denoising autoencoder (DAE), contractive autoencoder (CAE), and most recently marginalized denoising autoencoder (mDAE) provide strong framework for training neural networks that are robust against adversarial noises~ _cite_ . We propose Deep Contractive Networks (DCNs), which incorporate a layer-wise contractive penalty, and show that adversarials generated from such networks have significantly higher distortion. We believe our initial results could serve as the basis for training more robust neural networks that can only be misdirected by a substantial noise, in a way that is more attuned to how human perception performs.