Fast development of internet technologies and recent advancements in hand-held smart devices have boosted the visual contents sharing. The convenient access to internet and interests in socializing platforms have flooded the image based information contents such as human faces, sceneries, online products, etc. Such an enormous growth of image data has paved the way for emerging applications from different domains where the users are interested in retrieving images from a large-scale collection. Remarkably little has yet been produced to meet the visual content based retrieval challenges effectively for large-scale data. Unlike traditional search engines where retrieval is inherently dependent on index terms and tags related to images, content based image retrieval (CBIR) manipulates large multimedia database by characterizing them based on their respective visual features. Recent years have witnessed numerous achievements in CBIR~ _cite_ . In spite of large volumes of retrieval databases, the approximate nearest neighbor (ANN) searching methods have rendered satisfiable performances to capture visual contents with moderate retrieval time. Among many ANN searching methods, hashing nearest neighbor methods remain at the top~ _cite_ . The superior performance of hashing methods is credited to robust mapping functions that can project an instance, such as an image, into a compact binary code. The resultant binary codes have a shorter distance under the Hamming metric if their original instances are similar. In general, hashing methods are considered more effective than other ANN based methods regarding both searching speed and memory cost. Existing hashing methods can be divided into data-independent and data-dependent ones by considering whether training data are used. Data-dependent methods are also known as learning to hashing (LNH) . Taking benefits of the training data, LNH usually achieves much better performance compared with data-independent methods. Hence, the study of data-dependent LNH methods attracts more attention. Data-dependent learning to hashing can be further categorized into unsupervised and supervised methods by considering whether the supervised information is used. More information about learning to hashing can be found in the survey paper _cite_ . Among many supervised learning to hashing methods, deep supervised hashing, which is based on deep neural network structures, has shown an encouraging ongoing research progress, with the aim to learn an end-to-end structure that can directly map images into binary codes. Different from deep hashing, the classical approach of supervised LNH is generally composed of two steps: firstly extracting off-the-shelf hand-crafted features such as HOG~ _cite_, GIST~ _cite_, SIFT~ _cite_ and variants of SIFT~ _cite_, as the input of the hashing learning procedure; then learning the hashing function based on these visual features. Obviously, for this kind of LNH methods, the final performance is influenced not only by the mapping function, but also by the quality of used visual features. Previous research efforts on hashing learning such as supervised discrete hashing (SDH) _cite_, fast supervised hashing _cite_ and column sampling supervised hashing _cite_ mostly focus on the second step. By taking feature learning into account, deep hashing involves the convolution neural network (CNN) structure as a feature learning part of the hashing learning framework. As a result, deep hashing usually gives better performance than classical supervised LNH methods on the image retrieval task. To achieve better performance and training efficiency, the following two essential questions need to be fully exploited: Currently, the widely available supervised information of image data is the semantic label information. For example, CIFAR-N _cite_ and ImageNet _cite_ provide single class label for each image while NUS-WIDE _cite_ and COCO _cite_ assign multiple labels to a single image. Several methods generate similar or dissimilar pair-wise labels from these available class labels~ _cite_ . That is, two images from the same class are assigned with the similar label, otherwise they are treated as dissimilar. Then the pair-wise or ranking (triplet) loss is used for training _cite_ . The pair-wise or ranking (triplet) loss implicitly shows how the supervised information is used for the similarity preserving learning. However, it is very expensive to train with the pair-wise or triplet loss because the number of inputs is the number of training samples to the power of two for _cite_ or three for _cite_, respectively. Besides, how to sample the training pairs or triplets significantly impacts the final performance _cite_ . Another common solution is to directly use the softmax to guide the training process _cite_ . Training with softmax is much simpler compared to that with the pair-wise or ranking loss. But how the softmax serves the similarity preserving learning is not fully understood. For example, softmax is used by combining with the pair-wise or the ranking labels in~ _cite_ . The other challenge comes from solving the discrete optimization with the binary constraint. Under the scenario of deep hashing, images are mapped to binary codes in the form _inline_eq_ . Works in _cite_ directly move this discrete constraint to a regularization term that minimizes _inline_eq_, where _inline_eq_ is the continuous feature vector generated by the deep neural network. Though this relaxation is straightforward, it is unfeasible to train the neural network with standard back-propagation due to all zero-valued gradient of the _inline_eq_ function for non-zero inputs _cite_ . Another common approach introduces an active function, such as _inline_eq_ or _inline_eq_, \to restrict output within _inline_eq_ or _inline_eq_ as well as setting hyper parameters to push outputs close to the saturate parts _cite_ . Though this method eliminates the _inline_eq_ function, it faces the vanishing gradient problem due to the usage of _inline_eq_ or _inline_eq_ . Specifically, the outputs are forced to be close to the saturate part where gradients are extremely small. This problem becomes non-ignorable when the CNN part has deeper structures. In this paper, we propose a novel deep supervised hashing method, which effectively exploits the class label information. Different from previous deep supervised hashing that uses similar/dissimilar labels for training, our observation is that the class labels themselves are more natural and contain richer semantic concepts than the similar/dissimilar labels. Meanwhile, directly using the class labels is also beneficial for training since training with pair-wise or triplet loss function, which is generated by class labels, is computationally being expensive _cite_ . Based on these observations, we develop a novel loss function which directly uses the class labels to serve the similarity-preserving learning by punishing the overlap part among different classes in the embedding space. Because our model is designed to handle the deep supervised hashing through class wise level, we call it Deep Class-Wise Hashing (DCWH) . Besides, we extend our model to multi-label data. On the other hand, instead of directly solving the discrete optimization problem, we suggest a two-stage optimization strategy that guarantees our model can be effectively trained. From a high level point of view, we first solve an approximate version of the optimization problem with the hyper cube constraint. Instead of directly learning a discrete feature space, we propose first to project images into a hyper cube space, which is more suitable for the continuous nature of the CNN. Then, at the second stage, we continue refining our model to reduce the quantization error. Extensive experiments on four import benchmark datasets show that our model surpasses other state-of-the-art deep hashing methods. To summarize, the main contributions of this paper are as follows: The rest of this paper is organized as follows. Sec~ _ref_ briefly introduces recent research works in deep hashing and deep metric learning. The motivation of how we develop the class-wise model from the pair-wise loss is explained in Sec~ _ref_ . Based on the motivation, a class-wise deep hashing model along with the optimization strategy is presented in Sec~ _ref_ . In Sec~ _ref_, extensive experiments is conducted to compare with other state-of-the-art deep hashing methods. In Sec~ _ref_, several important properties of the proposed model are discussed. The conclusion and feature works of this paper are drawn in the final section.