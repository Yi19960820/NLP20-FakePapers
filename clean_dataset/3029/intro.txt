In recent years, research on human action recognition in images and videos has become a hot topic in the fields of computer vision, pattern recognition, and machine learning _cite_ . In the field of image recognition, features learned by convolutional neural networks (CNN) are often superior to hand-crafted ones _cite_ . Therefore, CNN applies to spatial feature extraction of human action. The temporal relationship between video frames provides additional motion information. Effective usage of the temporal information in the video can better analyze the potential information in the video and improve the recognition rate. Simonyan et al. _cite_ proposed a two-stream CNN network in which RGB and optical flow images were inputted into two separate networks to learn appearance and motion characteristics. The final result was the fusion of the prediction of two streams. However, space and time complexity of the pre-calculation of optical flow is very high. Even when using GPUs, optical flow calculation is the main bottleneck for two-stream CNN network. Ji et al. _cite_ proposed the ND CNN model, which used a ND convolution kernel to convolve three consecutive frames and obtained the spatial-temporal characteristics. Donahue et al. _cite_ proposed the LRCN model, which is a typical action-aware deep convolutional network composed of AlexNet _cite_ and LSTM _cite_ network. AlexNet extracts the intra-frame spatial features and fed them into an LSTM for modeling temporal relationship to obtains spatial and temporal information. We propose the Squeeze-and-Excitation Long-term Recurrent Convolutional Networks (SE-LRCN) for human action recognition, which uses the Squeeze-and-Excitation operations to implement the feature recalibration. Therefore, this paper also uses the Squeeze-and-Excitation (SE) module _cite_ to name this model. The framework of the proposed SE-LRCN is shown as in Fig.N. In SE-LRCN, the Squeeze-and-Excitation ResNet-N (SE-ResNet-N) _cite_ extracts the spatial feature, and we also propose Squeeze-and-Excitation Long Short-Term Memory (SE-LSTM) for modeling the temporal relationship. SE-ResNet-N and SE-LSTM achieve the attention extraction of pixel and frame granularity respectively. Taking into consideration the dependencies and degrees of importance of the feature channels of the pixel granularity when extracting the spatial features; Considering that the temporal relationship and degree of importance of the frame granularity when to perform temporal modeling. We evaluate the proposed model on two challenging benchmarks, HMDBN and UCFN, and achieves the competitive results with the state-of-the-art