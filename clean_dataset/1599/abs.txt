Compact neural networks are inclined to exploit ``sparsely-connected'' convolutions such as depthwise convolution and group convolution for employment in mobile applications. Compared with standard ``fully-connected'' convolutions, these convolutions are more computationally economical. However, ``sparsely-connected'' convolutions block the inter-group information exchange, which induces severe performance degradation. To address this issue, we present two novel operations named and to leverage the inter-group information. Our key idea is encoding the inter-group information with a narrow feature map, then combining the generated features with the original network for better representation. Taking advantage of the proposed operations, we then introduce the, an architectural unit specifically designed for compact networks. Finally, we propose a family of compact neural networks called based on ME modules. Extensive experiments on ILSVRC N dataset and PASCAL VOC N dataset demonstrate that MENet consistently outperforms other state-of-the-art compact networks under different computational budgets. For instance, under the computational budget of N MFLOPs, MENet surpasses ShuffleNet by N \% and MobileNet by N \% on ILSVRC N top-N accuracy, while by N \% and N \% on PASCAL VOC N mAP, respectively.