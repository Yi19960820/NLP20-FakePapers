Acoustic scene classification (ASC) refers to the identification of the environment in which the audios have been acquired, which associates a semantic label to each audio. In N, Sawhney proposed the first method to address the ASC problem in an MIT technical report _cite_ . A set of classes, including ``people'', ``voices'', ``subway'', ``traffic'' is recorded. An overall classification accuracy of N \% was obtained based on the recurrent neural networks and the K-nearest neighbor criterion. Indeed, the recognition of environments has become an important application in the field of machine listening, and ASC enables devices to make sense of their environments. The potential applications of ASC seem evident in several fields, such as security surveillance and context-aware services. In order to solve the problem of lacking common benchmarking datasets, the first Detection and Classification of Acoustic Scenes and Events (DCASE) N challenge _cite_ was organized by the IEEE Audio and Acoustic Signal Processing (AASP) Technical Committee. Many audio processing techniques have been proposed during the past years. The applications of deep learning in the ASC have witnessed a dramatic increase during last five years, especially the convolutional neural network (CNN) . Compared to the traditional method, which commonly involves training a Gaussian Mixture Model (GMM) on the frame-level features such as Mel-Frequency Cepstral Coefficients (MFCCs) _cite_, CNN-based methods can achieve better performance. However, most of the previous attempts aimed to apply the deep learning method by using one single channel (or just the average between the left and right channels) _cite_ . A robust Audio Scene classification model should be able to capture temporal patterns at different channels as additional cues may be embedded in the multi-channel recordings _cite_ . In this paper, we explore the use of multi-channel CNN for the ASC task, which achieves better accuracy with comparison to the standard CNN. On the other hand, the deep neural network architectures have a large number of parameters, and they are prone to overfitting. The easiest and most widely used approach to reduce overfitting is to employ larger datasets. As an alternative, data augmentation method can be used to improve the performance of neural network by artificially enlarging the dataset using label-preserving transformation. However, only a few attempts have been made for the data augmentation for audio scene classification. In this paper, we explore the use of mixup-based method for data augmentation _cite_, with the goal to obtain superior accuracy and robustness. In brief, mixup constructs virtual training examples, and the neural network can be trained by using the linear combinations of pairs of the representation of examples and their labels. Theoretically, mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of audio feature vectors should lead to linear interpolations of the associated targets _cite_ . Mixup can be implemented in a few lines of code, and induces the minimal computation overhead. Despite its simplicity, mixup allows a performance improvement using the DCASE N audio scene classification dataset. The paper is organized as follows. Section N discusses the relationship between our method and prior work, while, the multi-channel CNN classification method is presented in Section N. Section N describes the mixup method, and the experimental results are given in Section N. Section N gives the conclusion of this paper.