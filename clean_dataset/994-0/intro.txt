Consider a dynamic scene such as Figure~ _ref_, where you, as the camera wearer, are playing basketball. You need to make a decision with whom you will cooperate to maximize the overall benefit for your team. Looking ahead at your teammates, you make a conscious decision and then N-N seconds afterwards you perform a cooperative action such as passing the ball. In a team sport such as basketball, an effective cooperation among teammates is essential. Thus, in this paper, we aim to investigate whether we can use a single first-person image to infer with whom the camera wearer will cooperate N-N seconds from now? This is a challenging task because predicting camera wearer's cooperative intention requires N) inferring his/her momentary visual attention, N) decoding dominant social signals expressed by other players who want to cooperate, and N) knowing who your teammates are when the players are not wearing any team-specific uniforms. To make this problem even more challenging we ask a question: ``Can we infer cooperative basketball intention without manually labeled first-person data?''. Building an unsupervised learning framework is important because manually collecting basketball intention labels is a costly and a time consuming process. In the context of a cooperative basketball intention task, an annotator needs to have highly specific basketball domain knowledge. Such a requirement limits the scalability of the annotation process because such annotators are difficult to find and costly to employ. However, we conjecture that we can learn cooperative basketball intention in an unsupervised fashion by exploiting the signal provided by the first-person camera. What people see reflects how they are going to act. A first-person camera placed on a basketball player's head allows us to indirectly tap into that person's mind and reason about his/her internal state based on what the camera wearer sees. To do so we propose a novel cross-model EgoSupervision learning scheme, which allows us to learn the camera wearer's intention without the manually labeled intention data. Our cross-model EgoSupervision scheme works as follows. First we transform the output of a pretrained pose-estimation network such that it would approximately reflect the camera wearer's internal state such as his/her visual attention and intentions. Then, we use such transformed output as a supervisory signal to train another network for our cooperative basketball intention task. We show that such a learning scheme allows us to train our model without manually annotated intention labels, and achieve similar or even better results as the fully supervised methods do.