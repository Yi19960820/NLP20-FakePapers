Research in the area of knowledge graphs have largely focused on mining large corpora of text to extract entities, concepts, and the relations between them. Linking entities extracted from images to knowledge graphs and reasoning with them has received little attention. Knowledge graphs (KG) such DBpedia _cite_ and Yago _cite_ store entities and their relations in the form of RDF triples. Certain attributes contain pictures of the entity described, but most systems that use these KGs lack the ability to reason using these pictures. Pictures can provide information that can be used to improve a system's ability to perform tasks such as image search, visual verification _cite_ (i.e., to verify the validity of a fact by looking for proof in pictures) and visual question answering _cite_ (i.e. answering questions about the image) . To improve visual reasoning, Zhu et. al _cite_ use a knowledge graph representation to reason about object affordances _cite_, where the system tries to learn about the object, estimate the human pose to perform an action over it, and define the relative position of object with respect to the human. These tasks have become important in an age where smart-phones with cameras are the preferred mode of communication and to search for content on the web _cite_ . Object detection, scene description, pose estimation and other such tasks were commonly solved using SIFT _cite_ and HOG _cite_ features. But in recent years, deep neural networks have had tremendous success in diverse fields such as computer vision _cite_ _cite_, natural language processing _cite_ _cite_ and audio _cite_ . These developments, coupled with the availability of large quantities of data and high performance computing hardware such as GPUs (with CUDA and dedicated libraries like CuDNN), made remarkable performance gains possible. One of the popular deep neural network architectures, a Convolutional Neural Network (CNN), has substantially improved image classification compared to other existing methods as shown by Krizhevsky et. al. in N _cite_ . CNNs are also able to achieve state-of-the-art performance in many of the aforementioned tasks _cite_ _cite_ _cite_ . However, despite their success, intermittent work has been conducted to understand how deep neural networks learn features from the input data. Our focus is to extend the understanding of how deep networks operate by looking at the network's representations of spatial relations between objects in images. Spatial relations are difficult to learn as they are defined by the relative positions of objects in the image and are independent of the object itself (e.g., the relation next to is common to both scenarios: (a) A man is standing next to his dog (b) A child is sitting next to the chair) . Consider figure _ref_ where the desired output is to extract the relation next-to directly from the image and represent it as: \ Our research focuses on training a network to classify a subset of spatial relations only. Depending on the frame of reference, spatial relations are classified into three types: basic, deictic, or intrinsic _cite_ . Our network is trained on deictic relations, where the relation between two objects is specified with respect to the viewer's point of view (POV) . In this paper, we show how the network learns these relations by overlaying a generated heatmap on the original image that highlights parts of the image that are considered important to identify the relations. On further analysis, experiments reveal how a group of nodes are formed in the network which have a higher impact on the ability of the network to predict them. Figure _ref_ shows an example image and the overlayed heatmap. The regions in red are important to identify the relation behind . The following sections discuss the background and related work, the architecture used to train a model to predict spatial relations, the datasets on which the models are trained, and the observations from experiments conducted.