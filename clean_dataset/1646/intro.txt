A Generative Adversarial Network (GAN) ~ _cite_ is a generative model that can produce realistic samples from random vectors drawn from a known distribution. A GAN consists of a generator _inline_eq_ and a discriminator _inline_eq_, both of which are usually implemented as deep neural networks. The training of a GAN involves an adversarial game between the generator and the discriminator. In the context of images, the generator maps low-dimensional vectors from latent space to image space, creating images that are intended to come from the same distribution as the training data; the discriminator tries to classify between images generated by the generator (trying to assign score _inline_eq_) and real images from training data (trying to assign score _inline_eq_) . Ideally, the distribution of the images generated from the generator become indistinguishable from the distribution of real images in the training set, and the discriminator assigns _inline_eq_ to both generated and real images. In practice, this is hard to achieve and there is usually a gap between the learned distribution by generator and the real distribution. A conditional Generative Adversarial Network, sometimes called a cGAN~ _cite_ is an extension from GAN which allows for generating samples conditioned on certain external information. Such an extension takes the form of feeding the conditional vector into both the generator and the discriminator during the training process. After training, the generator can generate samples dictated by the condition from a random vector together with a conditional vector. The ability to control certain attributes of the generated samples is of crucial importance for a lot of applications, such as image inpainting~ _cite_, image manipulation~ _cite_, style transfer~ _cite_, future frame prediction~ _cite_, text-to-image~ _cite_ and image-to-image translation in general~ _cite_ . Recovering the latent vector as well as the conditional vector from an image can be useful. It is known that vectors that are close in latent and conditional space generates visually similar images, and algebraic operations in latent vector space often lead to meaningful corresponding operations in image space~ _cite_ . For a given image, being able to access the latent and conditional vector allow us to perform many tasks such as realistic image editing, data augmentation, inferences, retrieval, compression, and other insights of what the networks see and learn which can be significant to debugging, diagnosis, and other security related issues. The original cGAN framework and GAN framework in general do not provide a straightforward way of reverse back from an image to latent and conditional vector. We cover some of the previous work on recovering latent vector of a GAN in Section~ _ref_ . In this work, we show that it is also possible to recover the conditional vector from a cGAN for a known generator. While the recovery of latent vectors may become unreliable under the effect of mode collapse~ _cite_ when different latent vectors are mapped to a single image, the recovery of conditional vectors are usually robust, since it is rare for a successfully trained cGAN to map different conditional vectors to the same image. A very important point to make here is that it is not the same to recover from an image generated by the generator, and from a real image. Recovering the latent and conditional vector from generated image can be considered an reverse operation which the forward operation does exist. However, when recovering from a real image we are treating it as if it was generated by the generator whereas in fact such a mapping may not exist. Thus, it is more like a projection of a real image onto the manifold learned by the generator. Besides recovering from generated images, this more interesting question of whether sensible conditional information can be recovered from real images is investigated in this work.