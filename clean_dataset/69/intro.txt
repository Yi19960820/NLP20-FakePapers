In recent years, the paradigm of generating a reduced set of object location hypotheses (or window candidates) to be evaluated with a powerful classifier has become very popular in object detection. Most of the recent state-of-the-art detection methods _cite_ are based on such proposals. Using limited number of these proposals also helps with weakly supervised learning, in particular learning to localize objects without any bounding box annotations~ _cite_ . This approach can be seen as a two-stage cascade: First, selection of a reduced set of promising and class-independent hypotheses and second, a class-specific classification of each hypothesis. This pipeline has the advantage that, similarly to sliding window, it casts the detection problem to a classification problem. However, in contrast to sliding window, more powerful and time consuming detectors can be employed as the number of candidate windows is reduced. Methods for the generation of the window candidates are based on two very different approaches. The first approach uses bottom-up cues like image segmentation _cite_, object edges and contours _cite_ for window generation. The second approach is based on top-down cues which learn to separate correct object hypotheses from other possible window locations _cite_ . So far, the latter strategy seems to have inferior performance. In this paper we show that, with the proper features, accurate and fast top-down window proposals can be generated. We consider for this task the convolutional neural network (CNN) ``feature maps'' extracted from the intermediate convolutional layers of the Alexnet _cite_ trained on N classes of ImageNet. In the first part of this work we present a performance analysis of different CNN layers for generating proposals. More specifically, similarly to BING _cite_, we select a reduced set of window sizes and aspect ratios and slide them on each possible location of the feature map generated by a certain CNN layer. The relevance (or objectness) of the windows is learned using a linear classifier. As the proposal generation procedure should be fast, we base the feature aggregation for each candidate window on average pooling, which can be computed in constant time using integral images _cite_ . From this analysis we see that there is not a single best layer for candidate windows generation. Instead we notice that deeper layers, having a more semantic representation, perform very well in recalling the objects with a reduced set of hypotheses. Unfortunately, as noticed also for other tasks _cite_, they provide a poor localization of the object due to their coarseness. In contrast, earlier layers are better in accurately localizing the object of interest, but their recall is reduced as they do not represent strong object cues. Thus, we conclude that, for a good window candidate generation, we should leverage multiple layers of the CNN. However, even with the very fast integral images for the feature extraction, evaluating all window locations at all feature layers is too expensive. Instead we propose a method based on a cascade starting from the last convolutional layer (layer N) and going down with subsequent refinements until the initial layers of the net. As the flow of the cascade is inverse to the flow of the feature computation we call this approach an inverse cascade . Also, as we start from a coarse spatial window resolution, and throughout the layers we select and spatially refine the window hypotheses until a reduced and spatially well localized set of hypotheses, we call our method coarse-to-fine inverse cascade . An overview of our approach is illustrated in Fig.~ _ref_ . We evaluate the performance of the method in terms of recall vs. number of proposals as well as in terms of recall vs. object overlap. We show that in both evaluations the method is better than the current state of the art, and computationally very efficient. However, the best of the method comes when it is associated with a CNN-based detector _cite_ . In this case the approach does not need to compute any feature, because it reuses the same features already computed by the CNN network for detection. Thus, we can execute the full detection pipeline efficiently. In the next section, we describe related work. Next, in section N, we analyze the quality of different CNN layers for window proposal generation. Section N describes our inverse coarse-to-fine cascade. In section N we compare our method with the state-of-the-art, both in terms of object proposal generation as in terms of object detection performance. Section N concludes the paper.