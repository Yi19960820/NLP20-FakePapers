learning with convolutional neural networks (CNNs) has achieved state-of-the-art performance for automated medical image segmentation~ _cite_ . However, automatic segmentation methods have not demonstrated sufficiently accurate and robust results for clinical use due to the inherent challenges of medical images, such as poor image quality, different imaging and segmentation protocols, and variations among patients~ _cite_ . Alternatively, interactive segmentation methods are widely adopted, as integrating the user's knowledge can take into account the application requirements and make it easier to distinguish different tissues~ _cite_ . As such, interactive segmentation remains the state of the art for existing commercial surgical planning and navigation products. Though leveraging user interactions often leads to more robust segmentations, a good interactive method should require as little user time as possible to reduce the burden on users. Motivated by these observations, we investigate combining CNNs with user interactions for medical image segmentation to achieve higher segmentation accuracy and robustness with fewer user interactions and less user time. However, there are very few studies on using CNNs for interactive segmentation~ _cite_ . This is mainly due to the requirement of large amounts of annotated images for training, the lack of image-specific adaptation and the demanding balance of model complexity, time and memory space efficiency. The first challenge of using CNNs for interactive segmentation is that current CNNs do not generalize well to previously unseen object classes, as they require labeled instances of each object class to be present in the training set. For medical images, annotations are often expensive to acquire as both expertise and time are needed to produce accurate annotations. This limits the performance of CNNs to segment objects for which annotations are not available in the training stage. Second, interactive segmentation often requires image-specific learning to deal with large context variations among different images, but current CNNs are not adaptive to different test images, as parameters of the model are learned from training images and then fixed during the testing, without image-specific adaptation. It has been shown that image-specific adaptation of a pre-trained Gaussian Mixture Model (GMM) helps to improve segmentation accuracy~ _cite_ . However, transitioning from simple GMMs to powerful but complex CNNs in this context has not yet been demonstrated. Third, fast inference and memory efficiency are demanded for interactive methods. These can be relatively easily achieved for ND segmentations, but become much more problematic for ND volumes. For example, DeepMedic~ _cite_ works on local patches to reduce memory requirements but results in a slow inference. HighResNDNet~ _cite_ works on an entire volume with relatively fast inference but needs a large amount of GPU memory, leading to high hardware requirements. To make a CNN-based interactive segmentation method efficient to use, enabling CNNs to respond quickly to user interactions and to work on a machine with limited GPU resources (e.g, a standard desktop PC or a laptop) is desirable. DeepIGeoS~ _cite_ combines CNNs with user interactions and has demonstrated good interactivity. However, it has a lack of adaptability to unseen image contexts. The contributions of this work are four-fold. First, we propose a novel deep learning-based framework for interactive ND and ND medical image segmentation by incorporating CNNs into a bounding box and scribble-based binary segmentation pipeline. Second, we propose to use image-specific fine-tuning to adapt a CNN model to each test image independently. The fine-tuning can be either unsupervised (without additional user interactions) or supervised where user-provided scribbles will guide the learning process. Third, we propose a weighted loss function considering network and interaction-based uncertainty during image-specific fine-tuning. Fourth, we present the first attempt to employ CNNs to segment previously unseen objects. The proposed framework does not require annotations of all the organs for training. Thus, it can be applied to new organs or new segmentation protocols directly. For natural image segmentation, FCN~ _cite_ and DeepLab~ _cite_ are among the state-of-the-art performing methods. For ND biomedical image segmentation, efficient networks such as U-Net~ _cite_, DCAN~ _cite_ and Nabla-net~ _cite_ have been proposed. For ND volumes, patch-based CNNs were proposed for segmentation of the brain tumor~ _cite_ and pancreas~ _cite_, and more powerful end-to-end ND CNNs were proposed by V-Net~ _cite_, HighResNDNet~ _cite_, and ND deeply supervised network~ _cite_ . An extensive range of interactive segmentation methods have been proposed~ _cite_ . Representative methods include Graph Cuts~ _cite_, Random Walks~ _cite_ and GeoS~ _cite_ . Machine learning methods have been widely used to achieve high accuracy and interaction efficiency. For example, GMMs are used by GrabCut~ _cite_ to segment color images. Online random forests (ORFs) are employed by SlicSeg~ _cite_ for segmentation of fetal MRI volumes. In~ _cite_, active learning is used to segment ND Computed Tomography (CT) images. They have achieved more accurate segmentations with fewer user interactions compared with traditional interactive segmentation methods. To combine user interactions with CNNs, DeepCut~ _cite_ and ScribbleSup~ _cite_ propose to leverage user-provided bounding boxes or scribbles, but they employ user interactions as sparse annotations for the training set rather than as guidance for dealing with a single test image. ND U-Net~ _cite_ learns from annotations of some slices in a volume and produces a dense ND segmentation, but takes a long time for training and cannot be made responsive to user interactions. In~ _cite_, an FCN is combined with user interactions for ND RGB image segmentation, without adaptation for medical images. DeepIGeoS~ _cite_ uses geodesic distance transforms of scribbles as additional channels of CNNs for interactive medical image segmentation, but cannot deal with previously unseen object classes. Previous learning-based interactive segmentation methods often employ an image-specific model. For example, GrabCut~ _cite_ and SlicSeg~ _cite_ learn from the target image with GMMs and ORFs, respectively, so that they can be well adapted to the specific target image. Learning a model from a training set with image-specific adaptation during the testing has also been used to improve the segmentation performance. For example, an adaptive GMM has been used to address the distribution mismatch between a test image and the training set~ _cite_ . For CNNs, fine-tuning~ _cite_ is used for domain-wise model adaptation to address the distribution mismatch between different training sets. However, to the best of our knowledge, this paper is the first work to propose image-specific model adaptation for CNNs.