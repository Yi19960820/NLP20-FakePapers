Deep learning has achieved great successes in various computer vision tasks, such as ND image recognition ~ _cite_ and semantic segmentation ~ _cite_ . However, deep networks usually rely on large-scale labeled datasets for training. When it comes to ND data, such as medical volumetric data and point clouds, human labeling can be extremely costly, and often requires expert knowledge. Take medical imaging for example. With the rapid growth in the demand of finer and larger scale of computer-aided diagnoses (CAD), ND segmentation of medical images (such as CTs and MRIs) is acting as a critical step in biomedical image analysis and surgical planning. However, well-annotated segmentation labels in medical images require both high-level expertise of radiologists and careful manual labeling of object masks or surface boundaries. Therefore, semi-supervised approaches with unlabeled data occupying a large portion of the training data are worth exploring. In this paper, we aim to design a semi-supervised approach for ND data, which can be applied to diverse data sources, e.g. CT/MRI volumes and ND point clouds. Inspired by the success of co-training ~ _cite_ and its extension into single ND images~ _cite_, we further extend this idea into ND. Typical co-training requires at least two views (i.e. sources) of data, either of which should be sufficient to train a classifier on. Co-training minimizes the disagreements by assigning pseudo labels between each other view on unlabeled data. Blum and Mitchell~ _cite_ further proved that co-training has PAC-like guarantees on semi-supervised learning with an additional assumption that the two views are conditionally independent given the category. Since most computer vision tasks have only one source of data, encouraging view differences is a crucial point for successful co-training. For example, deep co-training ~ _cite_ trains multiple deep networks to act as different views by utilizing adversarial examples ~ _cite_ to address this issue. Another aspect of co-training to emphasize is view confidence estimation. In multi-view settings, given sufficient variance of each view, the quality of each prediction is not guaranteed and bad pseudo labels can be harmful if used in the training process. Co-training could benefit from trusting reliable predictions and degrading the unreliable ones. However, distinguishing reliable and unreliable predictions is challenging for unlabeled data because of lacking ground-truth. To address the above two important aspects, we propose an uncertainty-aware multi-view co-training (UMCT) framework, shown in Fig.~ _ref_ . First of all, we define the concept of ``view'' in our work as a data-model combination which combines the concepts of data source (classical co-training) and deep network model (deep co-training) . Although only one source of data is available, we can still introduce data-level view differences by exploring multiple viewpoints of ND data through spatial transformations, such as rotation and permutation. Hence, our multi-view approach naturally adapts to analyze ND data and can be integrated with the proposed co-training framework. We further introduce the model-level view differences by adopting ND pre-trained models to asymmetric kernels in ND networks, such as _inline_eq_ kernels. In this way, we can not only utilize the ND pre-trained weights but also train the whole framework in a full ND fashion _cite_ . Importantly, such design introduces ND biases in each view during training, leading to complementary feature representations in different views. During the training process, these disagreements between views are minimized through ND co-training, which further boosts the performance of our model. Another key component is the view confidence estimation. We propose to estimate the uncertainty of each view's prediction with Bayesian deep networks by adding dropout into the architectures _cite_ . A confidence score is computed based on epistemic uncertainty _cite_, which can act as a weight for each prediction. After propagation through this uncertainty-weighted label fusion module (ULF), a set of more accurate pseudo labels can be obtained for each view, which is used as supervision signal for unlabeled data. Our proposed approach is evaluated on the NIH pancreas segmentation dataset and the training/validation set of LiTS liver tumor segmentation challenge. It outperforms other semi-supervised methods by a large margin. We further investigate the influence of our approach when applied in a fully supervised setting, to see whether it can also assist training for each branch with sufficient labeled data. A fully-supervised method based on our approach achieved state-of-the-art results on LiTS liver tumor segmentation challenge and scored the second place in the Medical Segmentation Decathlon challenge, without using complicated data augmentation or model ensembles.