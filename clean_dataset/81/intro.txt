The changes of retinal vasculature contain substantial diagnostic information for many vascular and systematic diseases. Specifically, diseases may affect arterioles and venules differently. For example, in hypertensive patients, the size of arterioles usually shrinks faster than that of venules. While in diabetic patients, we usually observe the expansion of venules first. Therefore, accurately segmenting and classifying retinal arterioles and venules has a great potential to improve the diagnosis and management of these diseases. Methods for segmenting and classifying retinal arterioles and venules include two major types: image-processing methods _cite_ and deep learning based methods _cite_ . For image-processing methods, a vessel segmentation-arteriovenous classification strategy is usually adopted. A binary mask is first generated by retinal vessel segmentation of the fundus image. Furthermore, the centerlines of the vessels are computed from the binary mask using image morphological operations. Various hand-crafted image features are then extracted around vessel centerlines, which are used to classify the arterioles and the venules in the image. On the other hand, deep learning based methods are reported to classify arterioles and venules. Welikala et al _cite_ present a two-stage method for automated classification of arteriole and venule using deep learning. Retinal vessels are first segmented using a linear detector. The centerlines of arterioles and venules are then classified by a N-layer neural network. AlBadawi et al. _cite_ report a deep learning method that combines an encoding-decoding model and graph-based approach to classify arterioles and venules. These methods substantially improve segmentation accuracies as opposed to traditional image-understanding approaches, but the pipelined workflow may undermine the stability of the methods. We propose a novel architecture of deep convolutional neural network for segmenting and classifying arterioles and venules on retinal fundus images. At beginning, we first adopt the encoding-decoding structure (Unet) as the backbone network of our proposed model. However, the model generates poor segmentation and classification results with the classic convolutional layers in Unet. One explanation of this problem is that retinal vessels in fundus images strictly follow a topological distribution: the same type of blood vessels does not intersect itself. The fixed receptive fields of classic convolutional layers are insufficient to represent such global image information. Therefore, to improve the accuracies of segmentation and classification, we develop a special encoding path that couples InceptionVN modules and Cascading Dilated Convolutions (CDCs) on top of the backbone network. The model is thus able to extract and fuse high-level semantic features from multi-scale receptive fields. The network structure is shown in Fig. N. This network takes the original color fundus image as inputs and multi-class labels as outputs, which follows an end-to-end training process, requiring limited pre-and post-processing of the image data. All the image features are computed and utilized internally in the deep neural network, where no hand-crafted features or task-specific assumptions are included. The proposed method is evaluated on the DRIVE dataset _cite_ and has achieved state-of-the-art performance.