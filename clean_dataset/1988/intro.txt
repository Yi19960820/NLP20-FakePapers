This paper investigates, a paradigm in which the learner exploits as much well-annotated data as possible (\eg, ImageNet _cite_, COCO _cite_) and is also provided with potentially unlimited unlabeled data (\eg, from internet-scale sources) . It is a special regime of semi-supervised learning. However, most research on semi-supervised learning has labeled/unlabeled data by splitting a fully annotated dataset and is therefore likely to be by fully supervised learning with all annotations. On the contrary, omni-supervised learning is by the accuracy of training on all annotated data, and its success can be evaluated by how much it surpasses the fully supervised baseline. To tackle omni-supervised learning, we propose to perform knowledge distillation, inspired by _cite_ which performed knowledge distillation . Our idea is to generate annotations on unlabeled data using a model trained on large amounts of labeled data, and then retrain the model using the extra generated annotations. However, training a model on its own predictions often provides no meaningful information. We address this problem by ensembling the results of a single model run on different transformations (\eg, flipping and scaling) of an unlabeled image. Such transformations are widely known to improve single-model accuracy _cite_ when applied at test time, indicating that they can provide nontrivial knowledge that is not captured by a single prediction. In other words, in comparison with _cite_, which distills knowledge from the predictions of multiple models, we distill the knowledge of a single model run on multiple transformed copies of unlabeled data (see Figure~ _ref_) . Data distillation is a simple and natural approach based on ``self-training'' (\ie, making predictions on unlabeled data and using them to update the model), related to which there have been continuous efforts _cite_ dating back to the Ns, if not earlier. However, our simple data distillation approach can become realistic largely thanks to the rapid improvement of fully-supervised models _cite_ in the past few years. In particular, we are now equipped with accurate models that may make fewer errors than correct predictions. This allows us to trust their predictions on unseen data and reduces the requirement for developing data cleaning heuristics. As a result, data distillation does not require one to change the underlying recognition model (\eg, no modification on the loss definitions), and is a scalable solution for processing large-scale unlabeled data sources. To test data distillation for omni-supervised learning, we evaluate it on the human keypoint detection task of the COCO dataset _cite_ . We demonstrate promising signals on this real-world, large-scale application. Specifically, we train a Mask R-CNN model _cite_ using data distillation applied on the original labeled COCO set and another large unlabeled set (\eg, static frames from Sports-NM _cite_) . Using the distilled annotations on the unlabeled set, we have observed improvement of accuracy on the held-out validation set: \eg, we show an up to N points AP improvement over the strong Mask R-CNN baseline. As a reference, this improvement compares favorably to the _inline_eq_ N points AP improvement gained from training on a similar amount of extra data in _cite_ (using private annotations) . We further explore our method on COCO object detection and show gains over fully-supervised baselines.