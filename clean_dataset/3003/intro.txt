Deep learning (DL) is mainly motivated by the research of artificial intelligent, in which the general goal is to imitate the ability of human brain to observe, analyze, learn and make a decision, especially for complex problem _cite_ . This technique is in the intersection amongst the research area of signal processing, neural network, graphical modeling, optimization and pattern recognition. The current reputation of DL is implicitly due to drastically improve the abilities of chip processing, significantly decrease the cost of computing hardware and advance research in machine learning and signal processing _cite_ . In general, the model of DL technique can be classified into discriminative models, generative models, and hybrid model _cite_ . Discriminative models, for instance, are CNN, deep neural networks, and recurrent neural network. Some examples of generative models are deep belief networks (DBN), restricted Boltzmann machine, regularized autoencoders, and deep Boltzmann machines. On the other hand, hybrid model refers to the deep architecture use the combination of a discriminative and generative model. An example of this model is DBN to pre-train deep CNN, which can improve the performance of deep CNN over random initialization. Among all of hybrid DL techniques, this paper focuses on metaheuristic optimization for training a CNN. Although the sound character of DL to solve a variety of learning tasks, training is difficult _cite_ _cite_ _cite_ . Some examples of successful methods for training DL are Stochastic Gradient Descent, Conjugate gradient, Hessian-free Optimization and Krylov Subspace Descent. Stochastic Gradient Descent is easy to implement and also fast in the process for a case with many training samples. However, this method needs several manual tuning to make its parameters optimal, and also its process is principally sequential, as a result, it is hard to parallelize them with GPUs. Conjugate Gradient (CG) on the other side is easier to check for convergence as well as more stable to train. Nevertheless, CG is slow, so that it needs multicore CPUs and availability of a vast number of RAMs _cite_ . Hessian-free optimization (HFO) has been applied to train deep auto-encoders _cite_, proficient in handling under fitting problem, and more efficient than pre-training + fine tuning proposed by Hinton and Salakhutdinov _cite_ . On the other side, Krylov Subspace Descent (KSD) is more robust and simpler than HFO as well as look like to work better for the classification performance and optimization speed. However, KSD needs more memory than HFO _cite_ . In fact, techniques of modern optimization are heuristic or metaheuristic. These optimization techniques have been applied to solve any optimization problems in the research area of science, engineering, and even industry _cite_ . However, research about metaheuristic for optimize deep learning method is rarely conducted. One of paper is the combining of genetic algorithm (GA) and CNN, proposed by You Zhining and Pu Yunming _cite_ .Their model select the CNN characteristic by the process of recombination and mutation on GA, in which the model of CNN exists as individual in the algorithm of GA. Besides, in recombination process, only the layers weights and threshold value of CN (convolution in first layer) and CN (convolution in third layer) are changed in CNN model. In this paper, we compared the performance of three metaheuristic algorithms, i.e. simulated annealing (SA), differential evolution (DE) and harmony search (HS), for optimizing CNN.. The strategies by looking for the best value of the fitness function on the last layer using metaheuristic algorithm, then the results will be used again to calculate the weights and biases in the previous layer. In case of testing the performance of the proposed methods, we use MNIST dataset. This dataset is images of digital handwritten digits, in which it contains N, N training data and N, N testing data. All of the images have been centered and standardized with the size of N x N pixels. Each pixel of the image is represented by N for black, N for white and in between is a different shade of gray _cite_ . This paper is organized as follow: Section N is an introduction, Section N explains about the used metaheuristic algorithms, Section N describe the convolution neural networks, Section N gives a description of the proposed methods, Section N present result of simulation, and Section N is the conclusion.