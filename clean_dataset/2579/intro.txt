transformations aim to transform an input image into the desired output image, and they exist in a number of applications about image processing, computer graphics, and computer vision. For example, generating high-quality images from corresponding degraded (e.g. simplified, corrupted or low-resolution) images, and transforming a color input image into its semantic or geometric representations. More examples include, but not limited to, image de-noising~ _cite_, image in-painting~ _cite_, image super-resolution~ _cite_, image colorization~ _cite_, image segmentation~ _cite_, \etc. In recent years, convolutional neural networks (CNNs) are trained in a supervised manner for various image-to-image transformation tasks~ _cite_ . They encode input image into hidden representation, which is then decoded to the output image. By penalizing the discrepancy between the output image and ground-truth image, optimal CNNs can be trained to discover the mapping from the input image to the transformed image of interest. These CNNs are developed with distinct motivations and differ in the loss function design. One of the most straightforward approaches is to pixel-wisely evaluate output images~ _cite_, \eg, least squares or least absolute losses to calculate the distance between the output and ground-truth images in the pixel space. Though pixel-wise evaluation can generate reasonable images, there are some unignorable defects associated with the outputs, such as image blur and image artifacts. Besides pixel-wise losses, the generative adversarial losses were largely utilized in training image-to-image transformation models. GANs (and cGANs) ~ _cite_ perform an adversarial training process alternating between identifying and faking, and generative adversarial losses are formulated to evaluate the discrepancy between the generated distribution and the real-world distribution. Experimental results show that generative adversarial losses are beneficial for generating more realistic images. Therefore, there are many GANs (or cGANs) based works to solve image-to-image transformation tasks, resulting in sharper and more realistic transformed images~ _cite_ . Meanwhile, some GANs variants~ _cite_ investigated cross-domain image translations and performed image translations in absence of paired examples. Although these unpaired works achieved reasonable results in some image-to-image translation tasks, they are inappropriate for some image-to-image problems. For example, in image in-painting tasks, it is difficult to define the domain and formulate the distribution of corrupted images. In addition, paired information within training data are beneficial for learning image transformations, but they cannot be utilized by unpaired translation methods. Moreover, perceptual losses emerged as a novel measurement for evaluating the discrepancy between high-level perceptual features of the output and ground-truth images~ _cite_ . Hidden layers of a well-trained image classification network (\eg, VGG-N~ _cite_) are usually employed to extract high-level features (\eg, content or texture) of both output images and ground-truth images. It is then expected to encourage the output image to have the similar high-level feature with that of the ground-truth image. Recently, perceptual losses were introduced in aforementioned GANs-based image-to-image transformation frameworks for suppressing artifacts~ _cite_ and improving perceptual quality~ _cite_ of the output images. Though integrating perceptual losses into GANs has produced impressive image-to-image transformation results, existing works are used to depend on external well-trained image classification network (\eg VGG-Net) out of GANs, but ignored the fact that GANs, especially the discriminative network, also has the capability and demand of perceiving the content of images and the difference between images. Moreover, since these external networks are trained on specific classification datasets (\eg, ImageNet), they mainly focus on features that contribute to the classification and may perform inferior in some image transformation tasks (\eg, transfer aerial images to maps) . Meanwhile, since specific hidden layers of pre-trained networks are employed, it is difficult to explore the difference between generated images and ground-truth images from more points of view. In this paper, we proposed the perceptual adversarial networks (PAN) for image-to-image transformation tasks. Inspired by GANs, PAN is composed of an image transformation network _inline_eq_ and a discriminative network _inline_eq_ . Both generative adversarial loss and perceptual adversarial loss are employed. Firstly, similar with GANs, the generative adversarial loss is utilized to measure the distribution of generated images, \ie, penalizing generated images to lie in the desired target domain, which usually contributes to producing more visually realistic images. Meanwhile, to comprehensively evaluate transformed images, we devised the perceptual adversarial loss to form dynamic measurements based on the hidden layers of the discriminative network _inline_eq_ . Specifically, given hidden layers of the network _inline_eq_, the network _inline_eq_ is trained to generate the output image that has the same high-level features with that of the corresponding ground-truth. If the difference between images measured on existing hidden layers of the discriminator is smaller, these hidden layers will be updated to discover the discrepancy between images from a new point of view. Different from the pixel-wise loss and conventional perceptual loss, our perceptual adversarial loss undergoes an adversarial training process, and aims to discover and decrease the discrepancy under constantly explored dynamic measurements. In summary, our paper makes the following contributions: The rest of the paper is organized as follows: after a brief summary of previous related works in section~ _ref_, we illustrate the proposed PAN together with its training losses in section~ _ref_ . Then we exhibit the experimental validation of the whole method in section~ _ref_ . Finally, we conclude this paper with some future directions in section~ _ref_ .