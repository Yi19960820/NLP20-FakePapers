Recent advances prove the feasibility of end-to-end robot control by replacing the classic chain of perception, planning and control with a neural network that directly maps sensor input to control output~ _cite_ . For cars, direct perception~ _cite_ and end-to-end control~ _cite_ were showcased in the TORCS car racing game using Reinforcement Learning (RL) . As RL relies on try and error strategies an end-to-end driving prototype still seems too dangerous for real-life learning and there is still a lot of progress to be done as the first studies use simulators with simplified graphics and physics, and the obtained driving results lack realism. We propose a method (fig. _ref_) benefiting from recent asynchronous learning~ _cite_ and building on our preliminary work~ _cite_ to train an end-to-end agent in World Rally Championship N (WRCN), a realistic car racing game with stochastic behavior (animations, light) . In addition to remain close to real driving conditions we rely only on image and speed to predict the full longitudinal and lateral control of the car. Together with our learning strategy, the method converges faster than previous ones and exhibits some generalization capacity despite the significantly more complex environment that exhibits N of training tracks with various visual appearances (snow, mountain, coast) and physics (road adherence) . Although it is fully trained in a simulation environment the algorithm was tested successfully on real driving videos, and handled scenarios unseen in the training (e.g. oncoming cars) . Section~ _ref_ describes the few related works in end-to-end driving. Section~ _ref_ details our methodology and learning strategies. Exhaustive evaluation is discussed in~ _ref_ and generalization on real videos is shown in~ _ref_ .