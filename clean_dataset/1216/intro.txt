Learning ND scene geometry and scene flow from videos is an important problem in computer vision. It has numerous applications in different areas, including autonomous driving _cite_, robot navigation _cite_ and video analysis _cite_ . However, collecting ground truths for these tasks could be difficult. Lots of efforts and progresses have been made recently in unsupervised learning of depth _cite_ and optical flow _cite_ using neural network based methods. Both approaches have their own advantages and limitations. The depth approach exploits the geometric structure of the scene and decomposes the problem into two orthogonal ones. It can also leverage more frames in time and/or stereo information to add more constraints into the solution _cite_ . However, it assumes the entire scene is static and thus has difficulty dealing with moving objects. On the other hand, the optical flow approach can handle moving objects in principle. But it has difficulty in the region of complex structures and occluded areas. Based on the above observation, we propose a framework to jointly learn depth and optical flow by mutually leveraging the advantages from each other. We first estimate the moving region by comparing the optical flow and the rigid flow induced by the camera motion. With the estimated moving region mask, depth and camera motion estimations can be improved by not considering the reconstruction loss in the moving region. On the other hand, the optical flow performance can also be improved by learning from the more accurate rigid flow in the static region (see Fig. _ref_ for an example) . Some works _cite_ have tried to learn the two tasks jointly with monocular videos. We however decide to use stereo pairs as input to our framework for the following reasons. On one hand, the performance of the unsupervised depth estimation is much better with stereo input compared to monocular-based methods _cite_ . Given the fact that stereo cameras have a wide range applications in real world scenarios (e.g. on smartphones), we feel that unsupervised learning of depth with stereo pairs itself is a topic worth studying. Additionally, with stereo inputs both depth and visual odometry can recover the absolute scale which could be more useful in certain tasks like self-localization. On the other hand, the higher quality stereo-based depth can be utilized to improve estimations of other quantities with our newly designed modules. It has been noticed that directly estimating camera motion from two images can be difficult, because it would implicitly require the network to also estimate the scene structure _cite_ . Based on the high quality stereo-based depth, we propose a rigid alignment module to facilitate the camera motion estimation. More concretely, we refine the camera motion by rigidly aligning two consecutive point clouds whose correspondences are established through optical flow. Additionally, with the high quality depth and better ego-motion estimations, the rigid flow derived from ego-motion is much more accurate than monocular-based ones. Therefore we propose a flow consistency module which lets the optical flow learn from the rigid flow in the static regions. In summary, the key contributions of this work are: N) a unified framework for unsupervised learning of optical flow, depth, visual odometry and motion segmentation with stereo videos. N) a rigid alignment module for refining the ego-motion estimation. N) a flow consistency module for learning optical flow from rigid flow. Our method improves the state-of-the-art performance of unsupervised learning of depth and optical flow by a large margin. On KITTI N, for example, our method reduces the optical flow error from previous state-of-the-art unsupervised method _cite_ by N \%, and reaches the performance of the supervised methods.