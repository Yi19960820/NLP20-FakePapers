target of image denoising is to recover the original clean image under the additive white Gaussian noise corruption. Many classical patch-based algorithms which exploit natural image statistics exist, such as sparse redundant representation model _cite_ and non-local statistics model _cite_ . These methods are often well-engineered and some are widely considered as the current state-of-the-art, e.g. BMND algorithm _cite_ . Different from the above methods, recently the machine learning approach based on deep neural network has draw considerable attention. This approach utilizes neural network to approximate a denoising function from a noisy patch to a clean patch. Some network models have been proposed for this task, including stacked sparse autoencoder _cite_, convolutional neural network _cite_ and plain neural network _cite_ . It has been shown that a multi-layer enormous plain neural network trained on a large training set is able to achieve comparable denoising performance to BMND method _cite_ . Usually, larger network and larger training set can lead to better results _cite_ . In addition, some investigations have also been made to improve specific texture denoising _cite_, or to provide robustness to different noise types _cite_ or noise levels _cite_ . Although different architectures are adopted, the models mentioned above are all based on traditional tanh activation function. Whereas, recent emerging work has shown that non-saturating rectifier function performs better _cite_ and trains faster _cite_ than saturating tanh on image recognition task. In this letter, we study the use of rectifier function in deep neural network for image denoising. We choose the plain feed-forward neural network architecture because it has been shown to achieve state-of-the-art performance. As pointed out in _cite_, rectifier is a one-sided function which means its response to the opposite of a strongly excitatory input is zero. By empirically analyzing the orthogonality of the dictionary learned by rectifier neurons, we find that they tend to learn the reversed atoms due to the one-sided property. To remove such redundancy, we propose a dual-pathway architecture by combining two rectifier neurons with reversed input and output weights in the same hidden layer. This strategy results in an equivalent antisymmetric activation function which enables one node to respond to patterns with opposite polarities simultaneously. Thus there is greater chance to update the weights from two paths, which benefits the training of large neural networks on large datasets. We have already successfully used this model for non-blind image deconvolution _cite_ . Then we evaluate the proposed activation function against some typical non-linear units (e.g., sigmoid, tanh, rectifier and parametric rectifier _cite_) in deep network framework for image denoising. We make the comparisons on different noise levels to observe the performance progress during the model training. The experimental results show that our method outperforms all competitors. The improvements are significant especially when the noise is relatively small. Compared to other activation functions, our model achieves superior performances much faster. We provide a Matlab toolbox with the trained models to test our approach.