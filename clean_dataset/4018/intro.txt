Over the past few years, deep neural network architectures---convolutional architectures in particular---have time and again beaten state-of-the-art on large-scale image recognition tasks _cite_ . As a result, the application of convolutional neural networks (CNN) has become increasingly popular in a variety of fields. In medicine, deep learning is used as a tool to assist professionals of various subfields in their diagnoses, such as histopathology _cite_, oncology _cite_, pulmonology _cite_, etc . In the subfield of dermatology, CNNs have been applied to the problem of skin lesion classification, based on dermoscopy images, where they set a new state-of-the-art benchmark, matching---or even surpassing---medical expert performance _cite_ . The challenge remains, however, to understand the reasoning behind the decisions made by these networks, since they are essentially black box function approximators. This poses a problem when a neural network outputs a diagnosis, different from the diagnosis made by the medical expert, as there is no human interpretable reasoning behind the neural networks' diagnosis. In such a case, visualizations of the network could serve as a reasoning tool to the expert. In this paper, we train a CNN for binary classification on a skin lesion dataset, and inspect the features learned by the network, by visualizing its feature maps. In the next section, we first give an overview of the different visualization strategies for inspecting CNNs. Section N describes our CNN architecture and training procedure. In Section N we present and discuss the learned CNN features and we conclude the paper in Section N.