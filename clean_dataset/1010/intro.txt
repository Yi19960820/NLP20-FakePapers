Recent years have seen impressive improvements in the performance of computer vision systems, brought about by larger datasets~ _cite_, improvements in computational capacity and GPU computing, and the widespread adoption of deep convolutional neural network models~ _cite_ . However, the gains in computer vision on standard benchmark problems such as ImageNet classification or object detection~ _cite_ do not necessarily translate directly into improved capability in perception, and enabling a robot to perform complex tasks in unstructured real-world environments using visual perception remains a challenging open problem. Part of this challenge for robot perception lies in the fact that the effectiveness of modern computer vision systems hinges in large part on the training data that is available. If the objects for a task happen to fall neatly into the labels of dataset, then using a trained object detector for perception makes sense. However, as shown in Figure~ _ref_, objects outside the label space may be labeled incorrectly or not at all, and objects that the robot should distinguish may be labeled as being the same category. If the robot's environment looks too different from the detector's training data, the performance may suffer. These difficulties leave us with several unenviable alternatives: we can attempt to collect a large enough dataset for each task that we want the robot to do, painstakingly labeling our object of interest in a large number of images, or we can attempt to use the pretrained vision system directly, suffering from possible domain shift and a lack of flexibility. In both cases, we also have limited recourse when it comes to correcting failures: while we can add additional labeled data if the robot fails on a particular object, a single new labeled data-point is unlikely to correct the mistake. An alternative view of robotic vision has emerged in recent years with advances in deep reinforcement learning~ _cite_, end-to-end learning from demonstrations~ _cite_, and self-supervised learning~ _cite_ . These methods bypass the standard computer vision representation of class labels and bounding boxes and directly train task-specific models that predict actions or task success from raw visual observations. While these methods can overcome the challenges associated with large-scale semantic labeling and dataset bias by training directly on the task that the robot aims to solve, their ability to generalize is critically dependent on the distribution of training data. For example, if a robot must learn to pour liquid from a bottle into a cup, it can achieve instance-level proficiency with a moderate number of samples~ _cite_, but it must train on a huge number of bottles and cups in order to generalize at the category level. Switching from the standard vision framework to end-to-end training therefore allows us to bypass the need for costly human-provided semantic labels, but sacrifices the generalization that we can get from large computer vision datasets. In this work, we seek to develop a robotic vision framework that operates on sets of objects rather than raw pixels, and leverages prior datasets to learn a generic object concept model. Our principal insight is that, if the robot will be using learning (e.g., reinforcement learning or learning from demonstration) to perform the final task that is set out before it, it does not require precise labels or segmentation. It simply needs to be able to consistently localize visual cues in the observed image that correlate with the objects that are necessary for it to perform the task. However, to learn generalizable policies, the visual cues should be semantic in nature such that a policy trained on one object instance can function on a similar object when desired. We therefore take a two-stage approach to robotic vision: in the first stage, we construct a object-centric attentional prior based on an region proposal network. This stage requires a large amount of data, but does not require any task-specific data, and can therefore use a standard existing computer vision dataset. The second stage narrowly focuses this general-purpose attention by using a very small number of example trajectories, which can be provided by a human or collected automatically during the reinforcement learning process. This teaches the system to attend to task relevant objects, while still benefiting from the generalizable representations present in the general-purpose attention. Furthermore, because the second stage is trained with only a handful of example trajectories, it makes it easy for the user to correct mistakes or control the class of objects that the system generalizes to, simply by providing additional demonstrations. For example, if the user needs a policy specific to a particular type of cup, they can simply provide demonstrations with other cups present in the scene, illustrating that they should be ignored. If the user prefers broader category-level generalization, for example to cause a robot generalize across all types of fruits, they might provide demonstrations that show interactions with fruits of different types. In all cases, the total number of provided trajectories remains very small (less than N) . The main contribution of our work is a perception framework that facilitates generalization over objects and environments while requiring minimal data or supervision per task. Our method incorporates general-purpose object-centric priors in the form of an object attention trained on a large, generic computer vision dataset, and combines it with an extremely efficient task-specific attention mechanism that can either be learned from a very small number of demonstrations, or even specified directly by a human operator. We show that this framework can be combined with reinforcement learning to enable a real-world robotic system to learn vision-based manipulation skills. Our experiments demonstrate that our approach achieves superior generalization to an end-to-end trained approach, through the incorporation of prior visual knowledge via the general-purpose attention. We illustrate how the user can control the degree of generalization by including or excluding other objects in the demonstrations. Our source code is available online in a stand-alone ROS package: _url_ . A video of results is available here: _url_ .