Since Generative Adversarial Network (GAN) was presented by Goodfellow in N _cite_, GAN is proved to be very efficient in image generation tasks in many research fields _cite_ _cite_ _cite_ _cite_ . Specially, many researchers use GAN for face images generation. Researchers show that the extension of GAN can not only generate face images with different poses _cite_ _cite_, but also with different accessories _cite_ . However, when the target images are too complex, GAN is still insufficient to achieve objects generation precisely. In the other hand, different facial expression face images are always required to provide samples and labels for expression recognition _cite_ . And multi-view face images can also be used for both pose and facial expression analysis _cite_ _cite_ . However, insufficient facial images with different pose and facial expression bring us difficulty when using supervised machine learning methods. Although human face image datasets are not uncommon, a lot of datasets only include one frontal neutral face image for each subject _cite_ . In order to solve this problem, we aim to synthesize different facial expression images through limited neutral images _cite_ . Besides, image with different pose can enrich the diversity of the face samples and could be applied to the pose recognition. It is not easy to consider both pose and facial expressions simultaneously during image translation. We need a desired generative model to comprehend the current image as well as capturing the underlying data distribution. This is often a very difficult task, since a collection of image samples on one attribute may lie on a very complex manifold. Images with multiple attributes would be further more sophisticated to be modeled. As described in Figure~ _ref_, the image on the left is the input image and all the right images are the target images with N kinds of attributes. In order to handle this task, the model should be able to generate the target images based on the input image with the appointed attributes. Many research show that the extension of GAN is able to disentangle image information encoded by vector z and by condition y making them independent _cite_ _cite_ . We observe that vector z is able to encode persons' facial feature such as facial pose and hair styles with traditional GAN technologies. However, the traditional GAN based image-to-image methods meet difficulty when we try to translate a facial image into an image with appointed expression and pose. Inspired by other image translation concepts, we proposed an end-to-end image translation framework that includes two generators and two discriminators for facial images generation with multiple attributes. These generators and discriminators make up a pipeline architecture. The detail of our work is shown in Figure.~ _ref_ . The overview of our model can be found in Figure~ _ref_ .In consideration of the difficulty of generating images with multiple attributes at the same time, we divided the synthesizing process into two stages. We assign the two different attributes: facial expression and head pose, at the two different stages respectively. We denote two image generators as the expression generator _inline_eq_ and the pose generator _inline_eq_ for the two stages separately. Each generator has its respective discriminator. We denote the expression discriminator _inline_eq_ and the pose discriminator _inline_eq_ . Similar to the idea of _cite_, the _inline_eq_ and the _inline_eq_ is a simple image-to-image generative adversarial network and so do _inline_eq_ and _inline_eq_ . And the _inline_eq_ means the expression images that are synthesized by the expression generator and so do _inline_eq_ . At the first stage. We use one generators for the pose image synthesizing. The output image _inline_eq_ and real pose images are evaluated by the pose discriminator to improve the image output quality. And then we take _inline_eq_ as the input of the expression generator _inline_eq_ for the next stage. The output images from the expression generator _inline_eq_ are the final outputs of our model. Through this pipeline operation, we can break a difficult problem into several easy steps. It's proved that the conditional vector can represent the extension information when synthesizing images using GANs _cite_ _cite_ _cite_ . Zhang et al. and Reed et al.'s work also show that GANs with information are able to synthesize images conditioned on text descriptions and on spatial constraints such as bounding boxes or key points _cite_ _cite_ . Following this idea, we added two conditional vectors on the coded layer of two GANs respectively. It's believed that the coded layer includes high dimensional information. By adding conditional vectors on the coded layer, we can control the facial attribute and generate the appointed synthesized images. These additional information from latent space _cite_ can provides the generator additional constrains and force it generates the appointed images rather than generating casually. In order to make the encoder network focus on how to select the useful features, we used an additional discriminator on the bottom neck of the encoder-decoder network for parallel classfication task to enhance the feature extraction ability of the encoder network. The detail of this conditional adversarial architecture is described in section _ref_ and _ref_ . Inspired by Chen et al.'s work _cite_ and Gulrajani et al.'s work _cite_, we include the cascade loss and gradient penalty to improve the image generation quality _ref_ and _ref_ . The full loss function of our framework is described in section _ref_ . Please note that there is another possible pipeline sequence of Figure~ _ref_ . We define the sequence described in Figure~ _ref_ as PE (pose generator at first), and we define EP (expression generator at first) as its inverse sequence structure. We evaluated these two structures in section _ref_ . Our main contributions are summarized as follows: Experimental results demonstrate the combination of these methods lead to convincing performance.