Depth plays a crucial role in many computer vision applications and active ND sensors are becoming very popular. Nonetheless, such sensors may have severe shortcomings. For instance, the Kinect N is not suited at all for outdoor environments flooded by sunlight. Moreover, such sensor allows only for close range depth measurements. On the other hand, a popular active depth sensor perfectly suited for outdoor environments is LIDAR (e.g., Velodyne) . However, sensors based on such technology are typically expensive and often cumbersome for some practical applications. Thus, inferring depth with passive sensors based on standard imaging technology would be highly desirable being cheap, lightweight and suited for indoor and outdoor environments. In this context, acquiring images from different viewpoints allows inferring depth exploiting geometry constraints. On the other hand, estimating depth from a single image is indeed an ill-posed problem. Nonetheless, this latter approach would overcome some major constraints such as the need for simultaneous acquisition in binocular stereo or handling dynamic objects in depth-from-motion approaches. Although a geometrically ambiguous problem, Convolutional Neural Networks (CNNs) achieved outstanding results in monocular depth estimation by casting it as a learning task in both supervised and unsupervised manner. In particular, the latter paradigm addresses the hunger for data typical of deep learning tasks by training networks to produce a depth representation minimizing the warping error between images acquired from multiple points of view rather than the error with respect to difficult to source ground-truth depth labels. In this field, the work of Godard et al. _cite_ represents state-of-the-art for unsupervised monocular depth estimation. Deploying stereo imagery for training, a CNN learns to infer disparity from a single reference image and warps the target image accordingly to minimize the appearance error between the warped and the reference image. This strategy yields state-of-the-art performance _cite_ . The CNN is trained to infer disparity from a single reference image and the target image is warped accordingly minimizing the appearance error between warped and reference image. This way, the depth representation learned by the network is affected by artifacts in specific image regions inherited from the stereo setup (e.g., the left border using the left image as the reference) and in occluded areas. The post-processing step proposed in _cite_ partially compensates for these artifacts. However, it requires a double forward of the input image and its horizontally flipped version thus obtaining two predictions with artifacts, respectively, on the left and right side of depth discontinuities. Such issues are softened in the final map at the cost of doubling processing time and memory footprint. In this paper, we propose to explicitly take into account these artifacts training our network on imagery acquired by a trinocular setup. By assuming the availability of three horizontally aligned images at training time, our network learns to process the frame in the middle and produce inverse depth (i.e., disparity) maps according to all the available viewpoints. By doing so, we can attenuate the aforementioned occlusion artifacts because they occur in different regions of the estimated outputs. However, since trinocular setups are generally uncommon and hence datasets seldom available, we will show how to rely on popular stereo datasets such as CityScapes _cite_ and KITTI _cite_ to enforce our trinocular training assumption. Experimental results clearly prove that, deploying stereo pairs with a smart strategy aimed at emulating a trinocular setup, our-view work (NNet) is able anyway to learn a three-view representation of the scene as shown intuitively in Figure _ref_ and how it leads to more robust monocular depth estimation compared to state-of-the-art methods trained on the same binocular stereo pair with a conventional paradigm. Figure _ref_ highlights the behavior of NNet: we can see how disparity maps (b) and (d), from the point of view of two frames respectively on the left and right side of the reference image, show mirrored artifacts in occluded regions. Combining the two opposite views enables to compensate for these issues and produces a more accurate map (c) centered on the reference frame. Please note that KITTI does not explicitly contain trinocular views as those shown in Figure _ref_ and that this behavior is learned by NNet trained only on standard binocular data. Indeed, images and depth maps in (b) and (d) are inferred by our network. Exhaustive experimental results on the KITTI N stereo dataset _cite_ and the Eigen split _cite_ of the KITTI dataset _cite_ clearly show that NNet, trained on standard binocular stereo pairs, improves state-of-the-art methods for unsupervised monocular depth estimation, regardless of the cues deployed for training.