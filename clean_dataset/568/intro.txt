Computational ghost imaging (CGI) _cite_ has garnered attention in recent years as a promising single-pixel imaging method. In CGI, we project several known random patterns onto the object to be imaged and then use a lens to collect the light transmitted an object or reflected by an object. The light intensities are measured by a bucket detector, such as a photodiode. An image of the object is then created by calculating the correlations between the known random patterns and the measured light intensities. CGI can image objects even in noisy environments. Originally, CGI only measured the light intensity of objects, but methods have also been devised for measuring its phase _cite_ . The acquisition time for CGI schemes is long as they require a large number of illuminating random patterns to objects. Recently, the situation has been improved by using high-speed random pattern illumination _cite_ . In addition, three-dimensional _cite_ and multi-spectrum CGI _cite_ have been developed. Since random patterns are used to create the object images, the reconstructed images are contaminated by noise. To improve the quality of CGI images, improved correlation calculation methods have been devised, such as differential _cite_ and normalized CGI _cite_ . Iterative optimization schemes based on the Gerchberg--Saxton algorithm _cite_ as well as compressed sensing _cite_ have also been applied to CGI. In this study, we propose an approach to improve CGI image quality by using deep learning _cite_ and confirm our technique's effectiveness through simulations. Deep neural networks (DNNs) can learn features for the noisy images reconstructed by CGI schemes automatically. We used a dataset of N, N images and their CGI reconstructions to train a network. After training, the network could predict lower-noise images from new noisy CGI images that were not included in the training set. In Section N, we describe our DNN-based CGI scheme. Section N presents the simulation results and demonstrates the effectiveness of the proposed method. Finally, Section N presents the conclusions of this study.