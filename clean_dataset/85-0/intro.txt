Due to the fast development of Internet, different types of media data grow rapidly, e.g., texts, images and videos. These different types of data may describe the same events or topics. For example, the photos in Flickr are allowed users to give interactive comments. Hence, developing a retrieval model for multi-modal data is a desired requirement. Cross-modal retrieval, which takes one type of data as the query and return the relevant data of another type, is receiving increasing attention since it is a natural searching way for multi-modal data. The solution methods can be roughly divided into two categories~ _cite_: real-valued representation learning and binary representation learning. Since the low storage cost and fast retrieval speed of the binary representation, we only focus on cross-modal binary representation learning (i.e., Hashing) in this paper. To date, various cross-modal hashing algorithms~ _cite_ have been proposed for embedding correlations among different modalities of data. In the cross-modal hashing procedure, feature extraction is considered as the first step for representing all modalities of data, and then one project these multi-modal features into a common Hamming space for future search. Many methods~ _cite_ use shallow architecture for feature extraction. For example, collective matrix factorization hashing (CMFH) ~ _cite_ and semantic correlation maximization (SCM) ~ _cite_ use the hand-crafted features. Recently, deep learning has also been adopted for cross-modal hashing due to its powerful ability of learning good representations of data. The representative works of deep-network-based cross-modal hashing includes deep cross-modal hashing (DCMH) ~ _cite_, deep visual-semantic hashing (DVSH) ~ _cite_, pairwise relationship guided deep hashing (PRDH) ~ _cite_ and so on. In parallel, the computational model of ``attention" has drawn much interest due to its impressive result in various applications, e.g., image caption~ _cite_ . It is also desired for cross-modal retrieval problem. For example, as shown in Figure~ _ref_, given a query girl sits on donkey, if we can locate the more informative objects in image (e.g., the black regions), the more accuracy can be obtained. To the best of our knowledge, the attention mechanism has not been well explored for cross-modal hashing. In this paper, we propose an adversarial hashing network with attention mechanism for cross-modal hashing. Ideally, good attention masks should locate discriminative regions, which also mean the unattended regions of data are uninformative and hard to preserve similarities. Hence, in our proposed network, adaptive attention masks are generated for the multi-modal data, then the learned masks divide the data into attended samples (only keep foregrounds of the data) and unattended samples (only keep backgrounds of the data) . Hinging on such attention masks, a good discriminative hashing should preserve the similarities for both the foreground samples (which can be viewed as easy examples) and background samples (hard examples) for enhancing the robustness and performance of the learned hash functions. And the good generator should generate attention masks that make discriminator cannot preserve the similarities of the background samples, for unattended regions of data should not be discriminative. Based on this, we present a new adversarial model called HashGAN, which is illustrated in Figure~ _ref_ and consists of three major components: (N) feature learning module which uses CNN or MLP to extract high level semantic representations for the multi-modal data, (N) generative attention module which generates the adaptive attention masks and divides the feature representations into the attended and the unattended feature representations, and (N) discriminative hashing module which focus on learning the binary codes for the multi-modal data. HashGAN trains two adversarial networks alternatively: the discriminator is learned to preserve the similarities for both the easy foreground feature representations and the hard background feature representations, while the generator learns to produce masks that make the discriminator fails to keep similarities of the background feature representation. The adversarial retrieval loss and cross-modal retrieval loss are proposed to obtain good attention masks and powerful hash functions. The main contributions of our work are three-fold. First, we propose an attention-aware method for cross-modal hashing problem, which is able to detect the informative regions of multi-modal data. Second, we propose an HashGAN for learning effective attention masks and compact binary codes simultaneously. Third, we quantitatively evaluate the usefulness of attention in cross-modal hashing and our method yields better performances by comparing with several state-of-the-art methods.