Deep Neural Networks (DNNs) have shown remarkable success in many computer vision tasks such as image classification _cite_, object detection _cite_ and semantic segmentation _cite_ . Despite the high performance in large DNNs powered by cutting-edge parallel computing hardware, most of state-of-the-art network architectures are not suitable for resource restricted usage such as usages on always-on devices, battery-powered low-end devices, due to the limitations on computational capacity, memory and power. To address this problem, low-rank decomposition methods _cite_ have been proposed to minimize the channel-wise and spatial redundancy by decomposing the original network into a compact one with low-rank layers. Different from precedent works, this paper proposes a novel approach to design low-rank networks. Low-rank networks can be trained directly from scratch. However, it is difficult to obtain satisfactory results for several reasons. (N) ~ Low capacity: Compared with the original full rank network, the capacity of a low-rank network is limited, which causes difficulties in optimizing its performances. (N) ~ Deep structure: Low-rank decomposition typically doubles the number of layers in a network. The additional layers make numerical optimization much more challenging because of exploding and/or vanishing gradients. (N) ~ Rank selection: The rank of decomposed network is often chosen as a hyperparameter based on pre-trained networks; which may not be the optimal rank for the network trained from scratch. Alternatively, several previous works _cite_ attempted to decompose pre-trained models in order to get initial low-rank networks. However, the heuristically imposed low-rank could incur huge accuracy loss and network retraining is required to recover the performance of the original network as much as possible. Some attempts were made to use sparsity regularization _cite_ to constrain the network into a low-rank space. Though sparsity regularization reduces the error incurred by decomposition to some extent, performance still degrades rapidly when compression rate increases. In this paper, we propose a new method, namely Trained Rank Pruning (TRP), for training low-rank networks. We embed the low-rank decomposition into the training process by gradually pushing the weight distribution of a well functioning network into a low-rank form, where all parameters of the original network are kept and optimized to maintain its capacity. We also propose a stochastic sub-gradient descent optimized nuclear regularization that further constrains the weights in a low-rank space to boost the TRP. The proposed solution is illustrated in Fig.~ _ref_ . Overall, our contributions are summarized below.