Estimating depth information from single monocular images depicting general scenes is an important problem in computer vision. Many challenging computer vision problems have proven to benefit from the incorporation of depth information, to name a few, semantic labellings _cite_, pose estimations _cite_ . Although the highly developed depth sensors such as Microsoft Kinect nowadays have made the acquisition of RGBD images affordable, most of the vision datasets commonly evaluated among the vision community are still RGB images. Moreover, outdoor applications still rely on LiDAR or other laser sensors due to the fact that strong sunlight can cause infrared interference and make depth information extremely noisy. This has led to wide research interest on the topic of estimating depths from single RGB images. Unfortunately, it is a notoriously ill-posed problem, as one captured image scene may correspond to numerous real world scenarios _cite_ . Whereas for humans, inferring the underlying ND structure from a single image is effortless, it remains a challenging task for automated computer vision systems to do so since no reliable cues can be exploited, such as temporal information in videos, stereo correspondences, etc. Previous work mainly focuses on enforcing geometric assumptions, \eg, box models, to infer the spatial layout of a room _cite_ or outdoor scenes _cite_ . These models come with innate restrictions, which are limitations to model only particular scene structures and therefore are not applicable for general scene depth estimations. More recently, non-parametric methods _cite_ are explored, which consists of candidate images retrieval, scene alignment and then depth inference using optimizations with smoothness constraints. This is based on the assumption that scenes with semantically similar appearances should have similar depth distributions when densely aligned. However, this method is prone to propagate errors through the different decoupled stages and relies heavily on building a reasonable sized image database to perform the candidate retrieval. In recent years, efforts have been made towards incorporating additional sources of information, \eg, user annotations _cite_, semantic labellings _cite_ . In the recent work of _cite_, Ladicky \etal have shown that jointly performing depth estimation and semantic labelling can benefit each other. Nevertheless, all these methods use hand-crafted features. In contrast to previous efforts, here we propose to formulate the depth estimation as a deep continuous Conditional Random Fields (\crf) learning problem, without relying on any geometric priors or any extra information. \crf _cite_ is a popular graphical model for structured output predictions. While extensively studied in classification (discrete) domains, \crf has been less explored for regression (continuous) problems. One of the pioneer work on continuous \crf can be attributed to _cite_, in which it was proposed for global ranking in document retrieval. Under certain constraints, they can directly solve the maximum likelihood optimization as the partition function can be analytically calculated. Since then, continuous \crf has been successfully applied for solving various structured regression problems, \eg, remote sensing _cite_, image denoising _cite_ . Motivated by these successes, we here propose to use it for depth estimation, given the continuous nature of the depth values, and learn the potential functions in a deep convolutional neural network (\cnn) . Recent years have witnessed the prosperity of the deep \cnn _cite_ since the breakthrough work of Krizhevsky \etal _cite_ . CNN features have been setting new records for a wide variety of vision applications _cite_ . Despite all the successes in classification problems, deep \cnn has been less explored for structured learning problems, \ie, joint training of a deep \cnn and a graphical model, which is a relatively new and not well addressed problem. To our knowledge, no such model has been successfully used for depth estimations. Here, we bridge this gap by jointly exploring \cnn and continuous \crf, denoting this new method as a deep convolutional neural field (\dcnf) . Fully convolutional networks have recently been studied for dense prediction problems, \eg, semantic labelling _cite_ . Models based on fully convolutional networks have the advantage of highly efficient training and prediction. We here exploit this advance to speedup the training and prediction of our \dcnf model. However, the feature maps produced by the fully convolutional models are typically much smaller than the input image size. This can cause problems for both training and prediction. During training, one needs to downsample the ground-truth maps, which may lead to information loss since small objects might disappear. In prediction, the upsampling operation is likely to bring in degraded performance at the object boundaries. We therefore propose a novel superpixel pooling method to address this issue. It jointly exploits the strengths of highly efficient fully convolutional networks and the benefits of superpixels at preserving object boundaries. To sum up, we highlight the main contributions of this work as follows. Preliminary results of our work appeared in Liu \etal _cite_ . Our method exploits the recent advances of deep nets in image classification _cite_, object detection _cite_ and semantic segmentation _cite_, for single view image depth estimations. In the following, we give a brief introduction to the most closely related work. {\bf Depth estimation} Traditional methods _cite_ typically formulate the depth estimation as a Markov Random Field (\mrf) learning problem. As exact \mrf learning and inference are intractable in general, most of these approaches employ approximation methods, \eg, multi-conditional learning (MCL) or particle belief propagation (PBP) . Predicting the depths of a new image is inefficient, taking around N-Ns in _cite_ and even longer (Ns) in _cite_ . To make things worse, these methods suffer from lacking of flexibility in that _cite_ rely on horizontal alignment of images and _cite_ requires the semantic labellings of the training data available beforehand. More recently, Liu \etal _cite_ propose a discrete-continuous \crf model to take into consideration the relations between adjacent superpixels, \eg, occlusions. They also need to use approximation methods for learning and maximum a posteriori (MAP) inference. Besides, their method relies on image retrieval to obtain a reasonable initialization. In contrast, here we present a deep continuous \crf model in which we can directly solve the log-likelihood optimization without any approximations, since the partition function can be analytically calculated. Predicting the depth of a new image is highly efficient since a closed form solution exists. Moreover, we do not inject any geometric priors nor any extra information. On the other hand, previous methods _cite_ all use hand-crafted features in their work, \eg, texton, GIST, SIFT, PHOG, object bank, etc. In contrast, we learn deep \cnn for constructing unary and pairwise potentials of the \crf. Recently, Eigen \etal _cite_ proposed a multi-scale \cnn approach for depth estimation, which bears similarity to our work here. However, our method differs critically from theirs: they use the \cnn as a black-box by directly regressing the depth map from an input image through convolutions; in contrast we use a \crf to explicitly model the relations of neighboring superpixels, and learn the potentials (both unary and binary) in a unified \cnn framework. Recent work of _cite_ and _cite_ is relevant to ours in that they also perform depth estimation from a single image. The method of Su \etal _cite_ involves a continuous depth optimization step like ours, which also contains a unary regression term and a pairwise local smoothness term. However, these two works focus on ND reconstruction of known segmented objects while our method targets at depth estimation of general scene images. Furthermore, the method of _cite_ relies on a pre-constructed ND shape database of input object categories, and the work of _cite_ relies on class-specific object keypoints and object segmentations. In contrast, we do not inject these priors. {\bf Combining \cnn and \crf} In _cite_, Farabet \etal propose a multi-scale \cnn framework for scene labelling, which uses \crf as a post-processing step for local refinement. In the most recent work of _cite_, Tompson \etal present a hybrid architecture for jointly training a deep \cnn and an \mrf for human pose estimation. They first train a unary term and a spatial model separately, then jointly learn them as a fine tuning step. During fine tuning of the whole model, they simply remove the partition function in the likelihood to have a loose approximation. In contrast, our model performs continuous variable prediction. We can directly solve the log-likelihood optimization without using approximations as the partition function is integrable and can be analytically calculated. Moreover, during prediction, we have closed-form solutions to the MAP inference problem. Although no convolutional layers are involved, the work of _cite_ shares similarity with ours in that both continuous \crf's use neural networks to model the potentials. Note that the model in _cite_ is not deep and only one hidden layer is used. It is unclear how the method of _cite_ performs on the challenging depth estimation problem that we consider here. {\bf Fully convolutional networks} Fully convolutional networks have recently been actively studied for dense prediction problems, \eg, semantic segmentation _cite_, image restoration _cite_, image super-resolution _cite_, depth estimations _cite_ . To deal with the downsampled output issue, interpolations are generally applied _cite_ . In _cite_, Sermanet \etal propose an input shifting and output interlacing trick to produce dense predictions from coarse outputs without interpolations. Later on, Long \etal _cite_ present a deconvolution approach to put the upsampling into the training regime instead of applying it as a post-processing step. The \cnn model presented in Eigen \etal _cite_ for depth estimation also suffers from this upsampling problem---the predicted depth maps of _cite_ is N/N-resolution of the original input image with some border areas lost. They simply use bilinear interpolations to upsample the predictions to the input image size. Unlike these existing methods, we propose a novel superpixel pooling method to address this issue. It jointly exploits the strengths of highly efficient fully convolutional networks and the benefits of superpixels at preserving object boundaries.