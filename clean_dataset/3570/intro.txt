Fully Convolution Network (FCN) has evolved to be the network architecture in semantic segmentation after it is successfully adopted in _cite_ . As it is difficult (usually fails) to train a strong FCN from scratch, Long simply adapts the architecture of pre-trained classification network (e.g. VGG-N _cite_ trained on ImageNet _cite_ etc.) . Even though FCN significantly advances the performance of semantic segmentation, here we discuss its possible drawbacks so that we can build stronger segmentation network architectures. First and foremost, the pre-trained CNN is trained with low-resolution images (e.g. N _inline_eq_ N pixels), whereas input segmentation images are usually in high resolution (e.g. _inline_eq_ pixels) . The simple adaptation techniques adopted in FCN cannot effectively address this domain gap, which leads to less optimized segmentation performance of FCN. To elaborate, the feature maps that are used for classification in FCN have limited contextual fields, so predictions are inconsistent for local ambiguous regions. To close such a domain gap, one option is to adapt (downsample) the inputs. Unfortunately, this practice is found to deteriorate the performance of small-size objects. A more plausible and popular option is to modify network architectures (of FCN) so that they are fit for high-resolution inputs, and we will review and discuss these works in Section _ref_ . Additionally, Long _cite_ adopts two extra skip connections in FCN-Ns to aggregate three scale contextual predictions. Nonetheless, such limited scale fusion in segmentation networks is not expected to handle the significant variance of object scales across different images. Next, we present our Improved FCN (IFCN) to address those issues. We take the adaptation of VGG-N _cite_ as an example to demonstrate the architecture of IFCN. The key modification to FCN is that IFCN plugs in a context network between the pre-trained CNN and upsample layers. Thus, IFCN entails new parameters, which is crucial to fill the domain gap during the network fine-tuning on high-resolution segmentation images. The context network is a densely branched convolution network. Specifically, it is stacked with multiple convolution blocks, and shortcut branches are further added from each intermediate feature map. Functionally speaking, context network is able to significantly expand the receptive fields of feature maps, which is essential to contextualize local semantic predictions. In addition, shortcut branches are also important to ease the optimization difficulty of context network, as they provide shortcut paths for the propagation of error signals. Meanwhile, those shortcut branches enable IFCN to make predictions based on rich scale contexts. In consequence, IFCN is able to converge to a significantly better local optima than FCN on standard semantic segmentation images. Besides, IFCN discards the last two layers (,) which are specific to image classification _cite_ . By doing this, feature maps are more compact and the network size is reduced as well. We will elaborate the architecture design of context networks in Section _ref_ . We further add more skip connections from early feature maps of pre-trained CNN so that richer scale contexts are incorporated in semantic predictions. Similar to the notation of FCN-xs, we symbolize our segmentation network as IFCN-xs, where x denotes the sliding stride of the finest-resolution feature maps involved in prediction. Let's take FCN-Ns and IFCN-Ns for illustrative comparison. FCN-Ns has two skip connections from feature maps and . IFCN-Ns, on the other hand, adds skip connections for all the following feature maps starting with . In this perspective, IFCN-xs leverages significantly richer scale context for predictions than FCN-xs. As each feature map carries some distinct scale information, they are complementary to collectively handle the scale variance of objects. It is important to note that these skip connections incorporate very limited computation overhead. We will present the details of IFCN-xs in Section _ref_ . With these architecture modifications, IFCN marries context aggregation as well as rich-scale contextual prediction in an elegant framework. IFCN delivers a strong segmentation network architecture and it achieves new state-of-the-arts on standard semantic segmentation datasets including ADENK (ImageNet Scene Parsing) _cite_, Pascal Context _cite_, VOC N _cite_ and SUN-RGBD _cite_ . In Section _ref_, we also present detailed study to the architecture design of IFCN. Overall, IFCN doesn't involve any new computation layers comparing to FCN. Thus, it can be easily implemented with current deep learning libraries. We will release the code and model upon the acceptance of this paper.