Semantic segmentation can be described as assigning an object label to each pixel in an image. This assignment can be binary, {\em i.e.} foreground vs. background or multi-class, {\em i.e.} "sky", "building", "horse" etc. Semantic segmentation is a challenging computer vision task that combines image segmentation and object detection, and can be seen as a crucial step for image understanding. Recently, ConvNets have been producing state-of-the-art results in many computer vision tasks including semantic segmentation (_cite_) . However, successfully training ConvNets which can have large numbers of free parameters with existing methods requires large amounts of labeled data. It has been claimed (_cite_) that improvements have a correlation not only with the methodologies used but also the amount of training data available to the algorithm. Unfortunately, building large labeled datasets can be costly and time consuming. At a time when access to unlabeled data is very easy and cheap, devising new algorithms and methods to profit from these sources is imperative. The goal of these algorithms can be to improve current state of the art results by incorporating more unlabeled data in a "transductive learning" setting or achieve results comparable to the state of the art with significantly smaller amounts of labeled training data in a "semi-supervised learning" paradigm which is the goal of this work. Numerous methods and algorithms have been proposed for semi-supervised learning both in general and also for specific applications (_cite_) . Self-training and co-training can be mentioned as classic methods in semi-supervised learning literature (_cite_) . In this paper, we consider the problem of semantic segmentation when only sparse labels are available to the learning algorithm. This can be the case where a user labels a dataset only for some pixels per image which is a more realistic burden on the user than labeling all pixels in the training images. Motivated by this problem, we demonstrate how natural image characteristics, {\em i.e.} piecewise smoothness, can be used to develop novel unsupervised loss functions for learning ConvNets and demonstrate the advantages of our approach over the purely supervised setting.