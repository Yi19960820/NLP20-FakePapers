Images of faces provide relevant information for emotion perception. As humans we can infer an accurate first impression of somebody's emotions by observing their face. Multiple applications benefit from automated facial emotion recognition, such as human computer interaction (HCI) ~ _cite_, student engagement estimation~ _cite_, emotionally aware devices~ _cite_, or the improvement of expression production in autism disorder patients~ _cite_ . The state-of-the-art in automated facial expression analysis shows excellent performance in the controlled scenario, where images are acquired in studio environments. Nevertheless, the categorization of emotions, is still an unsolved problem. Besides the strong intra-class variability, facial expression algorithms must also deal with strong local changes in the illumination conditions, out of plane rotations, large variations in pose and point of view, and low resolution imaging. Recent advances in computer vision and particularly in object recognition suggest that new methods based on Deep Learning can improve the facial expression recognition task. Convolutional Neural Networks (CNNs) have represented a relevant breakthrough, especially since the last improvements on the ImageNet Challenge _cite_ . However, the amount of available data for this task is small, especially in all the possible configurations of pose, illumination and resolution. This supposes an inconvenience to exploit the training capacity of these networks, which need large amounts of training data. In this context the introduction of multi-task learning is particularly relevant, as it proved to successfully boost the performance of an individual task with the inclusion of other correlated tasks in the training process~ _cite_ . Thus, tasks with small amounts of data available can benefit from being trained simultaneously with other tasks, sharing a common feature representation and transferring knowledge between different domains. One of the main difficulties with multi-task approaches using different databases is the fact that not all the samples are labeled for all the tasks. In order to deal with this, classical approaches define different loss functions for each task and train alternatively for the different domains. This penalizes the tasks where sample labels are not available. One approach to solve this problem is a selective joint loss, but it only predicts probabilities for the label set to which the image belongs. Hence, in order to generate predictions for all the tasks, in this paper we propose a multi-label database-wise joint loss to overcome this problem. This paper makes the following contributions: (i) we formalize a novel dataset-wise selective sigmoid cross-entropy loss function to simultaneously train a multi-task, multi-label and multi-domain model. (ii) We validate that this new proposal outperforms single task CNNs and the classical multi-task approach using different databases. (iii) We show that the results of the joint learning of an unlabeled task are coherently correlated to the labeled task in the case of the emotion recognition problem.