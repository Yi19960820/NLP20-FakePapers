The work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification, and for human action recognition in particular? Thus far, the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths. We introduce a new temporal layer that models variable temporal convolution kernel depths. We embed this new temporal layer in our proposed ND CNN. We extend the DenseNet architecture-which normally is ND-with ND filters and pooling kernels. We name our proposed video convolutional network `Temporal ND ConvNet'~ (TND) and its new temporal layer `Temporal Transition Layer'~ (TTL) . Our experiments show that TND outperforms the current state-of-the-art methods on the HMDBN, UCFN and Kinetics datasets. The other issue in training ND ConvNets is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in ND ConvNets is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained ND CNN to a randomly initialized ND CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for ND CNNs. Thus, by finetuning this network, we beat the performance of generic and recent methods in ND CNNs, which were trained on large video datasets, e.g. Sports-NM, and finetuned on the target datasets, e.g. HMDBN/UCFN. The TND codes will be released soon .