Deep Neural Networks (DNNs) have been successfully applied in many artificial intelligence tasks, providing state-of-the-art performance and a remarkably small generalization error. On the other hand, DNNs often have far more trainable model parameters than the number of samples they are trained on and were shown to have a large enough capacity to memorize the training data . Thus, most statistical learning theory that explains generalization via hypothesis capacity struggle to explain the generalization ability of large artificial neural networks. In this work, we focus on a different approach to study generalization of DNNs, i.e., the connection between the robustness of a deep learning algorithm and its ability to generalize. have shown that if an algorithm is robust (\ie, its empirical loss does not change dramatically for perturbed samples), its generalization performance can also be guaranteed. However, in the context of DNNs, practitioners observe contradicting evidence between these two attributes. On the one hand, DNNs generalize well, and on the other, they are fragile to adversarial perturbation on the inputs~ . Nevertheless, adversarial training (methods based on generating adversarial examples to training examples and using them during training) have been shown to improve the generalization of deep neural network models~, indicating an implicit connection between the robustness of a neural net and its ability to generalize. Moreover, it was observed that dropout, coupled with adversarial training, is best at hindering memorization without reducing the modelâ€™s ability to learn . In order to solve this contradiction, we revisit the robustness argument in~ and present, to characterize the generalization performance of deep learning algorithms. Our proposed approach is not intended to give tight performance guarantees for general deep learning algorithms, but rather to pave a way for addressing the question: how can deep learning perform so well while being fragile to adversarial examples? Answering this question is difficult, yet we present evidence in both theory and simulation strongly suggesting that is crucial to the generalization performance of deep learning algorithms. Ensemble robustness concerns the fact that a randomized algorithm (\eg, Stochastic Gradient Descent (SGD), Dropout, Bayes-by-backprop, etc.) produces a distribution of hypotheses instead of a deterministic one. Therefore, ensemble robustness takes into consideration robustness of the of the hypotheses: even though some hypotheses may be sensitive to perturbation on inputs, an algorithm can still generalize well as long as most of the hypotheses sampled from the distribution are robust on average. () took a different approach and claimed that deep neural networks can generalize well despite nonrobustness. However, our definition of ensemble robustness together with our empirical findings suggest that deep learning methods are typically robust although being fragile to adversarial examples. Through ensemble robustness, we prove that the following holds with a high probability: randomized learning algorithms can generalize well as long as its output hypothesis has bounded sensitiveness to perturbation in average (see Theorem _ref_) . Specified for deep learning algorithms, we reveal that if hypotheses from different runs of a deep learning method perform consistently well in terms of robustness, the performance of such deep learning method can be confidently expected. Moreover, each hypothesis may be sensitive to some adversarial examples as long as it is robust on average. Although ensemble robustness may be difficult to compute analytically, we demonstrate an empirical estimate of ensemble robustness and investigate the role of ensemble robustness via extensive simulations. The results provide supporting evidence for our claim: ensemble robustness consistently explains the generalization performance of deep neural networks. Furthermore, ensemble robustness is measured solely on training data, potentially allowing one to use the testing examples for training and selecting the best model based on its ensemble robustness.