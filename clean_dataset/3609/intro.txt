These years have witnessed the success of convolutional neural networks (CNNs) in a wide range computer vision tasks, such as image classification, object detection and segmentation. The success of deep learning largely owes to the fast development of computing resources. Most of the deep learning models are trained on high-ended GPUs or CPU clusters. On the other hand, deeper networks impose heavy storage footprint due to the enormous amount of network parameters. For example, the N-layers VGG involves N MBytes of model parameters. Both the high computational and storage cost become impediments to popularize the deep neural networks to scenarios where either memory or computational resources are limited. The great interest to deploy deep learning systems on low-ended devices motives the research in compressing deep models to have smaller computation cost and memory footprints. Considerable efforts have been mounted to reduce the model size and speed up the inference of deep models. Denil et al. pointed out that network weights have a significant redundancy, and proposed to reduce the number of parameters by exploiting the linear structure of network _cite_, which motivated a series of low-rank matrix/tensor factorization based compression algorithms, e.g. _cite_ . Alternatively, multiple studies were devoted to discritizing network weights using vector quantization methods _cite_, which often outperformed the matrix/tensor factorization based methods _cite_ . Han et al. presented the deep compression method that integrates multiple compression methods to achieve a large reduction in model size _cite_ . Another line of work for model compression is to restrict network weights to low precision with a few bits. The advantage of this restriction is that an expensive floating-point multiplication operation can now be replaced by a sequence of cheaper and faster binary bit shift operations. This not only reduces the memory footprints but also accelerates the computation of the network. These approaches work well when pretrained weights are quantized into N-N bits~ _cite_ . When coming to extremely low bit networks, i.e. only one or two bits are used to represent weights~ _cite_, they only work well on simple datasets (e.g. MNIST and CIFARN), and usually incur a large loss on challenging datasets like ImageNet. In this work, we focus on compressing and accelerating deep neural networks with extremely low bits weights, and present a unified strategy for learning such low bits networks. We overcome the limitation of the existing approaches by formulating it as a discretely constrained non-convex optimization problem, which is usually referred to as mixed integer programs (MIP) . Given the NP hard nature of MIPs, we proposed a framework for learning extremely low bit neural network using the technique of alternating direction method of multipliers (ADMM) _cite_ . The main idea behind our method is to decouple the continuous variables from the discrete constraints using an auxiliary variable in the discrete space. This leads to a convenient form of the objective which is amenable to existing nonconvex optimization algorithms. Unlike previous low bit quantization methods _cite_ that incorporate an ad-hoc modification of the gradients for continuous weights, we simultaneously optimize in both continuous and discrete spaces, and connect the two solutions using an augmented Lagrangian. This is consistent with the previous observation from _cite_ that, by decoupling discrete constraints in MIP, one can use the information from the dual problem through ADMM to obtain a better upper bound. As a result of this reformulation, we can divide the problem of low bits quantized neural network into multiple subproblems which are significantly easier to solve. The main contributions of this paper are summarized as follows: