Deep neural networks (DNNs) learn hierarchical layers of representation from sensory input in order to perform pattern recognition _cite_ . Recently, these deep architectures have demonstrated impressive, state-of-the-art, and sometimes human-competitive results on many pattern recognition tasks, especially vision classification problems _cite_ . Given the near-human ability of DNNs to classify visual objects, questions arise as to what differences remain between computer and human vision. A recent study revealed a major difference between DNN and human vision~ _cite_ . Changing an image, originally correctly classified (e.g. as a lion), in a way imperceptible to human eyes, can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library) . In this paper, we show another way that DNN and human vision differ: It is easy to produce images that are completely unrecognizable to humans (Fig.~ _ref_), but that state-of-the-art DNNs believe to be recognizable objects with over N \% confidence (e.g. labeling with certainty that TV static is a motorcycle) . Specifically, we use evolutionary algorithms or gradient ascent to generate images that are given high prediction scores by convolutional neural networks (convnets) _cite_ . These DNN models have been shown to perform well on both the ImageNet _cite_ and MNIST _cite_ datasets. We also find that, for MNIST DNNs, it is not easy to prevent the DNNs from being fooled by retraining them with fooling images labeled as such. While retrained DNNs learn to classify the negative examples as fooling images, a new batch of fooling images can be produced that fool these new networks, even after many retraining iterations. Our findings shed light on current differences between human vision and DNN-based computer vision. They also raise questions about how DNNs perform in general across different types of images than the ones they have been trained and traditionally tested on.