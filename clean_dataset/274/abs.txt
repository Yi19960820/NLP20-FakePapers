We combine three methods which significantly improve the OCR accuracy of OCR models trained on early printed books: (N) The pretraining method utilizes the information stored in already existing models trained on a variety of typesets (mixed models) instead of starting the training from scratch. (N) Performing cross fold training on a single set of ground truth data (line images and their transcriptions) with a single OCR engine (OCRopus) produces a committee whose members then vote for the best outcome by also taking the top-N alternatives and their intrinsic confidence values into account. (N) Following the principle of maximal disagreement we select additional training lines which the voters disagree most on, expecting them to offer the highest information gain for a subsequent training (active learning) . Evaluations on six early printed books yielded the following results: On average the combination of pretraining and voting improved the character accuracy by N \% when training five folds starting from the same mixed model. This number rose to N \% when using different models for pretraining, underlining the importance of diverse voters. Incorporating active learning improved the obtained results by another N \% on average (evaluated on three of the six books) . Overall, the proposed methods lead to an average error rate of N \% when training on only N lines. Using a substantial ground truth pool of N, N lines brought the error rate down even further to less than N \% on average.