Visual object tracking is one of the fundamental and practical problems among the fields of computer vision research, and it has seen applications in automated surveillance, image stabilization, robotics and more. Given the initial bounding box annotation of an object, visual tracking algorithms aim to track the specified object throughout the subsequent part of the video without losing the object under various circumstances such as illumination change, blur, deformation, fast motion, and occlusion. Recently, with the increasing use of deep learning and convolutional neural networks (CNN) _cite_ in computer vision applications for their rich representation power and generalization capabilities _cite_, there have been numerous studies on utilizing the rich and general feature representation of the CNNs for visual tracking task _cite_ . Most algorithms incorporate the deep convolutional features used in object recognition systems _cite_ . On top of these feature representations, additional classifiers or correlation filters are trained for on-line adaptation to the target object _cite_ . While these methods were successful in obtaining high performance metrics in well-known benchmarks and datasets _cite_ using deep representations, the majority of these algorithms were not designed as an integrated structure, where two different systems (i.e. deep feature network system and target classifier systems) are built and trained separately, not being closely associated. This causes several problems when the framework is naively applied to the problem of visual tracking, where the classifier system is in constant need of being updated in order to adapt to the appearance changes of the target object, while the number of positive samples are highly limited. Since an update operation requires solving complex optimization problems for a given objective function using methods such as stochastic gradient descent (SGD) _cite_, Lagrange multipliers _cite_, and ridge regression _cite_, most tracking algorithms with deep representations run at low speeds under _inline_eq_ fps, thus making real-time applications unrealizable. Moreover, since the updates are often achieved by utilizing a handful of target appearance templates obtained in the course of tracking, while this strategy is inevitable, classifiers are prone to overfitting and losing generalization capabilities due to insufficient positive training samples. To deal with this prevalent overfitting problem, most algorithms incorporate a hand-crafted regularization term with a training hyper-parameter tuning scheme to achieve better results. Our approach tackles the aforementioned problems by building a visual tracking system incorporating a Siamese {matching network} for target search and a {meta-learner network} for adaptive feature space update. We use a fully-convolutional Siamese network structure analogous to _cite_ for searching the target object in a given frame, where target search can be done fast and efficiently using the cross-correlation operations between feature maps. For the meta-learner network, we propose a parameter prediction network inspired by recent advances in the meta learning methodology for few-shot learning problems _cite_ . The proposed meta-learner network is trained to provide the matching network with additional convolutional kernels and channel attention information so that the feature space of the matching network can be modified adaptively to adopt new appearance templates obtained in the course of tracking without overfitting. The meta-learner network only sees the gradients from the last layer of the matching network, given new training samples for the appearance. We also employ a novel training scheme for the meta-learner network to maintain the generalization capability of the feature space by preventing the meta-learner network from generating new parameters that cause overfitting of the matching network. By incorporating our meta-learner network, the target-specific feature space can be constructed instantly with a single forward pass without any iterative computation for optimization and is free from innate overfitting, improving the performance of the tracking algorithm. Fig. _ref_ illustrates the motivation of the proposed visual tracking algorithm. We show the effectiveness of our method by showing consistent performance gains in N different visual tracking datasets _cite_ while maintaining a real-time speed of N fps.