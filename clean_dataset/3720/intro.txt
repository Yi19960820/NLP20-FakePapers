The automatic detection and recognition of text in natural images,, is an important challenge for visual understanding. Text, as the physical incarnation of language, is one of the basic tools for preserving and communicating information. Much of the modern world is designed to be interpreted through the use of labels and other textual cues, and so text finds itself scattered throughout many images and videos. Through the use of text spotting, an important part of the semantic content of visual media can be decoded and used, for example, for understanding, annotating, and retrieving the billions of consumer photos produced every day. Traditionally, text recognition has been focussed on document images, where OCR techniques are well suited to digitise planar, paper-based documents. However, when applied to natural scene images, these document OCR techniques fail as they are tuned to the largely black-and-white, line-based environment of printed documents. The text that occurs in natural scene images is hugely variable in appearance and layout, being drawn from a large number of fonts and styles, suffering from inconsistent lighting, occlusions, orientations, noise, and, in addition, the presence of background objects causes spurious false-positive detections. This places text spotting as a separate, far more challenging problem than document OCR. The increase of powerful computer vision techniques and the overwhelming increase in the volume of images produced over the last decade has seen a rapid development of text spotting methods. To efficiently perform text spotting, the majority of methods follow the intuitive process of splitting the task in two: text detection followed by word recognition~ _cite_ . Text detection involves generating candidate character or word region detections, while word recognition takes these proposals and infers the words depicted. In this paper we advance text spotting methods, making a number of key contributions as part of this. Our main contribution is a novel text recognition method--this is in the form of a deep convolutional neural network (CNN) _cite_ which takes the to the network. Evidence is gradually pooled from across the image to perform classification of the word across a huge dictionary, such as the Nk-word dictionary evaluated in this paper. Remarkably, our model is trained, without incurring the cost of human labelling. We also propose an method to successfully train a model with such a large number of classes. Our recognition framework is exceptionally powerful, substantially outperforming previous state of the art on real-world scene text recognition, without using any real-world labelled training data. Our second contribution is a novel detection strategy for text spotting: the use of fast region proposal methods to perform word detection. We use a combination of an object-agnostic region proposal method and a sliding window detector. This gives very high recall coverage of individual word bounding boxes, resulting in around N \% word recall on both ICDAR N and Street View Text datasets with a manageable number of proposals. False-negative candidate word bounding boxes are filtered with a stronger random forest classifier and the remaining proposals adjusted using a CNN trained to regress the bounding box coordinates. Our third contribution is the application of our pipeline for large-scale visual search of text in video. In a fraction of a second we are able to retrieve images and videos from a huge corpus that contain the visual rendering of a user given text query, at very high precision. We expose the performance of each part of the pipeline in experiments, showing that we can maintain the high recall of the initial proposal stage while gradually boosting precision as more complex models and higher order information is incorporated. The recall of the detection stage is shown to be significantly higher than that of previous text detection methods, and the accuracy of the word recognition stage higher than all previous methods. The result is an end-to-end text spotting system that outperforms all previous methods by a large margin. We demonstrate this for the annotation task (localising and recognising text in images) across a large range of standard text spotting datasets, as well as in a retrieval scenario (retrieving a ranked list of images that contain the text of a query string) for standard datasets. In addition, the use of our framework for retrieval is further demonstrated in a real-world application--being used to instantly search through thousands of hours of archived news footage for a user-given text query. The following section gives an overview of our pipeline. We then review a selection of related work in Section~ _ref_ . Sections~N-N present the stages of our pipeline. We extensively test all elements of our pipeline in Section~ _ref_ and include the details of datasets and the experimental setup. Finally, Section~ _ref_ summarises and concludes. Our word recognition framework appeared previously as a tech report~ _cite_ and at the NIPS N Deep Learning and Representation Learning Workshop, along with some other non-dictionary based variants.