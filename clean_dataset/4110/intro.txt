progress in computer visual recognition, in particular image classification, object detection and segmentation or action recognition heavily relies on machine learning methods trained on large scale human annotated datasets. The level of annotation varies, spanning a degree of detail from global image or video labels to bounding boxes or precise segmentations of objects _cite_ . However, the annotations are often subjectively defined, primarily by the high-level visual recognition tasks generally agreed upon by the computer vision community. While such data has made advances in system design and evaluation possible, it does not necessarily provide insights or constraints into those intermediate levels of computation, or deep structure, that are perceived as ultimately necessary in order to design highly reliable computer vision systems. This is noticeable in the accuracy of state of the art systems trained with such annotations, which still lags significantly behind human performance on similar tasks. Nor does existing data make it immediately possible to exploit insights from an existing working system--the human eye--to potentially derive better features, models or algorithms. The divide is well epitomized by the lack of matching large scale datasets that would provide recordings of the workings of the human visual system, in the context of a visual recognition task, at different levels of interpretations including neural systems or eye movements. The human eye movement level, defined by image fixations and saccades, is potentially the less controversial to measure and analyze. It is sufficiently `high-level' or `behavioral' for the computer vision community to rule-out, to some degree at least, open-ended debates on where and what should one record, as could be the case, for instance with neural systems in different brain areas _cite_ . Besides, our goals in this context are pragmatic: fixations provide a sufficiently high-level signal that can be precisely registered with the image stimuli, for testing hypotheses and for training visual feature extractors and recognition models quantitatively. It can potentially foster links with the human vision community, in particular researchers developing biologically plausible models of visual attention, who would be able to test and quantitatively analyze their models on shared large scale datasets _cite_ . Some of the most successful approaches to action recognition employ bag-of words representations based on descriptors computed at spatial-temporal video locations, obtained at the maxima of an interest point operator biased to fire over non-trivial local structure (space-time `corners' or spatial-temporal interest points _cite_) . More sophisticated image representations based on objects and their relations, as well as multiple kernels have been employed with a degree of success _cite_, although it appears still difficult to detect a large variety of useful objects reliably in challenging video footage. Although human pose estimation could greatly disambiguate the interations between actors and manipulated objects, it is a difficult problem even in a controlled setting due to the large number of local minima in the search space _cite_ . The dominant role of sparse spatial-temporal interest point operators as front end in computer vision systems raises the question whether computational insights from a working system like the human visual system can be used to improve performance. The sparse approach to computer visual recognition is not inconsistent to the one of biological systems, but the degree of repeatability and the effect of using human fixations with computer vision algorithms in the context of action recognition have not been yet explored. In this paper we make the following contributions: The paper is organized as follows. In \S _ref_ we briefly review existing studies on human visual attention and saliency, as well as the state-of-the art computational models for automatic action recognition from videos. Our dataset and data collection methodology are introduced in \S _ref_ . In \S _ref_ we analyze inter-subject agreement and introduce two novel metrics for measuring spatial and sequential visual consistency in the video domain. In addition to showing remarkable visual consistency, human subjects also tend to fixate image structures that are semantically meaningful, which we illustrate in \S _ref_ . This naturally suggests that human fixations could provide useful information to support automatic action recognition systems. In section \S _ref_ we introduce our action recognition pipeline which we shall use through the remainder of the paper. Section \S _ref_ explores the action recognition potential of several interest point operators derived from ground truth human fixations and visual saliency maps. In \S _ref_ we turn our attention to the problem of human visual saliency prediction, and introduce a novel spatio-temporal human fixation detector trained using our human gaze dataset. Section \S _ref_ illustrates how predicted saliency maps can be integrated into a modern state-of-the-art end-to-end action recognition system. We draw our final conclusions in \S _ref_ .