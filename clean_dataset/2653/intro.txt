Neural networks have recently enjoyed an acceleration in popularity, with new research adding to several decades of foundational work. From multilayer perceptron (MLP) networks to the more prominent recurrent neural networks (RNNs) and convolutional neural networks (CNNs), neural networks have become a dominant force in the fields of computer vision, speech recognition, and machine translation _cite_ . Increase in computational speed and data collection have legitimized the training of increasingly complex deep networks. The flow of information from input to output is typically performed in a strictly sequential feed-forward fashion, in which for a network consisting of _inline_eq_ layers, nodes in the _inline_eq_ th layer receive input from the _inline_eq_ st layer, compute an output for each neuron through an activation function, and in turn use this output as an input for the _inline_eq_ st layer. A natural extension to this network structure is the addition of ``skip connections" between layers. Specifically, we are interested in the class of architectures in which the network of connections form a directed acyclic graph (DAG) . The defining property of a DAG is that it can always be decomposed into a topological ordering of _inline_eq_ layers, in which nodes in layer _inline_eq_ may be connected to layer _inline_eq_, where _inline_eq_ . A skip connection is a connection between nodes in layers _inline_eq_ and _inline_eq_, with _inline_eq_ . There has been an increasing interest in studying networks with skip connections which skip a small number of layers, with examples including Deep Residual Networks (ResNet) _cite_, Highway Networks _cite_, and FractalNets _cite_ . ResNets, for instance, use ``shortcut connections" in which a copy of previous layers is mapped through an identity mapping to future layers. Kothari and Agyepong _cite_ introduced ``lateral connections" in the form of a chain, with each unit in a hidden layer connected to the next. The full generality of neural networks for DAG architectures was considered in _cite_, which demonstrated superior performance of neural networks, entitled DenseNets, under a wide variety of skip connections. As an example of the efficacy of DAG architectures considered in _cite_, we consider AutoEncoders, a class of neural networks which provide a means of data compression. For an AutoEncoder, input data, such as a pixelated image, is also the desired output for a neural network. During an encoding phase, input is compressed through several hidden layers before arriving at a middle hidden layer, called the code, having dimension smaller than the input. The next phase is decoding, in which input from the code is fed through several more hidden layers until arriving at the output, which is of the same dimension as the input. The goal of compression is to minimize the difference between input data and output. In _cite_, Agarwal et. al introduced CrossEncoders and demonstrated its superior performance against AutoEncoders with no skip-connections. In Section _ref_, we extend the previous results to include the MNIST and Olivetti faces public datasets. We validate our results against several commonly used compression based performance metrics. Our main theoretical result is the convergence of backpropagation with DAG architectures using gradient descent with momentum. It is well known that feed-forward architectures converge under backpropagation, which is essentially gradient descent applied to an error function (see _cite_, for instance) . Updates for weights in backpropagation may be generalized to include a momentum term, which can help with increasing the convergence rate _cite_ . Momentum can help with escaping local minima, but concerns of overshooting require careful arguments for establishing convergence. Formal arguments for convergence have so far been restricted to simple classes of neural networks. Bhaya _cite_ and Torii _cite_ studied the convergence with backpropagation using momentum under a linear activation function. Zhang et al. _cite_ generalized convergence for a class of common nonlinear activation functions, including sigmoids, for the case of a zero hidden layer networks. Wu et al. _cite_ further generalized to one layer by demonstrating that error is monotonically decreasing under backpropagation iterations for sufficiently small momentum terms. The addition of a hidden layer required _cite_ to make the additional assumption of bounded weights during the iteration procedure. It is not evident whether applying the methods of _cite_ would generalize to networks with several hidden layers and skip connections, or if they would require stronger assumptions on boundedness of weights or the class of activation functions. We show in Section _ref_ that convergence indeed does hold, with similar assumptions to the proof of convergence of one hidden layer. In Theorem _ref_, we give the key inequality for proving Theorem _ref_, a recursive form for increments of error and output values of hidden layers after each iteration. This estimate allows us to show that for sufficiently small momentum parameters (including the case of zero momentum), error decreases with each iteration. Our approach to convergence is somewhat more explicit than the traditional proof of gradient descent, which minimizes a loss function without considering network architecture.