Hip fractures are a leading cause of death and disability among older adults. Hip fractures are also the most commonly missed diagnosis on pelvic radiographs, and delayed diagnosis leads to higher cost and worse outcomes. Computer-Aided Diagnosis (CAD) algorithms have shown promise for helping radiologists detect fractures, but the image features underpinning their predictions are notoriously difficult to understand. In this study, we trained deep learning models on N, N radiographs to classify fracture, five patient traits, and N hospital process variables. All N variables could be predicted from a radiograph (p \textless N), with the best performances on scanner model (AUC = N), scanner brand (AUC = N), and whether the order was marked “priority” (AUC = N) . Fracture was predicted moderately well from the image (AUC = N) and better when combining image features with patient data (AUC = N, p = Ne-N) or patient data plus hospital process features (AUC = N, p = Ne-N) . The model performance on a test set with matched patient variables was significantly lower than a random test set (AUC = N, p = N) ; and when the test set was matched on patient and image acquisition variables, the model performed randomly (AUC = N, N \% CI N-N), indicating that these variables were the main source of the model’s predictive ability overall. We also used Naive Bayes to combine evidence from image models with patient and hospital data and found their inclusion improved performance, but that this approach was nevertheless inferior to directly modeling all variables. If CAD algorithms are inexplicably leveraging patient and process variables in their predictions, it is unclear how radiologists should interpret their predictions in the context of other known patient data. Further research is needed to illuminate deep learning decision processes so that computers and clinicians can effectively cooperate.