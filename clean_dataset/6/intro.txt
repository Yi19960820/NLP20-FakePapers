Graphs are one of the most fundamental structures that have been widely used for representing many types of data. Learning on graphs such as graph semi-supervised learning, graph classification or graph evolution have found wide applications in domains such as bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. With remarkable successes of deep learning approaches in image classification and object recognition that attain ``superhuman'' performance, there has been a surge of research interests in generalizing convolutional neural networks (CNNs) to structures beyond regular grids, i.e., from ND/ND images to arbitrary structures such as graphs~ _cite_ . These convolutional networks on graphs are now commonly known as Graph Convolutional Neural Networks (GCNNs) . The principal idea behind graph convolution has been derived from the graph signal processing domain~ _cite_, which has since been extended in different ways for a variety of purposes~ _cite_ . In this paper, we expose three major limitations of the standard GCNN model commonly used in existing deep learning approaches on graphs, especially when applied to the graph classification problem, and explore ways to overcome these limitations. In particular, we propose a new model, referred to as {\em Graph Capsule Convolution Neural Networks (GCAPS-CNN)} . It is inspired by the notion of {\em capsules} developed in~ _cite_: capsules are new types of neurons which encapsulate more information in a local pool operation (e.g., a convolution operation in a CNN) by computing a small vector of highly informative outputs rather than just taking a scalar output. Our {\em graph capsule} idea is quite general and can be employed in any version of GCNN model either design for solving graph semi-supervised problem or doing sequence learning on graphs via Graph Convolution Recurrent Neural Network models (GCRNNs) . The first limitation of the standard GCNN model is due to the basic graph convolution operation which is defined--in its purest form--as the aggregation of node values in a local neighborhood corresponding to each feature (or channel) . As such, there is a potential loss of information associated with the basic graph convolution operation. This problem has been noted before~ _cite_, but has not attracted much attention until recently~ _cite_ . To address this limitation, we propose to improve upon the basic graph convolution operation by introducing the notion of {\em graph capsules} which encapsulate more information about nodes in a local neighborhood, where the local neighborhood is defined in the same way as in the standard GCCN model. Similar to the original capsule idea proposed in~ _cite_, this is achieved by replacing the scalar output of a graph convolution operation with a small vector output containing higher order statistical information per feature. Another source of inspiration for our proposed GCAPS-CNN model comes from one of the most successful graph kernels--the Weisfeiler-Lehman (WL)-subtree graph kernel~ _cite_ designed specifically for solving the graph classification problem. In WL-subtree graph kernel, node labels (features) are collected from neighbors of each node in a local neighborhood and compressed injectively to form a new node label in each iteration. The histogram of these new node labels are concatenated in each iteration to serve as a graph invariant feature vector. The important point to notice here is that due to the injection process, one can recover the exact node labels of local neighbors in each iteration without losing track of them. In contrast, this is not possible in the standard GCNN model as the input feature values of node neighbors are lost after the graph convolution operation. The second major limitation of the standard GCNN model is specific to its (in) ability in tackling the graph classification problem. GCNN models cannot be applied directly because they are equivariant ({\em not invariant}) with respect to the node order in a graph. To be precise, consider a graph _inline_eq_ with Laplacian _inline_eq_ and node feature matrix _inline_eq_ . Let _inline_eq_ be the output function of a GCNN model where _inline_eq_ are the number of nodes, input dimension and hidden dimension of node features, respectively. Then, _inline_eq_ is a permutation equivariant function, i.e., for any _inline_eq_ permutation matrix _inline_eq_ . This specific permutation equivariance property prevent us from directly applying GCNN to a graph classification problem, since it cannot provide any guarantee that the outputs of any two isomorphic graphs are always the same. Consequently, a GCNN architecture needs an additional graph permutation invariant layer in order to perform the graph classification task successfully. This invariant layer also needs to be differentiable for end-to-end learning. Very limited amount of efforts has been devoted to carefully designing such an invariant GCNN model for the purpose of graph classification. Currently the most common method for achieving graph permutation invariance is performing aggregation (i.e., summing) over all graph node values~ _cite_ . Though simple and fast, it can again incur significant loss of information. Likewise, using a max-pooling layer to achieve graph permutation invariance encounters similar issues. A few attempts have been made~ _cite_ that go beyond aggregation or max-pooling in designing graph permutation invariant GCNNs. In~ _cite_ the authors propose a global ordering of nodes by sorting them according to their values in the last hidden layer. This type of invariance is based on creating an order among nodes and has also been explored before in~ _cite_ . However, as discussed in Section~ _ref_, we show that there are some issues with this type of approach. A more tangential approach has been adopted in~ _cite_ based on group theory to design transformation operations and tensor aggregation rules that results in permutation invariant outputs. However, this approach relies on computing high order tensors which are computationally expensive in many cases. To that end, we propose a novel permutation invariant layer based on computing the covariance of the data whose output does not depend upon the order of nodes in the graph. It is also fast to compute since it requires only a single dense-matrix multiplication operation. Our last concern with the standard GCNN model is their limited ability in exploiting global information for the purpose of graph classification. The filters employed in graph convolutions are in essence local in nature and hence can only provide an ``average/aggregate view'' of the local data. This shortcoming poses a serious difficulty in handling graphs where node labels are not present; approaches which initialize (node) feature values using, e.g., node degree, are not much helpful in this respect. We propose to utilize global features (features that account for the full graph structure) using a family of graph spectral distances as proposed in~ _cite_ to remedy this problem. In summary, the major contributions of our paper are: We organize our paper into five sections. We start with the related work on graph kernels and GCNNs in Section~ _ref_, and present our core idea behind graph capsules in Section~ _ref_ . In Section~ _ref_, we focus on building a graph permutation invariant layer especially for solving the graph classification problem. In Section~ _ref_, we propose to equip our GCAPS-CNN model with enhanced global features to exploit the full graph structure for learning on graphs. Lastly in Section~ _ref_ we conduct experiments and show the superior performance of our proposed GCAPS-CNN model.