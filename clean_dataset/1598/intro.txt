In the context of large scale visual recognition, it is not uncommon for state-of-the-art convolutional networks to be trained for days or weeks before convergence~ . Performing exhaustive architecture search is quite challenging and computationally expensive. Furthermore, once a satisfactory architecture has been discovered, it can be extremely difficult to improve upon; small changes to the architecture more often decrease performance than improve it. In architectures containing fully-connected layers, naively increasing the dimensionality of such layers increases the number of parameters between them quadratically, increasing both the computational workload and the tendency towards overfitting. In settings where the domain of interest comprises thousands of classes, improving performance on specific subdomains can prove challenging, as the jointly learned features that succeed on the overall task on average may not be sufficient for correctly identifying the ``long tail'' of classes, or for making fine-grained distinctions between very similar entities. Side information in the form of metadata--for example, from Freebase~--often only roughly corresponds to the kind of similarity that would make correct classification challenging. In the context of object classification, visually similar entities may belong to vastly different high-level categories (e.g. a sporting activity and the equipment used to perform it), whereas two entities in the same high-level semantic category may bear little resemblance to one another visually. A traditional approach to building increasingly accurate classifiers is to average the predictions of a large ensemble. In the case of neural networks, a common approach is to add more layers or making existing layers significantly larger, possibly with additional regularization. These strategies present a significant problem in runtime-sensitive production environments, where a classifier must be rapidly evaluated in a matter of milliseconds to comply with service-level agreements. It is therefore often desirable to increase a classifier's capacity in a way that significantly improves performance while minimally impacting the computational resources required to evaluate the classifier; however, it is not immediately obvious how to satisfy these two competing objectives. We present a method for judiciously adding capacity to a trained neural network using the network's own predictions on held-out data to inform the augmentation of the network's structure. We demonstrate the efficacy of this method by using it to significantly improve upon the performance of a state-of-the-art industrial object recognition pipeline based on _cite_ with less than N \% extra computational overhead.