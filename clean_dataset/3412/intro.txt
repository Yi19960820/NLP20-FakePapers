In recent years, CNNs (convolutional neural networks) are being widely used in areas such as vision, NLP and other domains. While CNNs exhibit superior performance on a wide variety of tasks, their deployment calls for high-end devices due to their intensive computation (FLOPS) and memory requirements. This hinders their real-time usage on portable devices. While it may seem straightforward to address this problem by using smaller sized networks, redundancy of parameters seems necessary in aiding highly non-convex optimization during training to find effective solutions. Hence significant efforts are seen in recent days to address model compression. One line of research aims at devising efficient architectures _cite_ to be trained from scratch on a given task. While they have shown promising results, their generalizability across the tasks is not fully studied. Another prominent line of work _cite_ has focused on model compression to make CNNs more efficient in terms of computations (FLOPS) and memory requirements (Run Time Memory usage and storage space of the model) . These methods first train a large model for a given task and then prune the model until the desired compression is achieved. Model compression techniques can be broadly divided into the following categories. The first category _cite_ aims at introducing sparsity in the parameters of the model. While these approaches achieved good compression rate in model parameters, computations (FLOPS) and Total Runtime Memory (TRM) aren't improved. Such methods also require sparse libraries support to achieve the desired compression as mentioned in _cite_ The second category of methods _cite_ based on model compression using quantization. Often specialized hardware is required to achieve the required acceleration. These model compression techniques are specially designed for IoT devices. The third category of methods _cite_ based on the filter level pruning in the model. These approaches prune an entire filter based on some criteria/metrics and hence provide a structured pruning in the model. As for pruning the whole convolutional filter from the model reduces the depth of the feature maps for subsequent layers, these approaches give high compression rate regarding computations (FLOPS) and Total Runtime Memory (TRM) . Moreover, sparsity and quantization based methods can be applied in addition to these approaches to achieve better compression rates. As described above, filter level pruning approaches use a metric to identify the filter importance, and many heuristics have been used to identify the filter importance. _cite_ used the brute force approach to prune the filters from the model. They remove each filter sequentially and rank the importance of the filter based on their corresponding drop in the accuracy which seems to be impractical for large size networks on large-scale data-sets. Some of the works _cite_ use handcrafted metrics to calculate the filter importance. In the work of _cite_ they use _inline_eq_ norm of a filter to identify the filter importance. Another class of works _cite_ use data-driven metrics to identify the filter importance. _cite_ use the Taylor expansion to calculate the filter importance, which is motivated by optimal brain damage _cite_ . In this work, we propose a new method for filter level pruning based on the sensitivity of filters to auxiliary loss function. While most of the pruning methods use sparsity in some form, our approach is orthogonal to them by choice of auxiliary loss function (by driving filter values away from N) in the model. We evaluate our approach on a variety of tasks and show an impressive reduction in FLOPS across different architectures. We further demonstrate the generalizability of our approach by achieving competitive accuracy using a small model pruned for a different task. To further decrease the FLOPS and memory consumption, our method can be augmented with other pruning methods such as quantized weights, specialized network architectures devised for embedded devices, connection pruning, etc.