Deep learning based algorithms achieve superior performance in many problems in computer vision, including image classification, object detection and segmentation. However, it has been recently shown that algorithms based on Convolutional Neural Network (CNN) are vulnerable to {\em adversarial perturbations}, which are intentionally crafted noises that are imperceptible to human observer, but can lead to large errors in the deep network models when added to images. To date, most existing adversarial perturbations are designed to attack CNN image classifiers, \eg, _cite_ . Recently, attention has been shifted to finding effective adversarial perturbation to CNN-based object detectors _cite_ . Compared to image classification, finding effective perturbations for object detectors is more challenging, as the perturbation should affect not just the class label, but also the location and size of each object within the image. Existing methods _cite_ mostly design specific loss functions based on the final prediction to disturb object class labels. As such, these methods are model dependent, which require detailed knowledge of the network architectures. In this work, we develop a {\em Robust Adversarial Perturbation} (R-AP) method to universally attack deep proposal-based models that are fundamental to majority of object detectors and instance segmentation algorithms. Our method is based on the fact that a majority of recent object detectors and instance segmentation algorithms, \eg, _cite_ use a Region Proposal Network (RPN) to extract object-like regions, known as {\em proposals}, from an image and then process the proposals further to obtain object class labels and bounding boxes in object detection, and the instance class labels and region masks in instance segmentation. If a RPN is successfully attacked by an adversarial perturbation, such that no correct proposals generated, the subsequent process in the object detection or instance segmentation pipeline will be affected. Figure _ref_ overviews the proposed R-AP method. The investigation of adversarial perturbation on deep proposal-based models can lead to further understanding of the vulnerabilities of these widely applied methods. The efforts can also aid improving the reliability and safety of the derived technologies, including computer vision guided autonomous cars and visual analytics. The proposed R-AP method attacks a RPN based on the optimization of two loss functions: {\em (i)} the {\em label loss} and {\em (ii)} the {\em shape loss}, each of which targets a specific aspect of RPN. First, inspired by recent methods _cite_ that attacks CNN-based object detectors, we design the label loss to disturb the label prediction (which indicates whether a proposal represents an object or not) . Second, we also design a shape loss, which attacks the shape regression step in RPN, so that even if an object is correctly identified, the bounding box cannot be accurately refined to the object shape. Note that our R-AP method can be combined with existing adversarial perturbation method such as _cite_ to jointly attack corresponding network, since R-AP specifically focuses on attacking RPN, which is the intermediate stage of network compared to others which target the entire network. Experimental validations are performed on the MS COCO N _cite_, the current largest dataset used for training and evaluating mainstream object detectors and instance segmentation algorithms. Our experimental results demonstrate that the proposed R-AP attack can significantly degrade the performance of several state-of-the-art object detectors and instance segmentation algorithms, with minimal perceptible artifacts to human observers. Our contributions are summarized in the following: