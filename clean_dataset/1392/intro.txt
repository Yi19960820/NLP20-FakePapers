With the development of human-computer interaction (HCI), emotion recognition has become increasingly important. Since human's emotion contains many nonverbal cues, various modalities ranging from facial expressions, body gesture, voice to physiological signals can be used as the indicators of emotional states ~ _cite_ . In real-world applications, it is difficult to recognize human's emotional states only considering a single modality, because signals from different modalities represent different aspects of emotion and provide complementary information. Recent studies show that integrating multiple modalities can significantly boost the emotion recognition accuracy ~ _cite_ . The most successful approach to fuse the information from multiple modalities is based on deep multi-view representation learning ~ _cite_ . E.g., ~ _cite_ proposed a joint density model for emotion analysis with a multi-modal deep Boltzmann machine (DBM) ~ _cite_ . This multi-modal DBM is exploited to model the joint distribution over visual, auditory, and textual features. ~ _cite_ proposed a multi-modal emotion recognition method by using multi-modal autoencoders (MAE) ~ _cite_, in which the joint representations of Electroencephalogram (EEG) and eye movement signals were extracted. Nevertheless, there are still limitations with these deep multi-modal emotion recognition methods, e.g., their performances depend on the amount of labeled data and they could not handle incomplete data. By using the modern sensing equipments, we can easily collect massive emotion-related data from multiple modalities. But, the data labeling procedure requires lots of manual efforts. Therefore, in most cases only a small set of labeled samples is available, while the majority of whole dataset is left unlabeled. In addition to challenges with insufficient labeled data, one must often address the incomplete-data problem, i.e., not all modalities are available for every data point. Generally, we can identify various causes for incomplete data. E.g., unforeseeable sensor malfunction may fail to collect sensing information, thus providing us incomplete data with one or more missing modalities. Traditional multi-modal emotion recognition approaches ~ _cite_ only utilized the limited amount of labeled data, which may result in severe overfitting. Also, most of them neglect the missing modality issue, which greatly limits their applications in real-world scenarios. The most attractive way to deal with the aforementioned issues is semi-supervised learning (SSL) with incomplete data. SSL can improve model's generalization ability by exploiting both labeled and unlabeled data simultaneously ~ _cite_, and learning from incomplete data can guarantee the robustness of the emotion recognition system ~ _cite_ . In this paper, we show that the problems mentioned above can be resolved under a unified multi-view deep generative framework. For modeling the statistical relationships of multi-modality emotional data, a shared latent variable is transformed by different modality-specific generative networks to different data views (modalities) . Instead of treating each view equally, we impose a non-uniformly weighted Gaussian mixture assumption on the posterior approximation of the shared latent variables. This is critical for inferring the joint latent representation and the weight factor of each view from multiple modalities. During optimization, a second lower bound to the variational lower bound is derived to address the intractable entropy of a mixed Gaussians. To leverage the contextual information in the unlabeled data to augment the limited labeled data, we then extend our multi-view framework to SSL scenario. It is achieved by casting the semi-supervised classification problem as a specialized missing data imputation task. Specifically, we treat the unknown labels as latent variables and estimate them within a multi-view auto-encoding variational Bayes framework. We further extend the proposed SSL algorithm to the incomplete-data case by introducing latent variables for the missing views. Besides the unknown labels, the missing views are also integrated out so that the marginal likelihood is maximized with respect to model parameters. In this way, our SSL algorithm can utilize all available data: both labeled and unlabeled, as well as both complete and incomplete. Since the category information and the uncertainty of missing view are taken into account in the training process, our SSL algorithm is more powerful than traditional missing view imputation methods ~ _cite_ . We finally demonstrate the superiority of our framework and provide insightful observations on two real multi-modal emotion datasets.