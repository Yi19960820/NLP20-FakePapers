Light field refers to the collection of light rays in ND space. With a light field imaging system, light rays in different directions are recorded separately, unlike a traditional imaging system, where a pixel records the total amount of light received by the lens regardless of the direction. The angular information enables new capabilities, including depth estimation, post-capture refocusing, post-capture aperture size and shape control, and ND modelling. Light field imaging can be used in different application areas, including ND optical inspection, robotics, microscopy, photography, and computer graphics. Light field imaging is first described by Lippmann, who proposed to use a set of small biconvex lenses to capture light rays in different directions and refers to it as integral imaging _cite_ . The term "light field" was first used by Gershun, who studied the radiometric properties of light in space _cite_ . Adelson and Bergen used the term "plenoptic function" and defined it as the function of light rays in terms of intensity, position in space, travel direction, wavelength, and time _cite_ . Adelson and Wang described and implemented a light field camera that incorporates a single main lens along with a micro-lens array _cite_ . This design approach is later adopted in commercial light field cameras _cite_ . In N, Levoy and Hanrahan _cite_ and Gortler et al. _cite_ formulated light field as a ND function, and studied ray space representation and light field re-sampling. Over the years, light field imaging theory and applications have continued to be developed further. Key developments include post-capture refocusing _cite_, Fourier-domain light field processing _cite_, light field microscopy _cite_, focused plenoptic camera _cite_, and multi-focus plenoptic camera _cite_ . Light field acquisition can be done in various ways, such as camera arrays _cite_, optical masks _cite_, angle-sensitive pixels _cite_, and micro-lens arrays _cite_ . Among these different approaches, micro-lens array (MLA) based light field cameras provide a cost-effective solution, and have been successfully commercialized _cite_ . There are two basic implementation approaches of MLA-based light field cameras. In one approach, the image sensor is placed at the focal length of the micro-lenses _cite_ . In the other approach, a micro-lens relays the image (formed by the objective lens on an intermediate image plane) to the image sensor _cite_ . These two approaches are illustrated in Figure _ref_ . In the first approach, the sensor pixels behind a micro-lens (also called a lenslet) on the MLA record light rays coming from different directions. Each lenslet region provides a single pixel value for a perspective image; therefore, the number of lenslets corresponds to the number of pixels in a perspective image. That is, the spatial resolution is defined by the number of lenslets in the MLA. The number of pixels behind a lenslet, on the other hand, defines the angular resolution, that is, the number of perspective images. In the second approach, a lenslet forms an image of the scene from a particular viewpoint. The number of lenslets defines the angular resolution; and, the number of pixels behind a lenslet gives the spatial resolution of a perspective image. In the MLA-based light field cameras, there is a trade-off between spatial resolution and angular resolution, since a single image sensor is used to capture both. For example, in the first generation Lytro camera, an N megapixel image sensor produces NxN sub-aperture perspective images, each with a spatial resolution of about N megapixels. Such a low spatial resolution prevents the widespread adoption of light field cameras. In recent years, different methods have been proposed to tackle the low spatial resolution issue. Hybrid systems, consisting of a light field sensor and a regular sensor, have been presented _cite_, where the high spatial resolution image from the regular sensor is used to enhance the light field sub-aperture (perspective) images. The disadvantages of hybrid systems include increased cost and larger camera dimensions. Another approach is to apply multi-frame super-resolution techniques to the sub-aperture images of a light field _cite_ . It is also possible to apply learning-based super-resolution techniques to each sub-aperture image of a light field _cite_ . In this paper, we present a convolutional neural network based light field super-resolution method. The method has two sub-networks; one is trained to increase the angular resolution, that is, to synthesize novel viewpoints (sub-aperture images) ; and the other is trained to increase the spatial resolution of each sub-aperture image. We show that the proposed method provides significant increase in image quality, visually as well as quantitatively (in terms of peak signal-to-noise ratio and structural similarity index _cite_), and improves depth estimation accuracy. The paper is organized as follows. We present the related work in the literature in Section II. We explain the proposed method in Section III, present our experimental results in Section IV, and conclude the paper in Section V.