Object recognition, and more specifically object categorization, has seen unprecedented advances in recent years with development of convolutional neural networks (CNNs) _cite_ . However, most successful recognition models, to date, are formulated as supervised learning problems, in many cases requiring hundreds, if not thousands, labeled instances to learn a given concept class _cite_ . This exuberant need for large labeled datasets has limited recognition models to domains with N's to few N's of classes. Humans, on the other hand, are able to distinguish beyond _inline_eq_ basic level categories _cite_ . What is more impressive, is the fact that humans can learn from few examples, by effectively leveraging information from other object category classes, and even recognize objects without ever seeing them (\eg, by reading about them on the Internet) . This ability has spawned research in few-shot and zero-shot learning. Zero-shot learning (ZSL) has now been widely studied in a variety of research areas including neural decoding by fMRI images _cite_, character recognition~ _cite_, face verification _cite_, object recognition _cite_, and video understanding~ _cite_ . Typically, zero-shot learning approaches aim to recognize instances from the unseen or unknown testing {\em target} categories by transferring information, through intermediate-level semantic representations, from known observed {\em source} (or auxiliary) categories for which many labeled instances exist. In other words, supervised classes/instances, are used as context for recognition of classes that contain no visual instances at training time, but that can be put in some correspondence with supervised classes/instances. As such, a general experimental setting of ZSL is that the classes in target and source (auxiliary) dataset are disjoint and typically the learning is done on the source dataset and then information is transferred to the target dataset, with performance measured on the latter. This setting has a few important drawbacks: (N) it assumes that target classes cannot be mis-classified as source classes and vice versa; this greatly and unrealistically simplifies the problem; (N) the target label set is often relatively small, between ten _cite_ and several thousand unknown labels _cite_, compared to at least _inline_eq_ entry level categories that humans can distinguish; (N) large amounts of data in the source (auxiliary) classes are required, which is problematic as it has been shown that most object classes have only few instances (long-tailed distribution of objects in the world _cite_) ; and (N) the vast open set vocabulary from semantic knowledge, defined as part of ZSL~ _cite_, is not leveraged in any way to inform the learning or source class recognition. A few works recently looked at resolving (N) through class-incremental learning _cite_ which is designed to distinguish between seen (source) and unseen (target) classes at the testing time and apply an appropriate model--supervised for the former and ZSL for the latter. However, (N)--(N) remain largely unresolved. In particular, while (N) and (N) are artifacts of the ZSL setting, (N) is more fundamental. For example, consider learning about a {\em car} by looking at image instances in Fig. _ref_ . Not knowing that other motor vehicles exist in the world, one may be tempted to call anything that has N-wheels a {\em car} . As a result the zero-shot class {\em truck} may have large overlap with the {\em car} class (see Fig. _ref_ [SVR]) . However, imagine knowing that there also exist many other motor vehicles (trucks, mini-vans, {\em etc}) . Even without having visually seen such objects, the very basic knowledge that they {\em exist} in the world and are closely related to a {\em car} should, in principal, alter the criterion for recognizing instance as a {\em car} (making the recognition criterion stricter in this case) . Encoding this in our [SS-Voc] model results in better separation among classes. To tackle the limitations of ZSL and towards the goal of generic open set recognition, we propose the idea of semi-supervised vocabulary-informed learning . Specifically, assuming we have few labeled training instances and a large open set vocabulary/semantic dictionary (along with textual sources from which statistical semantic relations among vocabulary atoms can be learned), the task of semi-supervised vocabulary-informed learning is to learn a model that utilizes semantic dictionary to help train better classifiers for observed (source) classes and unobserved (target) classes in supervised, zero-shot and open set image recognition settings. Different from standard semi-supervised learning, we do not assume unlabeled data is available, to help train classifier, and only {\em vocabulary} over the target classes is known. \noindent Contributions: Our main contribution is to propose a novel paradigm for potentially open set image recognition: semi-supervised vocabulary-informed learning (SS-Voc), which is capable of utilizing vocabulary over unsupervised items, during training, to improve recognition. A unified maximum margin framework is used to encode this idea in practice. Particularly, classification is done through nearest-neighbor distance to class prototypes in the semantic embedding space, and we encode a set of constraints ensuring that labeled images project into semantic space such that they end up closer to the correct class prototypes than to incorrect ones (whether those prototypes are part of the source or target classes) . We show that word embedding (wordNvec) can be used effectively to initialize the semantic space. Experimentally, we illustrate that through this paradigm: we can achieve competitive supervised (on source classes) and ZSL (on target classes) performance, as well as open set image recognition performance with large number of unobserved vocabulary entities (up to _inline_eq_) ; effective learning with few samples is also illustrated. \noindent