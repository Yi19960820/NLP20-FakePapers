Image convolutions are basic operations for many image processing and computer vision applications. In this paper we will study the class of bilateral filter convolutions and propose a general image adaptive convolution that can be learned from data. The bilateral filter~ _cite_ was originally introduced for the task of image denoising as an edge preserving filter. Since the bilateral filter contains the spatial convolution as a special case, we will in the following directly state the general case. Given an image _inline_eq_ with _inline_eq_ pixels and _inline_eq_ channels, and for every pixel _inline_eq_, a _inline_eq_ dimensional feature vector _inline_eq_ (\eg, the _inline_eq_ position in the image _inline_eq_) . The bilateral filter then computes for all _inline_eq_ . Almost the entire literature refers to the bilateral filter as a synonym of the Gaussian parametric form _inline_eq_ . The features _inline_eq_ are most commonly chosen to be position _inline_eq_ and color _inline_eq_ or pixel intensity. To appreciate the edge-preserving effect of the bilateral filter, consider the five-dimensional feature _inline_eq_ . Two pixels _inline_eq_ have a strong influence _inline_eq_ on each other only if they are close in position color. At edges the color changes, therefore pixels lying on opposite sides have low influence and thus this filter does not blur across edges. This behaviour is sometimes referred to as ``image adaptive'', since the filter has a different shape when evaluated at different locations in the image. More precisely, it is the projection of the filter to the two-dimensional image plane that changes, the filter values _inline_eq_ do not change. The filter itself can be of _inline_eq_ dimensions _inline_eq_, in which case the multiplication in Eq.~ becomes an inner product. For the Gaussian case the filter can be applied independently per channel. For an excellent review of image filtering we refer to~ _cite_ . The filter operation of Eq.~ is a sparse high-dimensional convolution, a view first developed in~ _cite_ . An image _inline_eq_ is not sparse in the spatial domain, we observe pixels values for all locations _inline_eq_ . However, when pixels are understood in a higher dimensional feature space, \eg, _inline_eq_, the image becomes a sparse signal, since the _inline_eq_ values lie scattered in this five-dimensional space. This view on filtering is the key difference of the bilateral filter compared to the common spatial convolution. An image edge is not ``visible'' for a filter in the spatial domain alone, whereas in the ND space it is. The edge-preserving behaviour is possible due to the higher dimensional operation. Other data can naturally be understood as sparse signals, \eg, ND surface points. The contribution of this paper is to propose a general and learnable sparse high dimensional convolution. Our technique builds on efficient algorithms that have been developed to approximate the Gaussian bilateral filter and re-uses them for more general high-dimensional filter operations. Due to its practical importance (see related work in Sec.~ _ref_) several efficient algorithms for computing Eq.~ have been developed, including the bilateral grid~ _cite_, Gaussian KD-trees~ _cite_, and the permutohedral lattice~ _cite_ . The design goal for these algorithms was to provide a) fast runtimes and b) small approximation errors for the Gaussian filter case. The key insight of this paper is to use the permutohedral lattice and use it not as an approximation of a predefined kernel but to freely parametrize its values. We relax the separable Gaussian filter case from~ _cite_ and show how to compute gradients of the convolution (Sec.~ _ref_) in lattice space. This enables learning the filter from data. This insight has several useful consequences. We discuss applications where the bilateral filter has been used before: image filtering (Sec.~ _ref_) and CRF inference (Sec.~ _ref_) . Further we will demonstrate how the free parametrization of the filters enables us to use them in Deep convolutional neural networks (CNN) and allow convolutions that go beyond the regular spatially connected receptive fields (Sec.~ _ref_) . For all domains, we present various empirical evaluations with a wide range of applications.