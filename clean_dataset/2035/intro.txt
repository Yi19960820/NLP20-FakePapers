Since the impressive results of AlexNet deep convolutional neural network (DCNN) in the Image-Net Large-Scale Vision Recognition Challenge (ILSVRC) in N _cite_, DCNN research activity has seen exponential growth with the trend being deeper architectures accompanied by higher accuracies _cite_ . Following this trend, research in DCNN FPGA accelerators provides solutions that use high-end costly FPGA devices and aim at the datacenter rather than the mobile applications _cite_ . An exception to the-most-accurate-network trend in the DCNN architecture research, is SqueezeNet (SqN) _cite_, an AlexNet-level accuracy architecture which reduces dramatically the number of MACs and network parameters, requiring half of the MACs and fifty times less parameters compared to AlexNet. Even though the SqN DCNN architecture is more suitable than others for use in embedded mobile applications, it is still computationally very demanding and cannot be used in applications running on an embedded mobile processor. \par The contribution of this work is the design of SqueezeJet (SqJ), a small FPGA convolutional (conv) layer accelerator for SqN, that can be used as a coprocessor to an embedded mobile processor and enable the development of mobile computer vision (CV) applications. Specifically, the SqJ design: (N) deals with the challenge of the implementation of a single accelerator for multiple conv layers with variable input arguments, (N) implements streaming input/output (I/O) interfaces which, after the initialization phase, consume and produce data pixel-by-pixel, (N) uses a sophisticated hardware (HW) mechanism, which mimics software (SW) pointers to the rows of a two-dimensional array, taking advantage of the spatial locality of data and minimizing unnecessary data movement, (N) presents the possibilities of high-level synthesis (HLS) design by using the Xilinx Vivado HLS (VHLS) tool, (N) is implemented on a low-end FPGA system on chip (SoC) device, the Xilinx XCNZN, using the Xilinx SDSoC tool, and (N) it achieves N \% ILSVRCN top-N accuracy when it is used for the inference phase of SqN. To the best of the authors' knowledge, the current work presents the first low-end FPGA SoC (XCNZN) DCNN implementation which achieves N \% ILSVRCN top-N accuracy. The rest of this paper is organized as follows: Section _ref_ presents related work. Section _ref_ is an introduction to the conv layer's operation. Section _ref_ presents the architecture, the HLS design, and the implementation of the SqJ accelerator. Section _ref_ shows results related to the performance, the accuracy, and the power consumption of SqJ. Finally, Section _ref_ concludes the paper and proposes future work.