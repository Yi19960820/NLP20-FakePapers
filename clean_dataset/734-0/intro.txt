The computer vision community has been building datasets for decades, and as long as we have been building them, we have been fighting their biases. From the early days of COIL-N~ _cite_ the Corel Stock Photos and N Scenes datasets~ _cite_ up to and including newer datasets such as PASCAL VOC~ _cite_ and Imagenet~ _cite_, we have experienced bias: every sample of the world is biased in some way--viewpoint, lighting, etc. Our task has been to build algorithms that perform well on these datasets. In effect, we have ``hacked'' each new dataset-exploring it, identifying weaknesses, and in sometimes flawlessly fitting to it. For an in depth analysis of the evolution of datasets (and an enjoyable read) we refer the reader to~ _cite_ . In short, there are two main ways in which our community has addressed bias: making new datasets, and building bigger ones. By making new datasets we continuously get new samples of the visual world, and make sure our techniques handle more of its variability. By making our datasets larger we make it harder to over-fit to the dataset's individual idiosyncrasies. This approach has been remarkably successful. It requires, however, a great amount of effort to generate new datasets and label them with ground truth annotations. Even when great care has been taken to minimize the sampling bias it has a way of creeping in. As an example, for the task of object viewpoint estimation, we can observe clear bias in the distribution viewpoint angles when exploring real image datasets. Figure~ _ref_ shows the distribution of azimuth angles for the training sets of the car class of PASCAL VOC, and the CMUCar dataset. There is clear oversampling of some angles, mainly around _inline_eq_ and _inline_eq_ . In this work we explore the benefits of synthetically generated data for viewpoint estimation. ND viewpoint estimation is an ideal task for the use of renders, as it requires high of accuracy in labeling. We utilize a large database of accurate, highly detailed, ND models to create a large number of synthetic images. To diversify the generated data we vary many of the rendering parameters. We use the generated dataset to train a deep convolutional network using a loss function that is optimized for viewpoint estimation. Our experiments show that models trained on rendered data are as accurate as those trained on real images. We further show that synthetic data can be also be used successfully during validation, opening up opportunities for large scale evaluation of trained models. With rendered data, we control for viewpoint bias, and can create a uniform distribution. We can also adequately sample lighting conditions and occlusions by other objects. Figure~ _ref_ shows renders created for a single object, an IKEA bed. Note how we can sample the different angles, lighting conditions, and occlusions. We will, of course, have other types of bias, and this a combined approach--augment real image datasets with rendered data. We explore this idea below. We assert that a factor limiting the success of many computer vision algorithms is the of labeled data and the of the provided labels. For viewpoint estimation in particular the space of possible angles is immense, and collecting enough samples of every angle is hard. Furthermore, accurately labeling each image with the ground truth angle proves difficult for human annotators. As a result, most current ND viewpoint datasets resort to one of two methods for labeling data: (N) Provide coarse viewpoint information in the form of viewpoint classes (usually N to N) . (N) Use a two step process of labeling. First, ask annotators to locate about a dozen keypoints on the object (e.g., front-left most point on a car's bumper), then manually locate those same points in ND space on a preselected model. Finally, perform PnP optimization~ _cite_ to learn a projection matrix from ND points to ND image coordinates, from which angle labels are calculated. Both methods are unsatisfying. For many downstream applications a coarse pose classification is not enough, and the complex point correspondence based process expensive to crowdsource. By generating synthetic images one can create large scale datasets, with desired label granularity level.