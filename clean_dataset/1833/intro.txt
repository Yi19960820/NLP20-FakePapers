Assuming a set of _inline_eq_-dimensional vectors _inline_eq_, we aim to encode each vector _inline_eq_ into a _inline_eq_-dimensional code _inline_eq_, such that the code reconstructs the vector faithfully. When _inline_eq_ is the situation of interest, the most faithful coding is not unique. We hence consider the code to bear some structure, or in the Bayesian language, we want to impose some prior on the code. One possible structure/prior is reflected by the _inline_eq_-norm, i.e. by solving the following problem: where _inline_eq_ is a linear decoding matrix, and both _inline_eq_ and _inline_eq_ are constants. In real-world applications, we have difficulty in pre-determining the _inline_eq_ value, and the flexibility to learn the prior from the data becomes more important, which can be formulated as the following problem: where _inline_eq_ is a domain of _inline_eq_ values of interest. For example, _inline_eq_ defines a domain that ensures the constraint to be convex with regard to _inline_eq_ . In this paper, (_ref_) and (_ref_) are termed _inline_eq_-norm constrained coding, and for distinguishing purpose we refer to (_ref_) and (_ref_) as known _inline_eq_ and unknown _inline_eq_, respectively. For known _inline_eq_, if the decoding matrix is given a priori as _inline_eq_, then it is sufficient to encode each _inline_eq_ individually, by means of solving: or its equivalent, unconstrained form (given a properly chosen Lagrange multiplier _inline_eq_): Eq. (_ref_) is well known as an _inline_eq_-norm regularized least squares problem, which arises in many disciplines of statistics, signal processing, and machine learning. Several special _inline_eq_ values, such as N, N, N, and _inline_eq_, have been well studied. For example, when _inline_eq_, _inline_eq_ measures the number of non-zero entries in _inline_eq_, thus minimizing the term induces the code that is as as possible. While sparsity is indeed a goal in several situations, the _inline_eq_-norm minimization is NP-hard _cite_ . Researchers then proposed to adopt _inline_eq_-norm, which gives rise to a convex and easier problem, to approach _inline_eq_-norm. It was shown that, under some mild conditions, the resulting code of using _inline_eq_-norm coincides with that of using _inline_eq_-norm _cite_ . _inline_eq_-norm regularization was previously known in lasso regression _cite_ and basis pursuit _cite_ . Due to its enforced sparsity, it leads to the great success of compressive sensing _cite_ . The cases of _inline_eq_ lead to non-sparse codes that were also investigated. _inline_eq_-norm regularization was extensively adopted under the name of weight decay in machine learning _cite_ . _inline_eq_-norm was claimed to help spread information evenly among the entries of the resultant code, which is known as democratic _cite_ or spread representations _cite_, and benefits vector quantization _cite_, hashing _cite_, and other applications. Besides the above-mentioned special _inline_eq_ values, other general _inline_eq_ values were much less studied due to mathematical difficulty. Nonetheless, it was observed that general _inline_eq_ indeed could help in specific application domains, where using the special _inline_eq_ values might be overly simplified. For sparse coding, _inline_eq_-norms with _inline_eq_ all enforce sparsity to some extent, yet with different effects. In compressive sensing, it was known that using _inline_eq_-norm with _inline_eq_ needs fewer measurements to reconstruct sparse signal than using _inline_eq_-norm; regarding computational complexity, solving _inline_eq_-norm regularization is more difficult than solving _inline_eq_-norm but still easier than solving _inline_eq_-norm _cite_ . Further studies found that the choice of _inline_eq_ crucially affected the quality of results and noise robustness _cite_ . Xu endorsed the adoption of _inline_eq_-norm regularization and proposed an iterative half thresholding algorithm _cite_ . In image deconvolution, Krishnan and Fergus investigated _inline_eq_ and _inline_eq_ norms and claimed their advantages over _inline_eq_-norm _cite_ . For non-sparse coding, _inline_eq_-norms with _inline_eq_ and different _inline_eq_ 's have distinct impact on the solution. For example, Kloft argued for _inline_eq_-norm with _inline_eq_ in the task of multiple kernel learning _cite_ . Now let us return to the problem (_ref_) where the decoding matrix _inline_eq_ is . As discussed above, _inline_eq_-norm induces sparse representations, thus the special case of _inline_eq_-norm constrained coding with _inline_eq_ (similarly _inline_eq_) is to pursue a sparse coding of the given data. While sparse coding has great interpretability and possible relation to visual cognition _cite_, and was widely adopted in many applications _cite_, we are inspired by the studies showing that general _inline_eq_ performs better than special _inline_eq_, and ask: is general _inline_eq_-norm able to outperform _inline_eq_ / _inline_eq_ / _inline_eq_ / _inline_eq_-norm on a specific dataset for a specific task? If yes, then which _inline_eq_ will be the best (i.e. the case of unknown _inline_eq_) ? These questions seem not being investigated before, to the best of our knowledge. Especially,, i.e. _inline_eq_, is clearly different from sparse coding and is the major theme of our study. Analytically solving the _inline_eq_-norm constrained problems is very difficult as they are not convex optimization. When designing numerical solutions, note that there are two (resp. three) groups of variables in (_ref_) (resp. (_ref_)), and it is natural to perform alternate optimization over them iteratively. Previous methods for sparse coding mostly follow this approach, for example in _cite_ and in the well-known K-SVD method _cite_ . One clear drawback of this approach is the high computational complexity due to the iterative nature. A variant of using early-stopped iterative algorithms to provide fast solution approximations was discussed in _cite_, built on the overly strong assumption of close-to-orthogonal bases _cite_ . Besides the high complexity, it is not straightforward to extend the sparse coding methods to the cases of non-sparse coding, since they usually leverage the premise of sparse code heuristically but for general _inline_eq_ it is hard to design such heuristics. A different approach for sparse coding was proposed by Gregor and LeCun _cite_, where an iterative algorithm known as iterative shrinkage and thresholding (ISTA), that was previously used for _inline_eq_-norm regularized least squares, is unrolled and truncated to construct a multi-layer feed-forward network. The network can be trained end-to-end to act as a regressor from a vector to its corresponding sparse code. Note that truncation helps lower the computational cost of the network than the original algorithm, while training helps compensate the error due to truncation. The trained network known as learned ISTA (LISTA) is then a fast alternative to the original ISTA algorithm. Following the approach of LISTA, several recent works consider the cases of _inline_eq_-norm _cite_ and _inline_eq_-norm _cite_, as well as extend other iterative algorithms to their network versions _cite_ . However, LISTA and its following-up works are not applicable for solving the cases of general _inline_eq_, because they all refer to the same category of first-order iterative algorithms, i.e. the projected gradient descent (PGD) algorithms. For general _inline_eq_, the projection step in PGD is not analytically solvable. In addition, such works have more difficulty in solving the unknown _inline_eq_ problem. Beyond the family of PGD, another first-order iterative algorithm is the Frank-Wolfe algorithm _cite_, which is free of projection. This characteristic inspires us to adapt the Frank-Wolfe algorithm to solve the general _inline_eq_ problem. Similar to LISTA, we unroll and truncate the Frank-Wolfe algorithm to construct a network, termed Frank-Wolfe network (F-W Net), which can be trained end-to-end. Although the convergence speed of the original Frank-Wolfe algorithm is slow, we will show that F-W Net can converge faster than not only the original Frank-Wolfe and ISTA algorithms, but also the LISTA. Moreover, F-W Net has a novel, closed-form and nonlinear computation unit that is parameterized by _inline_eq_, and as _inline_eq_ varies, that unit displays the behaviors of several classic pooling operators, and can be naturally viewed as a cascade of a generalized normalization and a parametric activation. Due to the fact that _inline_eq_ becomes a (hyper) parameter in F-W Net, we can either set _inline_eq_ a priori or make _inline_eq_ learnable when training F-W Net. Thus, F-W Net has higher learning flexibility than LISTA and training F-W Net gracefully solves the unknown _inline_eq_ problem. To the best of our knowledge, we are the first to extend the sparse coding problem into _inline_eq_-norm constrained coding with general _inline_eq_ and unknown _inline_eq_ ; we are also the first to unroll-and-truncate the Frank-Wolfe algorithm to construct trainable network. Technically, we make the following contributions: The remainder of this paper is organized as follows. Section _ref_ reviews literatures from four folds: _inline_eq_-norm constrained coding and its applications, deep structured networks, the original Frank-Wolfe algorithm, and nonlinear units in networks, from which we can see that F-W Net connects and integrates these separate fields. Section _ref_ formulates F-W Net, with detailed discussions on its motivation, structure, interpretation, and implementation issues. Section _ref_ validates the performance of F-W Net on synthetic data under an extensive range of settings, as well as on a toy problem, handwritten digit recognition with the MNIST dataset. Section _ref_ discusses the proposed convolutional version of F-W Net. Section _ref_ then provides experimental results of using Conv F-W Net on image denoising and super-resolution, with comparison to some other CNN models. Section _ref_ concludes this paper. For reproducible research, our code and pre-trained models have been published online .