When we are interested in providing a description of an object or a human, we tend to use visual attributes to accomplish this task. For example, a laptop can have a wide screen, a silver color, and a brand logo, whereas a human can be tall, female, wearing a blue t-shirt and carrying a backpack. Visual attributes in computer vision are equivalent to the adjectives in our speech. We rely on visual attributes since (i) they enhance our understanding by creating an image in our head of what this object or human looks like; (ii) they narrow down the possible related results when we want to search for a product online or when we want to provide a suspect description; (iii) they can be composed in different ways to create descriptions; (iv) they generalize well as with some fine-tuning they can be applied to recognize objects for different tasks; and (v) they are a meaningful semantic representation of objects or humans that can be understood by both computers and humans. However, effectively predicting the corresponding visual attributes of a human given an image remains a challenging task _cite_ . In real-life scenarios, images might be of low-resolution, humans might be partially occluded in cluttered scenes, or there might be significant pose variations. Estimating the visual attributes of humans is an important computer vision problem with applications ranging from finding missing children to virtual reality. When a child goes missing or the police is looking for a suspect, a short description is usually provided that comprises such attributes (for example, tall white male, with a black shirt wearing a hat and carrying a backpack) . Thus, if we could efficiently identify which images or videos contain images of humans with such characteristics we could potentially reduce dramatically the labor and the time required to identify them _cite_ . Another interesting application is the ND reconstruction of the human body in virtual reality _cite_ . If we have such attribute information we can facilitate the reconstruction by providing the necessary priors. For example it is easier to reconstruct accurately the body shape of a human if we already know that it is a tall male with shorts and sunglasses than if no information is provided. In this work, we introduce CILICIA (CurrIculum Learning multItask ClassIfication Attributes) to address the problem of visual attribute classification from images of standing humans. Instead of using low-level representations, which would require extracting hand-crafted features, we propose a deep learning method to solve multiple binary classification tasks. CILICIA differentiates itself from the literature as: (i) it performs end-to-end learning by feeding a single ConvNet with the entire image of a human without making any assumptions about predefined connection between body parts and image regions; and (ii) it exploits the advantages of both multi-task and curriculum learning. Tasks are split into groups based on their labels' cross-correlation using hierarchical agglomerative clustering. The groups of tasks are learned in a curriculum learning scenario, starting with the one with the highest within-group cross-correlation and moving to the less correlated ones by transferring knowledge from the former to the latter. The tasks in each group are learned in a typical multi-task classification setup. Parts of this publication appear in our previous work _cite_ . However, in this work we have: When Vapnik and Vashist introduced the learning using privileged information (LUPI) paradigm _cite_, they drew inspiration from human learning. They observed how significant the role of an intelligent teacher was in the learning process of a student, and proposed a machine learning framework to imitate this process. Employing privileged information from an intelligent teacher at training time has recently received significant attention from the scientific community with remarkable results in areas ranging from object recognition _cite_ to biometrics _cite_ . Our work also draws inspiration from the way students learn in class. First, students find it difficult to learn all tasks at once. It is usually easier for them to acquire some basic knowledge first, and then build on top of that, by learning more complicated concepts. This can be achieved by learning in a hierarchical manner, which is commonly employed in the literature _cite_, or with a curriculum strategy. Curriculum learning _cite_ (presenting easier examples before more complicated and learning tasks sequentially, instead of all at the same time) imitates this learning process. It has the advantage of exploiting prior knowledge to improve subsequent classification tasks but it cannot scale up to many tasks since each subsequent task has to be learned individually. However, to maximize students' understanding a curriculum might not be sufficient by itself. Students also need a teaching paradigm that can guide their learning process, especially when the task to be learned is challenging. The teaching paradigm in our method is the split of visual attribute classification tasks that need to be learned by performing hierarchical agglomerative clustering. In that way, we exploit the advantages of both multi-task and curriculum learning. First, the ConvNet learns the group of tasks with the strongest intra cross-correlation in a multi-task learning setup, and once this process is completed, the weights of the respective tasks are used as an initialization for the more diverse tasks. During the training of the more diverse tasks, the prior knowledge obtained is leveraged to improve the classification performance. An illustrative example of our method is depicted in Figure~ _ref_ . Note that the proposed learning paradigm is not tied visual attribute classification domain and can be extended to other applications such as object recognition or _cite_ and domain adaptation _cite_ . In summary, this paper has the following contributions. First, we introduce CILICIA, a novel method of exploiting the advantages of both multi-task and curriculum learning by splitting tasks into groups by performing hierarchical agglomerative clustering. The tasks of each subgroup are learned in a joint manner. Thus, the proposed method learns better than learning all the tasks in a typical multi-task learning setup and converges faster than learning tasks one at a time. Second, we propose a scheme of transferring knowledge between the groups of tasks which speeds up the convergence and increases the performance. We performed extensive evaluations in three datasets of humans standing and achieved state-of-the-art results in all three of them. The remainder of the paper is organized as follows: in Section _ref_, a review of the related work in visual attributes, curriculum learning, and transfer learning is presented. Section _ref_ presents CILICIA, the proposed curriculum learning approach for multi-task classification of clusters of visual attributes. In Section _ref_, experimental results are reported, a detailed analysis of covariates is provided, and a discussion about the performance and the limitations of the proposed approach is offered. Finally, conclusions are drawn in Section _ref_ .