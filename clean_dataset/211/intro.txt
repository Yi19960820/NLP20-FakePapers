Single image depth estimation is an important task as it enables a variety of computer vision and graphics applications, such as ND scene reconstructions~ _cite_, depth-aware image re-rendering~ _cite_, and image refocus~ _cite_ \etc. It is given only a single RGB image as the input, however, aiming to estimate a depth map as output. In particular, indoor scenes have complex textures and structural variations which make it more difficult. Traditional methods use hand-crafted priors such as Karsch \etal~ _cite_ who use a depth transfer based on SIFT Flow~ _cite_ and Liu \etal~ _cite_ who combined predicted semantic information with depth features. Recently, deep learning based methods have shown superior performance by directly learning from a large amount of data ~ _cite_ . A high-quality depth map is expected to be both accurate and detailed. Existing deep neural network based methods have mostly focused on accuracy, but do not pay much attention to the important details~ _cite_ . An example is shown in Fig.~ _ref_, while both Xu \etal and our method have comparable accuracy; only our method preserves the structural detail and produces reasonable results in ND reconstruction. Other existing methods suffer from the same problem~ _cite_ . The loss of the detail comes from the fact that CNN use an intensive number of strided convolutions and spatial poolings; it reduces the resolution and loses the accurate location information. Leading to a smoothed and low-resolution output. Long \etal~ _cite_ and Ronneberger \etal~ _cite_ have proposed methods to improve the resolution of feature maps. However, the detailed edges still fail to align with the image. To preserve structural details and produce a sharp output, we propose two network modules. The first is Dense Feature Extractor (DFE) which combines ResNet~ _cite_ with dilated convolution. DFE uses dilated convolution to extract multi-scale features from an image while keeping the feature maps dense. We also introduce a Depth Map Generator (DMG) module which uses attention mechanism to regress the feature maps to depth in which we can allocate compute resources toward the most informative parts of an input signal according to the context. By combining the proposed DFE and DMG, our network can extract multi-scaled dense while informative features and fuse them effectively. Quantitative experiments on NYU Depth VN dataset~ _cite_ show our method is competitive with the state-of-the-art. Moreover, projected to point cloud and bokeh generation show that our method can preserve better structural details of the scene depth. We summarize our contributions as follows: