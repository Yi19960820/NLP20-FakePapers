Although there has been a large amount of progress in human activity recognition research in the past years _cite_, most of the existing works assume that region-of-interest (ROI) in videos are large enough. The assumption is that each video region corresponding to an activity has a high enough resolution, allowing the recognition model to capture detailed motion and appearance changes. However, there are several cases where this assumption does not hold. For instance, in far-field recognition scenarios (i.e., detecting human activities at a distance), humans are usually very far away from the camera and each ROI often has just a few pixels within. This happens commonly in visual surveillance cameras _cite_, required to cover a large area while having a low native resolution due to their cost. Furthermore, there are situations where one wants to intentionally avoid taking high-resolution (HR) videos because of a privacy concern. High resolution cameras including robot cameras and wearable cameras are becoming increasingly available at both public and private places, and we are afraid of them recording privacy-sensitive videos of us without consent. For example, if such camera system at home (for home security or smart home services) is cracked by a hacker, there is a risk of one's N/N private life being monitored/recorded by someone else. The paradigm of using extreme low resolution (e.g., NxN) anonymized videos for activity recognition is able to address such societal concern of unwanted video taking at the fundamental-level. Human faces in extreme LR videos are not identifiable (e.g., they are much smaller than NxN), naturally prohibiting the recognition process from accessing privacy-sensitive face information. This allows designing the device (e.g., a robot) that does not record HR videos while still recognizing what is going on around it for its operation. Although extreme low resolution videos are not the only privacy-preserving data (e.g., super-pixeled frames could also be privacy-preserving), they probably are the most computation (and hardware) efficient data to obtain/process and a number of recent research _cite_ studied such direction. Motivated by such demands, there were several previous studies on extreme low resolution object/activity recognition _cite_ . The learning in previous approaches was typically done by resizing each original high resolution training sample to a LR sample and using it as a training data. On the other hand, although the recognition methods are required to only use extreme low resolution data in the testing phase, it is a realistic assumption to use publicly available HR data (e.g., YouTube videos) for their learning in the training phase. Several previous works took such direction/assumption _cite_ for the better LR recognition and obtained promising results. However, most of the previous works were limited in the aspect that they seldom considered the intrinsic property of low resolution sensors: In LR images, due to the inherent limitation what a single pixel can capture from the scene, two images originated from the exact same scene often have totally different pixel (i.e., RGB) values. Camera transformations (particularly motion transformations _cite_) such as sub-pixel translations and rotations influence the image data significantly. Figure _ref_ shows an example. Depending on the transformations, LR images from the exact same scene become different visual data. In this paper, we propose a new low resolution classification approach that explicitly takes such property into account to enable better recognition of human activities from LR videos. The idea is that multiple LR videos (e.g., Figure _ref_) corresponds to a single HR video and thus should ideally be embedded to the same representation (to be used for the classification) . That is, the intermediate representations corresponding to these LR videos should be very similar, mapping the videos to the same point in the embedding space. Once such embedding space is jointly learned with its classifier, when a new LR video is provided in the testing phase, the model can map the video to its corresponding embedding location regardless of its (unknown) LR transform. This means that the classifier becomes invariant to sub-pixel transforms (e.g., affine transforms including translation, scaling, and rotation) of the LR camera. A new multi-Siamese Convolutional Neural Network (CNN) architecture is designed to learn the optimal embeddings for LR videos. We experimentally confirm that our concept of posing an additional constraint in the representation (i.e., embedding) learning that ``LR videos corresponding to the same HR videos should be identical/similar'' obtains better performance than the conventional approach of learning a classifier with the exact same number of augmented LR training videos. Our approach jointly optimizes the video representation and the classifier for the best LR activity recognition, obtaining superior performances to prior works.