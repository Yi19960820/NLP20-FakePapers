Deep neural networks are universal approximators that can learn any deterministic mapping _inline_eq_ given enough capacity. However, traditional neural networks are not universal approximators of conditional distributions _inline_eq_ . In the context of continuous data, neural networks with the mean squared error can be derived from maximum likelihood on a unimodal Gaussian distribution _inline_eq_ where the network _inline_eq_ predicts the expected value. Thus conventional networks could not learn output distributions with multiple modes. This kind of distribution occurs for instance when trying to predict an image _inline_eq_ based on a description _inline_eq_ . This distribution would be the set of images that fits the description-not a single image. In general, a similar situation occurs for most ill-posed or inverse problems-whenever the model does not have enough information to rule out all uncertainty over outcomes. In these situations, the unimodal prior forces the network to average the outcomes as illustrated in Figure _ref_ . This is problematic because in many cases this generates an invalid prediction and in the case of images this is exemplified by blurry average predictions. We observe that this occurs in several important applications of neural networks to the continuous domain--i.e. predicting the next frame of video or learning unsupervised models with regularized autoencoders . Stochastic feed-forward neural networks (SFNN) solve this problem with the introduction of stochastic latent variables to the network. The model can be seen as a mixture of neural networks where each configuration of stochastic variables defines a different neural network. This is efficiently achieved by sharing most of the parameters between configurations. While conventional neural networks fit a single conditional Gaussian to the data, the stochastic latent variables lead to fitting a mixture of conditional Gaussians. This a powerful extension since mixture of Gaussians are universal approximators of distributions . The network can model multi-modal distributions by learning a different network for each mode. _cite_ proposes training Sigmoid Belief Networks (SBN) which have only binary stochastic units. The resulting model makes piecewise-constant MAP predictions and thus is unsuitable for continuous problems--it cannot vary smoothly with the input, see Section~ _ref_ . _cite_ addresses this limitation with the addition of deterministic sigmoid units to each layer of the network. This yields a mixture of non-linear neural networks gated by a stochastic non-linear neural network. _cite_ showed improved results with this model but the training of the latent stochastic units with a high variance variational bound was a challenge. _cite_ suggested to avoid training the latent units, relying only on layers of deterministic units to shape the random distribution. This modification did not however eliminate a fundamental limitation of stochastic networks in which stochastic and deterministic units interact additively. In these networks, the gradients of the weights tied to deterministic units have much lower variance than those tied to stochastic units, which means it is harder assign credit to stochastic units and training prefers configuration using the deterministic ones as much as possible. In this paper, we propose a new class of stochastic networks called linearizing belief nets (LBN) that can learn any continuous distribution _inline_eq_ . This model combines deterministic variables with non-deterministic binary variables in a multiplicative fashion. This approach allows using linear deterministic units without losing modeling power. These linear units can be thought of as multiplicative skip connections that allows the gradient to flow without diffusion through deep networks~ . Furthermore, multiplicative interactions allow making tie-breaking choices which would be difficult to emulate with addition. Our experiments on facial expressions confirm that the model can successfully learn multimodal distributions. We demonstrate with image denoising that the model can attain state-of-the-art results modeling natural images-converging faster than even deterministic ReLU networks in some cases.