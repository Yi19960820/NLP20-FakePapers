In this paper, we propose Perceptual Adversarial Networks (PAN) for image-to-image transformations. Different from existing application driven algorithms, PAN provides a generic framework of learning to map from input images to desired images (Fig.~ _ref_), such as a rainy image to its de-rained counterpart, object edges to photos, semantic labels to a scenes image, ~ \etc. The proposed PAN consists of two feed-forward convolutional neural networks (CNNs): the image transformation network _inline_eq_ and the discriminative network _inline_eq_ . Besides the generative adversarial loss widely used in GANs, we propose the perceptual adversarial loss, which undergoes an adversarial training process between the image transformation network _inline_eq_ and the hidden layers of the discriminative network _inline_eq_ . The hidden layers and the output of the discriminative network _inline_eq_ are upgraded to constantly and automatically discover the discrepancy between the transformed image and the corresponding ground-truth, while the image transformation network _inline_eq_ is trained to minimize the discrepancy explored by the discriminative network _inline_eq_ . Through integrating the generative adversarial loss and the perceptual adversarial loss, _inline_eq_ and _inline_eq_ can be trained alternately to solve image-to-image transformation tasks. Experiments evaluated on several image-to-image transformation tasks (\eg, image de-raining, image inpainting, \etc) demonstrate the effectiveness of the proposed PAN and its advantages over many existing works.