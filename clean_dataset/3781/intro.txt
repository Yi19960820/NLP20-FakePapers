Deep convolutional neural networks (CNNs) _cite_ have achieved high performance in various tasks, such as image recognition _cite_, speech recognition _cite_, and sound recognition _cite_ . One of the biggest themes of research on image recognition has been network engineering. Many types of image recognition networks have been proposed mainly in ILSVRC competition _cite_ . Furthermore, training deep neural networks is difficult, and many techniques have been proposed to achieve a high performance: data augmentation techniques _cite_, new network layers such as dropout _cite_ and batch normalization _cite_, optimizers such as Adam _cite_, and so on. Thanks to these research studies, training deep neural networks has become relatively easy, with a stable performance, at least for image classification. At present, a novel approach is needed for further improvement. In _cite_, a simple and powerful learning method named {\it Between-Class learning (BC learning)} was proposed for deep sound recognition. BC learning aims to learn a classification problem by solving the problem of predicting the mixing ratio between two different classes. They generated between-class examples by mixing two sounds belonging to different classes with a random ratio. They then input the mixed sound to the model and trained the model to output the mixing ratio of each class. The advantages of BC learning are not limited only to the increase in variation of the training data. They argued that BC learning has the ability to impose constraints on the feature distributions, which cannot be achieved with standard learning, and thus the generalization ability is improved. They carefully designed the method of mixing two sounds, considering the difference in the sound energies, to achieve a satisfactory performance. As a result, BC learning improved the performance on various sound recognition networks, datasets, and data augmentation schemes, and they achieved a performance surpasses the human level in sound classification tasks. The question here is whether BC learning also performs well on images. The core idea of BC learning itself, i.e., mixing two examples and training the model to output the mixing ratio, can be used irrespective of the modality of input data. BC learning is applicable to sounds, because sound is a kind of wave motion and a mixture of multiple sound data still counts as a sound. However, an image is not a kind of wave motion, and mixing multiple image data does not visually make sense. We show an example of a mixed image in Fig.~ _ref_ (top) . A mixed image loses its objectness and does not count as an image. Therefore, it appears inappropriate to apply BC learning to images. However, the important thing is not how {\it humans} perceive the mixed data, but how {\it machines} perceive them. We argue that CNNs have an aspect of treating input data as waveforms, considering the recent studies on speech and sound recognition and the characteristics of image data as pixel values. We assume that CNNs recognize images by treating them as waveforms in quite a similar manner to how they recognize sounds. Thus, a mixture of two images is a mixture of two waveforms as shown in Fig.~ _ref_ (bottom), and it would make sense {\it for machines} . Therefore, what is effective for sounds would also be effective for images. We thus propose BC learning for images in this paper. First, we propose the simplest mixing method using internal divisions. Surprisingly, this mixing method proves to perform well. Second, we also propose an improved mixing method that treats images as waveform data (BC +) . In this method, we subtract the per-image mean value from each image. By doing this, we can treat each image as a zero-mean waveform similar to a sound. We then define the image energy as the standard deviation per image, and mix two images considering the image energy in quite a similar manner to what _cite_ did for sounds. This mixing method is also simple and easy to implement, and leads to a further improvement in performance. Experimental results show that BC learning also improves the performance of various types of image recognition networks from a simple network to the state-of-the-art networks. The top-_inline_eq_ error of ResNeXt-N (_inline_eq_ d) _cite_ on ImageNet-NK is improved from _inline_eq_ to _inline_eq_ by using the simplest BC learning. Moreover, the error rate of the state-of-the-art Shake-Shake Regularization _cite_ on CIFAR-N dataset is improved from _inline_eq_ to _inline_eq_ by using the improved BC learning (BC +) . Finally, we visualize the learned features and show that BC learning indeed imposes a constraint on the feature distribution. The contributions of this paper are as follows: The remainder of this paper is organized as follows. In Section _ref_, we provide a summary of BC learning for sound recognition _cite_ as a related work. We then propose BC learning for image recognition in Section _ref_, explaining the relationship with BC learning for sounds. In Section _ref_, we compare the performance of standard learning and BC learning, and demonstrate the effectiveness of BC learning. Finally, we conclude this paper in Section _ref_ .