We focus on ND object detection, which is a fundamental computer vision problem impacting most autonomous robotics systems including self-driving cars and drones. The goal of ND object detection is to recover the N DoF pose and the ND bounding box dimensions for all objects of interest in the scene. While recent advances in convolutional neural networks have enabled accurate ND detection in complex environments~ _cite_, the ND object detection problem still remains an open challenge. Methods for ND box regression from a single image, even including recent deep learning methods such as ~ _cite_, still have relatively low accuracy especially in depth estimates at longer ranges. Hence, many current real-world systems either use stereo or augment their sensor stack with lidar and radar. The lidar-radar mixed-sensor setup is particularly popular in self-driving cars and is typically handled by a multi-stage pipeline, which preprocesses each sensor modality separately and then performs a late fusion or decision-level fusion step using an expert-designed tracking system such as a Kalman filter~ _cite_ . Such systems make simplifying assumptions and make decisions in the absence of context from other sensors. Inspired by the successes of deep learning for handling diverse raw sensory input, we propose an early fusion model for ND box estimation, which directly learns to combine image and depth information optimally. Various combinations of cameras and ND sensors are widely used in the field, and it is desirable to have a single algorithm that generalizes to as many different problem settings as possible. Many real-world robotic systems are equipped with multiple ND sensors: for example, autonomous cars often have multiple lidars and potentially also radars. Yet, current algorithms often assume a single RGB-D camera~ _cite_, which provides RGB-D images, or a single lidar sensor~ _cite_, which allows the creation of a local front view image of the lidar depth and intensity readings. Many existing algorithms also make strong domain-specific assumptions. For example, MVND~ _cite_ assumes that all objects can be segmented in a top-down ND view of the point cloud, which works for the common self-driving case but does not generalize to indoor scenes where objects can be placed on top of each other. Furthermore, the top-down view approach tends to work well for objects such as cars, but does not for other key object classes such as pedestrians or cyclists . Unlike the above approaches, our architecture is designed to be domain-agnostic and agnostic to the placement, type, and number of ND sensors. As such, it is generic and can be used for a variety of robotics applications. In designing such a generic model, we need to solve the challenge of combining the heterogeneous image and ND point cloud data. Previous work addresses this challenge by directly transforming the point cloud to a convolution-friendly form. This includes either projecting the point cloud onto the image~ _cite_ or voxelizing the point cloud~ _cite_ . Both of these operations involve lossy data quantization and require special models to handle sparsity in the lidar image~ _cite_ or in voxel space~ _cite_ . Instead, our solution retains the inputs in their native representation and processes them using heterogeneous network architectures. Specifically for the point cloud, we use a variant of the recently proposed PointNet~ _cite_ architecture, which allows us to process the raw points directly. Our deep network for ND object box regression from images and sparse point clouds has three main components: an off-the-shelf CNN~ _cite_ that extracts appearance and geometry features from input RGB image crops, a variant of PointNet~ _cite_ that processes the raw ND point cloud, and a fusion sub-network that combines the two outputs to predict ND bounding boxes. This heterogeneous network architecture, as shown in Fig.~ _ref_, takes full advantage of the two data sources without introducing any data processing biases. Our fusion sub-network features a novel dense ND box prediction architecture, in which for each input ND point, the network predicts the corner locations of a ND box relative to the point. The network then uses a learned scoring function to select the best prediction. The method is inspired by the concept of spatial anchors~ _cite_ and dense prediction~ _cite_ . The intuition is that predicting relative spatial locations using input ND points as anchors reduces the variance of the regression objective comparing to an architecture that directly regresses the ND location of each corner. We demonstrate that the dense prediction architecture outperforms the architecture that regresses ND corner locations directly by a large margin. We evaluate our model on two distinctive ND object detection datasets. The KITTI dataset~ _cite_ focuses on the outdoor urban driving scenario in which pedestrians, cyclists, and cars are annotated in data acquired with a camera-lidar system. The SUN-RGBD dataset~ _cite_ is recorded via RGB-D cameras in indoor environments, with more than N object categories. We show that by combining PointFusion with an off-the-shelf ND object detector~ _cite_, we get comparable or better ND object detections than the state-of-the-art methods designed for KITTI~ _cite_ and SUN-RGBD~ _cite_ . To the best of our knowledge, our model is the first to achieve competitive results on these very different datasets, proving its general applicability.