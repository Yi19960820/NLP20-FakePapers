action recognition is an important problem in computer vision due to its wide applications in video surveillance, human computer interfaces, robotics, etc. Despite significant research efforts over the past few decades~ _cite_, accurate recognition of human actions from RGB video sequences is still an unsolved problem. With the advent of easy-to-use and low-cost depth sensors such as {MS} Kinect sensors, human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention and many applications have been developed~ _cite_ in recent years, due to the advantages of depth information over conventional RGB video, e.g. being insensitive to illumination changes and reliable to estimate body silhouette and skeleton~ _cite_ . Since the first work~ _cite_ reported in N, many methods~ _cite_ have been proposed using specifically hand-crafted feature descriptors extracted from depth. As the extraction of skeletons from depth maps~ _cite_ has become increasingly robust, more and more hand-designed skeleton features~ _cite_ have been devised to capture spatial configuration, and Dynamic Time Warpings (DTWs), Fourier Temporal Pyramid (FTP) or Hidden Markov Models (HMMs) are employed to model temporal information. However, these hand-crafted features are always shallow and dataset-dependent. Recently, Recurrent Neural Networks (RNNs) ~ _cite_ have also been adopted for action recognition from skeleton data. RNNs tend to overemphasize the temporal information especially when the training data is not sufficient, leading to overfitting. Up to date, it remains unclear how skeleton sequences could be effectively represented and fed to deep neural networks for recognition. For example, one can conventionally consider a skeleton sequence as a set of individual frames with some form of temporal smoothness, or as a subspace of poses or pose features, or as the output of a neural network encoder. Which one among these and other possibilities would result in the best representation in the context of action recognition is not well understood. In this paper, we present an effective yet simple method that represent both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, referred to as Joint Trajectory Maps (JTMs), as the input of ConvNets for action recognition. Such image-based representation enables us to fine-tune existing ConvNets models trained on ImageNet for classification of skeleton sequences without training the whole deep networks afresh. The three JTMs are complimentary to each other, and the final recognition accuracy is improved largely by a late score fusion method. One of the challenges in action recognition is how to properly model and use the spatio-temporal information. The commonly used bag-of-words model often ignores spatial information. On the other hand, HMMs or RNNs based methods are likely to overstress the temporal information. The proposed method addresses this challenge in a novel way by encoding as much the spatio-temporal information as possible (without a need to decide which one is important and how important it is) into images, and employing ConvNets to learn the discriminative one. Consequently, the proposed method outperformed the start-of-the-art methods on popular benchmark datasets. The main contributions of this paper include: This paper is an extension of the works presented in~ _cite_ . Unlike~ _cite_ where skeletons are assumed to have been sufficiently sampled and discrete joints are drawn onto images using a pen whose size is properly set, this paper employs joint trajectories and proposes to rotate skeletons to mimic multiple views for cross-view action recognition and data augmentation. In addition, this paper adopts multiply score fusion to improve the final recognition accuracy. Extensive experiments and detailed analysis are also presented in this paper. The rest of this paper is organized as follows. An overview of related works is given in Section~ _ref_ . Details of the proposed method are described in Section~ _ref_ . Evaluation of the proposed method on four datasets and analysis of the results are reported in Section~ _ref_ . Section~ _ref_ concludes the paper with remarks.