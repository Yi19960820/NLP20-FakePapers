Vision is one of the most promising domains for unsupervised learning. Unlabeled images and video are available in practically unlimited quantities, and the most prominent present image models---neural networks---are data starved, easily memorizing even random labels for large image collections~ _cite_ . Yet unsupervised algorithms are still not very effective for training neural networks: they fail to adequately capture the visual semantics needed to solve real-world tasks like object detection or geometry estimation the way strongly-supervised methods do. For most vision problems, the current state-of-the-art approach begins by training a neural network on ImageNet~ _cite_ or a similarly large dataset which has been hand-annotated. How might we better train neural networks without manual labeling? Neural networks are generally trained via backpropagation on some objective function. Without labels, however, what objective function can measure how good the network is? Self-supervised learning answers this question by proposing various tasks for networks to solve, where performance is easy to measure, i.e., performance can be captured with an objective function like those seen in supervised learning. Ideally, these tasks will be difficult to solve without understanding some form of image semantics, yet any labels necessary to formulate the objective function can be obtained automatically. In the last few years, a considerable number of such tasks have been proposed~ _cite_, such as asking a neural network to colorize grayscale images, fill in image holes, solve jigsaw puzzles made from image patches, or predict movement in videos. Neural networks pre-trained with these tasks can be re-trained to perform well on standard vision tasks (e.g. \image classification, object detection, geometry estimation) with less manually-labeled data than networks which are initialized randomly. However, they still perform worse in this setting than networks pre-trained on ImageNet. This paper advances self-supervision first by implementing four self-supervision tasks and comparing their performance using three evaluation measures. The self-supervised tasks are: relative position~ _cite_, colorization~ _cite_, the ``exemplar" task~ _cite_, and motion segmentation~ _cite_ (described in section~ _ref_) . The evaluation measures (section~ _ref_) assess a diverse set of applications that are standard for this area, including ImageNet image classification, object category detection on PASCAL VOC N, and depth prediction on NYU vN. Second, we evaluate if performance can be boosted by combining these tasks to simultaneously train a single trunk network. Combining the tasks fairly in a multi-task learning objective is challenging since the tasks learn at different rates, and we discuss how we handle this problem in section~ _ref_ . We find that multiple tasks work better than one, and explore which combinations give the largest boost. Third, we identify two reasons why a na {\"i} ve combination of self-supervision tasks might conflict, impeding performance: input channels can conflict, and learning tasks can conflict. The first sort of conflict might occur when jointly training colorization and exemplar learning: colorization receives grayscale images as input, while exemplar learning receives all color channels. This puts an unnecessary burden on low-level feature detectors that must operate across domains. The second sort of conflict might happen when one task learns semantic categorization (i.e. generalizing across instances of a class) and another learns instance matching (which should not generalize within a class) . We resolve the first conflict via ``input harmonization'', i.e. \modifying network inputs so different tasks get more similar inputs. For the second conflict, we extend our mutli-task learning architecture with a lasso-regularized combination of features from different layers, which encourages the network to separate features that are useful for different tasks. These architectures are described in section~ _ref_ . We use a common deep network across all experiments, a ResNet-N-vN, so that we can compare various diverse self-supervision tasks apples-to-apples. This comparison is the first of its kind. Previous work applied self-supervision tasks over a variety of CNN architectures (usually relatively shallow), and often evaluated the representations on different tasks; and even where the evaluation tasks are the same, there are often differences in the fine-tuning algorithms. Consequently, it has not been possible to compare the performance of different self-supervision tasks across papers. Carrying out multiple fair comparisons, together with the implementation of the self-supervised tasks, joint training, evaluations, and optimization of a large network for several large datasets has been a significant engineering challenge. We describe how we carried out the large scale training efficiently in a distributed manner in section~ _ref_ . This is another contribution of the paper. As shown in the experiments of section~ _ref_, by combining multiple self-supervision tasks we are able to close further the gap between self-supervised and fully supervised pre-training over all three evaluation measures. Self-supervision tasks for deep learning generally involve taking a complex signal, hiding part of it from the network, and then asking the network to fill in the missing information. The tasks can broadly be divided into those that use auxiliary information or those that only use raw pixels. Tasks that use auxiliary information such as multi-modal information beyond pixels include: predicting sound given videos~ _cite_, predicting camera motion given two images of the same scene~ _cite_, or predicting what robotic motion caused a change in a scene~ _cite_ . However, non-visual information can be difficult to obtain: estimating motion requires IMU measurements, running robots is still expensive, and sound is complex and difficult to evaluate quantitatively. Thus, many works use raw pixels. In videos, time can be a source of supervision. One can simply predict future~ _cite_, although such predictions may be difficult to evaluate. One way to simplify the problem is to ask a network to temporally order a set of frames sampled from a video~ _cite_ . Another is to note that objects generally appear across many frames: thus, we can train features to remain invariant as a video progresses~ _cite_ . Finally, motion cues can separate foreground objects from background. Neural networks can be asked to re-produce these motion-based boundaries without seeing motion~ _cite_ . Self-supervised learning can also work with a single image. One can hide a part of the image and ask the network to make predictions about the hidden part. The network can be tasked with generating pixels, either by filling in holes~ _cite_, or recovering color after images have been converted to grayscale~ _cite_ . Again, evaluating the quality of generated pixels is difficult. To simplify the task, one can extract multiple patches at random from an image, and then ask the network to position the patches relative to each other~ _cite_ . Finally, one can form a surrogate ``class'' by taking a single image and altering it many times via translations, rotations, and color shifts~ _cite_, to create a synthetic categorization problem. Our work is also related to multi-task learning. Several recent works have trained deep visual representations using multiple tasks~ _cite_, including one work~ _cite_ which combines no less than N tasks. Usually the goal is to create a single representation that works well for every task, and perhaps share knowledge between tasks. Surprisingly, however, previous work has shown little transfer between diverse tasks. Kokkinos~ _cite_, for example, found a slight dip in performance with N tasks versus N. Note that our work is not primarily concerned with the performance on the self-supervised tasks we combine: we evaluate on a separate set of semantic ``evaluation tasks.'' Some previous self-supervised learning literature has suggested performance gains from combining self-supervised tasks~ _cite_, although these works used relatively similar tasks within relatively restricted domains where extra information was provided besides pixels. In this work, we find that pre-training on multiple diverse self-supervised tasks using only pixels yields strong performance.