research in artificial neural networks has demonstrated their ability to perform well on a wide range of tasks including image, audio, and video processing and analysis in many domains _cite_ . In some applications, deep neural networks have been shown to outperform conventional machine learning methods and even human experts _cite_ . Most of the modern neural network architectures for computer vision include convolutional layers and thus are called convolutional neural networks (CNNs) . They have high computational requirements such that even modern central processing units (CPUs) are often not fast enough and specialized hardware, such as graphics processing units (GPUs), is needed _cite_ . However, there a compelling need for the use of deep convolutional neural networks on mobile devices and in embedded systems. This is particularly important for video processing in, for example, autonomous cars and medical devices _cite_, which demand capabilities of high-accuracy and real-time object recognition. Following properties of many modern high-performing CNN architectures make their hardware implementation feasible: Due to high regularity, size of the network can be easily varied, for example, by changing the number of convolutional blocks. In the case of field programmable gate arrays (FPGAs), this allows to program the network on different types of FPGAs, providing different processing speed. For example, implementation of higher number of convolutional blocks on an FPGA can directly lead to a speed-up in processing. Related direction in neural network research considers adapting them for the use on mobile devices, for example, see MobileNet _cite_ and SqueezeNet _cite_ . Mobile networks typically have reduced number of weights and require relatively small number of arithmetic operations. However, they are still executed at the software level and use floating-point calculations. For some tasks such as real-time video analysis that requires processing of N frames per second mobile networks still can be not fast enough without further optimization. In order to use an already trained neural network in a mobile device, a set of optimizations can be used to speed up computation. There exist a number of approaches to do so, including weight compression _cite_ or computation using low-bit data representations _cite_ . Since hardware requirements for neural networks keep increasing, there is a need for design and development of specialized hardware block for the use in ASIC and FPGA. The speed up can be achieved by following: For example, Qiu J. et al. _cite_ proposed an FPGA implementation of pre-trained deep neural networks from VGG-family _cite_ . They used dynamic-precision quantization with N \/N-bit data representation and singular vector decomposition to reduce the size of fully-connected layers, which led to smaller number of weights that had to be passed from the device the external memory. Zhang C. et al. _cite_ quantitatively analyzed computing throughput and required memory bandwidth for various CNNs using optimization techniques, such as loop tiling and transformation. This allowed their implementation to achieve a peak performance of N GFLOPS. Related approach is suggested in _cite_, which allowed to reduce power consumption by the compression of network weights. Higher level solution is proposed in _cite_, which considers the use of the OpenGL compiler for deep networks, such as AlexNet and VGG. Duarte et al. _cite_ have recently suggested the protocol for automatic conversion of neural network implementations in high-level programming language to intermediate format (HLS) and then into FPGA implementation. However, their work is mostly focused on the implementation of fully-connected layers. In this work we propose a design and implementation of FPGA-based CNN with fixed-point calculations that allows to achieve the exact performance of the corresponding software implementation on the live handwritten digit recognition problem. Due to the reduced number of parameters we avoid common issues with memory bandwidth. Suggested method can be implemented on a very basic FPGAs, but also is scalable for the use on FPGAs with large number of logical cells. Additionally, we demonstrate how existing open datasets can be modified in order to better adapt them for real-life applications. Finally, in order to promote the reproducibility of results, facilitate open-scientific development, and enable collaborative validation we make our source code, documentation, and all results from this study available online.