Deep Convolutional Neural Networks (DCNNs) have achieved remarkable success in various cognitive applications, particularly in computer vision _cite_ . They have shown human like performance on a variety of recognition, classification and inference tasks, albeit at a much higher energy consumption. One of the major challenges for convolutional networks is the computational complexity and the time needed to train large networks. Since training of DCNNs requires state-of-the-art accelerators like GPUs ~ _cite_, large training overhead has restricted the usage of DCNNs to clouds and servers. It is common to pre-train a DCNN on a large dataset (e.g. ImageNet, which contains N million images with N categories), and then use the trained network either as an initialization or a fixed feature extractor for the specific application ~ _cite_ . A major downside of such DCNNs is the inability to learn new information since the learning process is static and only done once before it is exposed to practical applications. In real-world scenarios, classes and their associated labeled data are always collected in an incremental manner. To ensure applicability of DCNNs in such cases, the learning process needs to be continuous. However, retraining these large networks using both previously seen and unseen data to accommodate new data, is not feasible most of the time. The training samples for already learned classes may be proprietary, or simply too cumbersome to use in training a new task. Also, to ensure data privacy, training samples should be discarded after use. Incremental learning plays a critical role in alleviating these issues by ensuring continuity in the learning process through regular model update based only on the new available batch of data. Nevertheless, incremental learning can be computationally expensive and time consuming, if the network is large enough. \par This paper focuses on incremental learning on deep convolutional neural network (DCNN) for image classification task. In doing so, we attempt to address the more fundamental issue: an efficient learning system must deal with new knowledge that it is exposed to, as humans do. To achieve this goal, there are two major challenges. First, as new data becomes available, we should not start learning from scratch. Rather, we leverage what we have already learned and combine them with new knowledge in a continuous manner. Second, to accommodate new data, if there is a need to increase the capacity of our network, we will have to do it in an efficient way. We would like to clarify that incremental learning is not a replacement of regular training. In the regular case, samples for all classes are available from the beginning of training. However, in incremental learning, sample data corresponding to new tasks become available after the base network is already trained and sample data for already learned task are no longer available for retraining the network to learn all tasks (old and new) simultaneously. Our approach to incremental learning is similar to transfer learning ~ _cite_ and domain adaptation methods ~ _cite_ . Transfer learning utilizes knowledge acquired from one task assisting to learn another. Domain adaptation transfers the knowledge acquired for a task from a dataset to another (related) dataset. These paradigms are very popular in computer vision. Though incremental learning is similar in spirit to transfer, multi-task, and lifelong learning; so far, no work has provided a perfect solution to the problem of continuously adding new tasks based on adapting shared parameters without access to training data for previously learned tasks. \par There have been several prior works on incremental learning of neural networks. Many of them focus on learning new tasks from fewer samples _cite_ utilizing transfer learning techniques. To avoid learning new categories from scratch, Fei-Fei et al. _cite_ proposed a Bayesian transfer learning method using very few training samples. By introducing attribute-based classification the authors _cite_ achieved zero-shot learning (learning a new class from zero labeled samples) . These works rely on shallow models instead of DCNN, and the category size is small in comparison. The challenge of applying incremental learning (transfer learning as well) on DCNN lies in the fact that it consists of both feature extractor and classifier in one architecture. Polikar et al. _cite_ utilized ensemble of classifiers by generating multiple hypotheses using training data sampled according to carefully tailored distributions. The outputs of the resulting classifiers are combined using a weighted majority voting procedure. This method can handle an increasing number of classes, but needs training data for all classes to occur repeatedly. Inspired form Polikar et al. _cite_, Medera and Babinec ~ _cite_ utilized ensemble of modified convolutional neural networks as classifiers by generating multiple hypotheses. The existing classifiers are improved in ~ _cite_ by combining new hypothesis generated from newly available examples without compromising classification performance on old data. The new data in ~ _cite_ may or may not contain new classes. Another method by Royer and Lampert ~ _cite_ can adapt classifiers to a time-varying data stream. However, the method is unable to handle new classes. Pentina et al. ~ _cite_ have shown that learning multiple tasks sequentially can improve classification accuracy. Unfortunately, for choosing the sequence, the data for all tasks must be available to begin with. Xiao et al. ~ _cite_ proposed a training algorithm that grows a network not only incrementally but also hierarchically. In this tree-structured model, classes are grouped according to similarities, and self-organized into different levels of the hierarchy. All new networks are cloned from existing ones and therefore inherit learned features. These new networks are fully retrained and connected to base network. The problem with this method is the increase of hierarchical levels as new set of classes are added over time. Another hierarchical approach was proposed in Roy et al. _cite_ where the network grows in a tree-like manner to accommodate the new classes. However, in this approach, the root node of the tree structure is retrained with all training samples (old and new classes) during growing the network. \par Li and Hoiem ~ _cite_ proposed `Learning without Forgetting' (LwF) to incrementally train a single network to learn multiple tasks. Using only examples for the new task, the authors optimize both for high accuracy for the new task and for preservation of responses on the existing tasks from the original network. Though only the new examples were used for training, the whole network must be retrained every time a new task needs to be learned. Recently, Rebuffi et al. ~ _cite_ addressed some of the drawbacks in ~ _cite_ with their decoupled classifier and representation learning approach. However, they rely on a subset of the original training data to preserve the performance on the old classes. Shmelkov et al. ~ _cite_ proposed a solution by forming a loss function to balance the interplay between predictions on the new classes and a new distillation loss which minimizes the discrepancy between responses for old classes from the original and the updated networks. This method can be performed multiple times, for a new set of classes in each step. However, every time it incurs a moderate drop in performance compared to the baseline network trained on the ensemble of data. Also, the whole process has substantial overhead in terms of compute energy and memory. \par Another way to accommodate new classes is growing the capacity of the network with new layers ~ _cite_, selectively applying strong per-parameter regularization ~ _cite_ . The drawbacks to these methods are the rapid increase in the number of new parameters to be learned ~ _cite_, and they are more suited to reinforcement learning ~ _cite_ . Aljundi et al. ~ _cite_ proposed a gating approach to select the model that can provide the best performance for the current task. It introduces a set of gating auto-encoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This method performs very well on image classification and video prediction problems. However, the training of autoencoders for each task requires significant effort. Incremental learning is also explored in Spiking Neural Networks (SNN) domain. An unsupervised learning mechanism is proposed by Panda et al. _cite_ for improved recognition with SNNs for on-line learning in a dynamic environment. This mechanism helps in gradual forgetting of insignificant data while retaining significant, yet old, information thus trying to addresses catastrophic forgetting. \par In the context of incremental learning, most work has focused on how to exploit knowledge from previous tasks and transfer it to a new task. Little attention has gone to the related and equally important problem of hardware and energy requirements for model update. Our work differs in goal, as we want to grow a DCNN with reduced effort to accommodate new tasks (sets of classes) by network sharing, without forgetting the old tasks (sets of classes) . The key idea of this work is the unique `clone-and-branch' technique which allows the network to learn new tasks one after another without any performance loss in old tasks. Cloning layers provides a good starting point for learning a new task compared to randomly initialized weights. The kernels learn quickly, and training converges faster. It allows us to employ fine-tuning in the new branch, saving training energy and time compared to training from scratch. On the other hand, branching allows the network to remember task specific weight parameters, hence the network does not forget old tasks (in task specific scenario) no matter how many new tasks it has learned. The novelty of this work lies in the fact that we developed an empirical mechanism to identify how much of the network can be shared as new tasks are learned. We also quantified the energy consumption, training time and memory storage savings associated with models trained with different amounts of sharing to emphasize the importance of network sharing from hardware point of view. Our proposed method is unique since it does not require any algorithmic changes and can be implemented in any existing hardware if additional memory is available for the supplementary parameters needed to learn new classes. There is no overhead of storing any data sample or statistical information of the learned classes. It also allows on-chip model update using a programmable instruction cache. Many of the state-of-the-art DNN accelerators support this feature. However, FPGAs are the kind of hardware architecture that is best suited for the proposed method. It offers highly flexible micro-architecture with reusable functional modules and additional memory blocks in order to account for dynamic changes. \par In summary, the key contributions of our work are as follows: We show that our proposed methodology leads to energy efficiency, reduction in storage requirements, memory access and training time, while maintaining classification accuracy without accessing training samples of old tasks.