In remote sensing, the satellite-or aircraft-based sensor technologies are used to capture and detect objects on Earth. Thanks to various propagated signals (e.g., electromagnetic radiation), remote sensing makes the data collection from dangerous or inaccessible areas possible, and therefore plays a significant role in many applications including monitoring, military information collection and land-use classification _cite_ . With the technological development of various satellite sensors, the volume of high-resolution remote sensing image data is increasing rapidly. Hence, proper compression of the satellite image becomes essential, which enables information exchange much more efficient, given a limited band width. Existing compression methods generally fall into two categories: lossless (e.g., PNG) and lossy (e.g., JPEG) _cite_ . The lossless methods usually provide better visual experience to users, but lossy methods often achieve higher compression ratios via non-invertible compression functions along with trade-off parameters to balance the data amount and the decompressed quality. Therefore the lossy compression schemes are always preferred by consumer devices in practice due to higher compression rate _cite_ . However, high compression rate comes with the cost of having compression artifacts on the decoded image, which is a barrier for many applications, such as image analysis. Therefore, there is a clear need for compression artifact reduction, which is able to gain visual quality of the decompressed image, which can influence the visual effect and low-level vision processing _cite_ . The compression artifacts are in relation to the schemes used for compression. Take JPEG compression as an example, blocking artifacts are caused by discontinuities at the borders when encoding adjacent _inline_eq_ pixel blocks, which are in the form of ringing effects and blurring due to the coarse quantization of the high frequency components. To deal with these compression artifacts, an improved version of JPEG, named JPEG N, is proposed, which adopts the wavelet transform to avoid blocking artifacts, but still undergoes ringing effects and blurring. As an excellent alternative, SPIHT _cite_ showed that using simple uniform scalar quantization, rather than complicated vector quantization, also yields superior results. Due to its simplicity, SPIHT has been successful on natural (portraits, landscape, weddings, etc.) and medical (X-ray, CT, etc.) images. Furthermore, its embedded encoding process has proved to be effective in a broad range of reconstruction qualities. For instance, it can code fair-quality portraits and high-quality medical images equally well (as compared with other methods in the same conditions) . However, in the field of remote sensing, the images usually suffer from severe artifacts after compression as shown in Fig.~ _ref_, which poses challenges to many high-level vision tasks, such as object detection _cite_, classification _cite_, and anomaly detection _cite_ . To cope with various compression artifacts, many conventional approaches have been proposed, such as filtering approaches _cite_, _cite_, _cite_, specific priors (e.g., the quantization table in DSC _cite_), and thresholding techniques _cite_ . Inspired by the great success of deep learning technology in many image processing applications, researchers start to exploit this powerful tool to reduce the compression artifact. Specifically, the Super-Resolution Convolutional Neural Network (SRCNN) _cite_ exhibits great potential of an end-to-end learning in image super-resolution. It is also pointed out that conventional sparse-coding-based image restoration model can be equally seen as a deep model. However, if we directly apply SRCNN to the compression artifact reduction task, the features extracted by its first layer are noisy, which will cause undesirable noisy patterns in reconstruction. Thus the three-layer SRCNN is not suitable for compressed image restoration, especially when dealing with complex artifacts. Thanks to transfer learning, ARCNN _cite_ has been successfully applied to image restoration tasks. However, without exploiting the multi-scale information, ARCNNs fail to solve more complicated compression artifact problems. Although many deep models with different architectures have been explored (e.g., _cite_) to solve the artifact reduction problem, there is little work incorporating different models in a unified framework to inherit their respective advantages. In this paper, a generic fusion network, dubbed as one-two-one (OTO) network, is developed for complex compression artifacts reduction. The general framework of the proposed OTO network is presented in Fig.~ _ref_ . Specifically, it consists of three sub-networks: a normal-scale network, a small-scale network with max pooling to increase the network receptive field, and a fusion network to perform principled fusion of the outputs from the summation and difference models. The summation model aggregates the low frequency information captured from different network scales, while the difference model is motivated by the Laplacian pyramid which is able to describe the high frequency information, such as detailed information. By combining the summation and difference models, both low and high frequency information of the image can be better characterized. This is motivated by the fact that adopting different schemes to process high frequency and low frequency information always benefits to low-level image processing applications, such as image denoising _cite_ and image reconstruction _cite_ . Most importantly, we provide an in-depth investigation into our OTO architecture based on the Taylor expansion, which shows that these two kinds of information are fused in a nonlinear scheme to gain more capacity to handle complicated image compression artifacts. From a theoretical perspective, this paper proposes a principled combination of different CNN models, providing the capability of coping with the extremely challenging task of the large blocking effect. Extensive experimental results verify that combining diverse models effectively boosts the performance. In a summary, we have the following contributions in this paper. For ease of explanation, we summarize main variables in Table _ref_ . The rest of the paper is organized as follows. Section _ref_ introduces the related works, and section _ref_ describes the details of the proposed method. Experiments and results are presented in section _ref_ . Finally, section _ref_ concludes the paper.