We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (\crf) and decision trees. In the literature, the potential functions of \crf are mostly defined as a combination of some pre-defined parametric models, and then methods like structured support vector machines (\ssvm) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve learning of potential functions on both unary and pairwise terms in \crf. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-N, Weizmann horse, Oxford flower) and multi-class (MSRC-N, PASCAL VOC N) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.