Deep learning has been overwhelmingly successful in a broad range of applications, such as computer vision, speech recognition / natural language processing, machine translation, bio-medical data analysis, and many more. Deep convolutional neural networks (CNN), in particular, have enjoyed huge success in tackling many computer vision problems over the past few years, thanks to the tremendous development of many effective architectures, AlexNet _cite_, VGG _cite_, Inception _cite_ and ResNet _cite_ to name a few. However, training these networks end-to-end with fully learnable convolutional kernels (as is standard practice) is (N) computationally very expensive, (N) results in large model size, both in terms of memory usage and disk space, and (N) prone to over-fitting, under limited data, due to the large number of parameters. On the other hand, there is a growing need for deploying, both for learning and inference, these systems on resource constrained platforms like, autonomous cars, robots, smart-phones, smart cameras, smart wearable devices, To address these drawbacks, several binary versions of CNNs have been proposed _cite_ that approximate the dense real-valued weights with binary weights. Binary weights bear dramatic computational savings through efficient implementations of binary convolutions. Complete binarization of CNNs, though, leads to performance loss in comparison to real-valued network weights. In this paper, we present an alternative approach to reducing the computational complexity of CNNs while performing as well as standard CNNs. We introduce the local binary convolution (LBC) layer that approximates the non-linearly activated response of a standard convolutional layer. The LBC layer comprises of fixed sparse binary filters (called anchor weights), a non-linear activation function and a set of learnable linear weights that computes weighted combinations of the activated convolutional response maps. Learning reduces to optimizing the linear weights, as opposed to optimizing the convolutional filters. Parameter savings of at least _inline_eq_ to _inline_eq_ can be realized during the learning stage depending on the spatial dimensions of the convolutional filters (_inline_eq_ to _inline_eq_ sized filters respectively), as well as computational and memory savings due to the sparse nature of the binary filters. CNNs with LBC layers, called {local binary convolutional neural networks (LBCNN)}, have much lower model complexity and are as such less prone to over-fitting and are well suited for learning and inference of CNNs in resource-constrained environments. Our theoretical analysis shows that the LBC layer is a good approximation for the non-linear activations of standard convolutional layers. We also demonstrate empirically that CNNs with LBC layers performs comparably to regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR-N, and ImageNet) while enjoying significant savings in terms of the number of parameters during training, computations, as well as memory requirements due to the sparse and pre-defined nature of our binary filters, in comparison to dense learnable real-valued filters. Related Work: The idea of using binary filters for convolutional layers is not new. BinaryConnect _cite_ has been proposed to approximate the real-valued weights in neural networks with binary weights. Given any real-valued weight, it stochastically assigns _inline_eq_ with probability _inline_eq_ that is taken from the hard sigmoid output of the real-valued weight, and _inline_eq_ with probability _inline_eq_ . Weights are only binarized during the forward and backward propagation, but not during the parameter update step, in which high-precision real-valued weights are necessary for updating the weights. Therefore, BinaryConnect alternates between binarized and real-valued weights during the network training process. Building upon BinaryConnect _cite_, binarized neural network (BNN) _cite_ and quantized neural network (QNN) _cite_ have been proposed, where both the weights and the activations are constrained to binary values. These approaches lead to drastic improvement in run-time efficiency by replacing most N-bit floating point multiply-accumulations by N-bit XNOR-count operations. Both BinaryConnect and BNN demonstrate the efficacy of binary networks on MNIST, CIFAR-N, and SVHN dataset. Recently, XNOR-Net _cite_ builds upon the design principles of BNN and proposes a scalable approach to learning binarized networks for large-scale image recognition tasks, demonstrating high performance on the ImageNet classification task. All the aforementioned approaches utilize high-precision real-valued weights during weight update, and achieve efficient implementations using XNOR bit count. XNOR-Net differs from BNN in the binarization method and the network architecture. In addition to network binarization, model compression and network quantization techniques _cite_ are another class of techniques that seek to address the computational limitations of CNNs. However, the performance of such methods are usually upper bounded by the uncompressed and unquantized models. Our proposed LBCNN is notably different from fully binarized neural networks and draws inspiration from . LBCNN, with a hybrid combination of fixed and learnable weights offers an alternate formulation of a fully learnable convolution layer. By only considering sparse and binary weights for the fixed weights, LBCNN is also able to take advantage of all the efficiencies, both statistical and computational, afforded by sparsity and weight binarization. We demonstrate, both theoretically and empirically, that LBCNN is a very good approximation of a standard learnable convolutional layer.