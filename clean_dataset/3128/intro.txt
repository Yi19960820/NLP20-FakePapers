Scene understanding started with the goal of creating systems that can infer meaningful configurations (e.g., parts, objects and their compositions with relations) from imagery like humans~ _cite_ . In computer vision research, significant progresses have been made in semantic scene labeling / segmentation (i.e., assigning the label for each pixel of the scene image) ~ _cite_ _cite_ _cite_ _cite_ . However, the problem of structured scene parsing (i.e., producing meaningful scene configurations) remains a challenge due to the following difficulties. To address these above issues, we develop a novel deep neural network architecture that automatically parses an input scene into a structured and meaningful configuration. Fig.~ _ref_ shows an illustration of our structured scene parsing, where our model identifies salient semantic objects in the scene and generates the hierarchical scene structure with the interaction relations among objects. Our model is inspired by the effectiveness of two widely successful deep learning techniques: convolutional neural networks (CNNs) ~ _cite_ _cite_ and recursive neural networks (RNNs) ~ _cite_ . The former category of models is widely applied for generating powerful feature representations in various vision tasks such as image classification and object recognition. Meanwhile, the RNN models (such as _cite_ _cite_ _cite_) have demonstrated as an effective class of models for predicting hierarchical and compositional structures in image and natural language understanding~ _cite_ . One important property of RNNs is the ability to recursively learn the representations in a semantically and structurally coherent way. In our deep CNN-RNN architecture, the CNN and RNN models are collaboratively integrated for accomplishing the scene parsing from complementary aspects. We utilize the CNN to layerwise extract features from the input scene image and generate the representations of semantic objects. Then, the RNN is sequentially stacked based on the CNN feature representations, generating the structured configuration of the scene. On the other hand, to avoid relying on the elaborative annotations, we propose to train our CNN-RNN model by leveraging the image descriptions. Our approach is partially motivated but different with the recently proposed methods for image-sentence embedding~ _cite_ _cite_ . In particular, we distill knowledge from the sentence descriptions for discovering scene structural configurations. In the initial stage, we decompose each sentence into a normalized semantic tree consisting of nouns and verb phrases by using a standard parser~ _cite_ and the WordNet _cite_ . Afterward, based on these semantic trees and their associated scene images, we train our model by developing an Expectation-Maximization method. Specifically, the semantic tree facilitates discovering the latent scene configuration in the two following aspects. i) The entities (\ie, nouns) determine the object category labels existing in the scene, and ii) the relations (\ie, verb phrases) over the entities assist to produce the scene hierarchy and object interactions. The two proportions of knowledge are incorporated into our learning objective together with the CNN and the RNN, respectively. Therefore, once the scene configuration is fixed, the parameters of the two neural networks are updated accordingly by the back propagation. The main contributions of our work are summarized as follows. i) We present a novel CNN-RNN framework for generating meaningful and hierarchical scene representations, which gains a deeper understanding of the objects in the scene compared to traditional scene labeling. The integration of CNN and RNN models is general to be extended to other high-level computer vision tasks. ii) We present a EM-type training method by leveraging text descriptions that associate with the training images. This method is cost-effective yet beneficial to introducing rich contexts and semantics. iii) Our extensive experiments on PASCAL VOC N demonstrate that the parsed scene representations are useful for scene understanding and our generated semantic segmentations are more favorable than those by other weakly-supervised scene labeling methods.