There have been many advances in deep neural network architectures in the past few years. One such improvement is a normalization technique called batch normalization _cite_ that standardizes the activations of layers inside a network using minibatch statistics. It has been shown to regularize the network as well as provide faster and more stable training. Another improvement comes from architectures that add so called shortcut paths to the network. These shortcut paths connect later layers to earlier layers typically, which allows for the stronger gradients to propagate to the earlier layers. This method can be seen in Highway Networks _cite_ and Residual Networks _cite_ . Other work has been done to find new activation functions with more desirable properties. One example is the exponential linear unit (ELU) _cite_, which attempts to keep activations standardized. All of the above methods are combating the vanishing gradient problem _cite_ that plagues deep architectures. With solutions to this problem appearing it is only natural to move to a system that will allow one to construct deeper architectures with as low a parameter cost as possible. Other work in this area has explored the use of complex and hyper-complex numbers, which are a generalization of the complex, such as quaternions. Using complex numbers in recurrent neural networks (RNNs) has been shown to increase learning speed and provide a more noise robust memory retrieval mechanism _cite_ . The first formulation of complex batch normalization and complex weight initialization is presented by _cite_ where they achieve some state of the art results on the MusicNet data set. Hyper-complex numbers are less explored in neural networks, but have seen use in manual image and signal processing techniques _cite_ . Examples of using quaternion values in networks is mostly limited to architectures that take in quaternion inputs or predict quaternion outputs, but do not have quaternion weight values _cite_ . There are some more recent examples of building models that use quaternions represented as real-values. In _cite_ they used a quaternion multi-layer perceptron (QMLP) for document understanding and _cite_ uses a similar approach in processing multi-dimensional signals. Building on _cite_ our contribution in this paper is to formulate and implement quaternion convolution, batch normalization, and weight initialization . There arises some difficulty over complex batch normalization that we had to overcome as their is no analytic form for our inverse square root matrix.