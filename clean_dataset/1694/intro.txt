Convolutional neural networks (CNNs) are the de-facto paragon for detecting the presence of objects in a scene, as portrayed by an image. CNNs are described as being ``approximately invariant'' to nuisance transformations such as planar translation, both by virtue of their architecture (the same operation is repeated at every location akin to a ``sliding window'' and is followed by local pooling) and by virtue of their approximation properties that, given sufficient parameters and transformed training data, could in principle yield discriminants that are insensitive to nuisance transformations of the data represented in the training set. In addition to planar translation, an object detector must manage variability due to scaling (possibly anisotropic along the coordinate axes, yielding different aspect ratios) and (partial) occlusion. Some nuisances are elements of a transformation group, \eg, the (anisotropic) location-scale group for the case of position, scale and aspect ratio of the object's support. The fact that convolutional architectures appear effective in classifying images as containing a given object regardless of its position, scale, and aspect ratio _cite_ suggests that the network can effectively manage such nuisance variability. However, the quest for top performance in benchmark datasets has led researchers away from letting the CNN manage all nuisance variability. Instead, the image is first pre-processed to yield {\em proposals}, which are subsets of the image domain (bounding boxes) to be tested for the presence of a given class (Regions-with-CNN _cite_) . Proposal mechanisms aim to remove nuisance variability due to position, scale and aspect ratio, leaving a ``Category CNN'' to classify the resulting bounding box as one of a number of classes it is trained with. Put differently, rather than computing the {\em posterior} distribution with nuisance transformations automatically marginalized, the CNN is used to compute the {\em conditional} distribution of classes given the data {\em and} a sample element that approximates the nuisance transformation, represented by a bounding box. If the goal is the nuisance itself (object support, as in {\em detection} _cite_) it can be found via maximum-likelihood ({\em max-out}) by selecting the bounding box that yields the highest probability of any class _cite_ . If the goal is the class regardless of the transformation (as in {\em categorization} _cite_), the nuisance can be approximately {\em marginalized out} by averaging the conditional distributions with respect to an estimation of the nuisance transformations _inline_eq_ . Now, if a CNN was an effective way of computing the marginals with respect to nuisance variability, there would be no benefit in conditioning and averaging with respect to (inferred) nuisance samples. This is a direct corollary of the Data Processing Inequality (DPI, Theorem N in _cite_) . Proposals are subsets of the whole image, so in theory less informative even after accounting for resolution/sampling artifacts (Fig.~ _ref_) . {\em A fortiori}, performance should further decrease if the conditioning mechanism is not very representative of the nuisance distribution, as is the case for most proposal schemes that produce bounding boxes based on adaptively downsampling a coarse discretization of the location-scale group _cite_ . Class posteriors conditioned on such bounding boxes discard the image outside it, further limiting the ability of the network to leverage on side information, or ``context''. Should the converse be true, \ie, should averaging conditional distributions restricted to proposal regions outperform a CNN operating on the entire image, that would bring into question the ability of a CNN to marginalize nuisances such as translation and scaling or else go against the DPI. In this paper we test this hypothesis, aiming to answer to the question: {\em How effective are current CNNs to reduce the effects of nuisance transformations of the input data, such as location and scaling?} To the best of our knowledge, this has never been done in the literature, despite the keen interest in understanding the properties of CNNs _cite_ following their empirical success. We are cognizant of the dangers of drawing sure conclusions from empirical evaluations, especially when they involve a myriad of parameters and exploit training sets that can exhibit biases. To this end, in Sect.~ _ref_ we describe a testing protocol that uses recognized existing modules, and keep all factors constant while testing each hypothesis. We first show that a baseline (AlexNet _cite_) with single-model top-N error of _inline_eq_ on ImageNet N Classification slightly {\em decreases} in performance (to _inline_eq_) when constrained to the ground-truth bounding boxes (Table _ref_) . This may seem surprising at first, as it would appear to violate Theorem N of _cite_ (on average, conditioning on the true value of the nuisance transformation must reduce uncertainty in the classifier) . However, note that the restriction to bounding boxes does not just condition on the location-scale group, but also on {\em visibility}, as the image outside the bounding box is ignored. Thus, {\em the slight decrease in performance measures the loss from discarding context by ignoring the image beyond the bounding box.} When we pad the true bounding boxes with a N-pixel rim, we show that, conditioned on such ``ground-truth-with-context'' indeed does decrease the error as expected, to _inline_eq_ . In Fig.~ _ref_ we show the classification performance as a function of the rim size all the way to the whole image for AlexNet and VGGN _cite_ . A _inline_eq_ rim yields the lowest top-N errors on the ImageNet validation set for both models. This also indicates that the context effectively leveraged by current CNN architectures is limited to a relatively small neighborhood of the object of interest. The second contribution concerns the {\em proper sampling} of the nuisance group. If we interpret the CNN restricted to a bounding box as a function that maps samples of the location-scale group to class-conditional distributions, where the proposal mechanism {\em down-samples} the group, then classical sampling theory _cite_ teaches that we should retain {\em not} the value of the function at the samples, but its {\em local average}, a process known as {\em anti-aliasing} . Also in Table _ref_, we show that simple uniform averaging of N and N samples of the isotropic {\em scale} group (leaving location and aspect ratio constant) reduces the error to _inline_eq_ and _inline_eq_ respectively. This is again unintuitive, as one expects that averaging conditional densities would produce less discriminative classifiers, but in line with recent developments concerning ``domain-size pooling'' _cite_ . To test the effect of such anti-aliasing on a CNN absent the knowledge of ground truth object location, we follow the methodology and evaluation protocol of _cite_ to develop a domain-size pooled CNN and test it in their benchmark classification of wide-baseline correspondence of regions selected by a generic low-level detector (MSER _cite_) . Our third contribution is to show that this procedure improves the baseline CNN by _inline_eq_--_inline_eq_ mean AP on standard benchmark datasets (Table _ref_ and Fig.~ _ref_ in Sect.~ _ref_) . Our fourth contribution goes towards answering the question set forth in the preamble: We consider two popular baselines (AlexNet and VGGN) that perform at the state-of-the-art in the ImageNet Classification challenge and introduce novel sampling and pruning methods, as well as an adaptively weighted marginalization based on the inverse R \'enyi entropy. Now, if {\em averaging} the conditional class posteriors obtained with various sampling schemes should improve overall performance, that would imply that the ``marginalization'' performed by the CNN is inferior to that obtained by sampling the group, and averaging the resulting class conditionals. _inline_eq_ This is indeed our observation, \eg, for VGGN, as we achieve an overall performance of _inline_eq_, compared to _inline_eq_ when using the whole image (Table~ _ref_) . There are, however, caveats to this answer, which we discuss in Sect.~ _ref_ . Our fifth contribution is to actually provide a method that performs at the state of the art in the ImageNet Classification challenge when using a single model. In Table _ref_ we provide various results and time complexity. We achieve a top-N classification error of _inline_eq_ and _inline_eq_ for AlexNet and VGGN, compared to _inline_eq_ and _inline_eq_ error when they are tested with _inline_eq_ regularly sampled crops _cite_, which corresponds to _inline_eq_ and _inline_eq_ relative error reduction, respectively. Data augmentation techniques such as scale jittering and an ensemble of several models _cite_ could be deployed along with our method. The source code implementing our method and the scripts necessary to reproduce the evaluation are available at _url_ . The literature on CNNs and their role in Computer Vision is rapidly evolving. Attempts to understand the inner workings of CNNs are being conducted _cite_, along with theoretical analysis _cite_ aimed at characterizing their representational properties. Such intense interest was sparked by the surprising performance of CNNs _cite_ in Computer Vision benchmarks _cite_, where many couple a proposal scheme _cite_ with a CNN. As our work relates to a vast body of work, we refer the reader to references in the papers that describe the benchmarks we adopt, namely _cite_, _cite_ and _cite_ . Bilen et. al. _cite_ also explore the idea of introducing proposals in classification. However, their approach leverages on a significantly larger number of candidates and focuses on sophisticated classifiers and post-normalization of class posteriors. Our investigation targets selecting a very small subset of the most discriminative candidates among generic object proposals, while building on popular CNN models.