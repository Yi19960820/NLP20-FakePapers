When performing convolutions on a finite domain, boundary rules are required as the kernel's support extends beyond the edge. For convolutional neural networks (CNNs), many discrete filter kernels ``slide'' over a ND image and typically boundary rules including,,, are used to extrapolate values outside the image. Considering a simple detection filter (a) applied to a diagonal feature (b), we see that no boundary rule is ever ideal: will create a black boundary halo (c), using the color will reduce but not remove the issue (d), and (e and _ref_ f) will create different kinks in a diagonal edge where the ground-truth continuation would be straight. In we visualize this as the error between the ideal response and the response we would observe at a location if a feature was present. In practical feature channels, these will manifest as false positive and negative images. These deteriorate overall feature quality, not only on the boundary but also inside. Another, equally unsatisfying, solution is to execute the CNN only on a ``valid'' interior part of the input image (crop), or to execute it multiple times and merge the outcome slide. Working in lower or multiple resolutions, the problem is even stronger, as low-resolution images have a higher percentage of boundary pixels. In a typical modern encoder-decoder _cite_, will eventually become boundary pixels at some step. Having a second thought on what a ND image actually is, we see, that the ideal boundary rule would be the one that extends the content exactly to the values an image taken with a larger sensor would have contained. Such a rule appears elusively hard to come by as it relies on information not observed. We cannot decide with certainty from observing the yellow part inside the image in b how the part outside the image continues--what if the yellow structure really stopped?--and therefore it is unknown what the filter response should be. However, neural networks have the ability to extrapolate information from a context, for example in in-painting tasks _cite_ . Here, this context is the image part inside the boundary. Given this observation, not every extension is equally likely. Most human observers would follow the Gestalt assumption of continuity and predict the yellow bar to continue at constant slope outside the image. Can a CNN do this extrapolation while extracting features? Addressing the boundary challenge, and making use of a CNN's extrapolating power, we propose the use of a novel boundary rule in CNNs. As such rules will have to depend on the image content and the spatial location of that content, we advocate to model them as a set of learned that simply replace the non-boundary filters when executed on the boundary. These boundary filters are supposed to produce exactly the same feature channels the non-boundary filters produce. Every boundary configuration (upper edge, lower left corner, etc.) has a different filter. This implies, that they incur no time or space overhead at runtime. At training-time, boundary and non-boundary filters are jointly optimized and no additional steps are required. It seems, that introducing more degrees of freedom increases the optimization challenge. However, introducing the right degrees of freedom, can actually turn an unsolvable problem into separate tasks that have simple independent solutions, as we conclude from a reduction of error both at the interior and at the edges, when using our method. After reviewing previous work and introducing our formalism, we demonstrate how using boundary conditions can improve the quality across a wide range of possible architectures () . We next show improvement in performance for tasks such as de-noising and de-bayering _cite_, colorization _cite_ as well as disparity and scene flow _cite_, in .