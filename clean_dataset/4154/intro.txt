Deep neural networks achieve state of the art performance on many problems, but are often very large in depth or width, and contain large numbers of parameters _cite_ . This has the drawback that they may be slow to execute or demand large memory to store, limiting their use in applications or platforms with low memory or fast execution requirements. This has led to a rapidly growing area of research on smaller and faster models. Achieving compact yet accurate models has been approached in a variety of ways including explicit frugal architecture design _cite_, model compression _cite_, pruning _cite_, binarisation _cite_ and most interestingly model distillation _cite_ . Distillation-based model compression relates to the observation _cite_ that small networks often have the same as large networks; but compared to large networks they are simply harder to train and find the right parameters that realise the desired function. That is, the limitation seems to lie in the difficulty of optimisation rather than in the network size _cite_ . To better learn a small network, the distillation approach starts with a powerful (deep and/or wide) teacher network (or network ensemble), and then trains a smaller student network to the teacher _cite_ . Mimicking the teacher's class probabilities _cite_ and/or feature representation _cite_ conveys additional information beyond the conventional supervised learning target. The optimisation problem of learning to mimic the teacher turns out to be easier than learning the target function directly, and the much smaller student can match or even outperform _cite_ the larger teacher. In this paper we explore a different but related idea to model distillation--that of . Distillation starts with a powerful large and pre-trained teacher network and performs one-way knowledge transfer to a small untrained student. In contrast, in mutual learning we start with a pool of untrained students who learn simultaneously to solve the task together. Specifically, each student is trained with two losses: a conventional supervised learning loss, and a mimicry loss that aligns each student's class posterior with the class probabilities of other students. Trained in this way, it turns out that each student in such a peer-teaching based scenario learns significantly better than when learning alone in a conventional supervised learning scenario. Moreover student networks trained in this way achieve better results than students trained by conventional distillation from a larger pre-trained teacher. Furthermore, while the conventional understanding of distillation requires a teacher larger and more powerful than the intended student, it turns out that in many cases mutual learning of several large networks also improves performance compared to independent learning. It is perhaps not obvious why the proposed procedure should work at all. Where does the additional knowledge come from, when the learning process starts out with all small and untrained student networks? Why does it converge to a good solution rather than being hamstrung by groupthink as `the blind lead the blind'. Some intuition about these questions can be gained by considering the following: Each student is primarily directed by a conventional supervised learning loss, which means that their performance generally increases and they cannot drift arbitrarily into groupthink as a cohort. With supervised learning, all networks soon predict the same (true) labels for each training instance; but since each network starts from a different initial condition, their estimates of the probabilities of the next most likely classes vary. It is these secondary quantities that provide the extra information in distillation _cite_ as well as mutual learning. Overall, mutual learning provides a simple but effective way to improve the generalisation ability of a network by training collaboratively with a cohort of other networks. Compared with distillation by a pre-trained static large network, collaborative learning by small peers even achieves better performance. Furthermore we observe that: (i) the efficacy increases with the number of networks in the cohort (by training on small networks only, more of them can fit on one GPU for effective mutual learning) ; (ii) it applies to a variety of network architectures, and to heterogeneous cohorts consisting of mixed big and small networks; and (iii) even large networks mutually trained in cohort improve performance compared to independent training. Finally, we note that while our focus is on obtaining a single effective network, the entire cohort can also be used as a highly effective ensemble model. {\bf Related Work} \quad The distillation-based approach to model compression has been proposed over a decade ago _cite_ but was recently re-popularised by _cite_, where some additional intuition about why it works--due to the additional supervision and regularisation of the higher entropy soft-targets--was presented. Initially, a common application was to distill the function approximated by a powerful model/ensemble teacher into a single neural network student _cite_ . But later, the idea has been applied to distill powerful and easy-to-train large networks into small but harder-to-train networks _cite_ that can even outperform their teacher. Recently, distillation has been connected more systematically to information learning theory _cite_ and SVM _inline_eq_ _cite_--an intelligent teacher provides privileged information to the student. Here we address dispensing with the teacher altogether, and allowing an ensemble of students to teach each other in mutual distillation. Other related ideas include Dual Learning _cite_ where two cross-lingual translation models teach each other interactively. But this only applies in this special translation problems where an unconditional within-language model is available to be used to evaluate the quality of the predictions, and ultimately provides the supervision that drives the learning process. In contrast, our mutual learning approach applies to general classification problems. While conventional wisdom about ensembles prioritises diversity _cite_, our mutual learning approach reduces diversity in the sense that all students become somewhat more similar by learning to mimic each other. However, our goal is not necessarily to produce a diverse ensemble, but to enable networks to find robust solutions that generalise well to testing data, which would otherwise be hard to find through conventional supervised learning.