The central problem of constructing an object detector can be decomposed into learning a confidence detection response function and estimating a search strategy. Most frequently, confidence functions are learned in a fully supervised setting, and the search is performed exhaustively. This approach is however not well suited to contemporary state-of-the-art systems, which are trained using large amounts of image or video data, and require complex multi-layer models _cite_ . First, manually annotating the large amounts of data needed by supervised algorithms is increasingly expensive. This underlines the need to exploit alternative sources of information to support system accuracy and efficiency. Second, the target search complexity is sometimes high in many practical object detection applications. Human eye movements can provide a rich source of supervision which, due to recent developments in eyetracking hardware, is increasingly less invasive and expensive. Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public _cite_ . Such data has been exploited at training time to support visual classification performance in video _cite_, but its usefulness as a training signal for detection, in the absence of any additional annotations, remains unexplored. Nor has detection performance benefited from insights derived from the `saccade-and-fixate' operating principles of the human visual system. In this paper, we propose general methods to learn detector confidence functions and search models from weak supervisory signals, circumventing the need for manual image annotations. Our main contributions are: