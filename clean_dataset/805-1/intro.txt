Since the inception of SHRP N, the presence or absence of work zones in a given NDS trip has been a desired piece of information. Prior efforts to conflate work zone occurrences with roadway information in the RID were not successful because they depended on N data provided by participating states. N data is only sparsely informative, indicating what segments of what highways had construction planned within a given time period. Whether actual construction equipment was present on the particular segment of highway over which a volunteer driver drove and at the time he or she drove is not an answerable question. In some cases, researchers have used N data to identify trips and their accompanying videos that supposedly contained a work zone, only to find themselves manually skimming through videos, sometimes finding what they were looking for and other times not _cite_ . Further, one of the six participating states did not manage to supply N-level data about work zones, making extraction of events from video the only option for those trips. This paper presents the SHRP N NDS Video Analytics (SNVA) software application, which aims to perform a complete and accurate accounting of work zone occurrences in the NDS video data set. To achieve this, SNVA combines a video decoder based on FFmpeg, an image scene classifier based on TensorFlow, and algorithms for reading timestamps off of video frames, for identifying the start and end timestamps of detected work zone events, and for exporting those events as records in CSV files. We organize the presentation of SNVA as follows: Section _ref_ discusses the motivations and methods behind our choice of deep learning framework and models; section _ref_ details our approach to the joint development of the work zone detection model and the data set used to train it; and section _ref_ describes our choice of hardware and software components, and highlights efforts made to optimize the video processing pipeline. In section _ref_ we present the expected future directions of SNVA development, and we conclude the paper in section _ref_ .