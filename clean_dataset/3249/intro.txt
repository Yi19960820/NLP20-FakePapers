As neural networks become increasingly popular, their ``black box" reputation is a barrier to adoption when interpretability is paramount. Understanding the features that lead to a particular output builds trust with users and can lead to novel scientific discoveries. proposed using gradients to generate saliency maps and showed that this is closely related to the deconvolutional nets of . Guided backpropagation is another variant which only considers gradients that have positive error signal. As shown in Figure N, saliency maps can be substantially improved by simply multiplying the gradient with the input signal, which corresponds to a first-order Taylor approximation of how the output would change if the input were set to zero; as we show, the layer-wise relevance propagation rules described in reduce to this approach, assuming bias terms are included in the denominators. Gradient-based approaches are problematic because activation functions such as Rectified Linear Units (ReLUs) have a gradient of zero when they are not firing, and yet a ReLU that does not fire can still carry information (Figure N) . Similarly, sigmoid or tanh activations are popular choices for the activation functions of gates in memory units of recurrent neural networks such as GRUs and LSTMs, but these activations have a near-zero gradient at high or low inputs even though such inputs can be very significant. We present DeepLIFT, a method for assigning feature importance that compares a neuron's activation to its `reference', where the reference is the activation that the neuron has when the network is provided a `reference input' (the reference input is defined according to what is appropriate for the task at hand) . This addresses the limitation of gradient-based approaches because the difference from the reference may be non-zero even when the gradient is zero.