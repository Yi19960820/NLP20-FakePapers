Robots that share spaces with people require the ability to detect, track and analyze humans for a variety of reasons including safety, user interaction, or efficient navigation through crowds~ _cite_ . Self-driving cars~ _cite_ or last-mile delivery robots, for example, need to recognize pedestrians and particularly also vulnerable road users such as elderly people, children or people in wheelchairs. This recognition task is well understood for image data~ _cite_, sometimes even specifically geared toward robust person detection~ _cite_, and with systems for pedestrian detection being commercially available for automotive use-cases. For other sensory modalities such as RGB-D or ND/ND range data, often needed in robotics, a growing number of detection methods and datasets exist. Many interesting approaches have been developed for detection in volumetric data, such as ND LiDAR, which can largely be grouped into two types. The first type uses ordered projections of the point cloud data~ _cite_ . The second, more recent type proposes novel neural network architectures which can consume unordered point cloud data and reason on them, \eg PointNet~ _cite_ and VoxelNet~ _cite_ . Despite the advent of increasingly performant and affordable ND/RGB-D sensors, ND horizontally mounted laser scanners are still a common part of the sensory setup on a large number of mobile service robots in human populated spaces, making them an interesting sensor for person detection. They are also one of the few certifiable sensors to ensure human safety \eg for transportation platforms in intralogistics or hospitals, have large fields of view, high accuracy, and high levels of robustness in wide ranges of conditions. In contrast, on-board cameras have several drawbacks in many robotic use-cases including limited field of view, ambiguous scale, and likely detection failures when subjects are close to the sensor, an important case for human-robot interaction. Existing ND range data based person detectors can roughly be grouped into two types. The first type consists of jump-distance based clustering of the measured laser points, computation of many hand-crafted features on these clusters, and use of a machine learning approach to classify the cluster as either person or background using said features~ _cite_ . The second type does the same thing, although for legs as opposed to persons, by tracking legs over time, and defining persons as pairs of legs which satisfy some hand-designed heuristics~ _cite_ . The latter type typically performs better on benchmark datasets and toy deployments, but the ``two leg assumption'' comes with some severe inherent drawbacks: N) It does not readily extend to other classes, \eg wheelchairs. N) It only works when the legs are visible, which is neither true in many real-world scenarios due to people wearing coats, dresses, hijabs, saris, \etc, nor in many professional scenarios due to work uniforms such as lab coats. This is a perfect example of why it is desirable to collect a diverse dataset and directly learn from data instead of hand-crafting large parts of a system. Learning from ND range data is difficult as evidenced by the relatively poorer performance of the jump distance-based methods when compared to the leg tracking methods. One big difference is that the latter often make use of rich temporal information. While deep learning has made impressive progress, also for detection on range data, so far our DROW~ _cite_ detector is the only deep learning based detector for ND range data. DROW obtained state-of-the-art detection results for walking aids and has performed well in multiple real-world deployments. In this paper we extend DROW to person detection. Given the multi-class nature of DROW, this is as straight forward as adding an additional class. However, given that persons in ND range data are usually covered by few points, person detection proved to be a challenging task for the DROW detector too. For every point in a laser scan the DROW detector predicts a class and possibly a vote to the closest object center. A final post-processing step then converts votes to actual predictions. We improve this post-processing step, which now allows us to predict confidences for each detection and improves the overall performance. Additionally, we slightly increase our convolutional neural network (CNN) size, further boosting detection scores. Orthogonal to these extensions, we experiment with a fully online temporal integration of a few previous scans and show that this results in a significant performance boost for person detection scores. Unlike a tracker, we do not perform any data association, but rather show how DROW's input pre-processing can be extended to include temporal context to be used by the network. We show how several fusion strategies in the network compare and obtain state-of-the-art results using our extended DROW detector. To facilitate all our experiments, we introduce a new dataset of people annotations for our original DROW dataset. We hand annotate persons for every frame which we previously annotated with wheelchair and walker center points. To the best of our knowledge, this is the largest dataset for person detection in ND range data recorded in real-world scenarios with over N hours of laser scan recordings. We thoroughly evaluate top performing existing detectors on this dataset, including retraining and tuning hyperparameters on our validation set. Since person detection is arguably a bigger field than walking aid detection in the robotics community, with the addition of person annotations, the DROW dataset is now by far more interesting and relevant. Our final results and the complete DROW dataset will be shared with the community to foster further research in this area. To summarize, our key contributions are as follows: [N] {\hspace* {-Npt} \vspace* {Npt}} [N] {\hspace* {-Npt} \vspace* {Npt}} [N] {{#N}}