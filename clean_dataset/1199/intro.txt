The breakthrough made by the convolutional neural network (ConvNet or CNN) on the ImageNet dataset was a watershed event that has transformed the fields of computer vision and speech recognition as well as related industries. While CNN has proven to be a powerful discriminative machine, researchers have recently become increasingly interested in the generative perspective of CNN. An interesting example is the recent work of Google deep dream (http: //deepdreamgenerator.com/) . Although it did not smash any performance records, it did capture people's imagination by generating interestingly vivid images. In this conceptual paper, we explore the generative perspective of CNN more formally by defining generative models based on CNN features, and learning these models by generating images from the models. Adopting the metaphor of Google deep dream, we let the generative models dream by generating images. But unlike the google deep dream, we learn the models from real images by making the dreams come true. Specifically, we propose to learn the FRAME (Filters, Random field, And Maximum Entropy) models using the highly nonlinear filters pre-learned by CNN at the convolutional layers. A FRAME model is a random field model that defines a probability distribution on the image space. The model is generative in the sense that images can be generated from the probability distribution defined by the model. The probability distribution is the maximum entropy distribution that reproduces the statistical properties of filter responses in the observed images. Being of the maximum entropy, the distribution is the most random distribution that matches the observed statistical properties of filter responses, so that images sampled from this distribution can be considered typical images that share the statistical properties of the observed images. There are two versions of FRAME models in the literature. The original version is a stationary model developed for modeling texture patterns . The more recent version is a non-stationary extension designed to represent object patterns . Both versions of the FRAME models can be sparsified by selecting a subset of filters from a given dictionary. The filters used in the FRAME model are the oriented and elongated Gabor filters at different scales, as well as the isotropic Difference of Gaussian (DoG) filters of different sizes. These are linear filters that capture simple local image features such as edges and blobs. With the emergence of the more expressive nonlinear filters learned by CNN at various convolutional layers, it is only natural to replace the linear filters in the original FRAME models by the CNN filters in the hope of learning more expressive models. We use the Langevin dynamics to sample from the probability distribution defined by the model. Such a dynamics was first applied to the FRAME model by _cite_, and the gradient descent part of the dynamics was interpreted as the Gibbs Reaction And Diffusion Equations (GRADE) . When applied to the FRAME model with CNN filters, the dynamics can be viewed as a recurrent generative form of the model, where the reactions and diffusions are governed by the CNN filters of positive and negative weights respectively. Incorporating CNN filters into the FRAME model is not an ad hoc utilitarian exploit. It is actually a seamless meshing between the FRAME model and the CNN model. The original FRAME model has an energy function that consists of a layer of linear filtering followed by a layer of pointwise nonlinear transformation. It is natural to follow the deep learning philosophy to add alternative layers of linear filtering and nonlinear transformation to have a deep FRAME model that directly corresponds to a CNN. More importantly, the learned FRAME model using CNN filters corresponds to a new CNN unit at the layer directly above the layer of CNN filters employed by the FRAME model. In particular, the non-stationary FRAME becomes a single CNN node at a specific position where the object appears, whereas the stationary FRAME becomes a special type of convolutional unit. Therefore, the learned FRAME model can be viewed as a generative version of CNN unit. In addition to learning a single CNN unit, we can also learn a new layer of multiple convolutional units from non-aligned images, so that each convolutional unit represents one type of local pattern. We call the resulting model the generative CNN model. It is a product of experts model, where each expert models a mixture of activation and inactivation of a local pattern. The rectified linear unit can be justified as an approximation to the energy function of this mixture model. The learning algorithm admits an interpretation in terms of the EM algorithm with a hard-decision E-step that detects the local patterns modeled by the convolutional units. The main purpose of this paper is to establish the conceptual correspondence between the generative FRAME model and the discriminative CNN, thus providing a formal generative perspective for CNN. Such a perspective is much needed because it may eventually lead to unsupervised learning of CNN in a generative fashion without the need for image labeling.