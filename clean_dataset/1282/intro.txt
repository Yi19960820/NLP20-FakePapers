The use of deep convolutional networks has recently advanced the accuracy of stereo matching algorithms considerably _cite_ . This improvement has been facilitated by the emergence of sizeable training sets, such as the KITTI datasets for autonomous driving~ _cite_, the new version of the Middlebury dataset~ _cite_, and, most recently, synthetic datasets of high quality _cite_ . The use of machine learning allows to tune the stereo matching process to handle characteristic image patterns. This allows to resolve various stereo ambiguities using semantic cues, surpassing the accuracy of more traditional approaches that use low-level cues and priors. Despite this success of deep learning methods in stereo, designing real-time algorithms as required by the majority of applications has proved challenging. The initial approach of _cite_ required over a minute to process a KITTI stereo-pair. A more recent (``fast'') variant discussed in the follow-up work~ _cite_ and a similar approach of _cite_ have brought down this time to as little as one second, which is still excessive for many applications. The computational bottleneck within methods _cite_ is in the matching process of high-dimensional descriptors of local appearance, which has to be done for all pairs of potentially matching pixels across the two views. The most recent method _cite_ streamlines this necessity by proposing a deep architecture that directly outputs the disparity map given the stereo-pair as an input. While the high-dimensional descriptors still have to be implicitly matched within their architecture, this matching process only happens at low resolution, while further processing results in the efficient upsampling. Here, we propose a new way to apply deep learning in order to improve the accuracy of stereo matching. In order to achieve real-time frame rate, we avoid learning high-dimensional descriptors and matching them and focus our learning-based effort on the cost-aggregation process. We thus use a simple linear combination of two classical and very fast similarity measures based on census transform _cite_ and sum-of-absolute-differences matching to define the overall matching costs for various pixels and disparities. To perform cost-aggregation, we smooth the obtained noisy matching costs using one of the fastest edge-preserving smoothing techniques, namely domain transform ~ _cite_ across the four directions. Crucially, we make the parameters of this cost-aggregation process spatially-varying and use a deep convolutional network to predict them on a per-pixel basis. Such prediction facilitates smoothing across parts belonging to same object and prevents smoothing across object boundaries. At test time, the deep learning module processes only one of the input images, and the complexity of this module is thus independent of the disparity range. Our experiments demonstrate that a combination of a simple matching process and a trainable domain transform-based cost aggregation is able to achieve a unique combination of a high frame-rate (e.g. \N fps for KITTI N dataset) and high matching accuracy (state-of-the-art for real-time methods) . The high accuracy is obtained via the end-to-end learning process that takes into account the pixel-level matching, the cost aggregation, and the final winner-takes-all disparity selection. The ultimate accuracy greatly benefits from initializing the weights within our deep network to the weights of the network trained to detect natural boundaries in images~ _cite_ . In the remainder of the paper, we discusses the related work (_ref_), detail the proposed method (_ref_), present the results of the experimental validation in _ref_, and conclude with a short discussion and outlook (_ref_) .