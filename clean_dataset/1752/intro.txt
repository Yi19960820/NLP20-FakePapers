In recent years, deep convolutional neural networks (CNNs) have proven to be highly effective general models for a multitude of computer vision problems . One such problem is, where the goal is to predict a fixed number of location coordinates corresponding to points of interest in an input image. A well-known instance of this problem is human pose estimation, for which CNNs are state-of-the-art. In this paper we study CNN-based solutions to coordinate regression, using the single-person pose estimation task as an exemplar. Such solutions may exhibit the desirable properties of spatial generalization and/or end-to-end differentiability. Spatial generalization is the ability of a model to generalize knowledge obtained at one location during training to another at inference time. If a spatially generalizable model observes a tennis ball in the top-left of an image during training, it should be able to successfully locate a similar tennis ball at a previously unseen location in a new image (\eg the bottom right) . It follows that this property will make a positive contribution to the overall generalization of a coordinate regression model, since the goal is to find items anywhere in the image. In general, the success of CNNs is understood to be a result of the high generalization ability afforded by spatially shared parameters . To maximize this advantage, care must be taken to avoid trainable layers which can overfit on global structure. note that ``fully connected layers are prone to overfitting, thus hampering the generalization ability of the overall network''. An end-to-end differentiable model can be composed with other differentiable layers to form a larger model without losing the ability to train using backpropagation . In the case of coordinate regression, being end-to-end differentiable means being able to propagate gradients all the way from the output numerical coordinates to the input image. It is possible to train a coordinate regression model without this property, such as by matching predicted heatmaps to target heatmaps generated from the ground truth locations. However, this approach cannot be used in architectures where the numerical coordinates are learned implicitly as intermediate values, including the prominent example of Spatial Transformer Networks . There are many CNN-based solutions to other computer vision tasks, such as classification and semantic segmentation, which exhibit both spatial generalization and end-to-end differentiability. However, existing solutions for coordinate regression sacrifice one property or the other. The most successful existing coordinate regression approach is to apply a loss directly to output heatmaps rather than numerical coordinates . Synthetic heatmaps are generated for each training example by rendering a spherical ND Gaussian centered on the ground truth coordinates. The model is trained to produce output images which resemble the synthetic heatmaps using mean-square-error loss. During inference, numerical coordinates are obtained from the model's output by computing the argmax of pixel values, which is a non-differentiable operation. Although this approach has good spatial generalization, it does have a few disadvantages. Most notably, gradient flow begins at the heatmap rather than the numerical coordinates () . This leads to a disconnect between the loss function being optimized (similarity between heatmaps) and the metric we are actually interested in (the distance between predicted coordinates and ground truth) . Only the brightest pixel is used to calculate numerical coordinates at inference time, but all of the pixels contribute to the loss during training. Making predictions based on the argmax also introduces quantization issues, since the coordinates have their precision tied to the heatmap's resolution. Another coordinate regression approach is to add a fully connected layer which produces numerical coordinates . An attractive (and sometimes) property of this approach is that it is possible to backpropagate all the way from the predicted numerical coordinates to the input image. However, the weights of the fully-connected layer are highly dependent on the spatial distribution of the inputs during training. To illustrate this point, consider an extreme situation where the training set consists entirely of coordinates located within the left-hand half of the image. Many of the fully connected layer's input activations will be useless, and as a result weights corresponding to the right-hand side of the image will not be trained properly. So although the convolutional part of the model is spatially invariant, the model as a whole will not generalize well to objects on the right-hand side of the image. This is an inefficient usage of the training data, and causes particularly bad performance on small datasets. We propose our (DSNT) layer as an alternative to existing approaches. The DSNT layer may be used to adapt existing CNN architectures, such as a pretrained ResNet, to coordinate regression problems. Our technique fully preserves the spatial generalization and end-to-end differentiability of the model, without introducing additional parameters. illustrates how the DSNT layer fits into the model as a whole in comparison to fully connected and heatmap matching approaches. summarizes the features that DSTN poses which selectively appear in fully connected (FC) and heatmap matching (HM) based approaches. We find that DSNT is able to consistently outperform the accuracy of heatmap matching and fully connected approaches across a variety of architectures on the MPII human pose dataset, and is therefore a suitable replacement in most situations. Our experiments show that state-of-the-art stacked hourglass models achieve higher accuracy when heatmap matching is replaced with DSNT. For ResNet-N models, DSNT outperforms heatmap matching by N \% with _inline_eq_ pixel heatmaps, and by N \% with _inline_eq_ pixel heatmaps. Since accuracy at low heatmap resolution is much better with DSNT, a wider variety of efficient architectures may be considered for coordinate regression. For instance, a simple ResNet-N network with DSNT is comparable in accuracy to an N-stack hourglass network, but exhibits triple the speed and half of the memory usage during inference. The DSNT layer presented in this paper is very similar to the soft-argmax operation of, which was developed in parallel with our own work. The soft-argmax has also been applied to different problem domains prior to this . However, we extend the idea further by proposing a regularization strategy which increases prediction accuracy. Additionally, we conduct a comprehensive set of experiments exploring configurations and properties of the operation, and the trade-off between accuracy and inference speed in the context of complete pose estimation models.