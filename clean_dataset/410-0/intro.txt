Although much progress has been made in image classification~ _cite_, detection~ _cite_ and segmentation~ _cite_, we are still far from reaching the goal of holistic scene understanding---that is, a model capable of recognizing the interactions and relationships between objects, and describing their attributes. While objects are the core building blocks of an image, it is often the relationships and attributes that determine the holistic interpretation of the scene. For example in Fig.~ _ref_, the left image can be understood as ``a man standing on a yellow and green skateboard", {and} the right image as ``a woman wearing a blue wet suit and kneeling on a surfboard". Being able to extract and exploit such visual information would benefit many real-world applications such as image search~ _cite_, question answering~ _cite_, and fine-grained recognition~ _cite_ . Visual relationships are a pair of localized objects connected via a predicate; for example, predicates can be actions (``kick''), comparative (``smaller than''), spatial (``near to''), verbs (``wear''), or prepositions (``with'') . Attributes describe a localized object, e.g., with color (``yellow'') or state (``standing'') . Detecting relationships and attributes is more challenging than traditional object detection~ _cite_ due to the following reasons: (N) There are a massive {number of} possible relationship and attribute types (e.g., N, N relationship types in Visual Genome~ _cite_), resulting in a greater skew of rare and infrequent types. (N) Each object can be associated with many relationships and attributes, making it inefficient to exhaustively search all {possible relationships} for each pair of objects. (N) A global, {holistic} perspective of the image is essential to resolve semantic {ambiguities} (e.g., ``woman wearing wetsuit'' vs. ``woman wearing shirt'') . Existing approaches~ _cite_ only predict a limited set of {relationship} types (e.g., N in Visual Phrase~ _cite_) and ignore semantic interdependencies between relationships and attributes by evaluating each region {within a scene separately} ~ _cite_ . It is impractical to exhaustively search all possibilities for each region, and also deviates from human perception. Therefore, it is preferable to have a more principled decision-making framework, which can discover all relevant relationships and attributes within a small number of search steps. To address the aforementioned issues, we propose a deep Variation-structured Reinforcement Learning (VRL) framework which sequentially detects relationship and attribute instances by exploiting global context cues. First, we use language priors to build a directed semantic action graph _inline_eq_, where the nodes are nouns, attributes, and predicates, connected by directed edges that represent semantic correlations (see Fig.~ _ref_) . This graph provides a highly-informative, compact representation that enables the model to learn rare relationships and attributes from frequent ones using shared graph nodes. For example, the semantic meaning of ``riding'' learned from ``person-riding-bicycle'' can help predict the rare phrase ``child-riding-elephant''. This generalizing ability allows VRL to handle a considerable number of possible {relationship} types. Second, existing deep reinforcement learning (RL) models~ _cite_ often require several costly episodes of trial and error to converge, even with a small action space, and our large action space would exacerbate this problem. To efficiently discover all relationships and attributes in a small number of steps, we introduce a novel variation-structured traversal scheme over the action graph which constructs small, adaptive action sets _inline_eq_ for each step based on the current state and historical actions: _inline_eq_ contains candidate attributes to describe an object; _inline_eq_ contains candidate predicates for relating a pair of objects; and _inline_eq_ contains new object instances to mine in the next step. Since an object instance may belong to multiple object categories which the object detector cannot distinguish, we introduce an ambiguity-aware object mining scheme to assign each object with the most appropriate category given the global scene context. Our variation-structured traversal scheme offers a very promising technique for extending the applications of deep RL to complex real-world tasks. Third, to incorporate global context cues for better reasoning, we explicitly encode the semantic embeddings of previously extracted phrases in the state vector. It {makes a better} tradeoff between increasing the input dimension and utilizing more historical context, compared to appending history frames~ _cite_ or binary action vectors~ _cite_ as in previous RL methods. Extensive experiments on the Visual Relationship Detection (VRD) dataset~ _cite_ and Visual Genome dataset~ _cite_ demonstrate that the proposed VRL outperforms {state-of-the-art methods} for both relationship and attribute detection, and also has good generalization capabilities for predicting unseen types.