Generative models, that are essential part of unsupervised learning, capture the structure/patterns in data by learning to generate samples that resemble the training data. Earlier work in generative modeling focused on graphical models and energy based models with latent variables, for example, Restricted Boltzmann Machines (RBM) ~ _cite_, Deep Belief Networks (DBN) ~ _cite_ . In these models, exact inference, normalization constant and its gradients are intractable and samples are usually obtained with expensive Markov Chain Monte Carlo (MCMC) techniques. Recent advances in deep learning have enabled us to bypass above mentioned challenges by using deep neural networks as parametrized functions that generate samples. Few dominant approaches that emerged in recent years are Variational AutoEncoders (VAEs) ~ _cite_, Generative Adversarial Networks (GANs) ~ _cite_, Generative Stochastic Networks~ _cite_, Deep Recurrent Attention Writer~ _cite_, Pixel Recurrent Neural Networks~ _cite_ and Pixel Convolutional Neural Networks~ _cite_ . Generative models have also helped to set benchmark results in semi-supervised learning _cite_ . Lately there have been attempts to use the advances in deep generative models for semi-supervised learning~ _cite_ and joint modeling of images and their visual descriptions~ _cite_ . In this work, we address the problem of jointly modeling images and their visual descriptions through deep generative models. Multimodal learning requires learning a correspondence between data across different modalities. In deep learning context, the first works towards multimodal learning is by N. Srivatsava et al. ~ _cite_ and J. Nigam~ et al. _cite_ . These models pose this problem as learning cross-modal representations at latent feature level and some models even try to learn a shared representation across modalities. This often becomes complex, because one modality rarely contains all the information about the other modality. In the case of images and text, the information present in image modality is much more dense than the information in text modality. VAEs are trained by maximizing the lower bound to the log-likelihood of the data and they include an inference network as part of the training procedure. Extensions such as~ _cite_ are proposed to increase the tightness of the lower bound. On the other hand, GANs are trained as a mini-max game between the generator and discriminator. Training of GANs only require a differentiable mapping from a latent space to the data space, without any requirement for inference network. Recently proposed InfoGAN~ _cite_ framework, a variation of GAN~ _cite_, defines image generation as a function of two sets of variables and is shown to learn interpretable representations of images. This is achieved by including a mutual information objective between a small subset of interpretable latent variables (denoted by _inline_eq_) and the observations. But as in GANs, it also lacks an inference mechanism to infer the latent state given an image. In this paper, we propose a new model, Variational Info GAN (ViGAN) that combines the InfoGAN model with VAE by using a subset of latent variables to represent the description of images. We also extend the architecture to include an inference mechanism and present a training procedure to jointly train the inference network and the model. We provide a training and sampling procedure for the proposed model. We denonstrate ViGAN's ability to generate new images conditioned on attributes, as well as modifying given images by changing attributes on three datasets namely, MNIST~ _cite_, LFW~ _cite_, celebA~ _cite_ .