The remarkable increase in performance of convolutional neural networks (CNNs) to solve computer vision problems is somehow diminished by the lack of understanding of the internal representations that capture the intrinsic image shapes. As we already mentioned before, the effects of the first convolutional layer in the net architecture can be easily understood. They act as feature detectors of specific spatial patterns that can be seen in the input image. Nevertheless, a second spatial convolution of these neuron activations entangles any intuition about the essential shape in the image. This problem is enlarged as we move forward in the hierarchy. On the other side, max pooling operators can be easily understood as simple image downsampling that just change the image resolution. One of the most relevant works that tackles this understanding problem is the work of Zeiler and Fergus~ _cite_ which is based on training a deconvolutional network. Same authors in ~ _cite_ exploit the deconvolution operation to visualize the image feature projections with the highest activation at the corresponding layer. A deconvolutional network has also been used by Dosovitskiy and Brox~ _cite_ . They use a given representation provided by a CNN to estimate the deconvolutional network that minimizes the image reconstruction error for a set of images. On the contrary, Mahendrans and Vedaldi~ _cite_ seek for the image whose representation minimizes the feature reconstruction error. Interestingly, object categorization using deconvolutional CNNs has been proved to be better than standard CNN when they are compared to electrophysiological data recorded during categorization tasks on primates. It has been shown by Cadieu \etal _cite_ . Other works that open the CNN black box, are those that try to understand how different scene variations are represented at the net levels. On one side the work of Aubry and Russell~ _cite_ the net is trained to encode a large synthetic dataset where the scene parameters such as ND point of view, lighting conditions or object style have been gradually changed with a ND CAD model. These images are represented with an standard CNN where the computed feature vectors can be qualitatively and quantitatively studied. Representation variations can be plotted in spaces of low dimensionality and can be used to predict similar variations in natural images. On another side, Dosovitskiy \etal _cite_ study natural scene variation represented with trained CNNs with the aim of generating accurate images of objects presenting unseen variations, such as, point of view, rotation or scale of the object. In this case, specific neurons can be studied as responsible of specific scene variations. In this work we present a new way to approach to the problem of visualizing internal representations, but focusing on understandig the filters themselves instead of the image responses. A similar idea has been outlined in a recent paper by Yosinski \etal _cite_ but with different assumptions. Our main contributions are twofold: In the following section we firstly deal with the definition and implementation of the inverse convolution, this will bring us to hypothesize about a new assumption. Correlation assumption will provide us with a new method to backproject the filters towards the image space.