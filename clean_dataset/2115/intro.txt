to a recent study _cite_ on `takeover time' in driverless cars, drivers engaged in secondary tasks exhibit larger variance and slower responses to requests to resume control. It is also well known that driver inattention is the leading cause of vehicular accidents. According to another study _cite_, N \% of crashes and N \% of near crashes involve driver distraction. Surveys on automotive collisions _cite_ demonstrated that drivers were less likely (N \%-N \%) to cause an injury related collision when they had one or more passengers who could alert them to unseen hazards. It is therefore essential for Advanced Driver Assistance Systems (ADAS) to capture these distractions so that the driver can be alerted or guided in case of dangerous situations. This ensures that the handover process between the driver and the self driving car is smooth and safe. Driver gaze is an important cue to recognize driver distraction. In a study on the effects of performing secondary tasks in a highly automated driving simulator _cite_, it was found that the frequency and duration of mirror-checking reduced during secondary task performance versus normal, baseline driving. Alternatively, Ahlstrom et al. _cite_ developed a rule based N-second 'attention buffer' framework which depleted when the driver looked away from the field relevant to driving (FRD) ; and it starts filling up when the gaze direction is redirected towards FRD. Driver gaze activity can also be used to predict driver behavior _cite_ . Martin et al. _cite_ developed a framework for modeling driver behavior and maneuver prediction from gaze fixations and transitions. While there has been a lot of research in improving personalized driver gaze zone estimation systems, there has been little progress in generalizing this task across different drivers, cars, perspectives and scale. We make an attempt in that direction using Convolutional Neural Networks (CNNs) . CNNs have shown tremendous promise in the fields of image classification, object detection and recognition. CNNs are also good at transfer learning. Oquab et al. _cite_ showed that image representations learned with CNNs on large-scale annotated datasets can be efficiently transferred to other visual recognition tasks. Therefore, instead of training a network from scratch, we adopt the transfer learning paradigm, where we finetune four different networks which have been trained to achieve state of the art results on the ImageNet _cite_ dataset. We analyze the effectiveness of each network in generalizing driver gaze zone estimation, by evaluating them on a large naturalistic driving dataset collected over N drives by N different subjects, in two different cars, each with slightly different camera settings and fields of view (Fig. _ref_) . The main contributions of this work are: a) A systematic ablative analysis of different CNN architectures and input strategies for generalizing driver gaze zone estimation systems b) Comparison of the CNN based model with some other state of the art approaches and, c) A large naturalistic driving dataset with extensive variability.