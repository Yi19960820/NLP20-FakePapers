Maximum a Posteriori (MAP) inference under the Bayesian framework is a popular method for solving various inverse problems in image processing. The MAP estimator is equivalent to an energy minimization problem, which consists of a data fidelity term and a signal prior term (also known as regularization term) . Roughly speaking, the priors fall into two main prior types. One is the analysis-based prior and the other is the synthesis-based one. Notation: In this paper our model presents a global prior over the entire image, in contrast to the common patch-based one. In order to distinguish between a patch and an image, we use the notation _inline_eq_ to indicate a patch (patch size: _inline_eq_, _inline_eq_ is odd), and _inline_eq_ to indicate an image (image size: _inline_eq_, with _inline_eq_) . We refer _inline_eq_ and _inline_eq_ with _inline_eq_ to the patch-based synthesis dictionary and analysis operator respectively. Furthermore, when the analysis operator _inline_eq_ is applied to the entire image _inline_eq_, we use the common sliding-window fashion to compute the coefficients _inline_eq_ for all _inline_eq_ patches in the N-D image form of _inline_eq_ . This result is equivalent to a multiplication of a sparse matrix _inline_eq_ and _inline_eq_, i.e., _inline_eq_ . We can group _inline_eq_ to _inline_eq_ separable sparse matrices _inline_eq_, where _inline_eq_ is associated with the _inline_eq_ row of _inline_eq_ (_inline_eq_) . If we consider _inline_eq_ as a N-D filter (_inline_eq_), we have: _inline_eq_ is equivalent to the result of convolving image _inline_eq_ with filter _inline_eq_ . Finally, we use _inline_eq_ that is expanded from the patch-based analysis operator _inline_eq_, to denote the global analysis operator associated with an entire image. Patch based analysis and synthesis model: Under the framework of MAP, the patch-based analysis model is given as the following minimization problem where _inline_eq_ is called analysis operator. The form of the penalty function _inline_eq_ depends on the prior utilized. For sparse representation, it can be _inline_eq_ or _inline_eq_ . The second type of prior is so-called synthesis prior. Basically, in the synthesis-based sparse representation model, a signal _inline_eq_ is called sparse over a given dictionary _inline_eq_, when it can be approximated as a linear combination of a few atoms from dictionary _inline_eq_ . This is formulated as following minimization problem using the MAP estimator. When we concentrate on the sparse prior, normally the penalty function _inline_eq_ is chose as _inline_eq_ . Learning patch based analysis and synthesis prior: In order to pursue better performance, an intuitive possibility is to make a better choice for the analysis operator _inline_eq_ and dictionary _inline_eq_ based on training. Indeed, there exist several typical and successful training algorithms for over-complete dictionary learning: (i) the K-SVD algorithm _cite_ (ii) On-line dictionary learning algorithm _cite_ (ii) efficient sparse coding algorithms _cite_ . However, compared to the extensive study for the training of the synthesis dictionary, the analysis operator learning problem has received relatively much less attention in the past decade, although the analysis model is the counterpart to the celebrated synthesis sparse model. But fortunately, it has been gaining more and more attention these two years. Consequently, there appear different algorithms for analysis operator learning _cite_ . Among existing analysis operator learning algorithms, the learning approach proposed by Peyr {\'e} and Fadili is very appealing since they consider this problem from a novel point of view. They interpret the action of analysis operator as convolution with some finite impulse response filters and they formulate the analysis operator learning task as a bi-level optimization problem _cite_ which is solved using a gradient descent algorithm. Contributions: Based on the investigation of existing dictionary and analysis operator learning algorithms, we find that (N) all the training approaches are based on patch priors; (N) the study of the later is immature since so far only few prior work has been tested with natural images _cite_ ; and (N) most analysis operator learning algorithms have to impose some non-convex constraints on the operator _inline_eq_ ; this therefore makes the corresponding optimization problems relatively complex and difficult to solve. Thus three questions arise: (N) can we formulate the image-based model using the patch priors? (N) is it possible to formulate the analysis operator learning problem in a relatively easy way? (N) can we compare two types of priors under an unified framework? We give answers to these questions in this paper.