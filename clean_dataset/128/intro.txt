Frame interpolation attempts to synthetically generate one or more intermediate video frames from existing ones, the simple case being the interpolation of one frame given two adjacent video frames. This is a challenging problem requiring a solution that can model natural motion within a video, and generate frames that respect this modelling. Artificially increasing the frame-rate of videos enables a range of applications. For example, data compression can be achieved by actively dropping video frames at the emitting end and recovering them via interpolation on the receiving end _cite_ . Increasing video frame-rate also directly allows to improve visual quality or to obtain an artificial slow-motion effect _cite_ . Frame interpolation commonly relies on optical flow _cite_ . Optical flow relates consecutive frames in a sequence describing the displacement that each pixel undergoes from one frame to the next. One solution for frame interpolation is therefore to assume constant velocity in motion between existing frames and interpolate frames via warping. However, optical flow estimation is difficult and time-consuming, and a good illustration of this is that the average run-time per _inline_eq_ frame of the top five performing methods of N in the Middlebury benchmark dataset _cite_ is N minutes . Furthermore, there is in general no consensus on a single model that can accurately describe it. Different models have been proposed based on inter-frame colour or gradient constancy, but these are susceptible to failure in challenging conditions such as occlusion, illumination or nonlinear structural changes. As a result, methods that obtain frame interpolation as a derivative of flow suffer from inaccuracies in flow estimation. Recently, deep learning approaches, and in particular, have set up new state-of-the-art results across many computer vision problems, and have also provided new optical flow estimation methods. In _cite_, optical flow features are trained in a supervised setup mapping two frames to their ground truth optical flow field. Spatial transformer networks _cite_ allow an image to be spatially transformed as part of a differentiable network, learning a transformation implicitly in an unsupervised fashion, hence enabling frame interpolation with an end-to-end differentiable network _cite_ . Choices in network design and training strategy can directly affect interpolation quality as well as efficiency. Multi-scale residual estimations have been repeatedly proposed in the literature _cite_, but only simple models based on colour constancy have been explored. More recently, training strategies have been proposed for low-level vision tasks to go beyond pixel-wise error metrics making use of more abstract data representations and adversarial training, producing visually more pleasing results _cite_ . An example of this notion applied to frame interpolation networks is explored very recently in _cite_ . In this paper we propose a real-time frame interpolation method that can generate realistic intermediate frames with high . It is the first model that combines the pyramidal structure of classical optical flow modeling with recent advances in spatial transformer networks for frame interpolation. Compared to naive CNN processing, this leads to a _inline_eq_ speedup with a _inline_eq_ dB increase in . Furthermore, to work around natural limitations of intensity variations and nonlinear deformations, we investigate deep loss functions and adversarial training. These contributions result in an interpolation model that is more expressive and informative relative to models based solely on pixel intensity losses as illustrated in and .