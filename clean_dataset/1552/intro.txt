Some natural images contain some degree of natural and artificial noise. These noises usually affect the visual quality of the original images, so the goal of image denoising is to reconstruct a reasonable estimate of the original image from the noisy image. Ideally, the resulting denoising image will not contain any noise or added artifacts. In the past few decades, many novel approaches have been proposed for image denoising _cite_ . One striking aspect of image denoising research is that a wide array of denoising strategies have remained popular, and in spite of vastly different approaches, many of these algorithms produce reasonably similar performance in terms of peak signal-to-noise ratio (PSNR) . For example, Fields of Experts (FoE) pursues an entirely parametric approach, by training Markov random fields with large N x N cliques to capture the statistics of small image patches. Over a set of six canonical images, FoE attained a PSNR of N for Gaussian noise with _inline_eq_ . A Gaussian scale mixtures also uses a parametric approach, and captures the joint statistics of neighboring Gabor filter coefficients. Over the same set of six images, mean PSNR was N for _inline_eq_ . Sparse dictionaries is a method that seeks to identify an optimal set of image patches to form the basis of a sparse LN norm. Over the same set of images, sparse dictionaries achieved a mean PSNR of N when the dictionary was trained from natural images, and N when trained on the noisy input image. Another method that exploits patterns found within the noisy input image is NL-means. However, NL-means uses a wholly non-parametric approach, by identifying similar patches within the noisy input image and averaging these together, weighed according to similarity and proximity. NL-means achieves a PSNR of N for _inline_eq_ on the same set of images. BMND is an algorithm with a similar strategy, but uses more sophisticated methods to combine similar image patches. This brief list of algorithms includes some that are parametric and others non-parametric, some that focus on matching natural scene statistics and others that focus on utilizing patterns from within the noisy input image, and some that use generatively trained probabilistic models, some discriminatively trained probabilistic models, and others that do not use probabilistic models at all. Numerous additional differences are evident between the implementation details of each approach. In spite of these significant differences in strategy, performance is reasonably similar between these varied algorithms. Continual improvements to denoising algorithms regularly change the dominant approach, and no category of denoising strategies has produced a clear enough victor to discourage further research in any other category. On the surface, this observation may suggest that image denoising algorithms are converging to some upper-bound on denoising performance. However, it is worth noting that each method is regarded as having different advantages and disadvantages. Strategies that perform best for low noise levels may not perform as well for high noise levels. Input images that contain many regular textures or patterns often benefit from non-local methods, while images with less internal regularities may benefit from algorithms trained from large suites of natural images. Additionally, one important quality of denoising methods is the ability to preserve sharp edges while removing noise. Methods that fail in this regard produce output that appear over-blurred. Non-local methods often perform well at maintaining sharp edges, as demonstrated by their residual images (the denoised image minus the true image) . These residual images show that methods like NL-means perform similarly near edges as they do near smooth regions. Other methods such as FoE show higher residual error near edges. Since FoE achieves a similar overall PSNR, it suggests that performance within smooth regions is higher for FoE. When multiple regression algorithms produce similar performance using significantly different approaches, and with distinct advantages and disadvantages, those algorithms are highly suitable for combination using ensemble learning methods. Ensemble learning is a method of combining multiple (possibly weak) predictors to produce one unified predictor of greater accuracy. While ensemble learning is a common and successful technique in machine learning, it has not been applied to image denoising. Ensemble learning methods benefit when the constituent algorithms are significantly different from one another. In this paper, we apply Bayesian ensemble learning methods to combine two of the most distinct denoising methods: Fields of Experts and NL-means. We use N natural images from the Berkeley Segmentation Benchmark for training _cite_ . Another set of N natural images from the Berkeley database are used for testing, along with N canonical images such as Barbara and Lena. For each level of input noise, the ensemble method achieved statistically significant improvement over both FoE and NL-means.