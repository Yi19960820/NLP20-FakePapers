Large-scale object classification and visual recognition have seen rising interest in the recent years. Data sets such as ImageNet~ _cite_ have helped scale up the number of classes represented in tasks such as object classification or detection. Many methods based on deep convolutional neural networks~ _cite_ have been developed recently to leverage the power of millions of training images distributed among thousands of classes. However, building a large data set, especially collecting large number of training images is very challenging and nevertheless this ends up representing only a fraction of the real visual world~ _cite_ . Transfer learning is a practical solution to bridge this gap as it allows to leverage knowledge and experience obtained from existing data to new domains. Specifically for object classification, knowledge from object categories which have labeled image samples can be transferred to new unseen categories which do not have training images. This task is referred to as zero-shot learning (ZSL) . There exist many directions in the literature to perform ZSL. These primarily differ in the type of knowledge source they tap in order to establish the connection between the unseen classes and the available visual information~ _cite_ . Among these directions, attribute-based knowledge transfer shows impressive performance~ _cite_ . By learning an intermediate layer of semantic attributes (\eg~colors or shapes), a novel class is then described with a subset of these attributes and its model is constructed accordingly based on the respective attribute classifiers. A major drawback of attribute-based approaches is that user supervision is needed to provide the description for each novel class. For example, for the new class ``leopard'' the user needs to describe it with a set of visual attributes in order to establish the semantic link with the learned visual vocabulary (\eg~the leopard has part paws, it exhibits a spotted pattern but does not live in water) . This amounts to providing manual class-attribute associations in the range of tens~ _cite_ to hundreds~ _cite_ of attributes for each new category. This is not only time consuming but often also requires domain-specific or expert knowledge~ _cite_ that the user is unlikely to have. It is more convenient and intuitive for the user to provide just the name of the unseen class rather than a lengthy description. Our goal is to remove this need for attribute supervision when performing zero-shot classification. We aim to automatically link a novel category with the visual vocabulary and predict its attribute association without user intervention. Thereby, we answer questions such as: Does the leopard live in the jungle? Does it have a striped pattern? (see~) . To this end, we propose a novel approach that learns semantic relations and automatically associates an unseen class with our visual vocabulary (\ie~the attributes) based solely on the class name. Using the predicted relations, we are able to construct a classifier of the novel class and conduct unsupervised zero-shot classification. Moreover, we demonstrate that our model is even able to automatically transfer the visual vocabulary itself across data sets which results in significant performance improvements at no additional cost. We demonstrate the effectiveness of such a model against state-of-the-art via extensive experiments.