The accuracy of convolutional neural networks (CNNs) _cite_ has been continuously improving _cite_, but the computational cost of these networks also increases significantly. For example, the very deep VGG models _cite_, which have witnessed great success in a wide range of recognition tasks _cite_, are substantially slower than earlier models _cite_ . Real-world systems may suffer from the low speed of these networks. For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection _cite_ and semantic segmentation _cite_ need to apply these models on higher-resolution images. It is thus of practical importance to accelerate test-time performance of CNNs. There have been a series of studies on accelerating deep CNNs _cite_ . A common focus of these methods is on the decomposition of one or a few layers. These methods have shown promising speedup ratios and accuracy on one or two layers and whole (but shallower) models. However, few results are available for accelerating models (\eg, _inline_eq_ N layers) . Experiments on complex datasets such as ImageNet _cite_ are also limited-\eg, the results in _cite_ are about accelerating a layer of the shallower AlexNet _cite_ . Moreover, performance of the accelerated networks as generic feature extractors for other recognition tasks _cite_ remain unclear. It is nontrivial to speed up, models for tasks like ImageNet classification. Acceleration algorithms involve not only the decomposition of layers, but also the optimization solutions to the decomposition. Data (response) reconstruction solvers _cite_ based on stochastic gradient descent (SGD) and backpropagation work well for simpler tasks such as character classification _cite_, but are less effective for complex ImageNet models (as we will discussed in Sec.~ _ref_) . These SGD-based solvers are sensitive to initialization and learning rates, and might be trapped into poorer local optima for regressing responses. Moreover, even when a solver manages to accelerate a single layer, the error of approximating multiple layers grow rapidly, especially for very deep models. Besides, the layers of a very deep model may exhibit a great diversity in filter numbers, feature map sizes, sparsity, and redundancy. It may not be beneficial to uniformly accelerate all layers. In this paper, we present an accelerating method that is effective for very deep models. We first propose a response reconstruction method that takes into account the nonlinear neurons and a low-rank constraint. A solution based on Generalized Singular Value Decomposition (GSVD) is developed for this nonlinear problem, without the need of SGD. Our explicit treatment of the nonlinearity better models a nonlinear layer, and more importantly, enables an reconstruction that accounts for the error from previous approximated layers. This method effectively reduces the accumulated error when multiple layers are approximated sequentially. We also present a rank selection method for adaptively determining the acceleration of each layer for a whole model, based on their redundancy. In experiments, we demonstrate the effects of the nonlinear solution, asymmetric reconstruction, and whole-model acceleration by controlled experiments of a N-layer model on ImageNet classification _cite_ . Furthermore, we apply our method on the publicly available VGG-N model _cite_, and achieve a N _inline_eq_ speedup with merely a N increase of top-N center-view error. The impact of the ImageNet dataset _cite_ is not merely on the specific N-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection _cite_ and semantic segmentation _cite_ . We exploit our method to accelerate the very deep VGG-N model for Fast R-CNN _cite_ object detection. With a N _inline_eq_ speedup of all convolutions, our method has a graceful degradation of N \% mAP (from N \% to N \%) on the PASCAL VOC N detection benchmark _cite_ . A preliminary version of this manuscript has been presented in a conference _cite_ . This manuscript extends the initial version from several aspects to strengthen our method. (N) We demonstrate compelling acceleration results on very deep VGG models, and are among the first few works accelerating very deep models. (N) We investigate the accelerated models for transfer-learning-based object detection _cite_, which is one of the most important applications of ImageNet pre-trained networks. (N) We provide evidence showing that a model trained from scratch and sharing the same structure as the accelerated model is inferior. This discovery suggests that a very deep model can be accelerated not simply because the decomposed network architecture is more powerful, but because the acceleration optimization algorithm is able to digest information.