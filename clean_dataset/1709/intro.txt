Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem for the needs addressed by the community such as automation and early identification of technical competence for surgeons in training. We approach the problem of video understanding of RAS videos as modeling the motions of surgeons. By analyzing the scene and object features, motion, low-level surgical gestures and the transitions among the gestures, we could model the activities taking place during surgical tasks _cite_ . The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries _cite_ . We propose to model the low-level surgical gestures; recurring common activity segments as described by Gao et al. _cite_ and the surgical tasks that are composed of gestures with a multimodal and multi-task learning approach. We use the different modalities of the visual features and motion cues (optical flow), and learn the temporal dynamics jointly. We argue that surgical tasks are better modeled by the visual features that are determined by the objects in the scene, while the low-level gestures are better modeled with motion cues. For example, the gesture Positioning the needle might take place in different tasks of Suturing and Needle Passing _cite_ . Or, on a higher level of actions, placing a Tie Knot might occur during a task of suturing on a tissue and a more specific and challenging task of Urethrovesical Anastomosis (UVA) _cite_ that involves stitching and reconnecting two parts together. If we rely on visual features of the object and the scenes only, these smaller segments of activities would have very different representations. Motion cues of low-level gestures, on the other hand, are independent of object and scene features. This helps us to classify common low-level gestures that are generic in nature and that reoccur in different tasks with different objects and under different settings better, and also helps reduce overfitting. However, we also believe that visual features are complementary to the motion cues, which are independent of object and scene features. The gesture ``Positioning the needle" might take place in different tasks of Suturing and Needle Passing, however is not likely to take place in Knot Tying tasks as a needle is not used for this task. In this paper, we address the problem of simultaneous classification of common low-level gestures and surgical tasks. While surgical task recognition, which is characteristically defined by the scene and the objects, is an easy task, the low-level gesture recognition remains a challenge. In our work, we focus on this challenging task by making use of complex relationships between the visual and motion cues, and the relationship between surgical activities at different levels of complexity. We propose a novel architecture that simultaneously classifies the common low-level gestures and the surgical tasks by making use of these joint relationships, combining shared and task-specific representations with a multi-task learning approach. Moreover, our architecture supports multimodal learning and is trained on both visual features and motion cues to achieve better performance. An overview of our method is shown in Figure _ref_ . We use the inputs of RGB frames for visual features and the RGB representation of the optical flow information of the same frames to refer to motion cues. We extract the high level features of these inputs using the convolutional neural networks we have trained on each task separately and use them as input to our recurrent joint model. After we convolve the two streams of input modality pairs, we concatenate the higher level features and use it as an input to our recurrent model that learns temporal dynamics of consequent frames.