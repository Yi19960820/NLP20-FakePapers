data with different modalities, including image, video, text, audio and so on, is now mixed together and represents comprehensive knowledge to perceive the real world. The research of cognitive science indicates that in human brain, the cognition of outside world is through the fusion of multiple sensory organs _cite_ . However, there exists the heterogeneity gap, which makes it quite difficult for artificial intelligence to simulate the above human cognitive process, because multimedia data with various modalities in Internet has huge quantity, but inconsistent distribution and representation. Naturally, cross-modal correlation exists among the heterogeneous data of different modalities to describe specific kinds of statistical dependencies _cite_ . For example, for the image and textual descriptions coexisting in one web page, they may be intrinsically correlated from content and share a certain level of semantic consistency. Therefore, it is necessary to automatically exploit and understand such latent correlation across the data of different modalities, and further construct metrics on them to measure how they are semantically relevant. For addressing the above issues, an intuitive idea is to model the joint distribution over the data of different modalities to learn the common representation, which can form a commonly shared space where heterogeneous data is mapped into. Thus, the similarities between them can be directly computed by adopting common distance metrics. Figure _ref_ shows an illustration of the above framework. In this way, the heterogeneity gap among the data of different modalities can be reduced, so that the heterogeneous data can be correlated together more easily to realize various practical application, such as cross-modal retrieval _cite_, where the data of different modalities can be retrieved at the same time by a query of any modality flexibly. Following the above idea, some methods _cite_ have been proposed to deal with the heterogeneity gap and learn the common representation by modeling the cross-modal correlation, so that the similarities between different modalities can be measured directly. These existing methods can be divided into two major categories according to their different models as follows: The first is in traditional framework, which attempts to learn mapping matrices for the data of different modalities by optimizing the statistical values, so as to project them into one common space. Canonical Correlation Analysis (CCA) _cite_ is one of the representative works, which has many extensions such as _cite_ . The second kind of methods _cite_ utilize the strong learning ability of deep neural network (DNN) to construct multilayer network, and most of them aim to minimize the correlation learning error across different modalities for the common representation learning. Recently, generative adversarial networks (GANs) _cite_ have been proposed to estimate a generative model by an adversarial training process. The basic model of GANs consists of two components, namely a generative model G and a discriminative model D . The generative model aims to capture the data distribution, while the discriminative model attempts to discriminate whether the input sample comes from real data or is generated from G . Furthermore, an adversarial training strategy is adopted to train these two models simultaneously, which makes them compete with each other for mutual promotion to learn better representation of the input data. Inspired by the recent progress of GANs, researchers attempt to apply GANs into computer vision areas, such as image synthesis _cite_, video prediction _cite_ and object detection _cite_ . Due to the strong ability of GANs in modeling data distribution and learning discriminative representation, it is a natural solution that GANs can be utilized for modeling the joint distribution over the heterogeneous data of different modalities, which aims to learn the common representation and boost the cross-modal correlation learning. However, most of the existing GANs-based works only focus on the unidirectional generative problem to generate new data for some specific applications, such as image synthesis to generate certain image by a noise input _cite_, side information _cite_ or text description _cite_ . Their main purpose is to generate new data of single modality, which cannot effectively establish correlation on multimodal data with heterogeneous distribution. Different from the existing works, we aim to utilize GANs for establishing correlation on the existing large-scale heterogeneous data of different modalities by common representation generation, which is a completely different goal to model the joint distribution over the multimodal input. For addressing the above issues, we propose Cross-modal Generative Adversarial Networks (CM-GANs), which aims to learn discriminative common representation with multi-pathway GANs to bridge the gap between different modalities. The main contributions can be summarized as follows. To the best of our knowledge, our proposed CM-GANs approach is the first to utilize GANs to perform cross-modal common representation learning. With learned common representation, heterogeneous data can be correlated by common distance metric. We conduct extensive experiments on cross-modal retrieval paradigm, to evaluate the performance of cross-modal correlation, which aims to retrieve the relevant results across different modalities by distance metric on the learned common representation, as shown in Figure _ref_ . Comprehensive experimental results show the effectiveness of our proposed approach, where our proposed approach achieves the best retrieval accuracy compared with N state-of-the-art cross-modal retrieval methods on N widely-used datasets: Wikipedia, Pascal Sentence and our constrcuted large-scale XmediaNet datasets. The rest of this paper is organized as follows: We first briefly introduce the related works on cross-modal correlation learning methods as well as existing GANs-based methods in Section II. Section III presents our proposed CM-GANs approach. Section IV introduces the experiments of cross-modal retrieval conducted on N cross-modal datasets with the result analyses. Finally Section V concludes this paper.