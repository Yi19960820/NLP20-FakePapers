Visual scene understanding~ _cite_ is a fundamental problem in computer vision. It aims at capturing the structural information in an image including the object entities and pair-wise relationships. As is shown in Figure~ _ref_, each entity and relation should be processed with a broader context to correctly understand the image at the semantic level. During recent years, deep neural network based object detection models such as Faster R-CNN~ _cite_ and YOLO~ _cite_ have achieved great improvements. However, such conventional object detection approaches cannot capture and infer the relationships within an image. Because of its ability to enrich semantic analysis and clearly describe how objects interact with each other~ (\eg ``a boy is riding a skateboard'' in Figure~ _ref_), generating scene graphs from images plays a significant role in multiple computer vision applications, such as image retrieval~ _cite_, image captioning~ _cite_, visual question answering~ _cite_ and video analysis~ _cite_ . The highly diverse visual appearances and the large numbers of distinct visual relations make scene graph generation a challenging task. Previous scene graph generation methods~ _cite_ locate and infer the visual relationship as a triplet in the form _inline_eq_ subject-predicate-object _inline_eq_, and the predicate is a word used to link a pair of objects, \eg _inline_eq_ boy-wearing-hat _inline_eq_ in Figure~ _ref_ . There exist various kinds of relationships between two objects, including spatial positions~ (\eg under, above), attributes/ prepositions~ (\eg with, of), comparatives~ (\eg taller, shorter) and actions/ verb~ (\eg play, ride) . Most of the existing works neglect the semantic relationship between the visual features and linguistic knowledge, and the intra-triplet connections. Moreover, previous works invariably utilize conventional deep learning models such as Convolutional Neural Networks~ (CNN) ~ _cite_ or Recurrent Neural Networks~ (RNN) ~ _cite_ for scene graph generation. These methods require to know the graph structure beforehand and contain computationally intensive matrix operations during approximation. In addition, most of them follow a step-by-step manner to capture the representation of nodes and edges, leading to neglect the global structure and information in whole image. Effectively extracting a whole joint graph representation to model the entire scene graph for reasoning is promising but remains an arduous problem. To address the aforementioned issues, we propose a novel that maps images to scene graphs. To be specific, the proposed method first adopts an object detection module to extract the location and category probability of each entity and relation. Then a semantic transformation module is introduced to translate entities and relation features as well as their linguistic representation into a common semantic space. In addition, we present a graph self-attention module to jointly embed an adaptive graph representation through measuring the importance of the relationship between neighboring nodes. Finally, a relation inference module is leveraged to classify each entity and relation by a Multi-Layer Perceptron~ (MLP), and to generate an accurate scene graph. Our main contributions are summarized as follows: