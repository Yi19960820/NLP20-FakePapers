How does a newborn agent learn about the world? When an animal (or robot) moves, its visual system is exposed to a shower of information. Usually, the speed with which something moves in the image is inversely proportional to its depth. \footnotemark As an agent continues to experience visual stimuli under its own motion, it is natural for it to form associations between the appearance of objects and their relative motion in the image. For example, an agent may learn that objects that look like mountains typically don't move in the image (or change appearance much) as the agent moves. Objects like nearby buildings and bushes, however, appear to move rapidly in the image as the agent changes position relative to them. This continuous pairing of images with motion acts as a kind of automatic supervision that could eventually allow an agent both to understand the depth of objects and to group pixels into objects by this predicted depth. Thus, by moving through the world, an agent may learn to predict properties (such as depth) of {\it static} scenes. A flurry of recent work has shown that {\it proxy tasks} (also known as {\it pretext} or {\it surrogate tasks}) such as colorization~ _cite_, jigsaw puzzles~ _cite_, and others~ _cite_, can induce features in a neural network that provide strong pre-training for subsequent tasks. In this paper, we introduce a new proxy task: estimation of relative depth from a single image. We show that a network that has been pre-trained, without human supervision, to predict relative scene depth provides a powerful starting point from which to fine-tune models for a variety of urban scene understanding tasks. Not only does this automatically supervised starting point outperform all other proxy task pre-training methods. For monocular depth understanding, it even performs better than the heavily supervised ImageNet pre-training, yielding results that are comparable with state-of-the-art methods. To estimate relative scene depths without human supervision, we resort to a recent motion segmentation technique~ _cite_ to estimate relative depth from geometric constraints between scene's motion field and camera motion. We apply it to simple, publicly available YouTube videos taken from moving cars. Since this technique estimates depth {\em up to an unknown scale factor}, we compute {\em relative depth} of the scene during the pre-training phase, where each pixel's value is in the range of _inline_eq_ denoting its depth percentile over the entire image. Unlike work that analyzes video paired with additional information about direction of motion~ _cite_, our agent learns from ``raw egomotion'' video recorded from cars moving through the world. Unlike methods that require videos of moving objects~ _cite_, we neither depend on, nor are disrupted by, moving objects in the video. Once we have relative depth estimates for these video images, we train a deep network to predict the relative depth of each pixel from a {\em single image}, \ie, to predict the relative depth {\em without} the benefit of motion. One might expect such a network to learn that an image patch that looks like a house and spans N pixels of an image (about N meters away) is significantly further away than a pedestrian that spans N image pixels (perhaps N meters away) . Figure _ref_ illustrates this prediction task and shows sample results obtained using a standard convolutional neural network (CNN) in this setting. For example, in the leftmost image of Fig.~ _ref_, an otherwise unremarkable traffic-light pole is clearly highlighted by its relative depth profile, which stands out from the background. Our hypothesis is that to excel at relative depth estimation, the CNN will benefit by learning to recognize such structures. The goal of our work is to show that pre-training a network to do relative depth prediction is a powerful proxy task for learning visual representations. In particular, we show that a network pre-trained for relative depth prediction (from automatically generated training data) improves training for downstream tasks including semantic segmentation, joint semantic reasoning of road segmentation and car detection, and monocular (absolute) depth estimation. We obtain significant performance gains on urban scene understanding benchmarks such as KITTI~ _cite_ and CityScapes~ _cite_, compared to training a segmentation model from scratch. Compared to nine other proxy tasks for pre-training, our proxy task consistently provides the highest gains when used for pre-training. In fact, our performance on semantic segmentation and joint semantic reasoning tasks comes close to that of equivalent architectures pre-trained with ImageNet~ _cite_, a massive labeled dataset. Finally, for the monocular (absolute) depth estimation,, using both VGGN~ _cite_ and ResNetN~ _cite_ architectures. As a final application, we show how our proxy task can be used for {\it domain adaptation} . One might assume that the more similar the domain of unlabeled visual data used for the proxy task (here, three urban scene understanding tasks) is to the domain in which the eventual semantic task is defined (here, semantic segmentation), the better the representation learned by pre-training. This observation allows us to go beyond simple pre-training, and effectively provide a domain adaptation mechanism. By adapting (fine-tuning) a relative depth prediction model to targets obtained from unlabeled data in a novel domain (say, driving in a new city) we can improve the underlying representation, priming it for better performance on a semantic task (\eg, segmentation) trained with a small labeled dataset from this new domain. In experiments, we show that pre-training on unlabeled videos from a target city, absent any labeled data from that city, consistently improves all urban scene understanding tasks. In total, our work advances two pathways for integrating unlabeled data with visual learning. Such methods of extracting knowledge from unlabeled data are likely to be increasingly important as computer vision scales to real-world applications; here massive dataset size can outpace herculean annotation efforts.