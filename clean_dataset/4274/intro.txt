Deep learning has achieved great results in many domains including computer vision. However, it is still far from being deployed in many real-world applications due to reasons including: Explaining the prediction of deep neural networks is a challenging task because they are complex models with large number of parameters. Recently, XAI has become a trending research area in which the goal is to develop reliable interpretation algorithms that explain the underlying decision making process. Designing such algorithms is a challenging task and considerable work _cite_ has been done to describe local explanations-explaining the model's output for a given input _cite_ . It has been shown that deep neural networks are vulnerable to adversarial examples. These carefully constructed samples are created by adding imperceptible perturbations to the original input for changing the final decision of the network. This is important for two reasons: (a) Such vulnerabilities could be used by adversaries to fool AI algorithms when they are deployed in real-world applications such as Internet of Things (IoT) _cite_ or self-driving cars _cite_ (b) Studying these attacks can lead to better understanding of how deep neural networks work and also possibly better generalization. In this paper, we design adversarial attack algorithms that not only fool the network prediction but also fool the network interpretation. Our main goal is to utilize such attacks as a tool to investigate the reliability of network interpretation algorithms. Moreover, since our attacks fool the network interpretation, they can be seen as a potential vulnerability in the applications that utilize network interpretation to understand the cause of the prediction (e.g., in health-care applications _cite_ .) We are interested in studying the reliability of the interpretation in highlighting true cause of the prediction. To this end, we use the adversarial patch method _cite_ to design a {\em controlled} adversarial attack setting where the adversary changes the network prediction by manipulating only a small region of the image. Hence, we know that the cause of the wrong prediction should be inside the patch. We show that it is possible to optimize for an adversarial patch that attacks the prediction without being highlighted by the interpretation algorithm as the cause of the wrong prediction. Grad-CAM _cite_ is one of the most well-known network interpretation algorithms that performs well on sanity check among state-of-the-art interpretation algorithms recently studied in _cite_ . Hence, we choose to study the correctness of Grad-CAM as a case study. Also, we show that our results even though tuned for Grad-CAM, can transfer directly to Occluding Patch _cite_ as another interpretation algorithm. As an example, in Figure _ref_, the original image (left) is correctly classified as ``French Bulldog''. On the top row, a targeted adversarial patch has successfully changed the prediction to ``Soccer Ball''. Since the adversary is able to manipulate only the pixels inside the patch, it is expected that the interpretation algorithm (e.g, Grad-CAM) for ``Soccer Ball'' category should highlight some patch pixels as the cause of the wrong prediction. This is shown in the first row, top-right image. However, in the bottom row, our adversarial patch algorithm, not only changes the prediction to ``Soccer Ball'', but also does it in a way that Grad-CAM does not highlight the pixels inside the patch. We argue that since the adversary can change only the patch pixels, we know that the cause of the wrong prediction should be inside the patch. So, the observation that Grad-CAM does not highlight the patch pixels reveals that Grad-CAM is not reliably highlighting the source of prediction. Note that in this setting, the target category is not chosen by the model and is randomly chosen by the adversary from all possible wrong categories (i.e., uniformly from N categories of ImageNet) . We believe this shows that the Grad-CAM algorithm is not {\em necessarily} showing the true cause of the prediction. We optimize the patch by adding a new term in the optimization of adversarial patches that suppresses Grad-CAM activation at the location of the patch while still encouraging the wrong prediction (target category) . We believe our algorithms can be used as a form of evaluation for future interpretation algorithms. Our attack is more practical since we are manipulating only a patch and hence is closer to real world applications. As a practical example, some applications in health-care are not only interested in the prediction, but also understanding the cause of it (e.g., what region of a medical image of a patient causes diagnosis of cancer.) We believe our attacks can be generalized beyond object classification to empower an adversary to manipulate the reasoning about some medical diagnosis. Such an attack can cause serious issues in health-care, for instance by manipulating medical records to charge insurance companies _cite_ . \noindent Our key contributions are summarized as follows: {\bf (N)} We introduce a novel algorithm to construct adversarial patches which fool both the classifier and the interpretation of the resulting category. {\bf (N)} With extensive experiments, we show that our method (a) generalizes from Grad-CAM to Occluding Patch _cite_, another interpretation method, (b) generalizes to unseen images (universal), (c) is able to fool GAIN _cite_, a model specifically trained with supervision on interpretation, and (d) is able to make the interpretation uniform to hide any signature of the attack. {\bf (N)} We use these attacks as a tool to assess the reliability of Grad-CAM, a popular network interpretation algorithm. This suggests that the community needs to develop more robust interpretation algorithms possibly using our tool as an evaluation method.