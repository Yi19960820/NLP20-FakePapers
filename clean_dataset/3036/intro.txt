Sparse representation has been successfully applied in various image processing and computer vision problems, such as image denoising, and image restoration. Dictionary learning is one way of obtaining sparse representations for signals with unknown precise model. The resulting sparse representation as a linear combination of atoms varies according to the type of dictionary learning techniques: Synthesis Dictionary Learning (SDL) and Analysis Dictionary Learning (ADL) . \par In contrast to SDL, which assumes that the interesting signal can be recovered by a dictionary with corresponding sparse coefficients, ADL is based on applying the dictionary to the data to yield sparse coefficients. Due to the success of dictionary learning in image restoration problems, task-driven dictionary learning methods are of great interest in many inference problems, such as image classification. There are broadly two strategies to address the task-driven dictionary learning method. The first strategy is to learn multiple class-specific sub-dictionaries to make the dictionary more structured, and to increase overall discrimination between different classes _cite_ . To be structured, the atoms in the dictionary are made to learn their own class labels. A class label for a new image can then be decided by comparing reconstruction error from different classes. Another strategy is to learn a shared dictionary for all classes and jointly learn a universal classifier to enforce more discriminative sparse representations _cite_ . All of the above mentioned techniques have been developed and implemented in the SDL framework, while ADL has increasingly received attention _cite_ . To the best of our knowledge, none of the standard ADL algorithm such as the analysis K-SVD _cite_ or the Sparse Null Space (SNS) pursuit _cite_ has addressed the task driven ADL problem. Shekhar _cite_ have adopted ADL together with SVM to digits and face recognition, and demonstrated that ADL is more stable under noise and occlusion with a competitive performance with SDL. Guo _cite_ integrated local topological structures and discriminative sparse labels into the ADL to yield a _inline_eq_ Nearest Neighbor method to classify images. Inspired by these past efforts and efficient coding of ADL, we propose an integration of structured subspace regularization and supervised learning into an ADL model to obtain a more structured discriminative and efficient approach to image classification. It has been shown, for example in the context of sparse subspace clustering _cite_, that the sparse representations of the data within a class share a low dimensional subspace. A structuring block diagonal matrix therefore is introduced to achieve these localized subspaces of the sparse codes. This yields more coherence for within-class sparse representations and more disparity for between-class representations. To induce additional robustness in the sought sparse representation, a one-against-all regression-based classifier is jointly learned, with a resulting optimization functional which we solve by a linearized alternating direction method (ADM) _cite_ . This approach is computationally more efficient than analysis K-SVD _cite_ and SNS pursuit _cite_ . Moreover, a great advantage of our algorithm is its extremely short on-line encoding and classification time. Our experiments demonstrate that our method achieves a better overall performances than the synthesis dictionary approach. The balance of this paper is organized as follows: In Section _ref_, we state and formulate the problem. We discuss the resulting solution to the optimization problem in Section _ref_ . The experimental validation and results are comprehensively presented in Section _ref_ . We finally provide some concluding comments in Section _ref_ .