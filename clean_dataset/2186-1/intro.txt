It has been estimated that humans can easily distinguish between approximately N basic object categories _cite_ and many more subordinate ones, such as different species of birds. Without seeing them, human beings can even recognize new unseen categories by leveraging other information (e.g. by reading text descriptions about object categories on the internet) . In contrast, encumbered with a lack of adequate data, generally machines can only recognize hundreds or thousands categories. To free recognition tasks from exuberant collecting of large labelled image datasets, zero-shot learning (ZSL) is gaining increasing attention in recent years, which aims to recognize instances from the new unseen categories which have no instances during training _cite_ . With the label sets between seen and unseen categories being disjoint, the key in the general methodology of ZSL is to establish the inter-class connections via intermediate semantic representations, either manually defined by human experts annotated attributes _cite_, or automatically extracted from auxiliary text sources _cite_ . Unseen categories can thus be predicted by transferring information from the training dataset. As a valuable knowledge base given in advance, in theoretical, the semantic representations of unseen categories are encouraged to be leveraged in any stage during ZSL. However, most recent works mainly focus on exploring these representations to construct a more effective classifier during testing. While, how to explore them during training to learn more generalized is equally important but still left far from being solved, since the quality of semantic representation predictor is much more rewarding _cite_ . In addition, due to the disjoint data distribution between seen and unseen classes, direct knowledge transfer will cause the domain shift problem during ZSL, leading to degraded performance. In this letter, we tackle these challenges with ideas from generative learning. We posit that there exists a latent space, as illustrated in Fig. _ref_, where each object category is encoded by a unique data (called prototype) essentially. Any type of object representations, e.g. texts or images, are generated from its corresponding prototypes from different perspectives. This supposition is inspired from the cognitive process of human beings, who have remarkably ability of generating various representations, e.g. images, audios, texts, from high-level category labels _cite_ . For example, it is almost effortless for people to imagine the different picture/audios, given the label 'penguin' and 'sparrow'. To mathematically formulate this institution, we assume that the latent prototypes obey a prior distribution where one can draw samples from. During the data generation process, the category prototype is first generated, from which then different observed representations can be developed. Based on this generation process, we further explore the semantic representations given beforehand to make the training process generalize well across unseen categories. A simple ZSL method encompassing different strategies is proposed to solve the domain shift problem by generating virtual unseen instances. Ahead of time, we further give a simple property about these semantic representations as a basic condition for their application in ZSL. Extensive experiments on real world datasets show our proposed method can achieve state-of-the-art results.