Facial analysis task has recently become an area of the great interest for many researchers and industrial works. It automatically infers the information regarding emotional states, levels of engagement, pain detection, action unit detection, facial landmark detection and facial expression recognition from nonverbal behavior _cite_ . These activities show the depth in this area. With the interest in improving Human-Computer Interaction (HCI), the study of human behavior and its state of mind is a must _cite_ . Expressions can be said as a gateway to humans state of mind for that particular instance. Thus for HCI, human's face plays a vital role. This paper aims to improve the facial action unit detection by drawing a relative bridge between expressions and the action units. Facial action units refer to the movement of the facial muscles such as raising eyebrows, opening mouth, lifting up cheeks etc. \These movements can express the human state of mind or some actions such as eating, speaking, or communicating through facial gestures _cite_ . Thus, Action Units (AUs) convey a lot of information. AUs and facial muscle movements share different types of relationships: one to one, one to many, many to one _cite_ . Not only some particular facial muscle movements represent some AUs, but several facial muscles movements can represent a particular AU and vice versa as well. These are set according to Facial Action Coding System (FACS) _cite_ . FACS is an anatomically-based comprehensive system to annotate the visible facial muscle movements. Human minds are trained to observe the facial muscle movements and then to relate with the expressions _cite_ . Many expression recognition tasks have shown the superior performance with the help of AUs as the input features _cite_ . Here, we are interested to predict AUs with the help of categorical emotion classes based features. Our motivation is to observe that whether expression based information could be beneficial for AUs prediction? To this end, we performed our experiments on the DISFA _cite_ and the EmotioNet _cite_ dataset. We train a network that can deal with the dense blocks with residual sharing, for the task of FAU detection. Secondly, to utilize facial expression features, we train our proposed network on RAF-DB dataset for basic emotion recognition. Using this model, we extract second last layer features and tried to fuse these features in the model for action unit detection. Results show the superior performance of the proposed network over previous state-of-the-art results. Later on, we also observed the cross modality performance of the proposed network. The key contributions of this paper are: