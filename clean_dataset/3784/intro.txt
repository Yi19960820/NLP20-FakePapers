Object recognition is one of the most challenging problems in computer vision, and is catalysed by the swift development of deep learning~ _cite_ in recent years. Various works have achieved exciting results on several RGB object recognition challenges~ _cite_ . However, there are several limitations for object recognition using only RGB information in many real world applications, as it projects the N-dimensional world into a N-dimensional space which leads to inevitable data loss. To amend those shortcomings of RGB images, using depth images as a complimentary is a plausible way. The RGB image contains information of color, shape and texture while the depth contains information of shape and edge. Those basic features can serve both as a strength or weakness in object recognition. For example, we are able to tell the difference between an apple and a table simply by the shape information from depth. However it is ambiguous when it comes to figure out whether it is an apple or an orange just by depth. When an orange plastic ball and an orange are placed together, it is equally difficult for us to tell the difference just by RGB image. This means that a simple combination of features from two modalities sometimes jeopardizes the discriminability of feature. Therefore, we are supposed to choose those shared and specific features more wisely. Thus, we believe a more elaborated combination of modality-specific and modality-correlated features will generate a more discriminative representation. \iffalse With the development of high-quality consumer depth cameras such as the Microsoft Kinect, numerous efforts have been made for RGB-D object recognition in recent years. Compared to RGB object recognition, the introduction of depth images greatly improves the recognition performance because depth information provides geometrical cues which are invariant to lighting and color variations, which are usually difficult to describe in RGB images. As there is a growing trend of the appearance of many depth-camera embedded devices like Google Tango~ _cite_ and Microsoft Hololens~ _cite_, the requirements and application potentials for RGB-D object recognition technology are growing rapidly. \fi There are two main procedures in RGB-D object recognition: feature representation~ _cite_ and object matching~ _cite_ . Compared with object matching, feature representation affects the performance of the object recognition system significantly, because real-world objects usually suffer from large intra-class discrepancy and inter-class affinity. A variety of methods have been proposed for RGB-D object representation recently \iffalse ~ _cite_ \fi, and they can be mainly classified into two categories: hand-crafted methods and learning-based methods. Methods in the first category design an elaborated hand-crafted descriptor for both the RGB and depth channels~ _cite_ for feature extraction. \iffalse Representative features include textons~ _cite_, color histograms~ _cite_, SIFT~ _cite_ and SURF~ _cite_, which describe objects from different aspects such as color, shape, and texture. \fi However, these hand-crafted methods usually require large amount of domain-specific knowledge, which is inconvenient to generalize to different datasets. Methods in the second category employ some machine learning techniques to learn feature representations in a data-driven manner, so that more data-adaptive discriminative information can be exploited~ _cite_ . However, most existing learning-based methods consider the RGB and depth information from two different channels individually, which ignores the sharable property and the interaction relationship between these two modalities. To address this, multi-modal learning approach recently has been presented for RGB-D object recognition. \iffalse Wang~ et al. ~ _cite_ proposed a multi-modal learning approach by extracting RGB and depth features within the deep learning framework, which can fully exploit the information of both RGB and depth modalities. While the correlation information of the RGB and depth information can be exploited, the modal-specific information has not been explicitly modelled in their method. Another weakness of that work is that, as they fixed the total dimension of shareable and specific feature, they didn't provide any analysis on how to choose the ratio between shareable dimension and specific dimension. Moreover, they treat the individual part and correlated part equally, which neglects the different discriminative power between them. \fi In this paper, we propose a correlated and individual multi-modal deep learning (CIMDL) method for RGB-D object recognition. Specifically, we develop a multi-modal learning framework to learn discriminative features from both the correlated and individual parts, and automatically learn the weights for different feature components in a data-driven manner. The basic pipeline of our proposed CIMDL is illustrated in Figure~N. \iffalse We first utilize two ways of deep CNNs to learn features from RGB and depth modality individually. Then, we feed the features learned from these two channels into a multi-modal learning layer to concatenate them. \fi This layer is designed for three purposes: N) generating the correlated part between these two modalities, N) extracting the discriminative part of features from both of these two modalities and N) learning the weights for the correlated and individual parts automatically for feature combination. \iffalse Finally, a feature vector containing both the correlated part and the individual part of the RGB and depth modalities is obtained at the top layer of the network, which is used as the final feature representation for RGB-D object recognition. \fi The parameters of the whole network are updated by using the back-propagation criterion. Experimental results on the RGB-D object~ _cite_ and NDND~ _cite_ datasets are presented to show the effectiveness of the proposed approach.