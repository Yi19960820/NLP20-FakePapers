In figure-ground segmentation, the regions of interest are conventionally defined by the provided ground truth, which is usually in the form of pixel-level annotations. Without such supervised information from intensive labeling efforts, it is challenging to teach a system to learn what the figure and the ground should be in each image. To address this issue, we propose an unsupervised meta-learning approach that can simultaneously learn both the {\em figure-ground concept} and the corresponding {\em image segmentation} . The proposed formulation explores the inherent but often unnoticeable relatedness between performing image segmentation and creating visual effects. In particular, to visually enrich a given image with a special effect often first needs to specify the regions to be emphasized. The procedure corresponds to constructing an internal representation that guides the image editing to operate on the target image regions. For this reason, we name such an internal guidance as the {\em Visual-Effect Representation} (VER) of the image. We observe that for a majority of visual effects, their resulting VER is closely related to image segmentation. Another advantage of focusing on visual-effect images is that such data are abundant from the Internet, while pixel-wise annotating large datasets for image segmentation is time-consuming. However, in practice, we only have access to the visual-effect images, but not the VERs as well as the original images. Taking all these factors into account, we reduce the meta-problem of figure-ground segmentation to predicting the proper VER of a given image for the underlying visual effect. Owing to its data richness from the Internet, the latter task is more suitable for our intention to cast the problem within the unsupervised generative framework. Many compositional image editing tasks have the aforementioned properties. For example, to create the {\tt color selectivo} effect on an image, as shown in Fig.~ _ref_, we can) identify the target and partition the image into foreground and background layers,) convert the color of background layer into grayscale, and) combine the converted background layer with the original foreground layer to get the final result. The operation of color conversion is local---it simply ``equalizes'' the RGB values of pixels in certain areas. The quality of the result depends on how properly the layers are decomposed. If a part of the target region is partitioned into the background, the result might look less plausible. Unlike the local operations, to localize the proper regions for editing would require certain understanding and analysis of the global or contextual information in the whole image. In this paper, we design a GAN-based model, called Visual-Effect GAN (VEGAN), that can learn to predict the internal representation (\ie, VER) and incorporate such information into facilitating the resulting figure-ground segmentation. We are thus motivated to formulate the following problem: Given an unaltered RGB image as the input and an image editing task with known compositional process and local operation, we aim to predict the proper VER that guides the editing process to generate the expected visual effect and accomplishes the underlying figure-ground segmentation. We adopt a data-driven setting in which the image editing task is exemplified by a collection of image samples with the expected visual effect. The task, therefore, is to transform the original RGB input image into an output image that exhibits the same effect of the exemplified samples. To make our approach general, we assume that corresponding pairs of input and output images are available in training, and therefore supervised learning is not applicable. That is, the training data does not include pairs of the original color images and the corresponding edited images with visual effects. The flexibility is in line with the fact that although we could fetch a lot of images with certain visual effects over the Internet, we indeed do not know what their original counterpart should look like. Under this problem formulation, several issues are of our interest and need to be addressed. First, {\em how do we solve the problem without paired input and output images?} We build on the idea of generative adversarial network and develop a new unsupervised learning mechanism (shown in Figs.~ _ref_ ~ \&~ _ref_) to learn the internal representation for creating the visual effect. The aims to predict the internal VER and the is to convert the input image into the one that has the expected visual effect. The compositional procedure and local operation are generic and can be implemented as parts of the architecture of a ConvNet. The has to judge the quality of the edited images with respect to a set of sample images that exhibit the same visual effect. The experimental results show that our model works surprisingly well to learn meaningful representation and segmentation without supervision. Second, {\em where do we acquire the collection of sample images for illustrating the expected visual effect?} Indeed, it would not make sense if we have to manually generate the labor-intensive sample images for demonstrating the expected visual effects. We show that the required sample images can be conveniently collected from the Internet. We provide a couple of scripts to explore the effectiveness of using Internet images for training our model. Notice again that, although the required sample images with visual effects are available on the Internet, their original versions are unknown. Thus supervised learning of pairwise image-to-image translation cannot be applied here. Third, {\em what can the VER be useful for, in addition to creating visual effects?} We show that, if we are able to choose a suitable visual effect, the learned VER can be used to not only establish the intended figure-ground notion but also derive the image segmentation. More precisely, as in our formulation the visual-effect representation is characterized by a real-valued response map, the result of figure-ground separation can be obtained via binarizing the VER. Therefore, it is legitimate to take the proposed problem of VER prediction as a surrogate for unsupervised image segmentation. We have tested the following visual effects:) {\tt black background}, which is often caused by using flashlight;) {\tt color selectivo}, which imposes color highlight on the subject and keeps the background in grayscale;) {\tt defocus/Bokeh}, which is due to depth of field of camera lens. The second column in Fig.~ _ref_ shows the three types of visual effects. For these tasks our model can be end-to-end trained from scratch in an unsupervised manner using training data that do not have either the ground-truth pixel labeling or the paired images with/without visual effects. While labor-intensive pixel-level segmentations for images are hard to acquire directly via Internet search, images with those three effects are easy to collect from photo-sharing websites, such as Flickr, using related tags.