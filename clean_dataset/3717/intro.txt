Metric learning can automatically learn a suitable metric from data, making it widely used in machine learning and data mining. From the perspective of feature learning, metric learning can learn a new discriminative feature space by feature transformation (Mahalanobis distance metric) . For Mahalanobis distance metric learning~ _cite_, one explainable and successful framework, the goal is to learn a metric function _inline_eq_ parameterized with a positive semi-definite parameter matrix _inline_eq_, which calculates the distance between samples _inline_eq_ and _inline_eq_ . _inline_eq_ can be mathematically decomposed as _inline_eq_, where _inline_eq_ is the linear transformation matrix, and _inline_eq_ is the rank of _inline_eq_ . However, these shallow metric learning algorithms usually learn just one metric space, one transformation space. Can we further improve this learnt metric space? Inspired by the hierarchical nonlinear learning characteristic of deep learning, we have an assumption that we may learn better metric space by learning new metric in the learnt feature space progressively (see Figure~ _ref_) . In order to verify this assumption, we present a hierarchical metric learning framework. In this framework, a representative metric learning algorithm is picked and taken as one metric layer, followed by a nonlinear layer, and then these layers are repeatedly stacked several times, to construct a metric-based deep network. Certainly, different metric learning algorithms can be integrated together as different metric layers. For simplicity and applicability, we implement an framework concretely. Specifically, one online metric learning (OML) algorithm is taken as a metric layer, followed by a nonlinear layer (ReLU), and then these two layers are repeatedly stacked several times. If we add a loss layer at the end of this network, this structure is very similar to Multilayer Perceptron (MLP) . However, in the proposed ODML, each metric layer is a relatively independent and intact OML algorithm with its own local loss, which is different from the hidden layer without supervision information in the traditional MLP. Recently, a series of algorithms have also been presented. The main difference between the proposed ODML and existing DML algorithms is that existing DML mainly utilize metric loss to train a better deep neural network, while ODML wants to reuse existing shallow metric learning algorithms and tries to reveal the essence of metric learning in a more transparent and theoretical way. From the perspective of training style, as each metric layer is a relatively independent OML algorithm, the parameter of each metric layer can be updated according to its own local loss during forward propagation (FP) . In this way, it is possible to train a network by only using FP strategy. The advantages of FP updating are that (N) the parameter updating is immediate, unlike the delayed updating of back propagation (BP) ; (N) when additional BP is adopted, FP updating can vastly accelerate the convergence. The second advantage happens to have the similar view of layer-wise unsupervised pre-training~ _cite_, which has a decisive position in the early stages of deep learning. However, the layer-wise unsupervised learning is unsupervised and lack of theoretical guarantee, which only acts as a pre-training operation (or a regularizer~ _cite_) . In contrast, FP updating in ODML is supervised and can serve as the primary training mode rather than a pre-training role (discussed in Section~ _ref_) . In fact, these two updating strategies (FP and BP) can be combined to train this metric-based deep network. Ideally, forward updating can explore new feature spaces sequentially, while backward updating can amend the exploration. Specifically, to facilitate the theoretical analysis of ODML, we design a new general Mahalanobis-based Online Metric Learning (MOML) algorithm. MOML has a convex objective function and enjoys a closed-form solution with few constraints at each time step. We also present a regret bound of MOML, which can guarantee its convergence. Through stacking MOML hierarchically, the ability of learning feature representation progressively can be guaranteed and explicable. In summary, the contributions of the proposed ODML can be exhibited as follows,