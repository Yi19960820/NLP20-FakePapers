Effective feature extraction and data representation are key factors to successful medical imaging tasks. Researchers usually adopt medical domain knowledge and ask for annotations from clinical experts. For example, using traditional image processing techniques such as filters or edge detection techniques to extract clinically relevant spatial features from images obtained by different image modalities, such as mammography~, lung computed tomography~ (CT) ~, and brain magnetic resonance imaging~ (MRI) ~ . The handcrafted features with supervised learning using expert-annotated labels work appropriately for specific scenarios. However, using predefined expert-derived features for data representation limits the chance to discover novel features. It is also very expensive to have clinicians and experts to label the data manually, and such labor-intensive annotation task limits the scalability of learning generalizable medical imaging representations. To learn efficient data representations of medical images, researchers recently have used different deep learning approaches and applied to various medical image machine learning tasks, such as image classification~, image segmentation~, or content-based image retrieval (CBMIR) ~ . CBMIR is a task that helps clinicians make decisions by retrieving similar cases and images from the electronic medical image database~ (Figure ~ _ref_) . CBMIR requires expressive data representations for knowledge discovery and similar image identification in massive medical image databases, and has been explored by different algorithmic approaches~ . However, the previous works of CBMIR focused more on using shallow learning algorithms, or combining single pre-trained CNN structure with other techniques~, which relies heavily on manually annotated, high-quality ground truth labeling. To mitigate these issues, we proposed a deep Siamese CNN~ (SCNN) that can learn fixed-length latent image representation from solely image pair information in order to reduce the dependency of using actual class labels annotated by human experts~ . We then evaluated the learned image representations on the task of CBMIR using a publicly available diabetic retinopathy~ (DR) fundus image dataset. We compared the image representations learned by the proposed deep SCNN with the single pre-trained supervised CNN architecture~ . The architecture of the proposed deep SCNN is illustrated in Figure~ _ref_ . The deep SCNN learns to differentiate an image pair by evaluating the similarity and relationship between the given images. Each image in the image pair is fed into one of the identical CNN, and the contrastive loss is computed between two outputs of CNNs. The model is an end-to-end structure to obtain a latent representation of the image, which can be used for further CBMIR task. The main contributions of this work are that we propose an end-to-end deep SCNN model for learning latent representations of medical images with minimal expert labeling efforts by reducing the multiclass problem to binary class learning problem, and apply them in the task of CBMIR using retina fundus images as a proof of concept. Experimental results show that SCNN's performance is comparable to that of the state-of-the-art CBMIR method using single supervised pre-trained CNN, but requires much less supervision for training.