Fully convolutional neural networks (F-CNNs) are being increasingly adopted for pixel/voxel-wise semantic segmentation of images in an end-to-end fashion. F-CNNs are typically constructed with a dumb-bell like architecture comprising of the encoder and decoder blocks in sequence~ _cite_ . One of the main architectural advances has been the introduction of connectivity amongst and within these blocks, which has in turn improved parameter optimization and gradient flow. Computational graph elements associated with such a connectivity can be broadly categorized into long-range and short-range connections. Long-range connections were first introduced by Ronnerberger et al. ~ _cite_ as skip connections between the encoder and decoder blocks and were demonstrated to improve information recovery and gradient flow. Short-range connections between convolutional layers were introduced in the seminal work on residual networks by He et al. ~ _cite_ . This idea was taken further within the work of densely-connected neural networks~ _cite_, wherein multiple convolutional layers were stacked in sequence along with connections that iteratively concatenate the feature maps with outputs of the previous layers. Introducing these short-range dense connections alleviate vanishing gradients, encourage feature reusability and strengthen information propagation across the network~ _cite_ . One commonality between design of the computational graph within the aforementioned architectures is the use of concatenation layers to aggregate information through these connections. Such a design increases the size of the output feature map along the feature channels, which in turn results in the need to learn filters with a higher number of parameters. Goodfellow et al. introduced the idea of competitive learning through maxout activations~ _cite_, which was adapted by Liao and Carneiro~ _cite_ for competitive pooling of multi-scale filter outputs. Both ~ _cite_ and~ _cite_ proved that the use of a maxout competitive unit boosts performance by creating a large number of dedicated sub-networks within a network that learns to target specific sub-tasks within the training task and reduces the number of parameters required. In this paper, we explore how such competitive units fare within a FCNN architecture targeted at biomedical image segmentation. We propose the Competitive Dense Fully Convolutional Network (CDFNet) by using competitive layers instead of concatenation by suitably adopting the DenseNet architecture proposed by Roy et al. in~ _cite_ . Particularly, we demonstrate that competitive units promote the formation of dedicated local sub-networks in each of the densely connected blocks within the encoder and the decoder paths. This in turn encourages sub-modularity through a network-in-network design that can learn more efficiently. Towards this, we propose two novel architectural elements targeted at introducing competition within the short-and long-range connections, as follows: The proof-of-concept for CDFNet is shown on the challenging task of whole-body segmentation in contrast-enhanced abdominal Magnetic Resonance Imaging (abMRI) scans as a part of the publicly available VISCERAL segmentation benchmark~ _cite_ .