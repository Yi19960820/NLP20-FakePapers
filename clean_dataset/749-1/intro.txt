This paper focuses on small organ ({\em e.g.}, the {\em pancreas}) segmentation from abdominal CT scans, which is an important prerequisite for enabling computers to assist human doctors for clinical purposes. This problem falls into the research area named {\em medical imaging analysis} . Recently, great progress has been brought to this field by the fast development of deep learning, especially convolutional neural networks~ _cite_ _cite_ . Many conventional methods, such as the graph-based segmentation approaches~ _cite_ or those based on handcrafted local features~ _cite_, have been replaced by deep segmentation networks, which typically produce higher segmentation accuracy~ _cite_ _cite_ . Segmenting a small organ from CT scans is often challenging. As the target often occupies a {\em small part} of input data ({\em e.g.}, less than _inline_eq_ in a ND image, see Figure~ _ref_), deep segmentation networks such as FCN~ _cite_ and DeepLab~ _cite_ can be easily confused by the background region, which may contain complicated and variable contents. This motivates researchers to propose a {\em coarse-to-fine} approach~ _cite_ with two {\em stages}, in which the coarse stage provides a rough localization and the fine stage performs accurate segmentation. But, despite state-of-the-art performance achieved in pancreas segmentation, this method suffers from {\em inconsistency} between its training and testing flowcharts, which is to say, the training phase dealt with coarse and fine stages individually and did not minimize a global energy function, but the testing phase assumed that these two stages can cooperate with each other in an iterative process. From another perspective, this also makes it difficult for multi-stage visual cues to be incorporated in segmentation, {\em e.g.}, the previous segmentation mask which carries rich information is discarded except for the bounding box. As a part of its consequences, the fine stage consisting of a sequence of iterations cannot converge very well, and sometimes the fine stage produced even lower segmentation accuracy than the coarse stage (see Section~ _ref_) . Motivated to alleviate these shortcomings, we propose a {\bf Recurrent Saliency Transformation Network} . The chief innovation is to relate the coarse and fine stages with a saliency transformation module, which repeatedly transforms the segmentation probability map from previous iterations as spatial priors in the current iteration. This brings us two-fold advantages over~ _cite_ . First, in the training phase, the coarse-scaled and fine-scaled networks are optimized jointly, so that the segmentation ability of each of them gets improved. Second, in the testing phase, the segmentation mask of each iteration is preserved and propagated throughout iterations, enabling multi-stage visual cues to be incorporated towards more accurate segmentation. To the best of our knowledge, this idea was not studied in the computer vision community, as it requires making use of some special properties of CT scans (see Section~ _ref_) . We perform experiments on two CT datasets for small organ segmentation. On the NIH {\em pancreas} segmentation dataset~ _cite_, our approach outperforms the state-of-the-art by an average of over _inline_eq_, measured by the average Dice-S {\o} rensen coefficient (DSC) . On another multi-organ dataset collected by the radiologists in our team, we also show the superiority of our approach over the baseline on a variety of small organs. In the testing phase, our approach enjoys better convergence properties, which guarantees its efficiency and reliability in real clinical applications. The remainder of this paper is organized as follows. Section~ _ref_ briefly reviews related work, and Section~ _ref_ describes the proposed approach. After experiments are shown in Sections~ _ref_ and~ _ref_, we draw our conclusions in Section~ _ref_ .