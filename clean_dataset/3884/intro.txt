In the conventional object classification tasks, samples of all classes are available for training a model. However, objects in the real world have a long-tailed distribution. In spite that images of common concepts can be readily found, there remains a tremendous number of concepts with insufficient and sparse visual data, thus making the conventional object classification methods infeasible. Targeting on tackling such an unseen object recognition problem, zero-shot learning has been widely researched recently. The underlying secret ensuring the success of zero-shot learning is to find an intermediate semantic representation (e.g. attributes or textual features) to transfer the knowledge learned from seen classes to unseen ones~ _cite_ . The majority of state-of-the-art approaches~ _cite_ consider zero-shot learning as a visual-semantic embedding problem. The paradigm can be generalized as training mapping functions that project visual features and/or semantic features to a common embedding space. The class label of an unseen instance is predicted by ranking the similarity scores between semantic features of all unseen classes and the visual feature of the instance in embedding space. Such a strategy conducts a one-to-one projection from semantic space to visual space. However, textual descriptions for categories and objects are inherently mapped to a variety of points in the image space. For example, ``a blue bird with white head'' can be the description of all birds with a blue body and a white head. This motivates us to study how adversarial training learns a one-to-many mapping with adding stochasticity. In this paper, we propose a generative adversarial approach for zero-shot learning that outperforms the state of the art by N \% and N \% on (CUB) ~ _cite_ and (NAB) ~ _cite_ datasets respectively. In this paper, we adopt a novel strategy that casts zero-shot learning as an imagination problem as shown in Fig.~ _ref_ . We focus on investigating how to hallucinate competent data instances that provide the intra-class diversity while keeping inter-class discrimination for unseen novel classes. Once this pseudo data is generated, a supervised classifier is directly trained to predict the labels of unseen images. Recent years witness the success of generative adversarial networks (GANs) ~ _cite_ to generate high compelling images. Our approach leverages GANs as a powerful computational model to imagine how unseen objects look like purely based on textual descriptions. Specifically, we first extract the semantic representation for each class from the Wikipedia articles. The proposed conditional generative model then takes as input the semantic representations of classes and hallucinates the pseudo visual features for corresponding classes. Unlike previous methods~ _cite_, our approach does not need any prior assumption of feature distribution and can imagine an arbitrary amount of plausible features indefinitely. The idea is conceptually simple and intuitive, yet the proper design is critical. Unlike attributes consisting of the discriminative properties shared among categories, Wikipedia articles are rather noisy as most words are irrelevant to visually recognizing the objects. Realizing that noise suppression is critical in this scenario, previous methods~ _cite_ usually involve complex designs of regularizers, such as _inline_eq_ norm in ~ _cite_ and autoencoder in ~ _cite_ . In this work, we simply pass textual features through additional fully connected (FC) layer before feeding it into the generator. We argue that this simple modification achieves the comparable performance of noise suppression and increases the ZSL performance of our method by _inline_eq_ (_inline_eq_ vs. _inline_eq_) on CUB dataset. Besides, the sparsity of training data (_inline_eq_ samples per class in CUB) makes GANs alone hardly simulate well the conditional distribution of the high dimensional feature (_inline_eq_) . As shown in Fig.~ _ref_ .c, the generated features disperse enormously and destroy the cluster structure in real features, thus hardly preserving enough discriminative information across classes to perform unseen image classification. To remedy this limitation, we proposed a visual pivot regularizer to provide an explicit guide for the generator to synthesize features in a proper range, thus preserving enough inter-class discrimination. Empirically, it aligns well the generated features as shown in Fig.~ _ref_ .b, and boosts the ZSL performance of our method from _inline_eq_ to _inline_eq_ on CUB. Succinctly, our contributions are three-fold: N) We propose a generative adversarial approach for ZSL (GAZSL) that convert ZSL to a conventional classification problem, by synthesizing the missing features for unseen classes purely based on the noisy Wikipedia articles. N) We present two technical contributions: additional FC layer to suppress noise, and visual pivot regularizer to provide a complementary cue for GAN to simulate the visual distribution with greater inter-class discrimination. N) We apply the proposed GAZSL to multiple tasks, such as zero-shot recognition, generalized zero-shot learning, and zero-shot retrieval, and it consistently outperforms state-of-the-art methods on several benchmarks.