neural networks have significantly advanced the state-of-the-art in computer vision~ _cite_, natural language processing~ _cite_, speech recognition~ _cite_, and robotics~ _cite_ . These networks are very effective at extracting high-level, complex abstractions of input data through a hierarchical learning process. Deep Convolutional Neural Networks (CNNs) achieve superior performance in visual object recognition tasks, and they have largely replaced hand-crafted features as the standard approach in this area. While deep learning is advantageous for large amounts of spatial data, it also has limitations. Using these networks for temporal data (e.g. video analysis) introduces several new challenges, typically addressed using Recurrent Neural Networks (RNNs) . Modern RNN models like Long Short-Term Memory (LSTM) are an effective way to handle temporal data, but they also tend to be difficult and expensive to train. In video analysis, LSTMs are often paired with CNNs, but this is likely to increase network and training complexity for most tasks. A simpler method for analyzing spatio-temporal data is desirable, and video analysis tasks are complex enough to reasonably test new methods. Video is now a ubiquitous source of spatio-temporal data, and interest in video analysis has risen due to the increasing presence of video data on the internet. Despite the rise of video data availability, video analysis remains a relatively under-examined area compared to image analysis. Many approaches still focus on hand-crafted features akin to those historically used in computer vision. Works applying deep learning to this domain with CNNs and hierarchical layers of LSTMs have shown results _cite_, but combining a CNN with hierarchical layers of LSTMs necessitates training a very large number of parameters, tuning many different hyper-parameters, and performing backpropagation through time. These requirements can be prohibitive, especially in real-world applications with size, weight, area, and power constraints (e.g. robotics, remote sensing, autonomous vehicles) . Visual information in video can be processed on a per-frame basis using CNNs, but training a randomly initialized deep network capable of operating on high resolution image data requires large amounts of data. Without sufficient training data, these networks are prone to overfitting. Unfortunately, many datasets do not contain enough labeled training samples for networks to converge effectively. In computer vision, this problem has been partially resolved using transfer learning to improve performance and decrease training time. This is usually achieved by initializing a new network with layers and trained weights drawn from a publicly available high-performance CNN model. In most cases, these source networks were built to achieve state-of-the-art results for ImageNet, a dataset that contains N different object categories over approximately one million labeled images. When trained on this kind of large natural image dataset, early convolutional layers in a CNN produce features with a surprising level of generality (i.e., features remain useful for most images) _cite_ . This feature generality is the primary characteristic which makes transfer learning with pre-trained CNNs so effective. Less work has been done in applying this kind of transfer learning to video datasets, and the best way to do this is still an open question. RNNs can be used for processing temporal information in video, but successes are limited to LSTM and its variants. RNNs are often are either inherently complex, or have other limitations (e.g., capacity in Hopfield networks) . Reservoir Computing (RC) offers some under-examined alternative RNN models, which are simple to construct and require minimal training. RC models are based on a neuroscientific model of corticostriatal processing loops~ _cite_, and they have been studied in many different applications _cite_ as an unconventional RNN. Two well known RC models are the Liquid State Machine (LSM) ~ _cite_ and the Echo State Network (ESN) ~ _cite_ . In this paper, we propose a novel neural network architecture, the Convolutional Drift Network (CDN) . CDNs produce per-frame appearance features from video using a deep CNN, and those features are pushed into a randomly initialized ESN reservoir. Since the ESN reservoir topology and weights are static after initialization, features produced by the CNN are the driving force in this network. Essentially, once a feature is produced and pushed into the ESN, it is left to propagate naturally, or `drift' through the fading memory representation produced by the Echo State property of ESNs. We evaluate our model on two video classification datasets. We also investigate how far we can simplify CDN training while still achieving competitive performance on this non-trivial classification task. Minimizing training complexity and resources is a step toward the eventual implementation of similar architectures on hardware for field-deployable systems. The specific contributions of this research are: