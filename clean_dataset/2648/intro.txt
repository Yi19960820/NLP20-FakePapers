Recent times have seen a huge explosion of digital images over the internet which has made large image databases prevalent. Given any database one might want to search for images semantically using a query image. Content Based Image retrieval (CBIR) explored in _cite_ provides a solution to the above mentioned problem by retrieving a set of similar images by the measure of a similarity metric between the feature representations of the query image and the member images of the database. Thus a general image retrieval pipeline consists of two steps: first, characterization of each image by rich discriminative features and second, performing a similarity search by some metric using these features to retrieve similar images. Feature representations for images are usually continuous-valued and thus running a nearest neighbour search on these representations for retrieval turns out to be very slow and computationally inefficient, especially for real-time applications in mobile devices or in databases with millions of images. Hence, there is a need to optimize this naive search technique under the constraints of both space and time. Hashing _cite_ is one such state-of-the-art technique for Approximate Nearest Neighbor (ANN) search _cite_ used due to faster retrieval speeds and reduced memory footprint. It involves learning a hash function for encoding continuous-valued image descriptors to compact binary codes while preserving their similarity and discriminative properties. The similarity between two such hash codes can be easily computed by their Hamming distance with the simple XOR operation. Recently, performance of traditional hashing methods have been bettered by deep hashing techniques like _cite_ which employ deep neural networks for learning the hash function. Consequently, our work uses deep hashing to address a special kind of image retrieval task called Texture Image Retrieval. Texture image retrieval may be defined as a type of CBIR which aims at searching for images having texture-patterns semantically similar to that of the query image. Texture being a low level visual attribute of an image's surface acts as a representative of the surface's roughness and also provides useful visual cues about the object's identity. Texture image retrieval has a variety of applications such as in digital library, multimedia web search, multimedia storage system and query-based video investigation. However, this task is very challenging for a couple of reasons. Firstly, due to the lack of large scale texture databases, this task is much less explored in the context of deep learning as compared to ordinary image retrieval. Secondly, for image retrieval tasks in general, the closeness among images is usually governed by the similarity among their high level features. However, in texture image retrieval where similarity is governed by texture patterns, similar semantics become difficult to capture owing to the fact that texture is a low level visual attribute. In our work, we introduce a deep hashing framework for texture image retrieval guided by a Texture Synthesis Network (TSN) _cite_ . We extract information at various levels from the intermediate layers of a pre-trained TSN and combine these multi-scale activations using channel-wise attention in a progressive manner to generate a powerful set of feature descriptors for texture images. The example-based texture synthesis approach aims at generating a texture image, double the size of input that faithfully captures all the visual properties of the input by preserving the large-scale structural features, the natural appearance and the spatial variation of local patterns. For this we use a generative adversarial network (GAN) where the generator aims to synthesize a larger image with an expanded view of the given input texture and the discriminator aims at comparing and classifying the generated textures with the corresponding ground truth images. Although this might resemble image super-resolution _cite_, they are different in various ways. Image super-resolution simply aims at enhancing the quality of the given image by increasing its resolution and sharpening it, whereas, texture synthesis aims at expanding the view of the given texture patch by injecting additional content consistent with the given input. Since the generator is able to synthesize larger textures from smaller patches, it is evident that all of the texture related information is recorded in the intermediate layers of the generator network. This information is used as the key source in our work for generating good descriptors for texture images. Channel-wise attention helps us to combine activation maps from various layers of the pre-trained TSN in a selective manner and reduce noise, such that channels with more information are given greater weightage than noisy and less informative ones. These continuous valued features are finally hashed to generate compact binary coded representations of the images. In addition to images from existent texture datasets, we also make use of images generated from our TSN to alleviate the problem of insufficient texture data for training which significantly improves the performance of our deep neural network. In this paper, we make the following novel contributions: