Fine-grained recognition refers to the task of distinguishing sub-ordinate categories, such as bird species~ _cite_, dog breeds~ _cite_, car models~ _cite_, flower categories~ _cite_, food dishes~ _cite_, etc. With the great potential in rivaling human experts, it has shown tremendous applications in real world ranging from e-commerce~ _cite_ to education~ _cite_ . Although great success has been achieved for basic-level recognition in the last few years~ _cite_, fine-grained recognition still faces two challenges. First, it is more difficult and time-consuming to gather a large amount of labeled fine-grained data because it calls for experts with specialized domain knowledge. In addition, the difference between fine-gained classes is very subtle. The most discriminative features are often not based on the global shape or appearance variation but contained in the mis-alignment of local parts or patterns. For instance, as shown in Fig.~ _ref_, the eye texture and beak shape are crucial to differentiate between Parakeet Auklet and Rhinoceros Auklet. To that end, the main body of previous research has focused on devising more discriminative features by detecting and aligning object parts. Nevertheless, most conventional methods~ _cite_ utilize manually defined parts to localize the regions, such as ``the head of a bird'', for fine-grained recognition. Relying on manually defined parts has several drawbacks: N) The precise part annotations are usually expensive to acquire. N) The strongly supervised part-based model might fail if some parts are occluded. N) For some fine-grained categories, it is very difficult to manually define parts for them. For example, it is very difficult to define parts for food recognition, as suggested in Fig.~ _ref_ . N) Most importantly, there is no clue that manually defined parts are optimal for all fine-grained recognition tasks. To overcome these problems, we propose a visual attention framework called {\em Fully Convolutional Attention Networks} (FCANs) for fine-grained recognition without part annotation. Given only image label, our framework utilizes reinforcement learning to simultaneously localize object parts and classify the object within the scene. Intuitively, the framework simulates human visual system that proceeds object recognition via a series of {\em glimpse} on object parts. At each glimpse, it strives to find the most discriminative location that can differentiate object's category given the previous observations. Similar to previous visual attention models~ _cite_, we employ the REINFORCE algorithm during training~ _cite_, where the action is the location of each glimpse, the state is the image and the locations of the previous glimpses, and the reward measures the classification correctness. The whole framework can be trained only by an image classification loss, thus requiring no manual part annotations. The visual attention approach is demonstrated to perform well on fine-grained recognition without requiring manually labeled object parts~ _cite_ . Compared to the previous reinforcement learning-based visual attention frameworks~ _cite_, the FCANs enjoy better computational efficiency as well as higher classification accuracy in fine-grained recognition. More concretely, our proposed framework improves the attention models in three ways: As a result, our proposed approach improves the recognition accuracy over previous reinforcement learning based methods~ _cite_ while being computationally more efficient. Our method also achieve competitive results against other state-of-the-art methods on multiple fine-grained datasets.