Deep Learning techniques have generated many of the state-of-the-art models~ _cite_ that reached impressive results on benchmark datasets like MNIST~ _cite_ . Such models are usually trained with variations of the standard Backpropagation method, with stochastic gradient descent (SGD) . In the field of shallow neural networks, there have been several developments to training algorithms that have sped up convergence~ _cite_ . This paper aims to bridge the gap between the field of Deep Learning and these advanced training methods, by combining Resilient Propagation (Rprop) ~ _cite_, Dropout~ _cite_ and Deep Neural Networks Ensembles. The Resilient Propagation~ _cite_ weight update rule was initially introduced as a possible solution to the ``vanishing gradients'' problem: as the depth and complexity of an artificial neural network increase, the gradient propagated backwards by the standard SGD backpropagation becomes increasingly smaller, leading to negligible weight updates, which slow down training considerably. Rprop solves this problem by using a fixed update value _inline_eq_, which is increased or decreased multiplicatively at each iteration by an asymmetric factor _inline_eq_ and _inline_eq_ respectively, depending on whether the gradient with respect to _inline_eq_ has changed sign between two iterations or not. This ``backtracking'' allows Rprop to still converge to a local minima, but the acceleration provided by the multiplicative factor _inline_eq_ helps it skip over flat regions much more quickly. To avoid double punishment when in the backtracking phase, Rprop artificially forces the gradient product to be _inline_eq_, so that the following iteration is skipped. An illustration of Rprop can be found in Algorithm~ _ref_ . Dropout~ _cite_ is a regularisation method by which only a random selection of nodes in the network is updated during each training iteration, but at the final evaluation stage the whole network is used. The selection is performed by sampling a _inline_eq_ from a Bernoulli distribution with _inline_eq_, where _inline_eq_ is the probability of node _inline_eq_ being muted during the weight update step of backpropagation, and _inline_eq_ is the, which is usually _inline_eq_ for the middle layers, _inline_eq_ or _inline_eq_ for the input layers, and _inline_eq_ for the output layer. For convenience this dropout mask is represented as a weight binary matrix _inline_eq_, covering all the weights in the network that can be used to multiply the weight-space of the network to obtain what is called a network, for the current training iteration, where each weight _inline_eq_ is zeroed out based on the probability of its parent node _inline_eq_ being muted. The remainder of this paper is structured as follows: