As the success of convolutional neural networks in computer vision, more and more researchers pay attention to their deployment on embedded sensors and mobile devices. Due to the enormous computational cost and storage overhead, it becomes an appealing subject on how to obtain a more efficient model without performance decline. Recently, many compression and acceleration methods have been proposed to solve this problem. One category of the most popular methods is pruning, which can induce sparsity to weight matrices so that only the non-zero parts are involved in computation and storage. _cite_ . Neuron pruning is one kind of structure pruning, which induces channel level and filter level sparsity of weight matrices so that it can directly compress the network into a slimmer one. To the best of our knowledge, most of state-of-the-art results are implemented in a layer-by-layer pruning and optimization manner _cite_ . However, as shown in Fig. _ref_, the survived neurons are inadequate to recover the original output due to the information loss, which accumulates errors as layer increases. In this way, its accuracy is deeply damaged and is difficult to restore the original performance. Their proposed greedy neuron selection methods actually attempt to ease this problem. However, without framework improved, this problem is unable to avoid, no matter how optimal the neuron selection is, especially when aggressive pruning. Intuitively, a better solution is to squeeze the useful information of the discarded neurons into the survived ones and then propagate to reconstruct the following pruned layers. Inspired by this idea, we propose a new layer-by-layer optimization framework for neuron pruning named Layer Decomposition-Recomposition Framework (LDRF) . In this framework, we elegantly generate the embedding space to each layer through cross-channel decomposition which serves as an information buffer. The information of the discarded neurons is squeezed into the survived ones in the embedding space before they are fed to reconstruct the following pruned layers. It efficiently avoids the emergence of information loss. Symmetrically, after pruning and reconstruction, each adjacent linear layers are recomposed to avoid network depth increase. Besides, via the analysis of the information propagation characteristic of Rectified Linear Units layer, i.e. ReLU _cite_, we propose a method to evaluate the pruning ratio range for each layer. It simplifies our optimization process and reduces our convergence time. We first conduct experiments on CIFAR-N _cite_ with VGG-N _cite_ to compare our method with an existing layer-by-layer pruning method _cite_ . Sadly, the majority of related works only compare the results after fine-tuning. In this experiment, the performance of our method significantly outperforms other methods before fine-tuning. Ultimately, we evaluate the generalization of our method on the large-scale ImageNet N classification dataset _cite_ with two most representative models, say VGG-N _cite_ and ResNet-N _cite_, and achieve state-of-the-art results by _inline_eq_ and _inline_eq_ speed-up with only {\bfseries N \%} and {\bfseries N \%} top-N accuracy drop respectively. Our major contributions are summarized as follows: