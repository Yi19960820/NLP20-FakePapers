Age progression/regression, also known as age synthesis, aims to aesthetically render given faces with aging or rejuvenating effect but still preserve personality. In recent years, age synthesis has become a hot topic in computer vision. It has a wide range of applications in various domains, e.g., finding missing person, age estimation, age-invariant verification, social entertainment, etc. However, due to the extreme challenges involving diverse genetics and living styles, rigid requirement for training datasets and large variation in illumination, age progression/regression is still a challenging task. In prior works, direct and step-by-step aging synthesis are mainly used for age progression/regression _cite_ . In direct aging synthesis, target age faces can be directly synthesized utilizing the relationships between input faces and their corresponding target age labels. Meanwhile, the step-by-step synthesis usually splits the long-term aging process into short-terms and merely concentrates on the transformation between adjacent age groups. Although the direct synthesis methods are easy to train, they can not synthesize satisfactory results with a long age span and often lose the original identity, while the step-by-step synthesis methods need to train a specific network for every two transformed age groups. In this paper, the proposed method is based on Conditional Generative Adversarial Nets (CGAN) _cite_, which can simultaneously achieve age progression and regression in the same framework with the given age labels. To remedy identity missing of direct synthesis methods and enhance the accuracy of age synthesis, an identity preserving loss and age preserving loss are also adopted in this method. Moreover, inspired by the observation that for adults, significant aging changes mainly occur in texture of facial subregions and only small changes happen in global facial configurations, we further propose GLCA-GAN with one global network generating the whole facial structure and three local networks processing diverse texture transformation of crucial facial subregions. We also employ a pixel loss to preserve detailed facial information of the input face. In addition, to further preserve most of the details in age-attribute-irrelevant areas, our generator learns the residual face, which is defined as the difference between the input face and its corresponding synthetic face. The main contributions of this work are as follows: N) We propose GLCA-GAN for age progression and regression. In GLCA-GAN, one global network is used for generating the whole facial structure and simulating the aging trend of the whole face, while three local networks are employed for processing diverse texture transformation of crucial facial subregions. N) We impose an age preserving loss to enhance the accuracy of age synthesis. To further ensure that the synthetic face and input face are belong to the same person, an identity preserving loss is adopted. In addition, a pixel loss is also used to preserve detailed facial information. N) Instead of manipulating a whole face, our generator learns the residual face between the input face and its corresponding synthetic face, which can accelerate the convergence process while preserving most of the details in age-attribute-irrelevant areas, e.g., clothes, background, etc. N) Our network can simultaneously achieve age progression and regression in the same framework with the given age labels and generate favorable results.