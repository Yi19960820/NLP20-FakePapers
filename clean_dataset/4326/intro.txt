Alzheimer's disease (AD) is a chronic neurodegenerative disorder that begins with short-term memory loss and develops over time, causing issues in conversation, orientation, and control of bodily functions . Early diagnosis of the disease is challenging and the diagnosis is usually made once cognitive impairment has already compromised daily living. Hence, developing robust, data-driven methods for disease progression modeling (DPM) utilizing longitudinal data is necessary to yield a complete perspective of the disease for better diagnosis, monitoring, and prognosis . Existing DPM techniques attempt to describe biomarker measurements as a function of disease progression through continuous curve fitting. In the AD progression literature, a variety of regression-based methods have been applied to fit logistic or polynomial functions to the longitudinal dynamic of each biomarker . However, parametric assumptions on the biomarker trajectories limit the applicability of such methods; in addition, none of the existing approaches considers the temporal dependencies among measurements. Furthermore, the available methods mostly rely on independent biomarker modeling and require alignment of subjects' trajectories--either as a pre-processing step or as part of the algorithm. Recurrent neural networks (RNNs) are sequence learning based methods that can offer continuous, non-parametric, joint modeling of longitudinal data while taking temporal dependencies amongst measurements into account . However, since longitudinal cohort data often contain missing values due to, for instance, dropped out patients, unsuccessful measurements, and/or varied trial design, standard RNNs require pre-processing steps for data imputation which may result in suboptimal analyses and predictions . Therefore, the lack of methods to inherently handle incomplete data in RNNs is evident . Long short-term memory (LSTM) networks are widely used types of RNNs developed to effectively capture long-term temporal dependencies by dealing with the exploding and vanishing gradient problem during backpropagation through time . They employ a memory cell with nonlinear reset units--so called constant error carousels (CECs), and learn to store history for either long or short time periods. Since their introduction, a variety of LSTM networks have been developed for different time-series applications . The vanilla LSTM, among others, is the most commonly used architecture that utilizes three reset gates with full gate recurrence and applies backpropagation algorithm through time using full gradients. Nevertheless, its complete topology can include biases and cell-to-gates (peephole) connections. The most common approach to handling missing data with LSTM networks is data interpolation pre-processing step, usually using mean or forward imputation. This two-step procedure decouples missing data handling and network training, resulting in a sub-optimal performance, and it is heavily influenced by the choice of data imputation scheme. Other approaches, update the architecture to utilize possible correlations between missing values' patterns and the target to improve prediction results . Our goal is different; we want to make the training of LSTM networks robust to missing values to more faithfully capture the true underlying signal, and to make the learned model generalizable across cohorts--not relying on specific cohort or demographic circumstances correlated with the target. In this paper, we propose a generalized method for training LSTM networks that can handle missing values in both target and predictor variables. This is achieved via applying the batch gradient descent algorithm together with normalizing the loss function and its gradients with respect to the number of missing points in target and input, to ensure a proportional contribution of each weight per epoch. The proposed LSTM algorithm is applied for modeling the progression of AD in the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) cohort based on magnetic resonance imaging (MRI) biomarkers, and the estimated biomarker values are used to predict the clinical status of subjects. Our main contribution is three-fold. Firstly, we propose a generalized formulation of backpropagation through time for LSTM networks to handle incomplete data and show that such built-in handling of missing values provides better modeling and prediction performances compared to using data imputation with standard LSTM networks. Secondly, we model temporal dependencies among measurements within the ADNI data using the proposed LSTM network via sequence-to-sequence learning. To the best of our knowledge, this is the first time such multi-dimensional sequence learning methods are applied for neurodegenerative DPM. Lastly, we introduce an end-to-end approach for modeling the longitudinal dynamics of imaging biomarkers--without need for trajectory alignment--and for clinical status prediction. This is a practical way to implement a robust DPM for both research and clinical applications.