Convolutional neural networks (CNNs) have dominated computer vision since deep CNNs, such as AlexNet~ _cite_, began winning the ImageNet Challenge: ILSVRC N~ _cite_ . To achieve high accuracy, researchers have been increasing the depth and complexity of CNNs to extreme boundaries of hardware limits (_cite_) . As a result, more and more computational power is required to run such complicated networks. Because of their limited computational power, as well as memory space, many real world applications, such as robotics, self-driving cars, augmented reality and smartphones are unable to afford such CNNs. Hence, the necessity and efficiency of so many layers and computationally intensive architectures is being challenged. Recently, researchers have been focusing on developing efficient, mobile-friendly neural networks such as MobileNet~ _cite_, ShuffleNet~ _cite_ and their improved versions (VNs) ~ _cite_ . These models significantly shrink the conventional CNNs without decreasing accuracy noticeably. The key to such reductions is lowering the number of operation between channel dimensions through group convolutions (GConv) _cite_ and depthwise convolutions (DWConv) _cite_ . In terms of complexity on the channel dimension, these convolutions utilize two extremes, _inline_eq_ and _inline_eq_ scales respectively, where _inline_eq_ is the number of groups in a GConv. Automatic machine learning (AutoML) uses these human designed architectures as an inductive bias for automatic architecture search, which provides state-of-the-art efficiency~ _cite_ . Inspired by models obtained from AutoML we identify that breaking the symmetry is important for efficient models. Intuitively, keeping a symmetric structure for each building block of the CNN could generate redundant features and thus the limited size of edge models might lead to low accuracy. We propose to break the symmetry in the channel dimensions instead, thereby introducing channel imbalance for log-scale computations. To our knowledge, channel imbalance has not been previously explored by AutoML nor by human designs, thus we choose this new architecture space to aim at efficiency. Our contributions' fit in research on sparse convolutions is shown in Figure~ _ref_ . We find that a key for sparsity/accuracy trade-off is to maintain connectivity between channel dimensions within residual blocks; in combination with channel imbalance this provides efficient flow of information. Therefore, we propose a new convolutional layer, named wavelet convolution (WConv) and its conjugate layer to keep connectivity: depthwise fast wavelet transform layer (DFWT) . WConv breaks the channel symmetry and reduces complexity to _inline_eq_ while DFWT takes negligible operations. DFWT itself is derived from an optimization through a formalism on connectivity that we develop to utilize the space of imbalanced channel dimensions. Hence, our building block for CNNs yields full connectivity and attains the efficiency/accuracy trade-off of MobileNetVN but requires log-scale complexity: an unmatched feat by the state-of-the-art. Using our building blocks, WConv and DFWT, we construct a new efficient CNN called WaveletNet . We compare WaveletNet to other extremely small models such as MobileNetVN/N and ShuffleNet, on CIFAR-N and ImageNet. We find that WaveletNet outperforms state-of-the-art models on CIFAR-N. Furthermore, we conduct an ablation study to signify the importance of the DFWT layer. We find that DFWT is important for boosting the accuracy of WConv, just like the Shuffle layer improves grouped convolution in ShuffleNet~ _cite_ . Our experiments on ImageNet achieve N \% error with only NM MACs (multiply-accumulate operations) and N parameters thereby showing that the accuracy of WaveletNet is comparable to that of MobileNetVN. Our contributions are as follows: we () inspired by AutoML first explore breaking the symmetry in channel dimensions, which may open a new path for human designed architecture search; () provide connectivity guidelines for development of efficient CNNs; () develop two new convolutional layers: WConv and DFWT that serve as building blocks for any CNN; () provide a state-of-the-art efficient CNN for edge devices.