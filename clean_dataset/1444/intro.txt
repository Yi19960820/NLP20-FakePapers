Deep learning and deep neural networks (DNNs) are the machine learning methods that use a neural network (NN) with many layers, which mimics information processing in the human brain. They have been used to innovate many types of applications in the real world, such as image recognition _cite_, audio analysis _cite_, and even more artistic works _cite_, One of the most important keys to the substantial advance of DNNs is representation learning and its result, learning representation. Most recent DNNs have been based on convolutional NNs (CNNs) _cite_, and trained using back propagation (BP) learning. It enables the representation learning by a discrimination task. However, it requires huge amount of labeled data, and the total amount of labeled data is very limited compared to those of unlabeled. Thus, efficient use of unlabeled data is getting crucial for DNNs. The BP learning basically extract optimal learning representation for the target discrimination task. It means the obtained learning representation is not completely generalized, but has some tendency caused by the task. In order to improve the generalization, the representation learning should not depend on any type of tasks, in other words, any kind of labels. Therefore, a method of representation learning by unlabeled data for CNNs is required, also from the viewpoint of generalization. One traditional learning method for NN, called competitive learning _cite_, is well-known for its strong representation learning. It is mainly applied for a self-organizing map, which is a single layer NN with a cortex-like topological structure _cite_, and Neocognitron, which is the progenitor of recent CNNs _cite_ . Competitive learning maximally uses input dataset information, and robustly obtains a large amount of filters with a high degree of freedom. However, it is not a supervised learning method and could not be applied for fine-tuning in the same manner as supervised BP learning. Therefore, if competitive learning can unify BP learning on CNNs, it is expected that it will enable more detailed and generalized recognition by DNNs. In this study, we propose a method to both integrate competitive learning into CNNs and enable effective pre-training using unlabeled data. We validated the method using a toy model on the MNIST handwritten dataset, and demonstrated that competitive learning accelerated the subsequent supervised BP learning and reduced the number of samples of labeled data in the initial learning stage.