Photographic text-to-image synthesis is a significant problem in generative model research _cite_, which aims to learn a mapping from a semantic text space to a complex RGB image space. This task requires the generated images to be not only realistic but also semantically consistent, i.e., the generated images should preserve specific object sketches and semantic details described in text. Recently, Generative adversarial networks (GANs) have become the main solution to this task. Reed \etal _cite_ address this task through a GAN based framework. But this method only generates _inline_eq_ images and can barely generate vivid object details. Based on this method, StackGAN _cite_ proposes to stack another low-to-high resolution GAN to generate _inline_eq_ images. But this method requires training two separate GANs. Later on, _cite_ proposes to bypass the difficulty of learning mappings from text to RGB images and treat it as a pixel-to-pixel translation problem _cite_ . It works by re-rendering an arbitrary-style _inline_eq_ training image conditioned on a targeting description. However, its high-resolution synthesis capability is unclear. At present, training a generative model to map from a low-dimensional text space to a high-resolution image space in a fully end-to-end manner still remains unsolved. This paper pays attention to two major difficulties for text-to-image synthesis with GANs. The first is balancing the convergence between generators and discriminators _cite_, which is a common problem in GANs. The second is stably modeling the huge pixel space in high-resolution images and guaranteeing semantic consistency _cite_ . An effective strategy to regularize generators is critical to stabilize the training and help capture the complex image statistics _cite_ . In this paper, we propose a novel end-to-end method that can directly model high-resolution image statistics and generate photographic images (see Figure _ref_ bottom) . The contributions are described as follows. Our generator resembles a simple vanilla GAN, without requiring multi-stage training and multiple internal text conditioning like _cite_ or additional class label supervision like _cite_ . To tackle the problem of the big leap from the text space to the image space, our insight is to leverage and regularize hierarchical representations with additional `deep' adversarial constraints (see Figure _ref_ top) . We introduce accompanying hierarchically-nested discriminators at multi-scale intermediate layers to play adversarial games and jointly encourage the generator to approach the real training data distribution. We also propose a new convolutional neural network (CNN) design for the generator to support accompanying discriminators more effectively. To guarantee the image diversity and semantic consistency, we enforce discriminators at multiple side outputs of the generator to simultaneously differentiate real-and-fake image-text pairs as well as real-and-fake local image patches. We validate our proposed method on three datasets, CUB birds _cite_, Oxford-N flowers _cite_, and large-scale MSCOCO _cite_ . In complement of existing evaluation metrics (e.g. Inception score _cite_) for generative models, we also introduce a new visual-semantic similarity metric to evaluate the alignment between generated images and conditioned text. It alleviates the issue of the expensive human evaluation. Extensive experimental results and analysis demonstrate the effectiveness of our method and significantly improved performance compared against previous state of the arts on all three evaluation metrics. All source code will be released.