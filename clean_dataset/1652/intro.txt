In recent years, the Convolutional Neural Network (CNN) has emerged as the primary model class for learning from signals such as audio, images, and video. Through the use of convolution layers, the CNN is able to exploit the spatial of the input space, and the translational (invariance) that is inherent in many learning problems. Because convolutions are (a shift of the input leads to a shift of the output), convolution layers preserve the translation symmetry. This is important, because it means that further layers of the network can also exploit the symmetry. Motivated by the success of CNNs, many researchers have worked on generalizations, leading to a growing body of work on . Generalization has happened along two mostly orthogonal directions. Firstly, the symmetry groups that can be exploited was expanded beyond pure translations, to other transformations such as rotations and reflections, by replacing convolutions with group convolutions . The feature maps in these networks transform as scalar fields on the group _inline_eq_ or a homogeneous space _inline_eq_ . We will refer to such networks as G-CNNs, because the transformation law for scalar fields is known as the regular representation of _inline_eq_ . Initially, regular G-CNNs were implemented for planar images, acted on by discrete translations, rotations, and reflections. Such discrete G-CNNs have the advantage that they are easy to implement, easy to use, fast, and result in improved results in a wide range of practical problems, making them a natural starting point for the generalization of CNNs. However, the concept is much more general: because G-CNNs were formulated in abstract group theoretic language, they are easily generalized to any group or homogeneous space that we can sum or integrate over . For instance, Spherical CNNs are G-CNNs for the ND rotation group _inline_eq_ acting on the sphere _inline_eq_ (a homogeneous space for _inline_eq_) . The second direction of generalization corresponds to a move away from scalar fields. Using connections to the theory of steerable filters and induced representations, the feature space was generalized to vector-and tensor fields, and even more general spaces (sections of homogeneous vector bundles) . We will refer to these networks as or G-CNNs, because the filters in these networks are steerable, and the associated transformation law is called the induced representation (see Fig. _ref_) . Thus, the general picture that has emerged is one of networks that use convolutions to map between spaces of sections of homogeneous vector bundles in a group equivariant manner. The classical CNN, mapping scalar fields (a.k.a. feature channels) on the plane to scalar fields on the plane in a translation equivariant manner, is but one special case. In this paper we study the general class of induced G-CNNs, and in particular the space of equivariant linear maps (intertwiners) between two induced representations associated with the input and output feature space of a network layer. We show that any equivariant map between induced representations can be written as a (twisted) convolution / cross-correlation, thus generalizing the results of _cite_, who showed this for regular representations . The induced representation has been studied extensively by physicists and mathematicians. The word ``induced'' comes from the fact that the transformation law of e.g. a vector field can be inferred from the transformation law of an individual vector under the action of a certain isotropy (or ``stabilizer'') subgroup of the symmetry group. For instance, when applying a ND rotation _inline_eq_ to a vector field _inline_eq_ on the sphere, each vector _inline_eq_ is moved to a new position _inline_eq_ by the ND rotation, and the vector itself is rotated in its tangent plane by a ND rotation _inline_eq_ (This is illustrated in Fig. _ref_ for a planar vector field) . Thus, we say that this vector field transforms according to the representation of _inline_eq_ induced by the canonical representation of _inline_eq_ . As another example, a higher order tensor transforms according to a different representation _inline_eq_ of _inline_eq_, so a tensor on the sphere transforms according to a different induced representation of _inline_eq_ . Induced representations are important in physics because they are the primary tool to construct irreducible representations, which enumerate the types of elementary particles of a physical (field) theory. In representation learning, the idea of irreducible representations as elementary particles has been applied to formalize the idea of ``disentangling'' or ``capsules'' that represent distinct visual entities, each of which has a certain . Indeed, we think of induced G-CNNs as the mathematically grounded version of Hinton's idea of capsules (sans dynamic routing, for now) . The general formalism of fiber bundles has also been proposed as a geometrical tool for modelling early visual processing in the mammalian brain . Although it is far too early to conclude anything, this convergence of physics, neuroscience, and machine learning suggests that field theories are not just for physicists, but provide a generally useful model class for natural and man-made learning systems. In order to understand and properly define the induced representation, we need some notions from group-and representation theory, such as groups, cosets, double cosets, quotients, sections, and representations. In section _ref_ we will define these concepts and illustrate them with two examples: the rotation group _inline_eq_ and Euclidean motion group _inline_eq_ . Although necessary for a detailed understanding of the rest of the paper, this section is rather dry and may be skimmed on a first reading. Induced representations are defined in section _ref_ . We present two of the many equivalent realizations of the induced representation. The first realization describes the transformation law for the vector space _inline_eq_ of sections of a vector bundle over _inline_eq_, such as vector fields over the sphere _inline_eq_ . This realization is geometrically natural, and such vector fields can be stored efficiently in computer memory, making them the preferred realization for implementations of induced G-CNNs. The downside of this realization is that, due to the use of an arbitrary frame of reference (choice of section), the equations describing it get quite cumbersome. For this reason, we also discuss the induced representation realized in the space _inline_eq_ of vector-valued functions on _inline_eq_, having a certain kind of symmetry (the space of Mackey functions) . We define a ``lifting'' isomorphism from _inline_eq_ to _inline_eq_ to show that they are equivalent: In section _ref_ we study the space of linear equivariant maps, or intertwiners, between two representations _inline_eq_ and _inline_eq_, induced from representations of subgroups _inline_eq_ and _inline_eq_ . Denoting this space by _inline_eq_ or _inline_eq_ (depending on the chosen realization of _inline_eq_ or _inline_eq_), we find that (of course) they are equivalent, and more importantly, that any equivariant map _inline_eq_ can be written as a special kind of convolution or correlation with an equivariant kernel _inline_eq_ on _inline_eq_ or _inline_eq_, respectively. Furthermore, these spaces of equivariant kernels, denoted _inline_eq_ and _inline_eq_, are shown to be equivalent to a space of kernels on the double coset space _inline_eq_, denoted _inline_eq_ . This is summarized in the following diagram of isomorphisms: The map _inline_eq_ takes a kernel _inline_eq_ to the ``neural network layer'' _inline_eq_, by using the kernel in a cross-correlation denoted _inline_eq_ . The map _inline_eq_ is defined similarly. That _inline_eq_ is an isomorphism means that any equivariant map _inline_eq_ can be written as a convolution with an appropriate kernel _inline_eq_ . The kernels in _inline_eq_ and _inline_eq_ have to satisfy certain equivariance constraints. These constraints can be largely resolved by moving to _inline_eq_, where finding a solution is typically easier. Using the results of this paper, finding a basis for the space of equivariant filters for a new group should be relatively straightforward. Having seen the main results derived in a relatively concrete manner, we proceed in section _ref_ to show how these results relate to Mackey's theory of induced representations, which is usually presented in a more abstract language. Then, in section _ref_, we show how to actually compute a basis for _inline_eq_ for the case of _inline_eq_ and _inline_eq_ .