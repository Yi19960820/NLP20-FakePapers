Audio-visual recognition~ (AVR) has been considered as a solution for speech recognition tasks when the audio is corrupted, as well as a visual recognition method used for speaker verification in multi-speaker scenarios.~The approach of AVR systems is to leverage the extracted information from one modality to improve the recognition ability of the other modality by complementing the missing information. The essential problem is to find the correspondence between the audio and visual streams, which is the goal of this work.~We propose the use of a coupled ND Convolutional Neural Network~ (ND-CNN) architecture that can map both modalities into a representation space to evaluate the correspondence of audio-visual streams using the learned multimodal features.~The proposed architecture will incorporate both spatial and temporal information jointly to effectively find the correlation between temporal information for different modalities.~By using a relatively small network architecture and much smaller dataset for training, our proposed method surpasses the performance of the existing similar methods for audio-visual matching which use ND CNNs for feature representation.~We also demonstrate that an effective pair selection method can significantly increase the performance. The proposed method achieves relative improvements over N \% on the Equal Error Rate~ (EER) and over N \% on the Average Precision~ (AP) in comparison to the state-of-the-art method.