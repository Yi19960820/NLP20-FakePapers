Humans can learn to identify new categories from few examples, even from a single one . Few-shot learning has recently attracted significant attention, as it aims to produce models that can generalize from small amounts of labeled data. In the few-shot setting, one aims to learn a model that extracts information from a set of support examples (set) to predict the labels of instances from a set. Recently, this problem has been reframed into the meta-learning framework _cite_, the model is trained so that given a set or task, produces a classifier for that specific task. Thus, the model is exposed to different tasks (or episodes) during the training phase, and it is evaluated on a non-overlapping set of new tasks . Two recent approaches have attracted significant attention in the few-shot learning domain:, and . In both approaches, the set and the set are embedded with a neural network, and nearest neighbor classification is used given a metric in the embedded space. Since then, the problem of learning the most suitable metric for few-shot learning has been of interest to the field~ . Learning a metric space in the context of few-shot learning generally implies identifying a suitable similarity measure (e.g. cosine or Euclidean), a feature extractor mapping raw inputs onto similarity space (convolutional stack for images or LSTM stack for text), a cost function to drive the parameter updates, and a training scheme (often episodic) . Although the individual components in this list have been explored, the relationships between them have not received considerable attention. In the current work we aim to close this gap. We show that taking into account the interaction between the identified components leads to significant improvements in the few-shot generalization. In particular, we show that a non-trivial interaction between the similarity metric and the cost function can be exploited to improve the performance of a given similarity metric via scaling. Using this mechanism we close more than the N \% gap in performance between the cosine similarity and the Euclidean distance reported in~ . Even more importantly, we extend the very notion of the metric space by making it task dependent via conditioning the feature extractor on the specific task. However, learning such a space is in general more challenging than learning a static one. Hence, we find a solution in exploiting the interaction between the conditioned feature extractor and the training procedure based on auxiliary co-training on a simpler task. Our proposed few-shot learning architecture based on task-dependent scaled metric achieves superior performance on two challenging few-shot image classification datasets. It shows up to N \% absolute accuracy improvement over the baseline (), and N \% over the state-of-the-art~ on the N-shot, N-way mini-Imagenet classification task, reaching N \% of accuracy, which is the best-reported accuracy on this dataset. We consider the episodic _inline_eq_-shot, _inline_eq_-way classification scenario. In this scenario, a learning algorithm is provided with a set _inline_eq_ consisting of _inline_eq_ examples for each of _inline_eq_ classes and a set _inline_eq_ for a task to be solved within a given episode. The set provides the task information via observations _inline_eq_ and their respective class labels _inline_eq_ . Given the information in the set _inline_eq_, the learning algorithm is able to classify individual samples from the set _inline_eq_ . Next, we define a similarity measure _inline_eq_ . Note that _inline_eq_ does not have to satisfy the classical metric properties (non-negativity, symmetry, subadditivity) to be useful in the context of few-shot learning. The dimensionality of metric input, _inline_eq_, will most naturally be related to the size of embedding created by a (deep) feature extractor _inline_eq_, parameterized by _inline_eq_, mapping _inline_eq_ to _inline_eq_ . Here _inline_eq_ is a list of parameters defining _inline_eq_, a list of weights in a neural network. The set of representations _inline_eq_ can directly be used to solve the few-shot learning classification problem by association. For example, Matching networks~ use sample-wise attention mechanism to perform kernel label regression. Instead, defined a feature representation _inline_eq_ for each class _inline_eq_ as the mean over embeddings belonging to _inline_eq_: _inline_eq_ . To learn _inline_eq_, they minimize _inline_eq_ using the softmax over prototypes _inline_eq_ to define the likelihood: _inline_eq_ . Metric Scaling: To our knowledge, this is the first study to (i) propose metric scaling to improve performance of few-shot algorithms, (ii) mathematically analyze its effects on objective function updates and (iii) empirically demonstrate its positive effects on few-shot performance. Task Conditioning: We use a task encoding network to extract a task representation based on the task's set. This is used to influence the behavior of the feature extractor through FILM . Auxiliary task co-training: We show that co-training the feature extraction on a conventional supervised classification task reduces training complexity and provides better generalization. Three main approaches for solving the few-shot classification problem can be identified in the literature. The first one, which is used in this work, is the meta-learning approach, learning a model that, given a task (set of labeled data), produces a classifier that generalizes across all tasks . This is the case of Matching Networks, which optionally use a Recurrent Neural Network (RNN) to accumulate information about a given task. In MAML, the parameters of an arbitrary learner model are optimized so that they can be quickly adapted to a particular task. In ``Optimization as a model'', a learner model is adapted to a new episodic task by a recurrent meta-learner producing efficient parameter updates. A more general approach was proposed by~, where the meta-learner is trained to represent entries from a set in an external memory. Similarly, adaResNet uses memory and the set to produce shift coefficients on the neuron activations of the set classifier. Many recent approaches focus on learning a metric on the episodic feature space. Prototypical networks use a feed-forward neural network to embed the task examples and perform nearest neighbor classification with the class centroids. The relation network approach by~ introduces a separate learnable similarity metric. SNAIL uses an explicit attention mechanism applicable both to supervised and to the sequence based reinforcement learning tasks. It has also been shown that these approaches benefit from leveraging unlabeled and simulated data~ . A second approach aims to maximize the distance between examples from different classes . Similarly, in, a contrastive loss function is used to learn to project data onto a manifold that is invariant to deformations in the input space. In the same vein, in, triplet loss is used for learning a representation for few-shot learning. The attentive recurrent comparators go beyond classical siamese approaches and use a recurrent architecture to learn to perform pairwise comparisons and predict if the compared examples belong to the same class. The third approach relies on Bayesian modeling of the prior distribution of the different categories like in, or who rely on hierarchical Bayesian modeling. As for task conditioning, _cite_ proposed conditional batch normalization for style transfer and visual reasoning. Differently, we modify the conditioning scheme to adapt it to few-shot learning, introducing _inline_eq_ priors, and auxiliary co-training. In the few-shot learning context, task conditioning ideas can be traced back to~, although in an implicit form as there is no notion of task embedding. In our work, we explicitly introduce a task representation (see Fig.~ _ref_) computed as the mean of the task class centroids (task prototypes) . This is much simpler than individual sample level LSTM/attention models in~ . Conditioning in~ is applied as a postprocessing of the output of a fixed feature extractor. We propose to condition the feature extractor by predicting its own batch normalization parameters thus making feature extractor behaviour task-dynamic without cumbersome fine-tuning on support set. In order to train the task conditioned architecture we use multitask training with a usual N-way classification task. Even though auxiliary co-training is beneficial for learning in general, ``little is known on multitask learning works and whether there are data characteristics that help to determine its success''~ . We show that combining task conditioning and auxiliary co-training is beneficial in the context of few-shot learning. The scaling and temperature adjustment in the softmax was discussed by~ in the context of model distillation. We propose to use it in the context of the few-shot learning scenario and provide novel theoretical and empirical results quantifying the effects of scaling parameter. The rest of the paper is organized as follows. Section~ _ref_ describes our contributions in detail. Section~ _ref_ highlights the importance of each contribution via an ablation study. The study is performed over two different benchmarks in the regime of N-shot, N-shot and N-shot learning to verify if conclusions hold across different setups. Finally, Section~ _ref_ concludes the paper and outlines future research directions.