The performance of machine learning algorithms is dependent on how the data is represented. In most methods, the quality of a data representation is itself dependent on prior knowledge imposed on the representation. Such prior knowledge can be imposed using domain specific information, as in SIFT, HOG, etc., or in learning representations using fixed priors like sparsity, temporal coherence, etc. The use of fixed priors became particularly popular while training deep networks . In spite of the success of these general purpose priors, they are not capable of adjusting to the context in the data. On the other hand, there are several advantages to having a model that can ``actively" adapt to the context in the data. One way of achieving this is to in a dynamic and context-sensitive manner. This will be the main focus of this work, with emphasis on visual perception. Here we propose a predictive coding framework, where a generative model uses ``top-down" information to empirically alter the priors used in the lower layers to perform ``bottom-up" inference. The centerpiece of the proposed model is extracting sparse features from time-varying observations using a . To this end, we propose a novel procedure to infer (or features) of a dynamical system. We then extend this feature extraction block to introduce a pooling strategy to learn invariant feature representations from the data. In line with other ``deep learning" methods, we use these basic building blocks to construct a hierarchical model using greedy layer-wise unsupervised learning. The hierarchical model is built such that the output from one layer acts as an input to the layer above. In other words, the layers are arranged in a Markov chain such that the states at any layer are only dependent on the representations in the layer below and above, and are independent of the rest of the model. The overall goal of the dynamical system at any layer is to make the best of the representation in the layer below using the top-down information from the layers above and the temporal information from the previous states. Hence, the name (DPCN) . The DPCN proposed here is closely related to models proposed in, where predictive coding is used as a statistical model to explain cortical functions in the mammalian brain. Similar to the proposed model, they construct hierarchical generative models that seek to infer the underlying causes of the sensory inputs. While use an update rule similar to Kalman filter for inference, proposed a general framework considering all the higher-order moments in a continuous time dynamic model. However, neither of the models is capable of extracting discriminative information, namely a sparse and invariant representation, from an image sequence that is helpful for high-level tasks like object recognition. Unlike these models, here we propose an efficient inference procedure to extract locally invariant representation from image sequences and progressively extract more abstract information at higher levels in the model. Other methods used for building deep models, like restricted Boltzmann machine (RBM), auto-encoders and predictive sparse decomposition, are also related to the model proposed here. All these models are constructed on similar underlying principles: (N) like ours, they also use greedy layer-wise unsupervised learning to construct a hierarchical model and (N) each layer consists of an encoder and a decoder. The key to these models is to learn both encoding and decoding concurrently (with some regularization like sparsity, denoising or weight sharing), while building the deep network as a feed forward model using only the encoder. The idea is to approximate the latent representation using only the feed-forward encoder, while avoiding the decoder which typically requires a more expensive inference procedure. However in DPCN there is no encoder. Instead, DPCN relies on an efficient inference procedure to get a more accurate latent representation. As we will show below, the use of reciprocal top-down and bottom-up connections make the proposed model more robust to structured noise during recognition and also allows it to perform low-level tasks like image denoising. To scale to large images, several convolutional models are also proposed in a similar deep learning paradigm . Inference in these models is applied over an entire image, rather than small parts of the input. DPCN can also be extended to form a convolutional network, but this will not be discussed here.