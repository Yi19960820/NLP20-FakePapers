Image inpainting is the process of reconstructing a missing region in an image such that the inpainted area is visually consistent with its neighboring pixels and the inpainted image overall looks realistic. Traditional approaches to this problem either require that the filling information is available in the image _cite_ or rely on the availability of a large photo database to retrieve the missing region _cite_ . These approaches work well for the background inpainting problem, i.e., filling the missing part with image patches similar to its background, as inpainting can be performed through pattern matching. However, when it comes to complete a missing part of an object where no existing patches can be matched or retrieved, the traditional approaches may fail. For example, if a mouth is missing, it is not possible to synthesize the mouth using image patches from other face parts. Instead, image inpainting in this case requires semantic knowledge about faces, e.g., location, shape, color and texture of face parts. To address this object completion problem in image inpainting, recent models _cite_ propose to use generative adversarial networks (GANs) for more semantically consistent results. However, for generative models, the semantic understanding is implicitly learned through adversarial training. There are no direct constraints on the structure of the target object and hence the inherent semantic understanding is not always guaranteed. Fortunately, in recent years, the success of deep learning has made the semantic labels of objects accessible. In this work, we investigate the possibility of introducing the semantic knowledge of face labels to the adversarial training of face completion for better induction of semantic understanding. We focus on helping the inpainting model better understand the underlying structure of faces through the collaborative learning of other face related tasks. We argue that current approaches using generative inpainting models alone may not be able to produce structurally realistic results in some cases. For example, when an eye is missing from the image, the inpainting model should be able to predict the missing eye's location and shape based on the facial symmetry. However, as shown in the first row of Figure _ref_, the generative image inpainting model trained without using our proposed collaborative method produces a structurally unrealistic face (Figure _ref_ (d) top) with the inpainted eye smaller and darker than the eye outside the corrupted region. In contrast, a collaboratively trained model can keep the structural consistency between the inpainted region and the nearby context (Figure _ref_ (c) top) . In addition, we also find that models trained in this manner tend to produce visually consistent results among tasks. As demonstrated in Figure _ref_ (d) bottom, the segmentation result is closely aligned with the inpainting result other than the ground truth. This provides a clear evidence that they are inherently helping each other during training and the knowledge is shared instead of individually learned. To this end, we propose an innovative image-to-image generative network for face completion. The proposed method formulates a collaborative GAN to facilitate the direct learning of multiple tasks. For the generator, the network outputs multiple channels for each task and has them share most of the network parameters for better collaborative learning. We also stand apart from the existing inpainting models by introducing skip connections between the encoder and decoder _cite_ . For the discriminators, we apply conditional GAN (cGAN) _cite_ for better transformation quality and have dedicated discriminators for each task. For the loss function, we introduce an inpainting concentrated scheme to allow the model focusing on the inpainting itself instead of autoencoding the context. Our experimental results demonstrate the effectiveness of the proposed design and better feature representations can be obtained with the proposed collaborative GAN. Comparing with other generative models without using collaborative adversarial learning, our approach consistently produces remarkably more realistic inpainting results. Comparing with single task adversarial learning, our joint approach produces better performances on all tasks.