This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs _cite_ that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD) . More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (\eg, _inline_eq_ N) layers are approximated. For the widely used very deep VGG-N model _cite_, our method achieves a whole-model speedup of N _inline_eq_ with merely a N \% increase of top-N error in ImageNet classification. Our N _inline_eq_ accelerated VGG-N model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector _cite_ .