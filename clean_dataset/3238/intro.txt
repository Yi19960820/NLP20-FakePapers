Multiple kernel learning (MKL) is a classic yet powerful machine learning technique for integrating heterogeneous features in the reproducing kernel Hilbert space (RKHS) by linear or nonlinear combination of base kernels. It has been demonstrated successfully in many applications such as object detection in computer vision _cite_ as one of the joint winners in VOCN _cite_ . In the literature many MKL formulations have been proposed. For instance, proposed a block-_inline_eq_ regularized formulation for binary classification, and later proposed using _inline_eq_ norm as regularization in MKL. As a classic example we show the objective in SimpleMKL _cite_ as follows: where the set _inline_eq_ denote training data, _inline_eq_ and _inline_eq_ denote the classifier parameters, _inline_eq_ denote the weights for _inline_eq_ kernels lying on the _inline_eq_-simplex space, _inline_eq_ denote the slack variables for the hinge loss, _inline_eq_ is a predefined regularization constant, _inline_eq_ denotes the matrix transpose operator, and _inline_eq_ denotes the feature map in RKHS for the _inline_eq_-th kernel _inline_eq_ so that _inline_eq_ . Based on the dual we can rewrite Eq. _ref_ as follows: Here _inline_eq_ denote the Lagrangian variables in the dual. To optimize Eq. _ref_ typically alternating optimization algorithms are developed, that is, learning _inline_eq_ in Eq. _ref_ while fixing _inline_eq_ and learning _inline_eq_ in Eq. _ref_ by solving a kernel support vector machine (SVM) problem while fixing _inline_eq_ . Such family of algorithms in MKL are called {\em wrapper methods} . From this perspective Eq. _ref_ essentially learns a kernel mixture by {\em linear} combination of base kernels. Accordingly we can write the {\em decision function}, _inline_eq_, in SimpleMKL for a new sample _inline_eq_ as follows: Localized MKL (LMKL) _cite_ and its variants such as _cite_ are another family of MKL algorithms which learn {\em localized} (\ie data-dependent) kernel weights. For instance, propose replacing _inline_eq_ in Eq. _ref_ with a function _inline_eq_, leading to the following objective: where _inline_eq_ denotes the {\em gating function} that takes data _inline_eq_ as input and outputs the weight for the _inline_eq_-th kernel. The decision function is as follows: Comparing Eq. _ref_ with Eq. _ref_, we can see that LMKL essentially {\em relaxes} the conventional MKL by introducing the data-dependent function _inline_eq_ in LMKL that may better capture data properties such as distributions for learning kernel weights. Since the learning capability of _inline_eq_ is high, previous works usually prefer regularizing it using explicit expressions such as Gaussian similarity in _cite_ . Beyond learning linear mixtures of kernels, some works focus on learning nonlinear kernel combination such as involving product between kernels _cite_ or deep MKL _cite_ that embeds kernels into kernels. Among these works, the primal formulations may be nontrivial to write down explicitly. Instead some works preferred learning the decision functions directly based on Empirical Risk Minimization (ERM) . For instance, in _cite_ the nonlinear kernel mixtures are fed into the dual of an SVM to learn the parameters for both kernel mixtures and the classifiers. MKL, including multi-class MKL _cite_ and multi-label MKL _cite_, can be further generalized to multi-task learning (MTMKL) _cite_ . However, these research topics are out of scope of this paper. {\bf Motivation:} From the dual perspective, solving kernel SVMs such as Eq. _ref_ involves a constrained quadratic programming (QP) problem for computing the Lagrangian variables that determine {\em linear} decision boundaries in RKHS. This is the key procedure that consumes most of the computation. Moreover, in the literature of LMKL it lacks of a principle for learning the gating functions rather than manually tuning the functions such as enforcing Gaussian distributions as prior. In addition the optimal decision functions may not be necessarily linear in RKHS for the sake of accuracy. Therefore, it is highly desirable to have principles for learning both gating and decision functions in LMKL. Deep neural networks (DNNs) have been proven as a universal approximator for an arbitrary function _cite_ . In fact recently researchers have started to apply DNNs as efficient solvers to some classic numerical problems such partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimensional spaces _cite_ . Conventionally solving these numerical problems with large amount of high dimensional data is very time-consuming. In contrast DNNs are able to efficiently output approximate solutions with sufficient precision. {\bf Contributions:} Based on the considerations above, we propose a simple neural network, namely LMKL-Net, as an efficient solver for LMKL. As a learning principle we parameterize the gating function as well as the classifier in LMKL using an attentional network (AN) and a multilayer perceptron (MLP), respectively, without enforcing any (strong) prior explicitly. We expect that by fitting the data, the network can approximate both underlying optimal functions properly. The localized weights learned from AN guarantee that the kernel mixtures are indeed valid kernels. Empirically we demonstrate the superiority of LMKL-Net over the state-of-the-art MKL solvers in terms of accuracy, memory, and running speed.