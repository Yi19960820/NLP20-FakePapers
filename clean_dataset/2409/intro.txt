Recently, deep learning has been successfully applied to a diverse range of research areas including computer vision _cite_, natural language processing _cite_, and robotics _cite_ . When deep networks are kept in a cyber environment without interacting with an actual physical system, mis-predictions or malfunctioning of the system may not cause catastrophic disasters. However, when using deep learning methods in a real physical system involving human beings, such as an autonomous car, safety issues must be considered appropriately _cite_ . In May N, in fact, a fatal car accident had occurred due to the malfunctioning of a low-level image processing component of an advanced assisted driving system (ADAS) to discriminate the white side of a trailer from a bright sky _cite_ . In this regard, Kendall and Gal _cite_ proposed an uncertainty modeling method for deep learning estimating both aleatoric and epistemic uncertainties indicating noise inherent in the data generating process and uncertainty in the predictive model which captures the ignorance about the model. However, computationally-heavy Monte Carlo sampling is required which makes it not suitable for real-time applications. In this paper, we present a novel uncertainty estimation method for a regression task using a deep neural network and its application to learning from demonstration (LfD) . Specifically, a mixture density network (MDN) _cite_ is used to model underlying process which is more appropriate for describing complex distributions _cite_, e.g., human demonstrations. We first present an uncertainty modeling method when making a prediction with an MDN which can be acquired with a single MDN forward path without Monte Carlo sampling. This sampling-free property makes it suitable for real-time robotic applications compared to existing uncertainty modeling methods that require multiple models _cite_ or sampling _cite_ . Furthermore, as an MDN is appropriate for modeling complex distributions _cite_ compared to a density network used in _cite_ or standard neural network for regression, the experimental results on autonomous driving tell us that it can better represent the underlying policy of a driver given complex and noise demonstrations. The main contributions of this paper are twofold. We first present a sampling-free uncertainty estimation method utilizing an MDN and show that it can be decomposed into two parts, explained and unexplained variances which indicate our ignorance about the model and measurement noise, respectively. The properties of the proposed uncertainty modeling method is analyzed through three different cases: absence of data, heavy measurement noise, and composition of functions scenarios. Using the analysis, we further propose an uncertainty-aware learning from demonstration (Lfd) method. We first train an aggressive controller in a simulated environment with an MDN and use the explained variance of an MDN to switch its mode to a rule-based conservative controller. When applied to a complex real-world driving dataset from the US Highway _inline_eq_ _cite_, the proposed uncertainty-award LfD outperforms compared methods in terms of safety of the driving as the out-of-distribution inputs, which are often refer to as covariate shift _cite_, are successfully captured by the proposed explained variance. The remainder of this paper is composed as follows: Related work and preliminaries regarding modeling uncertainty in deep learning are introduced in Section _ref_ and Section _ref_ . The proposed uncertainty modeling method with an MDN is presented in Section _ref_ and analyzed in Section _ref_ . Finally, in Section _ref_, we present an uncertainty-aware learning from demonstration method and successfully apply to an autonomous driving task using a real-world driving dataset by deploying and controlling an virtual car inside the road.