The automatic analysis of surgical videos is at the core of many potential assistance systems for the operating room. The localization of surgical tools, in particular, is required in many applications, such as the analysis of tool-tissue interactions, the development of novel human-robot assistance platforms and the automated annotation of video databases. In the literature, surgical tool localization has traditionally been approached with fully supervised methods _cite_, with the most recent localization and segmentation methods relying on deep learning _cite_ . However, training fully supervised approaches require the data to be fully annotated with spatial information, which is tedious and expensive. This may explain why the datasets used so far for tool localization are small, namely in the order of a few thousand images and with a maximum of N-N sequences, as described in the recent review _cite_ . This then limits the applicability and generalizability of the approaches that can be developed. Recently, it has been shown that when a convolutional neural network is trained for the task of classification, the convolutional layers of the network learn general notions about the detected objects. Some recent works have used this fact to successfully localize objects in images without explicitly training for localization _cite_ . The proposed deep learning approaches directly output spatial heat maps, where the detected position corresponds to the strongest activations. This is achieved by replacing all fully connected layers with equivalent convolutions or removing them altogether. The resulting architectures are called fully convolutional networks (FCNs) . Others have extended this approach to address the challenging task of semantic segmentation with weak supervision _cite_ . In the medical community as well, weakly supervised learning (WSL) has been applied to tasks such as detection of cancerous regions in medical images _cite_ . Along with the recent release of large public surgical video datasets, such as CholecN _cite_, which contains N complete cholecystectomy videos fully annotated with binary tool presence information (_inline_eq_ NK frames in total), WSL techniques can potentially help develop tool localization methods that can scale up to larger datasets containing much more variability. In this paper, we propose a method for detecting and localizing surgical tools. It is based on weakly-supervised learning using only image-level labels and does not require any spatial annotation. Our contributions are twofold: (N) we propose the first surgical tool localization approach based on weakly-supervised learning; (N) we demonstrate our approach on the largest public endoscopic video dataset to date, namely CholecN _cite_ .