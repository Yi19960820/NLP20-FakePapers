(CNNs) have brought about great advances in tasks such as object recognition, object detection, and semantic segmentation in several years. However, the number of parameters required for CNNs that have generally good performance tends to be very large, which imposes memory requirements and computational cost that exceed the capabilities of mobile and compact devices. To solve the problems, various techniques _cite_ have been proposed for making CNNs more efficient and increasing the speed of inference. In these works, network pruning is an important approach for removing redundant parameters from the models. \par Research into the pruning methods are roughly divided at two levels: the neuron level and the channel level. At the neuron level, the number of parameters is reduced by severing connections between spatial neurons in the convolutional layer or the neurons in the fully connected layer. At the channel level (channel pruning), the connections of all structural elements that respond to a particular channel are dropped for input and output channels in the convolutional layer; pruning is performed in sets of groups. This method differs from the neuron-level pruning (\eg, in _cite_) in that it does not require any special implementation since the shape of the weight matrix is reduced. However, since deletions are performed in sets of groups, the influence on the precision is significant and the problem setup is more difficult than with neuron-level methods. The channel pruning methods have several difficulties that require designing the criteria for evaluating the importance of channels and set the compression ratio for each layer. Especially, the latter is serious problem because the many existing methods~ _cite_ need the ratios as hyper-parameters for the pruning. In general, the problem will be more difficult when using deeper models. \par In this paper, we propose a channel pruning method for pre-trained models. Figure _ref_ a shows an overview of our approach. In this method, the importance of channels is evaluated using neural networks (we call attention modules) connected immediately before all target layers in the pre-trained model. Although these attention modules need to be trained, the modules are able to infer the importance of the channels. Furthermore, it is optimized in all levels of layers since the attention module for a lower level is trained by considering the gradient of the pre-trained model and the gradient of the upper-level attention module. \par The major contributions of this paper are summarized as follows: