In the past five years, deep neural network (DNN) based models have achieved super-human performance on the ILSVRC NK recognition task. However, most existing object recognition models, particularly those DNN-based ones, require hundreds of image samples to be collected for each object class; many of the object classes are rare and it is thus extremely hard, sometimes impossible to collect sufficient training samples, even with social media. One way to bypass the difficulty in collecting sufficient training data for object recognition is zero-shot learning (ZSL) . The goal of ZSL is to recognise a new/unseen class without any training samples from the class. A ZSL model typically assumes that each class name is embedded in a semantic space. With this semantic space and a visual feature space representing the appearance of an object in an image, it chooses a joint embedding space (which is mostly formed with the semantic space) and learns a projection function so that both the visual features and the semantic vectors are embedded in the same space. Under the inductive ZSL setting, the projection function is learned only with the seen class training samples and then directly used to project the unseen class samples to the joint embedding space. The class label of a test unseen class sample is assigned to the nearest unseen class prototype. One of the biggest challenges in ZSL is the domain gap between the seen and unseen classes. As mentioned above, the projection functions learned from the seen classes are applied to the unseen class data. However, the unseen classes are often visually very different from the seen ones; therefore the domain gap between the seen and unseen class domains can be big, meaning that the same projection function may not be able to project an unseen class image to be close to its corresponding class name in the joint embedding space for correct recognition. To tackle the domain gap challenge, most previous works focus on learning more transferrable projection functions. Specifically, many ZSL models resort to transductive learning, where the unlabeled test samples from unseen classes are used to learn the projection functions. However, such an approach assumes the access to the whole test dataset beforehand, and thus deviates from the motivation of ZSL: the unseen class samples are rare. These transductive ZSL models are thus limited in their practicality in real-world scenarios. Moreover, other than projection function learning, deep feature learning has been largely overlooked in ZSL. Most previous works, if not all, simply use visual features extracted by convolutional neural network (CNN) models pretrained on ImageNet . We argue that deep feature learning and projection function learning should be equally important for overcoming the domain gap challenge. In this paper, we propose a novel inductive ZSL model that leverages the superclasses as the bridge between seen and unseen classes to narrow the domain gap. The idea is simple: an unseen class is not strictly `zero-shot' if it falls into a superclass that contains one or more seen classes. Exploiting the shared superclass among seen and unseen classes thus provides a means for narrowing the domain gap. In this work, we generate the superclasses by a data-driven approach, without the need of a human-annotated taxonomy. Specifically, we construct a tree-structured class hierarchy that consists of multiple superclass layers and a single class layer. In this tree-structured hierarchy, we take both seen classes and unseen classes as the leaves, and generate the superclasses by clustering over the semantic representations of all seen and unseen class names (see the orange box in Fig.~ _ref_) . Moreover, by exploiting the superclasses from the class hierarchy, the proposed model aims to narrow the domain gap in two aspects: deep feature learning and projection function learning. To learn transferrable visual features, we propose a novel deep feature learning model based on the superclasses from the class hierarchy. Specifically, a CNN model is first designed for both class and superclass classification. Since the seen and unseen classes may have the same superclasses, directly training this CNN model enables us to learn transferrable deep features. To further strengthen the feature transferability, we explicitly encode the hierarchical structure among classes/superclasses into the CNN model, by plugging recurrent neural network (RNN) components into the network. Overall, a novel CNN-RNN architecture is designed for transferrable deep feature learning, as illustrated in the green box in Fig.~ _ref_ . To learn a transferrable projection function for inductive ZSL, we propose a novel projection function learning method by utilising the superclasses to align the two domains, as shown in the blue box in Fig.~ _ref_ . Specifically, we formulate the projection function learning on each superclass layer as a graph regularised self-reconstruction problem. An efficient iterative algorithm is developed as the solver. The results of multiple superclass layers are combined to boost the performance of projection function learning on the single class layer. Importantly, our model can be easily extended to a closely related problem--few-shot learning (FSL) . Specifically, the above transferrable feature and projection learning methods involved in our model are employed to solve the FSL problem without any modification. Experiments on the widely-used mini-ImageNet dataset show that our model achieves the state-of-the-art results. This suggests that the class hierarchy is also important for narrowing down the domain gap in the FSL problem. Our contributions are: (N) A novel inductive ZSL model is proposed to align the seen and unseen class domains by utilising the superclasses shared across the two domains. To our best knowledge, this is the first time that the superclasses generated by data-driven clustering have been leveraged in both feature learning and projection learning to narrow the domain gap for inductive ZSL. (N) Due to the domain alignment using the superclasses from the class hierarchy, we have created the new state-of-the-art for ZSL and FSL.