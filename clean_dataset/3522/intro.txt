neural networks _cite_ have contributed to a series of advances in tackling image recognition and visual understanding problems _cite_ . They have been applied in many areas of engineering and science _cite_ . Increasing the network depth is known to improve the model capabilities, which can be seen from AlexNet~ _cite_ with N layers, VGG~ _cite_ with N layers, and GoogleNet~ _cite_ with N layers. However, increasing the depth can be challenging for the learning process because of the vanishing/exploding gradient problem~ _cite_ . Deep residual networks _cite_ avoid this problem by using identity skip-connections, which help the gradient to flow back into many layers without vanishing. The identity skip-connections facilitate training of very deep networks up to thousands of layers that helped residual networks win five major image recognitions tasks in ILSVRC~N _cite_ and Microsoft COCO~N _cite_ competitions. However, an obvious drawback of residual networks is that every percentage of improvement requires significantly increasing the number of layers, which linearly increases the computational and memory costs _cite_ . On CIFAR-N image classification dataset, deep residual networks with N-layers and N-layers reach a test error rate of _inline_eq_ and _inline_eq_ respectively, while the N-layer has six times more computational complexity than the N-layer. On the other hand, wide residual networks _cite_ have N times fewer layers while outperforming the original residual networks. It seems that the power of residual networks is due to the identity skip-connections rather than extremely increasing the network depth. Nevertheless, a recent study supports that deep residual networks act like ensembles of relatively shallow networks~ _cite_ . This is achieved by showing the existence of exponential paths from the output layer to the input layer that gradient information can flow. Also, observations show that removing a layer from a residual network, during the test time, has a modest effect on its performance. Additionally, it shows that most of the gradient updates during optimization come from ensembles of relatively shallow depth. Moreover, residual networks do not resolve the vanishing gradient problem by preserving the gradient through the entire depth of the network. Instead, they avoid the problem by ensembling exponential networks of different length. This raises the importance of multiplicity that refers to the number of possible paths from the input layer to the output layer _cite_ . Inspired by these observations, we introduce multi-residual networks (Multi-ResNet) which increase the multiplicity of the network, while keeping its depth fixed. This is achieved by increasing the number of residual functions in each residual block. We then show that the accuracy of a shallow multi-residual network is similar to a deep N-layer residual network. This supports that deep residual networks behave like ensembles instead of a single extremely deep network. Next, we examine the importance of effective range which is the range of paths that significantly contribute towards gradient updates. We show that for a residual network deeper than a threshold _inline_eq_, increasing the number of residual functions leads to a better performance than increasing the network depth. This leads to a lower error rate for the multi-residual network with the same number of convolutional layers as the deeper residual network. Experiments on ImageNet, CIFAR-N, and CIFAR-N datasets show that multi-residual networks improve the accuracy of deep residual networks and outperform almost all of the existing models. We demonstrate that a N-layer Multi-ResNet with two residual functions in each block outperforms the top-N accuracy rate of a N-layer ResNet by _inline_eq_ on the ImageNet N classification dataset _cite_ . Also, using moderate data augmentation (flip/translation), multi-residual networks achieve an error rate of _inline_eq_ and _inline_eq_ on CIFAR-N and CIFAR-N receptively (based on five runs) . This is N and N improvement compared to the residual networks with identity mappings _cite_ with almost the same computational and memory complexity. The proposed multi-residual network achieves a test error rate of _inline_eq_ and _inline_eq_ on CIFAR-N and CIFAR-N. Concurrent to our work, ResNeXt _cite_ and PolyNet _cite_ achieved second and third place in the ILSVRC~N classification task . Both models increase the number of residual functions in the residual blocks similar to our model, while PolyNet inserts higher order paths into the network as well. Eventually, a model parallelism technique has been explored to speed up the proposed multi-residual network. The model parallelism approach splits the calculation of each block between two GPUs, thus each GPU can simultaneously compute a portion of residual functions. This leads to the parallelization of the block, and consequently the network. The resulting network has been compared to a deeper residual network with the same number of convolutional layers that exploits data parallelism. Experimental results show that in addition to being more accurate, multi-residual networks can also be up to N faster. In summary, the contributions of this research are: The rest of the paper is organized as follows. Section~ _ref_ details deep residual networks and other models capable of improving the original residual networks. The hypothesis that residual networks are exponential ensembles of relatively shallow networks is explained in Section~ _ref_ . The proposed multi-residual networks and the importance of the effective range are discussed in Section~ _ref_ . Supporting experimental results are presented in Section~ _ref_ . Concluding remarks are provided in Section~ _ref_ . A pre-print version of this paper _cite_ is available at _url_, and the code to reproduce the results can be found at _url_ .