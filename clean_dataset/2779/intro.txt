Mobile devices are ubiquitous and they are used for various applications such as mobile banking, e-business and social media. These devices store confidential and critical data which if lost/stolen can cause harm to the user. Therefore, secure, convenient and fast authentication methods are required to unlock the devices. Most of the modern mobile devices rely on biometric based authentication~ _cite_ such as face and fingerprint recognition to validate the identity of the user. However, biometric authentication on mobile devices pose several challenges. A primary challenge in acquiring the biometric data from mobile phones is that it is highly unconstrained. For touch-less sensing (e.g. capturing faces), the quality of the image can be adversely affected by factors such as variation in illumination conditions, distance from the subject, indoor/outdoor scenarios, quality of the front and back camera, and motion blur due to movement of the device/subject. Different mobile sensors for capturing biometric data pose a cross sensor matching problem, as different camera sensors have different imaging properties. This introduces heterogeneity in the captured data (e.g., indoor vs outdoor, front camera vs back camera resolution), and it makes biometric recognition on mobile devices an interesting and challenging problem. Periocular region as a biometric modality _cite_ has been gaining attention. It refers to using the regions around the eye for identity recognition. The periocular region is generally available even in unconstrained scenarios with a non cooperative subject and it can be especially useful in situations where the other information such as face is partially occluded. Figure _ref_ illustrates the use of mobile periocular recognition in unconstrained environments. It requires no additional capturing overhead which is useful while capturing using a mobile device. Feasibility of periocular region as a biometric trait was explored by Park \etal _cite_ . Thereafter, there has been significant research advancements in this area. Detailed surveys of periocular recognition are provided by Alonso-Fernandez \etal _cite_ and Nigam \etal _cite_ . A large number of techniques have performed periocular recognition on data obtained with high quality sensors in constrained conditions but there has been increasing focus on the less constrained scenarios as well. Many popular methods relied on hand crafted features like HOG, SIFT and LBP for the periocular and iris information _cite_ . Tan \etal _cite_ use filters applied on input data for providing discriminative features for segmentation and recognition. Nie \etal _cite_ use convolutional restricted Boltzmann machine along with handcrafted feature extraction for improved performance. Recently, deep convolutional neural networks have gained immense popularity for ocular recognition. Zhao and Kumar _cite_ use explicit semantic information to extract better features and improve performance of the CNN. Proen \c ca \etal _cite_ generate artificial samples belonging to multiple classes by interchanging ocular parts from different subjects for data augmentation thereby improving the training process. Several works have also explored the problem of periocular recognition by capturing data using mobile devices. De Frietas \etal _cite_ model the inter session variability in the data from the enrollment time to the test time. Raghavendra \etal _cite_ utilize coupled autoencoders and Maximum Response (MR) based texture features for mobile periocular recognition. Another approach by Raja \etal _cite_ used pooling of sparse filtered features. Zhang \etal _cite_ use the fusion of iris and periocular region information with weighted concatenation to obtain a joint representation. In this paper, a novel heterogeneity aware deep embedding framework for periocular recognition is proposed specifically for scenarios where the images are captured in unconstrained settings. The proposed method works by obtaining the heterogeneity invariant feature representations of the periocular images via a deep convolutional neural network. The deep CNN model is trained via the proposed heterogeneous aware loss metric based on the identity of the subjects and tries to enforce a margin between the clusters of images of a particular identity/class in the embedding space. The embeddings of the same subject/classes are brought close to each other and that of other subjects are pushed away from each other in the output embedding space of the deep CNN model. In addition to that, the loss function ensures that the model produces heterogeneity aware embeddings. Experiments are performed on three popular periocular databases and comparison with existing algorithms demonstrate state-of-the-art results. The remaining paper is arranged as follows: Section _ref_ contains details of the proposed algorithm. The database used and experiment protocols are discussed in Section _ref_ while the results are discussed in Section _ref_ .