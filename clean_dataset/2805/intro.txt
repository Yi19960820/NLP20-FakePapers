The absence of the ability to hear sounds is a huge obstacle to smooth and natural communication for the hearing-impaired people in a predominantly hearing world. In many social situations, the hearing-impaired people necessarily need help from professional sign language interpreters to communicate with the hearing people even when they have to reveal their very private and sensitive information. Moreover, the hearing-impaired people are more vulnerable in various emergency situations due to the communication barriers due to the absence of the hearing ability. As a consequence, the hearing-impaired people easily become isolated and withdrawn from society. This leads us to investigate the possibility of developing an artificial intelligence technology that understands and communicates with the hearing-impaired people. However, sign language recognition or translation is a very challenging problem since the task involves a interpretation between visual and linguistic information. The visual information consists of several parts such as body movement and facial expression of a signer~ _cite_ . To interpret the collection of the visual information as natural language sentences is also one of tough challenges to realize the sign language translation problem. In order to process a sequence, there have been several interesting variants of recurrent neural networks (RNNs) proposed including long short-term memory (LSTM) ~ _cite_ and gated recurrent units (GRUs) ~ _cite_ . These architectures have been successfully employed to resolve many problems involving the process of sequential data such as machine translation and image captioning~ _cite_ . Moreover, many researchers working on the field of image and video understanding have raised the level that seemed infeasible even a few years ago by learning their neural networks with a massive amount of training data. Recently, many neural network models based on convolutional neural network (CNNs) exhibited excellent performances in various visual tasks such as image classification~ _cite_, object detection~ _cite_, semantic segmentation~ _cite_, and action recognition~ _cite_ . Understanding sign languages requires a high level of spatial and temporal understanding and therefore, is regarded as very difficult with the current level of computer vision and machine learning technology~ _cite_ . It should be noted that sign languages are different from hand (finger) languages as the hand languages only represent each letter in an alphabet with the shape of a single hand~ _cite_ while the linguistic meaning of each sign is determined by subtle difference of shape and movement of body, hands, and sometimes by facial expression of the signer~ _cite_ . More importantly, the main difficulty comes from the lack of dataset for training neural networks. Many sign languages represent different words and sentences of spoken languages with temporal sequences of gestures comprising continuous pose of hands and facial expressions. This implies that there are uncountably many combinations of the cases even to describe a single human intention with the sign language. Hence, we restrict ourselves to the task of translating sign language in various emergency situations. We construct the first Korean sign language dataset collected from fourteen professional signers who are actually hearing-impaired people and named it the KETI sign language dataset. The KETI sign language dataset consists of N, N high-resolution videos that recorded the Korean signs corresponding to N words and N sentences related to various emergency situations. Using the KETI sign language dataset, we present our sign language translation model based on the well-known off-the-shelf human keypoint detector and the sequence-to-sequence translation model. To the best of our knowledge, this paper is the first to exploit the human keypoints for the sign language translation problem. Due to the inherent complexity of the dataset and the problem, we present an effective normalization technique for the extracted human keypoints to be used in the sign language translation. We implement the proposed ideas and conduct various experiments to verify the performance of the ideas with the test dataset. The main contributions of this paper are highlighted as follows: