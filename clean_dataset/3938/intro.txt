Image classification and annotation are two important tasks in computer vision. In image classification, one tries to describe the image globally with a single descriptive label (such as {\it coast}, {\it outdoor}, {\it inside city}, etc.), while annotation focuses on tagging the local content within the image (such as whether it contains `` {\it sky} '', a `` {\it car} '', a `` {\it tree} '', etc.) . Since these two problems are related, it is natural to attempt to solve them jointly. For example, an image labeled as {\it street} is more likely to be annotated with `` {\it car} '', `` {\it pedestrian} '' or `` {\it building} '' than with `` {\it beach} '' or `` {\it see water} ''. Although there has been a lot of work on image classification and annotation separately, less work has looked at solving these two problems simultaneously. Work on image classification and annotation is often based on a topic model, the most popular being latent Dirichlet allocation or LDA~ _cite_ . LDA is a generative model for documents that originates from the natural language processing community but that has had great success in computer vision for scene modeling~ _cite_ . LDA models a document as a multinomial distribution over topics, where a topic is itself a multinomial distribution over words. While the distribution over topics is specific for each document, the topic-dependent distributions over words are shared across all documents. Topic models can thus extract a meaningful, semantic representation from a document by inferring its latent distribution over topics from the words it contains. In the context of computer vision, LDA can be used by first extracting so-called ``visual words'' from images, convert the images into visual word documents and training an LDA topic model on the bags of visual words. Image representations learned with LDA have been used successfully for many computer vision tasks such as visual classification~ _cite_, annotation~ _cite_ and image retrieval~ _cite_ . Although the original LDA topic model was proposed as an unsupervised learning method, supervised variants of LDA have been proposed~ _cite_ . By modeling both the documents' visual words and their class labels, the discriminative power of the learned image representations could thus be improved. At the heart of most topic models is a generative story in which the image's latent representation is generated first and the visual words are subsequently produced from this representation. The appeal of this approach is that the task of extracting the representation from observations is easily framed as a probabilistic inference problem, for which many general purpose solutions exist. The disadvantage however is that as a model becomes more sophisticated, inference becomes less trivial and more computationally expensive. In LDA for instance, inference of the distribution over topics does not have a closed-form solution and must be approximated, either using variational approximate inference or MCMC sampling. Yet, the model is actually relatively simple, making certain simplifying independence assumptions such the conditional independence of the visual words given the image's latent distribution over topics. Recently, an alternative generative modeling approach for documents was proposed by . Their model, the Document Neural Autoregressive Distribution Estimator (DocNADE), models directly the joint distribution of the words in a document, by decomposing it through the probability chain rule as a product of conditional distributions and modeling each conditional using a neural network. Hence, DocNADE doesn't incorporate any latent random variables over which potentially expensive inference must be performed. Instead, a document representation can be computed efficiently in a simple feed-forward fashion, using the value of the neural network's hidden layer. also show that DocNADE is a better generative model of text documents and can extract a useful representation for text information retrieval. In this paper, we consider the application of DocNADE in the context of computer vision. More specifically, we propose a supervised variant of DocNADE (SupDocNADE), which models the joint distribution over an image's visual words, annotation words and class label. The model is illustrated in Figure~ _ref_ . We investigate how to successfully incorporate spatial information about the visual words and highlight the importance of calibrating the generative and discriminative components of the training objective. Our results confirm that this approach can outperform the supervised variant of LDA and is a competitive alternative for scene modeling.