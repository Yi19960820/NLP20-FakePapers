Predicting human motion over a significant time horizon is a challenging problem with applications in a variety of domains. For example in human computer interaction, human detection and tracking, activity recognition, robotics and image based pose estimation it is important to model and predict the most probable sequence of human motions in order to react accordingly and in a timely manner. Despite the inherent stochasticity and context dependency of natural motion, human observers are remarkably good at predicting what is going to happen next, exploiting assumptions about continuity and regularity in natural motion. However, formulating this domain knowledge into strong predictive models has been proven to be difficult. Integrating spatio-temporal information into algorithmic frameworks for motion prediction is hence either done via simple approximations such as optical flow~ _cite_ or via manually designed and activity specific spatio-temporal graphs~ _cite_ . Given the learning capability of deep neural networks and recurrent architectures in particular, there lies enormous potential but also many challenges in learning statistical motion models directly from data that can generalize over a range of activities and over long time horizons. Embracing this challenge we propose a new augmented recurrent neural network (RNN) architecture, dubbed \modelnamefull (\DAELSTMthreeLR) . Our model is capable of extracting both structural and temporal dependencies directly from the training data and does not require expert designed and task dependent spatio-temporal graphs for input as is the case in prior work~ _cite_ . Our work treats the two aspects of the task, namely the inherent constraints imposed by the skeletal configuration and the constraints imposed by temporal coherence explicitly. Using a feed forward network for pose filtering and an RNN for temporal filtering, reduces drift due accumulation of error over time. We demonstrate this in a number of side-by-side comparisons to the state-of-the-art. Specifically, we leverage de-noising autoencoders to learn the spatial structure and dependencies between different joints of the human skeleton while an LSTM network models temporal aspects of the motion. Contrary to related work that uses autoencoders to project the input data into a lower-dimensional manifold _cite_, our model directly operates in the joint angle domain of the human skeleton. Although we use an autoencoder-like architecture it does not bear real resemblance to encoding-decoding in the usual sense of latent representation learning. We simply use the autoencoder to de-noise skeletal poses at every time step, i.e. our auto encoder takes a pose as input and produces the filtered version of it in the same domain. During training we perturb the inputs with random noise, as is common practice in de-noising tasks, but additionally use dropout layers on the inputs to randomly remove entire joint positions from the training samples. Therefore, to be able to accurately reconstruct entire poses the network has to leverage information about the spatial dependencies between adjacent joints to correctly infer positions of the missing joints. Hence this training regime forces the network to implicitly recover the spatial configuration of the skeleton. The proposed model learns to predict the most likely pose at time _inline_eq_ given the history of poses up to time _inline_eq_ . Putting this model into recurrence allows for synthesis of novel and realistic motion sequences. We experimentally demonstrate that separating pose reconstruction and temporal modeling improves performance over settings where the autoencoder is primarily used for representation learning. While the architecture is simple, it captures both the spatial and temporal components of the problem well and improves prediction accuracy compared to the state-of-the-art on two publicly available datasets. In the domain of generative motion models, the lack of appropriate evaluation protocols to asses the quality and naturalness of the generated sequences is a commonly faced issue. The generated sequences need to be perceptually similar to the training data but clearly one does not simply want to memorize and replicate the training data. To better assess this generative nature of the task we furthermore contribute an evaluation protocol that quantifies how natural a generated sequence is over arbitrarily long time horizons. To assess naturalness we propose to train a separate classifier to predict action class labels. Intuitively the longer a sequence can be classified to belong to the same action category as the seed sequence the higher the quality of the prediction. We evaluate the proposed model on the N dataset of Ionescu et al. _cite_ and the more recent dataset of Holden et al. _cite_ in a pose forecasting task. Our model outperforms the N-layer LSTM baseline and two state-of-the-art models _cite_ both in terms of short and long horizon predictions. Furthermore, we detail results from the proposed evaluation protocol and demonstrate that this can be used to analyze the performance of such generative tasks.