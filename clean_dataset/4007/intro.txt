Deep Neural Nets (s) have become present day de-facto standard for any modern machine learning task. The flexibility and power of such structures have made them outperform other methods in solving some really complex problems of speech [_cite_] and object [_cite_] recognition. We exploit the power of such structures in an based application for word prediction and retrieval with a single model. Optical character recognition () is the task of converting images of typed, handwritten or printed text into machine-encoded text. It is a method of digitizing printed texts so that it can be electronically edited, searched, stored more compactly, displayed on-line and used in machine processes such as machine translation, text-to-speech and text mining. From character recognition to word prediction, s in recent years have gained much awaited traction in mainstream applications. With its usage spanning across handwriting recognition, print text identification, language identification etc.~ s have humongous untapped potential. In our present work we show an end-to-end, deep neural net, based architecture for word prediction and retrieval. We conceptualize the problem as that of a sequence to sequence learning and use based architecture to first encode input to a fixed dimension feature and later decode it to variable length output. Recurrent Neural Networks () architecture has an innate ability to learn data with sequential or temporal structure. This makes them suitable for our application. Encoder network reads the input sequence one step at a time and converts it to an expressive fixed-dimensional vector representation. Decoder network in turn converts this fixed-dimensional vector (Figure _ref_) to the text output. Encoder-Decoder framework has been applied to many applications recently. [_cite_] used recurrent encoder-decoder for character-level language modelling task where they predict the next character given the past predictions. It has also been used for language translation [_cite_] where a complete sentence is given as input in one language and the decoder predicts a complete sentence in another language. Vinyals et al. [_cite_] presented a model based on a deep recurrent architecture to generate natural sentences describing an image. They used a convolutional neural network as encoder and a recurrent decoder to describe images in natural language. Zaremba et al. [_cite_] used sequence to sequence learning for evaluating short computer programs, a domain that have been seen as too complex in past. Vinyals et al. [_cite_] proposed neural conversational networks based of sequence to sequence learning framework which converses by predicting the next sentence given the previous sentence (s) in a conversation. In the same spirit as _cite_, we formulate the problem as a sequence to sequence mapping problem to convert an input (text) image to its corresponding text. In this paper, we investigate the expressiveness and learnability of s in sequence to sequence learning regime for printed text . We demonstrate that sequence to sequence learning is suitable for word prediction task in a segmentation free setting. We even show the expressiveness of the learnt deep word image embeddings (from Encoder network of prediction) on image retrieval task. In (majority of) cases where standard models do not convert a variable length input to a fixed dimensional output, we are required to use Dynamic Time Warping () for retrieval which tends to be computationally expensive and slow. Converting variable length samples to fixed dimensional representation gives us access to fast and efficient methods for retrieval in fixed dimensional regime â€“-approximate nearest neighbour.