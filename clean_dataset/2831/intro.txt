Context features play a crucial role in many vision classification problems, such as semantic segmentation _cite_, object detection _cite_ and pose estimation _cite_ . As illustrated by the toy example in Fig. _ref_, when performing classification on the blurry white objects with similar appearance, if the semantic histogram from the whole image has a higher bin on the class ``sea'', then the object is more likely to be classified as a ``boat''; if the histogram has a higher bin on the class ``sky'', then it is more likely to be classified as a ``bird''. The semantic context thus acts as an important indicator for this classification task. Context features could be mainly categorized into statistical and non-statistical ones depending on whether they abandon the spatial orders of the context information. On the one hand, for most deep learning methods that gain increasing attention in recent years, non-statistical context features dominate. Some examples include _cite_ for object detection and _cite_ for semantic segmentation. On the other hand, statistical context features were mostly used in conventional classification methods with hand-crafted features. Commonly used statistical features include histogram, Bag-of-Words (BoW) _cite_, Fisher vector _cite_, Second-order pooling _cite_, etc. Such global context features performed successfully with hand-crafted low-level features at their times. However, they were much less studied since the popularity of deep learning. There are a limited number of deep learning methods that tried to incorporate statistical features into deep neural networks. Such examples include the deep Fisher network _cite_ that incorporate Fisher vector and orderless pooling _cite_ that combines with Vector of Locally Aggregated Descriptors (VLAD) . Both methods aim to improve the image classification performance. However, when calculating the statistical features, both methods fix the network parameters and simply treat features by deep networks as off-the-shelf features. In such a way, the deep networks and the statistical operations are not jointly optimized, which is one of the key factors for the success of deep networks. In this work, we introduce a learnable histogram layer for deep neural networks. Unlike existing deep learning methods that treat statistical operations as a separate module, our proposed histogram layer is able to back-propagate (BP) errors and learn optimal bin centers and bin width during training. Such properties make it possible to be integrated into neural networks and end-to-end trained. In this way, the appearance and statistical features in a neural network could effectively adapt each other and thus lead to better classification accuracy. The proposed learnable histogram layer could be used for various applications. We propose the HistNet-SS network for semantic segmentation and the HistNet-OD network for object detection. Both networks are built based on state-of-the-art deep neural networks with the learnable histogram layer. Jointly training the HistNets in an end-to-end manner helps convolution layers learn more discriminative feature representations and boosts the final accuracy. Thus our contributions of this paper can be summarized as three-fold: