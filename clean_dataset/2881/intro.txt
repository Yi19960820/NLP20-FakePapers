Free-hand sketch is the simplest form of human visual rendering. Albeit with varying degrees of skill, it comes naturally to humans at young ages, and has been used for millennia. Today it provides a convenient tool for communication, and a promising input modality for visual retrieval. Prior sketch studies focus on sketch recognition _cite_ or sketch-based image retrieval (SBIR) . SBIR methods can be further grouped into category-level _cite_ and instance-level fine-grained SBIR (FG-SBIR) _cite_ . This dichotomy corresponds to how a sketch is created--based on a category-name or a (real or mental) picture of a specific object instance. These produce different granularities of visual cues (e.g., prototypical vs. specific object detail) . As argued in _cite_, it is fine-grained sketches of specific object instances that bring practical benefit for image retrieval over the standard text modality. Modelling fine-grained object sketches and matching them with corresponding photo images containing the same object instances is extremely challenging. This is because photos are exact perspective projections of a real world scene or object, while free-hand sketches are iconic abstractions with different geometry, and selected choice of included detail. Moreover, sketches are drawn by people of different backgrounds, drawing abilities and styles, and different subjective perspectives about the salience of details to include. Thus two people can draw very different sketches of the same object as shown in Fig.~ _ref_ (a) photo _inline_eq_ sketch. A closer inspection of the human sketching process reveals that it includes two components. As shown in _cite_, a sketcher typically first deploys long strokes to draw iconic object contours, followed by shorter strokes to depict visual details (e.g., shoes laces or buckles in Fig.~ _ref_ (a)) . Both the iconic contour and object details are important for recognising the object instance and matching a sketch with its corresponding photo. The contour is informative about object subcategory (e.g., a boot or trainer), while the details distinguish instances within the subcategory--modelling both are thus necessary. However, they have very different characteristics demanding different treatments. The overall geometry of the sketch contour experiences large and user-specific distortion compared to the true edge contour of the photo (compare sketch contour in Fig.~ _ref_ (a) with photo object contour in Fig.~ _ref_ (b)) . Photo edge contours are an exact perspective projection of the object boundary; and free-hand sketches are typically an orthogonal projection at best, and usually much more distorted than that--if only because humans seem unable to draw long smooth lines without distortion _cite_ . In contrast, distortion is less of an issue for shorter strokes in the object detail part. But choice and amount of details varies by artist (e.g., buckles in Fig.~ _ref_ (a)) . In this paper, for the first time, we propose to model human sketches by inverting the sketching process. That is, instead of modelling the forward sketching pass (i.e., from photo/recollection to sketch), we study the inverse problem of translating sketches into visual representations that closely resemble the perspective geometry of photos. We further argue that this inversion problem is best tackled on two levels by separately factorising out object contours and the salient sketching details. Such factorisation is important for both modelling sketches and matching them with photos. This is due to the differences mentioned above: sketch contours are consistently present but suffer from large distortions, while details are less distorted but more inconsistent in their presence and abstraction level. Both parts can thus only be modelled effectively when they are factorised. We tackle the first level of inverse-sketching by proposing a novel deep image synthesis model for style transfer. It takes a sketch as input, restyles the sketch into natural contours resembling the more geometrically realistic contours extracted from photo images, while removing object details (see Fig.~ _ref_ (b)) . This stylisation task is extremely difficult because (a) Collecting a large quantity of sketch-photo pairs is infeasible so the model needs to be trained in an unsupervised manner. (b) There is no pixel-to-pixel correspondence between the distorted sketch contour and realistic photo contour, making models that rely on direct pixel correspondence such as _cite_ unsuitable. To overcome these problems, we introduce a new cyclic embedding consistency in the proposed unsupervised image synthesis model. It forces the sketch and unpaired photo contours to share some support in a common low-dimensional semantic embedding space. We next complete the inversion in a discriminative model designed for matching sketches with photos. It importantly utilises the synthesised contours to factor out object details to better assist with sketch-photo matching. Specifically, given a training set of sketches, their synthesised geometrically-realistic contours, and corresponding photo images, we develop a new FG-SBIR model that extracts factorised feature representations corresponding to the contour and detail parts respectively before fusing them to match against the photo. The model is a deep Siamese neural network with four branches. The sketch and its synthesised contours have their own branches respectively. A decorrelation loss is applied to ensure the two branch's representations are complementary and non-overlapping (i.e., factorised) . The two features are then fused and subject to triplet matching loss with the features extracted from the positive and negative photo branches to make them discriminative. The contributions of this work are as follows: (N) For the first time, the problem of factorised inverse-sketching is defined and identified as a key for both sketch modelling and sketch-photo matching. (N) A novel unsupervised sketch style transfer model is proposed to translate a human sketch into a geometrically-realistic contour. (N) We further develop a new FG-SBIR model which extracts an object detail representation to complement the synthesised contour for effective matching against photos.