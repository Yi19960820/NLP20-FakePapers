vehicles need to perceive and understand their surroundings (such as road users, free space, and other road scene semantics) for decision making, path planning, etc. Since Nissan introduced the surround view camera system in N on the Infiniti EXN, many TierNs and OEMs are actively developing such technology. Besides Infiniti and Nissan, automakers such as BMW, Audi, Mercedes Benz, Lexus, and Toyota offer similar systems in their production vehicles. The system usually consists of four fisheye cameras mounted around the vehicle to provide N-degree surroundings, which helps eliminate all blind spots during critical and precise maneuvers. Based on the surround view cameras, this paper explores the N-degree road scene understanding. Thanks to the methodology of Convolutional Neural Network (CNN) based semantic segmentation, in recent years, road scene understanding has achieved huge progress using narrow-angle or even wide-angle conventional cameras _cite_ . Conventional cameras follow well the pinhole camera model: all straight lines in the real world are projected as straight lines in the image. However, images captured by fisheye cameras have strong distortions. As distortions bring difficulties in image processing, fisheye images are usually undistorted before use _cite_ . However, image undistortion hurts image quality (especially at image boundaries) _cite_ and leads to information loss. An example of image undistortion is shown in Fig.~ _ref_ . We consider that the segmented results on raw fisheye images can be used as an information source for other tasks. For example, visual odometry system for fisheye cameras _cite_ can use the semantics to improve the performance like visual semantic odometry (VSO) _cite_ . This paper explores CNN based semantic segmentation on raw surround view images, as illustrated in Fig.~ _ref_ . \newpage Two challenging aspects are considered. The first is an effective deep learning model to handle fisheye images. Fisheye images have severe distortion which is unavoidable during the process of projecting an image of a hemispheric field onto a plane _cite_ . The degree of distortion is related to the distance between the camera and the objects, and also to the radial angle. The distortions are not uniform over all spatial areas _cite_ . This brings CNN models the demand for modeling large and unknown transformations. CNNs already have shown a remarkable representing ability to handle distortions with the help of large-scale datasets which contain diverse scenes. The ability largely originates from the large capacity of deep models like VGGNet _cite_, GoogleNet _cite_ and ResNet _cite_ . Besides, handcrafted structures, for example, pyramid pooling module _cite_, also contribute to the representational power. However, regular CNNs have inherently limited ability to model large, unknown geometric distortions _cite_ . The CNNs have fixed structures, such as fixed filter kernels, fixed receptive field sizes, and fixed pooling kernels. There lack internal mechanisms to handle the geometric distortions. Interested readers may refer to _cite_ for details. The second is about training datasets for the deep neural networks. So far, state-of-the-art CNN-based semantic segmentation methods require large-scale pixel-level annotated images for parameter optimization. The annotating process is time-consuming and expensive work, yet several road scene datasets have already been created _cite_ and contribute to the development of semantic segmentation algorithms. However, there are few large-scale annotated datasets of semantic segmentation for surround view cameras. In our previous works _cite_, a fisheye dataset is generated from Cityscapes dataset for a forward-looking conventional camera. However, it is not enough for surround view cameras. First, the image composition of cameras in different directions varies a lot. For example, as shown in Fig.~ _ref_, a forward-looking camera usually captures the rear view of front vehicles, but a sideways-looking camera captures the side view of surround vehicles. Second, the Cityscapes dataset is collected from cities in Europe; the model trained using such dataset may not be suitable for applications in regions outside Europe. This paper is a considerable extension of our previous conference publication _cite_ . We further address the method of road scene semantic segmentation using surround view cameras with a more comprehensive set of improvements and experiments. The main contributions w.r.t. the previous work are as follows. First, a more effective module is proposed to handle images with large distortions. We do not use the OPP module proposed in _cite_ because it does not show improvements with the highly efficient ERFNet. Instead, we explore the deformable convolution _cite_ to handle fisheye images. To address the spatial correspondence problem _cite_, the Restricted Deformable Convolution (RDC) is proposed to further restrict deformable convolution for pixel-wise prediction tasks. Second, zoom augmentation is redefined as the operation of transforming existing conventional images to fisheye images, and a zoom augmentation layer with a CUDA implementation is implemented for online training. Conventional datasets are used to augment the surround view images via the zoom augmentation method. Finally, a multi-task learning architecture is presented to train an end-to-end semantic segmentation model for real-world surround view images by combining a small number of real-world images and a large number of transformed images. We introduce the idea of AdaBN _cite_ to bridge the distributional gap between real-world images and transformed images. In addition, the Hybrid Loss Weightings (HLW) is proposed to improve the generalization ability by introducing auxiliary losses with different loss weightings. This paper is organized as follows: Section II reviews related works. Section III introduces the RDC, whereas Section IV describes the method of converting the existing datasets to fisheye datasets. Section V presents the training strategy. And Section VI demonstrates quantitative experiments.