Recently, tremendous advances in semantic segmentation have been made~ _cite_ ~ _cite_ ~ _cite_ ~ _cite_ . These approaches often rely on deep convolutional neural networks (CNN) ~ _cite_ trained on a large-scale classification dataset~ _cite_, which is then transfered to the segmentation task based on the mask annotations~ _cite_ ~ _cite_ ~ _cite_ . However, the annotation for pixel-wise segmentation masks usually requires considerable human effort. In addition, the construction of a semantic segmentation dataset covering diverse appearances, view-points or scales of objects is also costly and difficult. These limitations hinder the development of semantic segmentation which generally requires large-scale data for training. {While a large collection of fully annotated images are difficult to obtain, weakly-labeled yet related videos are abundant on video sharing websites, e.g., YouTube.com, especially for human segmentation task. Intuitively, when a human instance is moving in the video, the inherent motion cues with the aid of an imperfect human detector may potentially help identify the human masks out of the background~ _cite_ _cite_ _cite_ . Thus in this paper, we target at using the video-context derived human masks from raw YouTube videos to iteratively train and update a good segmentation neural network, instead of using a limited number of single image mask annotations like in traditional approaches.} {The video-context is used to infer the human masks by exploiting spatial and temporal contextual information over video frames.} Note that our framework can be applied to general object segmentation tasks, especially for moving objects. {This paper focuses on human segmentation, as human-centric videos are the most common on YouTube.} {Figure~ _ref_ provides an overview of our unified framework containing two integrated steps, i.e., the video-context guided human mask inference and the CNN-based human segmentation network learning.} In the first step, given a raw video, we extract the supervoxels, which are the spatio-temporal analogs of superpixels, to provide a bottom-up volumetric segmentation that tends to preserve object boundaries and motion continuousness~ _cite_ . The spatio-temporal graph is built on the superpixels within supervoxels. To remove the ambiguity in determining the instances of interest in the video, we resort to an imperfect human detector~ _cite_ and region proposal method~ _cite_ to generate the candidate segmentation masks. These masks are then combined with the confidence maps predicted by the currently trained CNN to provide the unary energy of each node. The graph-based optimization is then performed to find optimal human label assignments of the nodes by maximizing both appearance consistency within the neighboring nodes and the long-term label consistency within each supervoxel. In the second step, the video-context derived human masks extracted from massive raw YouTube videos are then utilized to train and update a deep convolutional neural network (CNN) . {One important issue in training with those raw videos is the existence of noisy labels within these extracted masks. To effectively reduce the influence of noisy data,} we utilize the sample-weighted loss during the network optimization. The trained network in turn makes better segmentation predictions for the key frames in each video, which can help refine the video-context derived human masks. This process iterates to gradually update the video-context derived human masks and the network parameters until the network is mature. We evaluate our method on the PASCAL VOC N segmentation benchmark~ _cite_ . Our very-weakly supervised learning framework by using raw YouTube videos achieves significantly better performance than the previous weakly supervised methods (i.e., using box annotations) ~ _cite_ ~ _cite_ as well as the fully supervised (i.e., using mask annotations) methods~ _cite_ ~ _cite_ ~ _cite_ . By combining with limited annotated data, our weakly supervised variant (i.e., using the box annotations on VOC) and the semi-supervised variant (i.e., using the mask annotations on VOC) yield superior accuracies than the previous methods~ _cite_ ~ _cite_ using extensive extra Nk annotations on Microsoft COCO~ _cite_ . {Note that the general image-level supervision is also utilized in our approach as we pre-train our neural network on ImageNet.}