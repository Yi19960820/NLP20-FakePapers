Recent research has shown that Deep Convolutional Neural Network (CNN) can achieve human-competitive accuracy on various image recognition tasks _cite_ . However, several recent studies have suggested that the mapping learned by CNN from input image data to the output classification results, is not continuous. That is, there are some specific data points (or possibly some continuous regions) in the input space whose classification labels can be changed by adding even very small perturbations. Such modification is called ``adversarial perturbation'' in the case that potential adversaries wish to abuse such a characteristic of CNN to make it misclassify _cite_ . By using various optimization methods, tiny well-tuned additive perturbations which are expected to be imperceptible to human eyes but be able to alter the classification results drastically, can be calculated effectively. In specific, adding the adversarial perturbation can lead the target CNN classifier to either a specific or arbitrary class, both are different from the true class. In this research, we propose and evaluate a black-box method of generating adversarial perturbation based on differential evolution, a natural inspired method which makes no assumptions about the problem being optimized and can effectively search very wide area of solution space. Our proposal has mainly the following contribution and advantages compared to previous works: The rest of the paper is as follows: Section~N introduces previous attack methods and their features, as well as compares with the proposed method. Section~N describes why and how to use DE to generate effective adversarial perturbation under various settings. In Section~N, several measures are proposed for evaluating the effectiveness of DE based attack. Section~N discusses the experimental results and points out possible future extension.