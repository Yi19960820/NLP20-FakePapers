Face has been one of the main targets for image enhancement tasks. In particular, upsampling a low-resolution face image to a high-resolution one has been an important problem, called by its own name of face hallucination~ _cite_ . The problem is formulated as follows. Given a low-resolution _inline_eq_ face image _inline_eq_, our goal is to obtain a photorealistic high-resolution _inline_eq_ face image _inline_eq_ whose down-sampled version is equal to _inline_eq_, where _inline_eq_ is the upsampling factor. The relation can be written as where _inline_eq_ and _inline_eq_ are the low and high-resolution images stacked into column vectors and _inline_eq_ is an _inline_eq_ sparse matrix implementing low-pass filtering and down-sampling. To invert this largely (_inline_eq_-times) under-determined linear system and recover the high-resolution image, additional constraints are needed. We approximate the solution of this linear inverse problem using a deep neural network where facial constraints are explicitly modeled and learned using training data. Our considerations for the proposed face upsampling network are inspired by the face hallucination work of Liu~ \etal~ _cite_ . Similar to~ _cite_, we utilize the following three constraints to regularize the under-determined problem. (N) Global constraint: The reconstructed high-resolution face image should satisfy holistic constraints such as shape, pose, and symmetry, and should include detailed characteristic facial features such as eyes and nose. (N) Local constraint: Statistics of the reconstructed local image regions should match that of high-resolution face image patches (e.g., smooth regions with sharp boundaries), and should include face-specific details. (N) Data constraint: The reconstruction should be consistent with the observed low-resolution image and satisfy Eq.~ . Liu~ \etal~ _cite_ used a two-step approach according to these constraints. First a global face reconstruction is acquired using an eigenface model, which is a linear projection operation. In the second step details of the reconstructed face are enhanced by non-parametric patch transfer from a training set where consistency across neighboring patches is enforced through a Markov random field. This method produces high-quality results when the face images are near frontal, well aligned, and lighting conditions are controlled. However, when these assumptions are violated, the simple linear eigenface model fails to produce satisfactory global reconstruction. In addition, the patch transfer does not scale well with large datasets due to the nearest-neighbor (NN) search. In this paper, we present a deep network architecture that resembles Liu~ \etal's framework~ _cite_ but solves the aforementioned problems for accurate and efficient face hallucination. Our network consists of the two sub-networks: the first one implements holistic face reconstruction according to the global constraints, and the second one enhances face-specific details and enforces local patch statistics. However, they are learned jointly using a large amount of training data, providing an optimized structure for upsampling. Moreover, the feed-forward operation provides computational efficiency in the test time. In extensive experiments using two benchmark datasets captured under controlled and uncontrolled setups, we show that our algorithm outperforms the state-of-the-art algorithms. Our main contributions can be summarized as follows: (N) We present a deep interpretation of the global-local face hallucination framework~ _cite_ . (N) We design a deep network architecture that replaces the original two-step approach with an end-to-end learning and feed-forward operation, improving both the accuracy and speed. (N) We learn the deep network by minimizing a combination of reconstruction error and a learned face quality loss in adversarial setting, which produces high resolution images with improved visual quality. (N) We conduct extensive comparisons with the state-of-the-art algorithms and demonstrate that our algorithm outperforms them both qualitatively and quantitatively. Face hallucination is the single-image super-resolution (SR) problem specific to face images. Single-image SR algorithms developed for generic images share the same formulation in (_ref_) . To invert the under-determined system, local constraints are enforced as priors based on image statistics~ _cite_ and exemplar patches~ _cite_ . Global constraints are typically not available for the generic SR problem, which limits the plausible upsampling factor. Yang~ \etal's recent study~ _cite_ showed that _inline_eq_ upsampling results in the lower bound of the human perceptual scores. Liu~ \etal~ _cite_ used a global constraint for face hallucination based on eigenfaces~ _cite_, and proposed a two-step approach where the initial global reconstruction is improved by local non-parametric patch transfer~ _cite_ . As described above, the simple eigenface model has a difficulty when the datasets include large pose and illumination variations, and their local refinement process is computationally expensive due to the NN patch search. Ma~ \etal~ _cite_ assumed that training and test images are precisely aligned and searched the NN patches of a target pixel in the test image only at the specific pixel location in the training images. Using the location-specific patches provides global constraints implicitly, as long as the images are well-aligned. Yang~ \etal~ _cite_ partitioned a face image into three groups of facial components, contours, and smooth regions based on a facial landmark detection~ _cite_ . They used the NN search for each of the facial components with the training images, while for contours and smooth regions edge-based statistics and NN patch search were used. The result was generated by integrating gradient maps from the three groups and imposing them on the high-resolution image. Their method relies on the facial landmark detection and thus the result degrades for low-resolution input images where the landmark localization is typically inaccurate. In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification~ _cite_ and object detection~ _cite_ to face recognition~ _cite_, segmentation~ _cite_, and video event detection~ _cite_ . These methods have also been replacing highly optimized hand-designed algorithms in low-level vision tasks such as image denoising~ _cite_, image enhancement~ _cite_, and SR~ _cite_ . Dong~ \etal~ _cite_ proposed super-resolution convolutional neural network (SRCNN) for generic SR. They interpreted it as a deep network version of the conventional sparse coding methods~ _cite_ . SRCNN provides the state-of-the-art performance for generic SR, but not for face-specific SR as we compare in experiments. More recently, Wang~ \etal~ _cite_ proposed an improved deep model for generic SR that also takes into account self similarities. Zhou~ \etal~ _cite_ presented bi-channel convolutional neural network (BCCNN) for face-specific SR. They used a convolutional neural network architecture whose output was blended with the bicubic upsampled image using a weighting factor which is also predicted from the network. The last layer of this network linearly combines high-resolution basis images, which corresponds to a global face reconstruction and smooths out the person-specific details. Basic building blocks of our algorithm are well known neural network architectures such as encoder~ _cite_, convolutional~ _cite_, and deconvolutional~ _cite_ neural nets. Our architectural design enables effective learning of global and local constraints that are important for face upsampling task using these well-known building blocks. Recently, generative adversarial networks (GANs) ~ _cite_ have been proposed as an alternative to learn deep generative models. In GAN framework, a generative network learns to generate samples from a given data distribution, while simultaneously a discriminative network learns to identify the samples that are generated from this network. Since then, GANs have been successfully used for image~ _cite_, scene~ _cite_, and sequence synthesis~ _cite_ tasks. In this paper, we use GAN framework to learn a discriminative network which evaluates face quality, while at the same time optimizing the face super-resolution network according to the learned quality measure.