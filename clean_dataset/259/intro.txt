The purpose of this paper is to situate the problem of dataset bias in the context of medical imaging, and to present a framework to handle the problem. The availability of datasets has played a central role in advancing the state of the art for learning algorithms in general, and for imaging research in particular. However, all datasets are finite attempts at encapsulating the practically infinite world to allow the imaging community a playing field for evolving and testing their algorithms. In practice, every dataset inadvertently brings along its idiosyncrasies due to study participant selection, image acquisition variabilities, and annotation biases. This has given rise to the dataset bias problem _cite_ . Perhaps the first telling introduction to the problem was by Torralba and Efros _cite_ . They demonstrated the problem by learning a classifier to name the dataset of a random image from N object recognition datasets. The classifier was trained on randomly sampled N images from each dataset, and surprisingly it performed with a classification accuracy of N \%, which is significantly better than chance (N/N = N \%) . They also went on to show that with more training data, the classification accuracy could be increased with no immediate signs of saturation. This is a serious problem and it essentially boils down to the following. Given images with the same object category, one could tell which dataset they belong to; this means that every dataset leaves a footprint on its images, and during the course of learning a classifier, we are inadvertently learning idiosyncrasies of the dataset along with learning general properties of the object category. Does a similar situation arise in the context of medical imaging studies? We will investigate this shortly. In this paper, we identify a hitherto unnoticed link between handling dataset bias and methods to preserve privacy for fair machine learning. We draw our motivation from one particular method which aims at learning fair representations by taking the input features to a latent space such that datapoints with different values of sensitive information (e.g., race or gender) become as indistinguishable as possible while maintaining discriminablity with respect to a classification task _cite_ . We propose to adapt this method such that we learn a latent space in which the data points are indistinguishable in terms of the dataset they belong to, while satisfying the classification requirements . Two main differences between our work and _cite_ must be noted. First, the sensitive variable in _cite_ is binary and we modify the objective function to handle any number of datasets. Second, the classification part of the objective in _cite_ is also geared for binary classification, and we generalize this part as well to handle more than two classes. One of the earliest approaches to explicitly handle the dataset bias problem for object recognition in visual databases was presented by Khosla et al. _cite_ . In this work, the authors learn a support vector machine (SVM) classifier using images from multiple datasets and decompose the SVM weights into two parts: one part is dataset specific, while the other part is common to all datasets. A notable work on domain adaptation for medical imaging is _cite_ . The work that comes closest in spirit to the proposed method in this paper is domain adversarial training of neural networks _cite_ . However, the methods in _cite_ and _cite_ both use a part of target dataset for training. The method presented in this paper does not use the target dataset at all; our experiments are purely leave-one-dataset-out. Through this paper we hope to introduce the link between dataset bias and fair machine learning, as well as encourage more work along this line to better address the problem of dataset bias.