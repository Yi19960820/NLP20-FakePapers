In this paper, we consider the problem of hashing, which is concerned with learning binary embeddings of data in order to enable fast approximate nearest neighbor retrieval. We take a task-driven approach, and seek to optimize learning objectives that closely match test-time performance measures. Nearest neighbor retrieval performance is frequently measured using ranking-based evaluation metrics, such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG) _cite_, but the optimization of such metrics has been deemed difficult in the hashing literature _cite_ . We propose a novel learning to rank formulation to tackle these difficult optimization problems, and our main contribution is a gradient-based method that directly optimizes ranking metrics for hashing. Coupled with deep neural networks, this method achieves state-of-the-art results. Our formulation is inspired by a simple observation. When performing retrieval with binary vector encodings and the integer-valued Hamming distance, the resulting ranking usually contains, and different tie-breaking strategies can lead to different results (Fig.~ _ref_) . In fact, ties are a common problem in ranking, and much attention has been paid to it, including in Kendall's classical work on rank correlation _cite_, and in the modern information retrieval literature _cite_ . Unfortunately, the learning to hash literature largely lacks tie-awareness, and current evaluation protocols rarely take tie-breaking into account. Thus, we advocate using ranking evaluation metrics, which implicitly average over all permutations of tied items, and permit efficient closed-form evaluation. Our natural next step is to learn hash functions by optimizing tie-aware ranking metrics. This can be seen as an instance of learning to rank with listwise loss functions, which is advantageous compared to many other ranking-inspired hashing formulations. To solve the associated discrete and NP-hard optimization problems, we relax the problems into their continuous counterparts where closed-form gradients are available, and then perform gradient-based optimization with deep neural networks. We specifically study the optimization of AP and NDCG, two ranking metrics that are widely used in evaluating nearest neighbor retrieval performance. Our results establish the new state-of-the-art for these metrics in common image retrieval benchmarks.