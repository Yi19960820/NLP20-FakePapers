Human action recognition has been an active research topic in computer vision due to its wide range of applications, such as smart surveillance and human-computer interactions. In the past decades, research on action recognition mainly focused on recognising actions from conventional RGB videos. In the previous video-based motion action recognition, most researchers aimed to design hand-crafted features and achieved significant progress. However, in the evaluation conducted by Wang et al. _cite_, one interesting finding is that there is no universally best hand-engineered feature for all datasets. Recently, the release of the Microsoft Kinect brings up new opportunities in this field. The Kinect device can provide both depth maps and RGB images in real-time at low cost. Depth maps have several advantages compared to traditional color images. For example, depth maps reflect pure geometry and shape cues, which can often be more discriminative than color and texture in many problems including object segmentation and detection. Moreover, depth maps are insensitive to changes in lighting conditions. Based on depth data, many works _cite_ have been reported with respect to specific feature descriptors to take advantage of the properties of depth maps. However, all of them are based on hand-crafted features, which are shallow high-dimensional descriptions of local or global spatio-temporal information and their performance varies from dataset to dataset. Deep Convolutional Neural Networks (ConvNets) have been demonstrated as an effective class of models for understanding image content, offering state-of-the-art results on image recognition, segmentation, detection and retrieval _cite_ . With the success of ImageNet classification with ConvNets _cite_, many works take advantage of trained ImageNet models and achieve very promising performance on several tasks, from attributes classification _cite_ to image representations _cite_ to semantic segmentation _cite_ . The key enabling factors behind these successes are techniques for scaling up the networks to millions of parameters and massive labelled datasets that can support the learning process. In this work, we propose to apply ConvNets to depth map sequences for action recognition. An architecture of Hierarchical Depth Motion Maps (HDMM) + N Channel Convolutional Neural Network (NConvNets) is proposed. HDMM is a technique that can transform the problem of action recognition to image classification and artificially enlarge the training data. Specifically, to make our algorithms more robust to viewpoint variations, we directly process the ND pointclouds and rotate the depth data into different views. To make full use of the additional body shape and motion information from depth sequences, each rotated depth frame is first projected onto three orthogonal Cartesian planes, and then for each projection view, the absolute differences (motion energy) between consecutive and sub-sampled frames are accumulated through an entire depth video sequence. To weight the importances of different motion energy, a weighted factor is used to make the motion energy more important for the recent poses than the past ones. Three HDMMs are constructed after above steps and three ConvNets are trained on the HDMMs. The final classification scores are combined by late fusion of the three ConvNets. We evaluate our method on the MSRActionND, MSRActionNDExt, UTKinect-Action and MSRDailyActivityND datasets individually and achieve results which are better than or comparable to the state-of-the-art. To further verify the robustness of our method, we combine the last three datasets into a single one and test the proposed method on it. The results show that that our approach could achieve consistent performance without much degradation in performance on the combined dataset. The main contributions of this paper can be summarized as follows. First of all, we propose a new architecture, namely, HDMM + NConvNets for depth-based action recognition, which achieves state-of-the-art results on four datasets. Secondly, our method can handle view variant cases for action recognition to some extent due to the simply and directly processing of ND pointclouds. Lastly, a large dataset is generated by combining the existing ones to evaluate the stability of the proposed method, because the combined dataset contains large variances of within actions, background, viewpoint and number of samples of each action across the three datasets. The remainder of this paper is organized as follows. Section N reviews the related work on deep learning on ND video action recognition and action recognition using depth sequences. Section N describes the proposed architecture. In Section N, various experimental results and analysis are presented. Conclusion and future work are made in Section N.