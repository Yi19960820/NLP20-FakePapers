The interest at lifelogging devices, such as first-person (wearable) cameras, being able to collect daily user information is recently increased. These cameras capable of frequently capturing images that record visual information of our daily life known as ``visual lifelogging'' in order to create a visual diary with activities of first-person life with unprecedented details~ _cite_ . Since, the wearable camera can collect a huge number of images by non-stop image collection capacity (N-N per minute, NK-NK per day and NK-NK per year) . The analysis of these egocentric photo-streams (images) can improve the people lifestyle; by analyzing social pattern characterization ~ _cite_ and social interactions~ _cite_, as well as generating storytelling of first-person days~ _cite_ . In addition, the analysis of these images can greatly affect on human behaviors, habits, and even health~ _cite_ . One of the personal tendencies of people is food events that can badly affected on their health. For instance, some people can eat more if they see and senses (e.g. smell) food that constantly feel them hungry immediately~ _cite_ . Thus, monitoring and determining the duration of food intakes will help to improve people food behaviour. The motivation behind this research is twofold. Firstly, using a wearable camera is to capture images related to food places, where the users are engaged within foods (see Fig.~ _ref_) . Consequently, these images of visual lifelogging can give a unique opportunity to work on food pattern analysis from the first-person viewpoint. Secondly, the analysis of everyday information (entering, leaving and stay time, see in Fig.~ _ref_) of visited food places can enable a novel health care application that can help to analyze the food eating patterns of people and prevent the diseases related to food, like obesity, diabetes and heart diseases. Early work of places or scene recognition in conventional images was mainly motivated by two large scale places or scene datasets, i.e., PlacesN~ _cite_ and SUNN~ _cite_) with millions of labeled images. The semantic classes of these datasets are defined by their labels by representing the entry-level of an environment. The images of datasets were collected form the internet with a large diversity. However, the two datasets failed to record the real involvement of first-person with food environment and the characterization of the first-person activity. In turn, wearable cameras can able to capture the scenes from a more intimate perspective by its ego-vision system. Thus, we built a new in-house private dataset, so-called ``EgoFoodPlaces'', with details involvement information of places that can help to classify the food places or environment to solve the first-person food pattern characterization. With diversity of food places (cafeterias, bars, restaurants, ..etc.) traditional methods of feature extraction (e.g., HOG and SIFT) and classification (e.g., Support Vector Machine (SVM) and Neural Network (NN)) ~ _cite_ are not sufficient to deal with this complex problem of food places recognition. Thus, this paper aims to use deep learning models (e.g., Convolutional Neural Network (CNN)) that will help us to automatically select and extract key features and also to construct new ones for different food places. One of recent architectures of deep networks used for classification and segmentation tasks is Atrous Convolution Networks proposed in~ _cite_ . That networks can encode contextual information by using filters or pooling operations at multiple rates with different sizes of neighbourhoods. Thus, in this paper, we propose to use these networks in our deep model to improve the classification rate with ResNet networks. In addition to detect important structures as well as small details of the input images, we rescale the input images in a multi-scale space (i.e., a pyramid of images with different resolutions) . The main contributions of this work is summarized as follows: The paper is organized as follows. Section N explores the proposed approach. In turn, Section N describes our in-house dataset and demonstrate the experimental results and discussions. Finally, conclusion and future work are shown in section N.