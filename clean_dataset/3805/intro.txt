Recently, convolutional neural networks (CNNs) have achieved a great success in many computer vision and machine learning tasks. This success facilitates the development of industrial applications using CNNs. However, there are two major challenges for practical use of these networks, especially on resource-limited devices: As a consequence, there has been growing interest in model speedup and compression. It is common to sacrifice a little prediction accuracy in exchange for smaller model size and faster running speed. In the literature, a major technique is based on the idea of low rank matrix and tensor approximations. In _cite_, low rank matrix factorization was used on the weight matrix of the final softmax layer. () decomposed the weight matrix as a product of two smaller matrices and one of the matrices was carefully constructed as a dictionary. In _cite_, model approximation is followed by fine-tuning on the training data. () also took the nonlinear activation functions into account when doing approximation. Low rank technique can also be applied on the weight tensors of convolutional layers. () used a shared set of separable (rank-N) filters to approximate the original filters. () exploited the redundancy that exists between different feature channels and filters. () applied CP-decomposition, a type of tensor decomposition, on the weight tensors. In this paper, we explore a framework for approximating the weight matrices and weight tensors in neural networks by sum of Kronecker products. We note that as the bases for low rank factorizations like SVD or CP-decomposition are outer products of vectors, approximation by these bases can only exploit the redundancy . In contrast, as the Kronecker product generalizes the outer product from vectors to matrices of arbitrary shape, we may use the Kronecker product to exploit redundancy between local patches of shape. Figure _ref_ demonstrates a case when approximating by Kronecker product would produce less reconstruction error than outer products with the same number of parameters for image pixel value matrix. Intuitively, similar situation may also exist for weight matrices and tensors in convolutional networks, and in these cases our method may produce approximate models that run faster and have less number of parameters at the same level of accuracy loss. On the other hand, with similar number of parameters, our method can advances previous state-of-the-art. The rest of this paper is organized as follows. In Section N, we introduce the Kronecker layer. We discuss some details about implementing the Kronecker layer in Section N. We extend our technique to convolutional layer in Section N. Section N analyses the result of using Kronecker layers on some benchmark datasets. Section N discusses some related work not yet covered. Finally, Section N concludes the paper and discusses future work.