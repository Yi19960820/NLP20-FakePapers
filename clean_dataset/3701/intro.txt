The sensitive data, such as medical imaging data, genetic sequences, financial records and other personal information, is often managed by independent organizations like hospitals and companies _cite_ . Many deep learning (DL) algorithms prefer to use as much data as possible distributed in different organizations for training, because the performance of these DL algorithms directly depends on the amount of high-quality data not only for rarely occurring patterns but also for the robustness to the outliers _cite_ . In practice, however, directly sharing data between different organizations is of great difficulties due to many reasons including privacy protection, legal risk consideration and conflict of interests. Therefore, it has become an important research topic for both academy and industry to fully employ the data of different organizations for training DL models without centralizing the data, while achieving similar performance compared to centralized training after moving all data together. Recently, there has been a trend to use collaborative solvers to train a global model on geo-distributed, multi-datacenter data without directly sharing data between different data centers _cite_ . Specifically, several participants independently train the DL models for a while, and periodically aggregate their local updates to construct a shared model. Only parameters are exchanged and all the training data is kept in the original places _cite_ . However, there are several challenges for this approach: In this work, we propose a multi-datacenter based collaborative deep learning method (denoted as co-learning), which (N) minimizes the performance gap between the centralized and decentralized modes, (N) minimizes the inter-datacenter communication cost during the co-training procedure over WANs, (N) is applicable to a wide variety of deep network architectures without any change. The co-learning approach proposes two strategies to improve the performance of a shared model in distributed learning, based on the conventional model averaging method. First, we adopt the modified cyclical learning rate _cite_, so as to avoid falling into the local optima during the local training procedure. Second, we enlarge the number of local epochs when the difference between two consecutive shared models decreases to be less than a threshold, so as to increase the diversity between local models and reduce the inter-datacenter communication cost. The synchronization period is extended from milliseconds or seconds to ten of minutes or even hours. Surprisingly, despite the claims from previous studies _cite_, we find that model averaging in the decentralized mode can provide competitive performance compared to the traditional centralized mode. Extensive experiments are conducted on three different tasks: image classification, text classification and audio classification. Using the co-learning method, we have tested various state-of-the-art neural network architectures including VGGNet _cite_, ResNet _cite_, DenseNet _cite_ and Capsule architectures _cite_ . All the experiments reveal that the proposed co-learning approach can provide superior performance in the decentralized mode. In summary, the main contributions include: The remainder of this paper is organized as follows. Section N descries the related work, while Section N presents the details of our co-learning approach. Section N describes the experimental results, the discussion and conclusion are given in Section N.