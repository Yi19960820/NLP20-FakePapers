Sensing (CS) _cite_, _cite_, _cite_ is an effective approach for acquiring sparse signals where both sensing and compression are performed at the same time. Since there are numerous examples of natural and artificial signals that are sparse in the time, spatial or a transform domain, CS has found numerous applications. These include medical imaging, geophysical data analysis, computational biology, remote sensing and communications. In the general CS framework, instead of acquiring _inline_eq_ samples of a signal _inline_eq_, _inline_eq_ random measurements are acquired where _inline_eq_ . This is expressed by the underdetermined system of linear equations: where _inline_eq_ is the known measured vector and _inline_eq_ is a random measurement matrix. To uniquely recover _inline_eq_ given _inline_eq_ and _inline_eq_, _inline_eq_ must be sparse in a given basis _inline_eq_ . This means that where _inline_eq_ is _inline_eq_, i.e., _inline_eq_ has at most _inline_eq_ non-zero elements. The basis _inline_eq_ can be complete; i.e., _inline_eq_, or over-complete; i.e., _inline_eq_ where _inline_eq_ (compressed sensing for over-complete dictionaries is introduced in _cite_) . From and: where _inline_eq_ . Since there is only one measurement vector, the above problem is usually called the Single Measurement Vector (SMV) problem in compressive sensing. In distributed compressive sensing, also known as the Multiple Measurement Vectors (MMV) problem, a set of _inline_eq_ sparse vectors _inline_eq_ is to be jointly recovered from a set of _inline_eq_ measurement vectors _inline_eq_ . Some application areas of MMV include magnetoencephalography, array processing, equalization of sparse communication channels and cognitive radio _cite_ . Suppose that the _inline_eq_ sparse vectors and the _inline_eq_ measurement vectors are arranged as columns of matrices _inline_eq_ and _inline_eq_ respectively. In the MMV problem, _inline_eq_ is to be reconstructed given _inline_eq_: In, _inline_eq_ is assumed to be jointly sparse, i.e., non-zero entries of each vector occur at the same locations as those of other vectors, which means that the sparse vectors have the same support. Assume that _inline_eq_ is jointly sparse. Then, the necessary and sufficient condition to obtain a unique _inline_eq_ given _inline_eq_ is _cite_: where _inline_eq_ is the number of rows in _inline_eq_ with non-zero energy and _inline_eq_ of a given matrix is the smallest possible number of linearly dependent columns of that matrix. _inline_eq_ gives a measure of linear dependency in the system modelled by a given matrix. In the SMV problem, no rank information exists. In the MMV problem, the rank information exists and affects the uniqueness bounds. Generally, solving the MMV problem jointly can lead to better uniqueness guarantees than solving the SMV problem for each vector independently _cite_ . In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: N) greedy methods _cite_ like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, N) relaxed mixed norm minimization methods _cite_, or N) Bayesian methods like _cite_ where a posterior density function for the values of _inline_eq_ is created, assuming a prior belief, e.g., _inline_eq_ is observed and _inline_eq_ should be sparse in basis _inline_eq_ . The selection of one of the above methods depends on the requirements imposed by the specific application. The MMV reconstruction methods stated above do not rely on the use of training data. However, for many applications, a large amount of data similar to the data to be compressed by CS is available. Examples are camera recordings of the same environment, images of the same class (e.g., flowers, buildings, ....), electroencephalogram (EEG) of different parts of the brain, etc. In this paper, we address the following questions in the MMV problem when training data is available: Please note that we want to address the above questions `` without adding any complexity or adaptability '' to the encoder. In other words, our aim is not to design an optimal encoder, i.e., optimal sensing matrix _inline_eq_ or the sparsifying basis _inline_eq_, for the given training data. The encoder would be as simple and general as possible. This is specially important for applications that use sensors having low power consumption due to a limited battery life. However, the decoder in these cases can be much more complex than the encoder. For example, the decoder can be a powerful data processing machine. To address the above questions, we propose the use of a two step greedy reconstruction algorithm. In the first step, at each iteration of the reconstruction algorithm, and for each column of _inline_eq_ represented as _inline_eq_, we first find the conditional probability of each entry of _inline_eq_ being non-zero, given the residuals of all previous sparse vectors (columns) at that iteration. Then we select the most probable entry and add it to the support of _inline_eq_ . The definition of the residual matrix at the _inline_eq_ th iteration is _inline_eq_ where _inline_eq_ is the estimate of the sparse matrix _inline_eq_ at the _inline_eq_ th iteration. Therefore in the first step, we find the locations of the non-zero entries. In the second step we find the values of these non-zero entries. This can be done by solving a least squares problem that finds _inline_eq_ given _inline_eq_ and _inline_eq_ . _inline_eq_ is a matrix that includes only those atoms (columns) of _inline_eq_ that are members of the support of _inline_eq_ . To find the conditional probabilities at each iteration, we propose the use of a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells and a softmax layer on top of it. To find the model parameters, we minimize a cross entropy cost function between the conditional probabilities given by the model and the known probabilities in the training data. The details on how to generate the training data and the training data probabilities are explained in subsequent sections. Please note that this training is done only once. After that, the resulting model is used in the reconstruction algorithm for any test data that has not been observed by the model before. Therefore, the proposed reconstruction algorithm would be almost as fast as the greedy methods. The block diagram of the proposed method is presented in Fig. _ref_ and Fig. _ref_ . We will explain these figures in detail in subsequent sections. To the best of our knowledge, this is the first model-based method in MMV sparse reconstruction that is based on a deep learning bottom up approach. Similar to all deep learning methods, it has the important feature of learning the structure of _inline_eq_ from the raw data automatically. Although it is based on a greedy method that selects subsets that are not necessarily optimal, we experimentally show that by using a properly trained model and only one layer of LSTM, the proposed method significantly outperforms well known MMV baselines (e.g., SOMP) as well as the well known Bayesian methods for the MMV problem (e.g., Multitask Bayesian Compressive Sensing (MT-BCS) _cite_ and Sparse Bayesian Learning for temporally correlated sources (T-SBL) _cite_) . We show this on two real world datasets. We emphasize that the computations carried at the encoder mainly include multiplication by a random matrix. The extra computations are only needed at the decoder. Therefore an important feature of compressive sensing (low power encoding) is preserved. Exploiting data structures besides sparsity for compressive sensing has been extensively studied in the literature _cite_ . In _cite_, it has been theoretically shown that using signal models that exploit these structures will result in a decrease in the number of measurements. In _cite_, a thorough review on CS methods that exploit the structure present in the sparse signal or in the measurements is presented. In _cite_, a Bayesian framework for CS is presented. This framework uses a prior information about the sparsity of _inline_eq_ to provide a posterior density function for the entries of _inline_eq_ (assuming _inline_eq_ is observed) . It then uses a Relevance Vector Machine (RVM) _cite_ to estimate the entries of the sparse vector. This method is called Bayesian Compressive Sensing (BCS) . In _cite_, a Bayesian framework is presented for the MMV problem. It assumes that the _inline_eq_ ``tasks'' in the MMV problem in, are not statistically independent. By imposing a shared prior on the _inline_eq_ tasks, an empirical method is presented to estimate the hyperparameters and extensions of RVM are used for the inference step. This method is known as Multitask Compressive Sensing (MT-BCS) . In _cite_, it is experimentally shown that the MT-BCS outperforms the method that applies Orthogonal Matching Pursuit (OMP) on each task, the Simultaneous Orthogonal Matching Pursuit (SOMP) method which is a straightforward extension of OMP for the MMV problem, and the method that applies BCS for each task. In _cite_, the Sparse Bayesian Learning (SBL) _cite_ is used to solve the MMV problem. It was shown that the global minimum of the proposed method is always the sparsest one. The authors in _cite_, address the MMV problem when the entries in each row of _inline_eq_ are correlated. An algorithm based on SBL is proposed and it is shown that the proposed algorithm outperforms the mixed norm (_inline_eq_) optimization as well as the method proposed in _cite_ . The proposed method is called T-SBL. In _cite_, a greedy algorithm aided by a neural network is proposed to address the SMV problem in . The neural network parameters are calculated by solving a regression problem and are used to select the appropriate column of _inline_eq_ at each iteration of OMP. The main modification to OMP is replacing the correlation step with a neural network. They experimentally show that the proposed method outperforms OMP and _inline_eq_ optimization. This method is called Neural Network OMP (NNOMP) . In _cite_, an extension of _cite_ with a hierarchical Deep Stacking Netowork (DSN) _cite_ is proposed for the MMV problem. `` The joint sparsity of _inline_eq_ is an important assumption in the proposed method ''. To train the DSN model, the Restricted Boltzmann Machine (RBM) _cite_ is used to pre-train DSN and then fine tuning is performed. It has been experimentally shown that this method outperforms SOMP and _inline_eq_ in the MMV problem. The proposed methods are called Nonlinear Weighted SOMP (NWSOMP) for the one layer model and DSN-WSOMP for the multilayer model. In _cite_, a feedforward neural network is used to solve the SMV problem as a regression task. Similar to _cite_ (if we assume that we have only one sparse vector in _cite_), a pre-training phase followed by a fine tuning is used. For pre-training, the authors have used Stacked Denoising Auto-encoder (SDA) _cite_ . Please note that an RBM with Gaussian visible units and binary hidden units (i.e., the one used in _cite_) has the same energy function as an auto-encoder with sigmoid hidden units and real valued observations _cite_ . Therefore the extension of _cite_ to the MMV problem will give similar performance as that of _cite_ . In _cite_, a reconstruction method is proposed for sparse signals whose sparsity patterns change slowly with time. The main idea is to replace Compressive Sensing (CS) on the observation _inline_eq_ with CS on the Least Squares (LS) residuals. LS residuals are calculated using the previous estimation of the support. In _cite_, a reconstruction method is proposed to recover sparse signals with a sparsity pattern that slowly changes over time. The main idea is to use Sparse Bayesian Learning (SBL) framework. Similar to SBL, a set of hyperparameters are defined to control the sparsity of signals. The main difference is that the prior for each coefficient also involves the coefficients of the adjacent temporal observations. In _cite_, a CS algorithm is proposed for time-varying sparse signals based on the least-absolute shrinkage and selection operator (Lasso) . A dynamic Lasso algorithm is proposed for the signals with time-varying amplitudes and support. The rest of the paper is organized as follows: In section _ref_, the basics of Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) cells are briefly explained. The proposed method and the learning algorithm are presented in section _ref_ . Experimental results on two real world datasets are presented in section _ref_ . Conclusions and future work directions are discussed in section _ref_ . Details of the final gradient expressions for the learning section of the proposed method are presented in Appendix _ref_ .