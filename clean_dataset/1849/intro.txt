We are witnessing the impressive development in connecting computer vision and natural language, from the arguably mature visual detection~ _cite_ to the burgeoning visual captioning and question answering~ _cite_ . However, most existing efforts to the latter vision-language tasks attempt to directly bridge the visual model (\eg, CNN) and the language model (\eg, RNN), but fall short in modeling and understanding the relationships between objects. As a result, poor generalization ability was observed as those models are often optimized on specialized datasets for specific tasks such as image captioning or image QA.~ _cite_ . As illustrated in Figure~ _ref_, we take a step forward from the lower-level object detection and a step backward from the higher-level language modeling, focusing on the visual relations between objects in an image. We refer to a visual relation as a triplet, where the predicate can be verb (--), spatial (--), preposition (--), and comparative (--) ~ _cite_ . Visual relations naturally bridge the vision and language by placing objects in a semantic context of what, where, and how objects are connected with each other. For example, if we can detect--and--successfully, the reasoning behind the answer ``gray'' to the question asked in Figure~ _ref_ will be explicitly interpretable using dataset-independent inference, \eg, QA over knowledge bases~ _cite_, and thus permits better generalization or even zero-shot learning~ _cite_ . In this paper, we present a convolutional localization network for visual relation detection dubbed V isual Trans lation E mbedding network (VTransE) . It detects objects and predicts their relations simultaneously from an image in an end-to-end fashion. We highlight two key novelties that make VTransE effective and distinguishable from other visual relation models~ _cite_: Translation Embedding . Since relations are compositions of objects and predicates, their distribution is much more long-tailed than objects. For _inline_eq_ objects and _inline_eq_ predicates, one has to address the fundamental challenge of learning _inline_eq_ relations with few examples~ _cite_ . A common solution is to learn separate models for objects and predicates, reducing the complexity to _inline_eq_ . However, the drastic appearance change of predicates makes the learning even more challenging. For example, appearance largely varies from--to--. To this end, inspired by Translation Embedding (TransE) in representing large-scale knowledge bases~ _cite_, we propose to model visual relations by mapping the features of objects and predicates in a low-dimensional space, where the relation triplet can be interpreted as a vector translation, \eg, _inline_eq_ _inline_eq_ . As shown in Figure~ _ref_, by avoiding learning the diverse appearances of--with large variance, we only need to learn the translation vector in the relation space, even though the subjects and/or objects can be quite diverse. Knowledge Transfer in Relation . Cognitive evidences show that the recognition of objects and their interactions is reciprocal~ _cite_ . For example, and detections serve as the context for prediction, which in turn constrains the articulation of the two objects, and thus benefiting object detection. Inspired by this, we explicitly incorporate knowledge transfer between objects and predicates in VTransE. Specifically, we propose a novel feature extraction layer that extracts three types of object features used in translation embedding: classeme (\ie, class probabilities), locations (\ie, bounding boxes coordinates and scales), and RoI visual features. In particular, we use the bilinear feature interpolation~ _cite_ instead of RoI pooling~ _cite_ for differentiable coordinates. Thus, the knowledge between object and relation---confidence, location, and scale---can be transfered by a single forward/backward pass in an end-to-end fashion. We evaluate the proposed VTransE on two recently released relation datasets: Visual Relationship~ _cite_ with N, N images and N, N unique relations, and Visual Genome~ _cite_ with N, N images and N, N unique relations. We show significant performance improvement over several state-of-the-art visual relation models. In particular, our purely visual VTransE can even outperform the multi-modal method with vision and language priors~ _cite_ in detection and retrieval, and a bit shy of it in zero-shot learning. In summary, our contributions are as follows: N) We propose a visual relation detection model dubbed Visual Translation Embedding network (VTransE), which is a convolutional network that detects objects and relations simultaneously. To the best of our knowledge, this is the first end-to-end relation detection network; N) We propose a novel visual relation learning model for VTransE that incorporates translation embedding and knowledge transfer; N) VTransE outperforms several strong baselines on visual relation detection by a large performance gain.