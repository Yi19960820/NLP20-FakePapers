With the proliferation of off-the-shelf, downloadable networks such as VGG-N, overfeat, R-CNN and several others in the caffe model zoo, it has become common practice in the computer vision community to simply fine-tune one of these networks for any task~ _cite_ . These networks are usually trained on a large dataset such as Imagenet and Pascal~ _cite_ . The proponents of these networks argue that these networks have learnt image representations that are pertinent for most datasets that deal with natural images. Under the assumption that all these datasets are and are derived from a similar distribution this might as well be true. Even with such networks, features that are unique to each datasets do matter. While fine-tuning of an already trained network works to a certain extent, these features are not in a traditional manner on the target dataset but are simply copied. There is also no guarantee that these features are the best representations for the target dataset, although there is some validity in expecting that such a representation might work well, since after all it was learnt from a large enough dataset. Most computer vision scientists do not attempt to train a new architecture from scratch (random initializations) . Training even a mid-sized deep network with a small dataset is a notoriously difficult task. Training a deep network, even those with mid-level depth require a lot of supervision in order to avoid weight explosion. On most imaging datasets, with image sizes being _inline_eq_, the memory insufficiency of a typical GPU restricts the mini-batches to less than _inline_eq_ . Using small mini-batches and small datasets lead to very noisy and untrustworthy gradients. This leads to weight explosions unless the learning rates are made sufficiently smaller. With smaller learning rates, learning is slowed down. With smaller mini-batches learning is unstable. One way to avoid such problems is by using regularization. By regularizing we can penalize the gradients for trying to make the weights go higher and higher. Batch Normalization is another technique that is quite commonly used to keep weight explosion under check~ _cite_ . Even with these regularization techniques, the difficulty of training a deep network from scratch leads most computer vision scientists to use pre-trained networks. There are several reasons why one might favour a smaller or a mid-sized network even though there might be a better solution available using these large pre-trained networks. Large pre-trained networks are computationally intensive and often have a depth in excess of _inline_eq_ layers. The computational requirement of these networks do not make them easily portable. Most of these networks require state-of-the-art GPUs to work even in simple feed forward modes. The impracticality of using pre-trained networks on smaller computational form factors necessitates the need to learn smaller network architectures. The quandary now is that smaller networks architectures cannot produce powerful enough representations. Many methods have been recently proposed to draw additional supervision from large well-trained networks to regularize a new network while learning from scratch~ _cite_ . All of these works were inspired from the Dark Knowledge (DK) approach~ _cite_ . All these techniques use at most one layer of supervision on top of the softmax supervision and try to use this technique to learn more deeper networks better. Figure~ _ref_ shows a conceptualization of this idea. In this paper, we try and make a shallower mentee network learn the same representation as a larger, well-trained mentor network at various depths of its network hierarchy. Mentorship happens by tagging on to the loss of the mentee network, a dissimilarity loss for each layer that we want mentored. To the best of our knowledge, there hasn't been any work that has regularized more than one layer this way. There also hasn't been any work that has trained a mid-sized network from a larger and deeper network from scratch. We study some idiosyncratic properties for some novel configurations of mentee networks. We argue that such mentoring avoids weight explosion. Even while using smaller mini-batches, mentee networks get ample supervision and are capable of stable learning even at high learning rates. We show that mentee networks produce a better generalization performance than an independently learnt baseline network. We also show that mentee networks are better transferable than the independently learnt baselines and are also a good initializer. We also show that mentee networks can learn good representations from very little data and sometimes even without supervision from a dataset in an unsupervised fashion. The rest of the paper is organized as follows: section~ _ref_ discusses related works, section~ _ref_ details the mentored learning, section~ _ref_ discusses designs for experiments, section~ _ref_ produces results and section~ _ref_ provides concluding remarks.