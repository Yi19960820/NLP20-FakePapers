networks usually consist of neurons that have equal learning capabilities because the mathematical models of neurons are identical for all the neurons in a network. Neurons are trained by capturing the relation between their inputs and outputs. Thus, all the neurons in a network have an equal chance of learning input features. Consequently, without further inspections, one cannot tell whether a feature learned by one neuron is more or less important than features learned by other neurons. In this work, we provide network neurons with unequal feature-learning abilities; thus, some neurons learn more important features than others. Neural networks are often trained using backpropagation _cite_ . Errors between network outputs and target outputs are propagated backward to update the weights of nodes in previous layers of the network. The updates are proportional to both the inputs and the so-called sensitivities of the nodes. By assigning different activation functions to nodes in a layer, we allow the nodes to have different sensitivities to the same inputs. We use a set of functions parameterized by a single parameter as activation functions to assign node-wise variant sensitivities. Specifically, the slopes of the activation functions are controllable via this parameter. Thus, nodes with smaller node indices are assigned higher sensitivities. Features learned by a network with a set of node-wise variant activation functions are sorted by importance. When nodes in a trained network are removed individually, from last to first, the network accuracy gradually deteriorates at increasingly larger increments. A network with a set of node-wise variant activation function has the ability to learn not only the features that represent the inputs but also the importance of the learned features. We call deep networks containing nodes with unequal and asymmetric learning abilities ``deep asymmetric networks.'' Deep networks have achieved great successes across a wide range of fields; however, they are usually computationally expensive and memory intensive. By designing an efficient network that requires less computation but provides the same performance, one can deploy the network on a system with small computational power or perform more tasks on a system with high computational power. Many studies have investigated designing more efficient deep networks _cite_ . In many approaches, the importance of nodes in a trained network are evaluated with a certain measure such as _inline_eq_, _inline_eq_ norms, and correlation _cite_ . Then the nodes showing smaller measures are pruned from the network. The performance of pruned networks depends on how effectively nonessential nodes are identified by the measure. We use the ability of asymmetric networks to learn the importance of features to design more efficient deep networks. Because the nodes in a deep asymmetric networks sort features by importance, the nodes can be pruned from least to most important to meet network computational complexity and memory requirements. The ability to learn the importance of features is validated with both a simple network in an auto-associative setting using Gaussian data and the MNIST dataset _cite_ and deep convolutional neural networks (CNN) for object recognition using the CIFAR-N dataset _cite_ and for action recognition using NTU RGB + D action recognition dataset _cite_ . After the asymmetric networks are trained, we can analyze the individual contribution of each node to the reconstruction error and recognition accuracy. The experimental results show that the reconstruction error and the accuracy are increasingly influenced by the nodes with smaller indices than by those with larger indices, which indicates that the features learned by the asymmetric networks are sorted in the order of their importance. We applied the feature-sorting property to prune the deep asymmetric CNN without a loss of recognition accuracy. Using the asymmetric technique, we were also able to prune deep networks transferred from famous complex networks. To do this, we prepared VGG _cite_ and ResNet _cite_ for a facial expression recognition task. Then, we transferred the weights from the famous networks to the asymmetric networks and trained them on the CK + dataset _cite_ . The asymmetric networks pruned by the proposed procedure result in smaller (and thus more efficient) networks but exhibit no loss of accuracy. We also compared our pruning to results reported in studies using the MNIST using CIFAR-N dataset and ResNet using ImageNet dataset. The rest of this paper is organized as follows. Section _ref_ proposes the use of node-wise variant activation functions in asymmetric deep networks. The network architecture and training are introduced in Section _ref_ . In section _ref_, the feature-sorting property of asymmetric networks is analyzed using a simple shallow network. Section _ref_ presents the pruning procedure used to design more efficient deep asymmetric networks. A pruning algorithm is given in Section _ref_, and a review of pruning methods is given in Section _ref_ . Experimental validations are presented in Section _ref_ for shallow, deep, and transferred deep asymmetric networks in Sections _ref_ . Finally, conclusions are given in Section _ref_ .