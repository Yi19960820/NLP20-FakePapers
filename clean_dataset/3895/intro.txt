With the availability of big data and GPU computing, deep learning has become a powerful technique which raises the benchmarks of classification and detection challenges in computer vision . Using huge databases such as ImageNet which has more than a million annotated images, deep convolutional neural networks (CNNs) such as AlexNet, VGGNet, and GoogLeNet were proposed with impressive performance in visual pattern recognition. In medical imaging, however, such large datasets are usually unavailable especially in ND analysis. Given the large variety of possible clinical conditions in an imaging modality, it is very challenging to build a sufficiently large dataset with samples of desired abnormalities, and the datasets are usually highly unbalanced. If such datasets are directly used to train classifiers, low classification performance can be expected. Furthermore, different from the annotation tasks of ImageNet, annotations of medical images require radiologists or trained experts to ensure the clinical relevance. Therefore, medical image annotations can be time consuming and expensive. As a consequence, it is highly beneficial if we can reduce the number of annotated samples without decreasing the performance of the classification task. To address the above difficulties, transfer learning using ImageNet pre-trained CNNs has been a common approach in medical image analysis . In _cite_, the N off-the-shelf features from the first fully-connected layer of a pre-trained OverFeat model were extracted for pulmonary nodule detection from computed tomography (CT) images through a linear support vector machine (SVM) . In _cite_, hand-crafted features such as the histogram of oriented gradients were combined with features from selected convolutional and fully-connected layers of a pre-trained CNN-M model for cardiac semantic level classification from computed tomography angiography (CTA) images using a SVM. In _cite_, four different medical imaging applications on three imaging modalities were used to study the differences between CNNs trained from scratch and CNNs fine-tuned from pre-trained models. Using the AlexNet architecture, this study showed that the fine-tuned CNNs performed better as the number of training samples reduced. In _cite_, by studying different CNN architectures such as AlexNet, GoogLeNet, and VGGNet on thoraco-abdominal lymph node detection and interstitial lung disease classification, it was shown that models trained from scratch or fine-tuned from ImageNet pre-trained CNNs consistently outperform applications using off-the-shelf CNN features. Recently, algorithms for building medical image applications on limited training data without ImageNet pre-trained CNNs have been studied. In _cite_, auxiliary labels generated by an existing automated software tool were used to pre-train a segmentation network. The pre-trained network was fine tuned by error corrective boosting which selectively focuses on classes with erroneous segmentations. In _cite_, by learning a statistical appearance model from few sample images, a large number of synthetic image pairs were generated with the associated ground-truth deformation. These synthetic data were used to fine-tune a pre-trained FlowNet for dense image registration, and it was shown that data-driven, model-based augmentation approach outperforms generic but highly unspecific methods. In _cite_, a semi-supervised learning architecture based on the generative adversarial nets was proposed for cardiac abnormality classification in chest X-rays. This architecture allows both labeled and unlabeled data to contribute to the model thus the required number of labeled data can be largely reduced. There are several limitations of using ImageNet pre-trained CNNs on medical image analysis. First of all, if the features of the fully-connected layers are used, the input images need to be resized to match the training images (e.g. N _inline_eq_ N _inline_eq_ N for VGGNet) . This leads to unnecessary increase in computation for smaller images, or reduced details for larger images. Secondly, the size of the pre-trained model may be unnecessarily large for medical image applications. Using VGGNet as an example, its architecture was proposed to classify N classes of non-medical images. Such a large number of classes is uncommon in medical image analysis and thus such a large model may be unnecessary. Thirdly, to the best of our knowledge, there are no publicly available models equivalent to ImageNet pre-trained CNNs for ND image analysis. Datasets with millions of annotated ND medical images are publicly unavailable, and the computational cost of training or using an equivalent model in ND can be computationally difficult. To overcome the above limitations, here we propose to use a pre-trained segmentation network as the feature source to reduce the number of samples required to train a classification network. By using a segmentation network pre-trained on data similar to those of the classification task, we can achieve high classification performance with very limited training data. The power of this framework can be partly explained by curriculum learning . Similar to the learning of humans and animals, machine learning can benefit from presenting concepts which are meaningfully organized from simple to complex. In our framework, instead of a classification task which involves complex and abstract concepts such as disease categories, we first train the machine to perform a segmentation task which involves simpler concepts such as shapes and structures. This is similar to radiologists who need to learn anatomical and functional concepts before being able to perform diagnosis. In fact, when pixel-level semantic labels are provided by radiologists or automated software tools for training, the segmentation network learns the anatomical knowledge from experts. There are several reasons to use a segmentation network as the feature source. First of all, different ND and ND segmentation networks are available, and all of them show impressive capabilities of segmenting complicated biological or medical structures with relatively few training samples and straightforward training strategy. In _cite_, it is shown that classification networks trained on image-level labels can provide useful localization information. For segmentation networks, there are tens of thousands of pixel-level labels providing the shape, structural, and semantic information per image. Therefore, the amount of useful features in a segmentation network can be large even with relatively few training samples. Secondly, compared with generative models such as the generative adversarial nets, the trainings of segmentation networks are more straightforward and require less data. Furthermore, using features from pre-trained segmentation networks is as simple as using those from pre-trained classification networks. Although training segmentation networks requires pixel-level labels, as our concentration is not on segmentation accuracy, semantic labels generated by existing software tools or even non-semantic labels generated by techniques such as intensity thresholding can provide useful features depending on the complexities of the classification problems. Using our framework on a three-class brain tumor classification problem of ND magnetic resonance (MR) images, we achieved N \% accuracy on N testing samples with N training samples. When applying to a nine-class semantic level classification problem of ND cardiac CTA images, we achieved N \% accuracy on N testing samples with N training samples. The preliminary results of this work were reported in _cite_ . Further improvements and experiments are reported in this paper: In this paper, the network architectures of the proposed and tested frameworks and the corresponding training strategy are presented in Section _ref_ . Section _ref_ presents the experiments on brain tumor classification and Section _ref_ presents the experiments on cardiac semantic level classification. Section _ref_ discusses the findings from the experiments and Section _ref_ provides the conclusion.