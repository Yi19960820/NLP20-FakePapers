, the launch of operational optical broadband (multispectral) satellites has successfully boosted the usage of multispectral data for various tasks such as urban monitoring, management of natural resources, ecosystem, and disasters prediction. There has been a growing interest in large-scale land cover mapping of urban _cite_, agriculture monitoring _cite_ _cite_, and mineral exploration _cite_, since high-quality multispectral (MS) satellite imagery is openly available on a global scale (e.g., Sentinel-N and Landsat-N) . However, MS data fails to discriminate spectrally similar classes due to its broad spectral bandwidth. Hyperspectral imaging can acquire richer spectral information that enables high discrimination ability but its coverage from space is much narrower than the one of MS imaging due to the limitations of imaging devices and satellite techniques. This trade-off naturally motivates us to ponder a question: This is as a typical cross-modal feature learning problem. Researchers have proposed a variety of multi-modal feature learning algorithms by introducing additional information, which can be roughly categorized into two parts: fusion-based joint feature learning (FJFL) _cite_ and alignment-based shared feature learning (ASFL) _cite_ . The main difference between FJFL and ASFL is illustrated in Fig. _ref_ . FJFL aims to learn discriminative features by absorbing the different properties from multi-modal data. FJFL fuses the different sources at the data level to diversify the information and then to further learn the higher-level feature representation. One intuitive way for FJFL is to directly learn a joint data representation at the feature level. At present, this is the mainstream approach for multi-modal data analysis _cite_ . For example, by embedding the height information from LiDAR into MS (HS) data, Ghamisi _cite_ learned multi-fold features from HS and LiDAR correspondences for a multi-modal classification task. Iyer _cite_ provided a graph-based new perspective for feature extraction and segmentation of multi-modal images and achieved a desirable result. The resulting discriminative features are beneficial for improving the performance of some high-level applications, especially classification _cite_ _cite_, object detection _cite_, image/video analysis _cite_, and spectral unmixing _cite_ . Image fusion can be also regarded as a part of FJFL when feature learning is applied subsequently. For instance, hyperspectral and multispectral (HS-MS) data fusion enhances the spectral resolution of MS data by fusing it with the low-spatial-resolution HS data _cite_ . The fused HS-MS product can be then seen as a new input for further discriminative feature learning. Behind the advancement of FJFL, the is the prerequisite. This limitation undoubtedly results in a poor fit for cross-modal data analysis, in particular for cross-modal feature learning _cite_ . In our MS-HS case, the cross-modal learning refers to a problem that given a large-scale MS image and a limited HS area partially overlapping with the MS data (see Fig. _ref_ for example), we learn the low-dimensional embedding representation from the limited amount of MS-HS correspondences and transfer the learned features to the rest of MS data for improving the performance of large-scale land-cover and land-use mapping. During the process, we expect to transfer the discrimination capability learnt from the rich spectral information into MS data through the learned common subspace in order to more effectively identify some challenging classes that are hardly recognized by MS data due to its poor spectral information. Please note that we just start a preliminary investigation of cross-modal learning (MS-HS) in this paper, that is, the MS and HS images share the same land-cover classes. Unlike FJFL, ASFL is more apt for cross-modal feature learning, since ASFL can adaptively shuttle back and forth between the different modalities or domains by means of the learned common subspace. Matasci _cite_ linearly projected the hyperspectral data of the source and target domains into a common feature space where the gap between domains in hyperspectral image classification are expected to be reduced. Kulis _cite_ addressed the issue of visual domain adaption by learning a nonlinear transformation in kernel space, with the application to general object recognition. In _cite_, a probabilistic framework was proposed to align the class distributions of two domains for robust hyperspectral image classification. Manifold alignment (MA) _cite_ is also a powerful tool for modeling this kind of issue. Inspired by MA, Tuia _cite_ aligned multi-view remote sensing images on manifolds by fully allowing for the spectral variabilities between the different angle imageries, yielding a significant improvement of classification performance. It should be noted that these methods mentioned above only consider the differences of a unimodality between the source and target domains at the level of original features, but they fail to investigate the transferability of multi-modality since the different modalities usually hold the different feature dimensions. Although these approaches can build connections between features or instances, a poorly connected relationship between the learned common subspace and label information is still hindering the low-dimensional feature representation from being more discriminative. We propose a cross-modality feature learning framework, called common subspace learning (CoSpace), that learns the shared feature representation (common subspace) from partial HS-MS correspondences. Extensive experiments are conducted on simulated MS and partially overlapped real HS data based on two airborne HS datasets: the University of Houston and Chikusei datasets. MS data is generated from HS data by using the spectral response functions (SRFs) of Sentinel-N. We re-label the training and testing classes on the datasets to meet the problem setting of cross-modal feature learning and further to make them more challenging (see Section III for details) . Our contributions can be specifically unfolded as follows: The remainder of this paper is organized as follows. In section II, we first clarify our motivation and then propose the methodology of the CoSpace model, finally elaborate on the corresponding ADMM-based optimization algorithm. Section III presents the experimental results and analysis on two different HS-MS datasets both qualitatively and quantitatively. Finally, some conclusions are drawn in Section IV.