Convolutional neural networks (CNNs) have gained overwhelming success for visual recognition owing to their representational ability. Recent work has shown that the depth of representation is of crucial importance to the performance of CNNs~ _cite_ . Eldan~ \etal conclude that depth is a determinant factor of the expressiveness of neural networks~ _cite_ . Several studies~ _cite_ have also been conducted to investigate the width of the representation, it however does not seem to be the major concern of recent network architecture designs. The possibility to utilize the representational power of both wide and deep representations under a given model complexity has remained an unexplored problem. Increasing depth by simply stacking more layers leads to optimization difficulties as the information flow gets gradually obscured by each layer during propagation~ _cite_ in deep networks. An intuitive approach to ameliorate this problem is introducing shortcut connections towards farther layers to enable direct access to the guiding signal through propagation. This method has been shown particularly effective by various recently proposed state-of-the-art networks~ _cite_, with its effectiveness further confirmed by visualizing the loss landscape of such networks~ _cite_ . Despite the fact that shortcut connections make it viable to optimize extremely deep networks as they help preserve the information flow, they seem to change the expected behaviors of deep networks. In many ways, these networks exhibit the property of having weak dependencies between consecutive layers while layers generally share strong dependencies in regular deep networks~ _cite_ . It is reported that ResNets~ _cite_, a typical network architecture with heavy use of shortcut connections, behave similarly to ensembles of many shallow networks~ _cite_, suggesting that such networks could be viewed from the aspect of a collection of several mostly independent subsections. This quality of having independent subsections and the aggregation nature of skip connections provide us the insight to link networks with shortcut connections to pseudo-wide networks. The benefit of having wide representations lies in the fact that it allows for a larger feature space by introducing higher feature throughput to the network, however we argue that wide convolutional layers are not the only means to achieve this goal. A wide representation could also be collectively formulated by aggregating many transformations with small kernels. The shortcut connections are a case of aggregated transformations as they aggregate outputs from many layers, thus there is no surprise when we observe certain properties that resemble the behavior of a wide network on a deep and thin network with shortcut connections. We propose a novel aggregation-based convolutional layer (SeqConv) to construct networks with the benefits of both wide and deep representations following the aggregation nature of shortcut connections. We divide a regular wide convolutional layer into several groups and place in-layer shortcut connections between each group. We then aggregate the outputs from all groups in sequence to formulate a collective wide representation. The SeqConv layer not only preserves the width of a regular convolutional layer, but as well introduces a hierarchical multi-path micro-architecture that is capable of representing heterogeneous kernels~ _cite_ . The representation capability of the layer is thus greatly enhanced, it is possible for a single SeqConv layer to produce multi-scale representation~ _cite_ with deep hierarchical features. Our contributions in this paper are threefold: