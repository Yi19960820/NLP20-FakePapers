\noindent Current researches on person Re-Identification (ReID) mainly focus on two lines of tasks depending on still images and video sequences, respectively. Recent years have witnessed the impressive progresses in image based person ReID, \eg, deep visual representations have significantly boosted the ReID performance on image based ReID datasets~ _cite_ . Being able to explore plenty of spatial and temporal cues, video based person ReID has better potentials to address some challenges in image based person ReID. Fig.~ _ref_ shows several sampled frames from person tracklets. As shown in Fig.~ _ref_ (a), solely relying on visual cues is hard to identify those two persons wearing visually similar clothes. However, they can be easily distinguished by gait cues. Meanwhile, video based person ReID could also leverage the latest progresses in image based person ReID. The two persons in Fig.~ _ref_ (b) show similar gait cues, but can be easily distinguished by their spatial and appearance cues. It is easier to infer that, extracting and fusing spatial and temporal cues is important for video based person ReID. Existing studies on video based person ReID have significantly boosted the performance on existing datasets. Those works can be summarized into two categories,, N) extracting frame-level features and generating video feature thorough pooling or weight learning ~ _cite_, and N) extracting frame-level features then applying the Recurrent Neural Networks (RNN) to generate video features~ _cite_ . Both those two categories of methods first treat each video frame independently. The feature genearted by pooling strategy are generally not affected by the order of video frames. The RNN only builds temporal connections on high-level features, hence is not capable to capture the temporal cues on image local details. Therefore, more effective way of acquiring spatial-temporal feature should still be investigated. Recently, ND Convolutional Neural Network (CNN) is introduced to learn the spatial-temporal representation in other video tasks like action recognition~ _cite_ . Through sliding convolution kernels on both spatial and temporal dimensions, ND CNN encodes both the visual appearance and the temporal cues across consecutive frames. Promising performances have been reported in many studies~ _cite_ . Because a single ND convolution kernel can only cover short temporal cues, researcher usually stack several ND kernels together to gain the stronger temporal cue learning ability. Although showing better performance, stacked ND convolutions results in substantial growth of parameters, \eg, the widely used CND~ _cite_ network reaches the model size of NMB with only N ND convolution layers, almost N times to the N parameters of ResNetN ~ _cite_ . Too many parameters not only make ND CNNs computationally expensive, but also leads to the difficulty in model training and optimization. This makes ND CNN not readily applicable on video based person ReID, where the training set is commonly small and person ID annotation is expensive. This work aims to explore the rich temporal cues for person ReID through applying ND convolution, while mitigating the shortcomings in existing ND CNN models. A Multi-scale ND (MND) convolution layer is proposed as a more efficient and compact alternatives to traditional ND CNN layer. MND layer is implemented using several parallel temporal convolution kernels with different temporal ranges. Several MND layers are inserted into a ND CNN architecture. The resulting MND convolution network (MND CNN) introduces marginal parameters to the ND CNN, but gains the multi-scale temporal cues modeling ability. Compared with existing ND CNNs, MND CNN is more compact and easier to train. To further refine the learned temporal cues by MND convolution layer, a Residual Attention Layer (RAL) is proposed to jointly learn spatial and temporal attention masks. With RAL, more important spatial and temporal cues can be kept and the noises can be depressed, enabling MND CNN to extract discriminative temporal feature. We further introduce a ND CNN to learn and extract the spatial and appearance features from video sequences. This ND CNN and the MND CNN compose a two-stream CNN architecture, where the extracted spatial and temporal features are fused for video based person ReID. Extensive experiments demonstrate that our method outperforms a wide range of state-of-art methods on three widely used benchmarks datasets,, ~ _cite_, ~ _cite_ and ~ _cite_ . Moreover, we achieve a reasonable trade-off between ReID accuracy and model size. Introducing only about NMB parameter overhead to the ND CNN, MND CNN boosts the mAP of ND CNN from N to N on . The ND CNN model IND~ _cite_ achieves mAP of N with NMB parameters. Compared with IND, MND performs better and saves about NMB of parameters, thus could be a better temporal feature learning model for video based person ReID. The contribution of this work can be summarized into two aspects. N) we propose a MND convolution layer as a more compact and efficient alternative to ND CNN layer. MND layer makes multi-scale temporal feature learning with a compact neural network possible. To our best knowledge, this is the first attempt of introduce ND convolution in person ReID. N) we further propose the RAL to learn spatial-temporal attention masks, and use a ND CNN to extract complementary spatial and appearance features. Those components further boost the video based person ReID performance.