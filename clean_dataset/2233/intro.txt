Representation learning from sequence data has many applications including action and activity recognition from videos _cite_, gesture recognition _cite_, music classification from audio clips _cite_, and gene regulatory network analysis from gene expressions _cite_ . In this paper we focus on activity and action recognition in videos, which is important for many real life applications including human computer interaction, sports analytic, and elderly monitoring and healthcare. Neural network-based supervised learning of representations from sequence data has many advantages compared to hand-crafted feature engineering. However, capturing the discriminative behaviour of sequence data is a very challenging problem; especially when neural network-based supervised learning is used, which can overfit to irrelevant temporal signals. In video sequence classification, and especially in action recognition, a key challenge is to obtain discriminative video representations that generalize beyond the training data. Moreover, a good video representation should be invariant to the speed of the human actions and should be able to capture long term time evolution information, i.e., ~the temporal dynamics. In action recognition a key challenge is to extract and represent high-level motion patterns, dynamics, and evolution of appearance of videos. One can argue that end-to-end learning of video representations are the key to successful human action recognition. However, it is extremely hard problem due to massive amount of video data that is required to learn such end-to-end video representations. A further challenge is to encode dynamics efficiently and effectively from variable length sequences. This calls for novel spatio-temporal neural network architectures. Recent success in action and activity recognition has been achieved by modelling evolving temporal dynamics in video sequences _cite_ . Some methods use linear ranking machines to capture first order dynamics _cite_ . Other methods encode temporal information using RNN-LSTMs on video sequences _cite_, but at the cost of many more model parameters. To further advance activity recognition it is beneficial to exploit temporal information at multiple levels of granularity in a hierarchical manner and thereby capture more complex dynamics of the input sequences _cite_ . As frame based features improve, \eg, from a convolutional neural network (CNN), it is important to exploit information not only in the spatial domain but also in the temporal domain. Several recent methods have obtained significant improvements in image categorisation and object detection using very deep CNN architectures _cite_ . Motivated by these deep hierarchies _cite_, we argue that learning a temporal encoding at a single level is not sufficient to interpret and understand video sequences, and that a temporal hierarchy is needed. In addition, we argue that end-to-end learning of video representations are necessary for reliable human action recognition. In recent years CNNs have become very popular for automatically learning representations from large collections of static images. Many tasks in computer vision, such as image classification, image segmentation and object detection, have benefited from such automatic representation learning _cite_ . However, it is unclear how one may extend these highly successful CNNs to sequence data; especially, when the intended task requires capturing dynamics of video sequences (e.g., ~action and activity recognition) . Indeed, capturing the discriminative dynamics of a video sequence remains an open problem. Some authors have proposed to use recurrent neural networks (RNNs) _cite_ or extensions, such as long short term memory (LSTM) networks _cite_, to classify video sequences. However, CNN-RNN/LSTM models introduce a large number of additional parameters to capture sequence information. Consequently, these methods need much more training data. For sequence data such as videos, obtaining labelled training data is significantly more costly than obtaining labels for static images. This is reflected in the size of datasets used in action and activity recognition research today. Even though there are datasets that consist of millions of labelled images (e.g., ~ImageNet~ _cite_), the largest fully labelled action recognition dataset, UCFN, consists of barely more than N, N videos _cite_ . Some notable efforts to create large action recognition datasets include the Sports-NM~ _cite_, the YouTube-NM~ _cite_ and the ActivityNet dataset~ _cite_ . The limitation of Sports-NM and YouTube-NM is that they are constructed from weakly labelled human annotations and sometimes annotations are very noisy. Furthermore, ActivityNet only consist of N, N high quality annotated videos, which is insufficient for learning good video representations. Despite recent efforts in building good action recognition datasets~ _cite_, it is highly desirable, therefore, to develop frameworks that can learn discriminative dynamics from video data without the cost of additional training data or model complexity. Perhaps the most straightforward CNN-based method for encoding video sequence data is to apply temporal max pooling or temporal average pooling over the video frames. However, these methods do not capture any valuable time varying information of the video sequences _cite_ . In fact, an arbitrary reshuffling of the frames would produce an identical video representation under these pooling schemes. Rank-pooling _cite_, on the other hand, attempts to encode time varying information by learning a linear ranking machine, one for each video, to produce a chronological ordering of the video's frames based on their appearance (i.e., ~the hand-crafted or CNN features) . The parameters of the ranking machine (i.e., ~fit linear model) are then used as the video representation. However, unlike max and average pooling, it was previously unclear how the CNN parameters can be fine-tuned to give a more discriminative representation when rank-pooling is used since there is no closed-form formula for the rank-pooling operation and the derivative of its input arguments with respect to the rank-pool output not obvious. The original rank pooling method of Fernando \etal _cite_ obtained good activity recognition performance using hand-crafted features. Given a sequence of video frames, the rank pooling method returns a vector of parameters encoding the dynamics of that sequence. The vector of parameters is derived from the solution of a linear ranking SVM optimization problem applied to the entire video sequence, \ie, at a single level. We extend that work in two important directions that facilitates the use of richer CNN-based features to describe the input frames and allows the processing of more complex video sequences. First, we show how to learn discriminative dynamics of video sequences or vector sequences using rank pooling-based temporal pooling. We show how the parameters of the activity classifier, shared parameters of video representations, and the CNN features themselves can all be learned jointly using a principled optimization framework. A key technical challenge, however, is that the optimization problem contains rank pooling as a subproblem---itself a non-trivial optimization problem. This leads to a large-scale bilevel optimization problem _cite_ with convex inner-problem, which we propose to solve by stochastic gradient descent. The result is a higher capacity model than Fernando \etal _cite_, which is tuned to produce features that are discriminative for the task at-hand. Concisely, we learn during learning by propagating back the errors from the final classification layer to learn both video representation and a good classifier. Second, we propose a hierarchical rank-pooling scheme that encodes a video sequence at multiple levels. The original video sequence is divided into multiple overlapping video segments. At the lowest level, we encode each video segment using rank pooling to produce a sequence of descriptors, one for each segment, which captures the dynamics of the small video segments (see Figure~ _ref_) . We then take the resulting sequence, divide that into multiple subsequences, and apply rank pooling to each of these next-level subsequences. By recursively applying rank pooling on the obtained segment descriptors from the previous layer, we capture higher-order, non-linear, and more complex dynamics as we move up the levels of the hierarchy. The final representation of the video is obtained by encoding the top-level dynamic sequence using yet one more rank pooling. This strategy allows us to encode more complicated activities thanks to the higher capacity of the model. In summary, our proposed hierarchical rank pooling model consists of a feed forward network starting with a frame-based CNN and followed by a series of point-wise non-linear operations and rank pooling operations over subsequences as illustrated in Figure~ _ref_ . Our main contributions are then: (N) a novel discriminative dynamics learning framework in which we learn discriminative frame-based CNN features for the task at-hand in an end-to-end manner or joint learning of parameters of video representation using rank pooled discriminative video representation, and the classifier parameters, (N) a novel temporal encoding method called . Our proposed method is useful for encoding dynamically evolving frame-based CNN features, and we are able to show significant improvements over other effective temporal encoding methods. This paper is an extension of our two recent conference papers~ _cite_ . In this journal version we provide a broad overview of the action recognition progress and extend the related work section. Here we unify the learning of discriminative rank pooling and full end-to-end parameter learning using the same bilevel optimization framework. Some additional experiments and analysis are also included. The rest of the paper is organised as follows. Related work is discussed in~ followed by a brief background to rank pooling and some preliminaries in~ . We present our discriminative networks in~ and discuss how the resulting representation can be used to classify videos. In~ we show how all the parameters of the discriminative networks can be learned. Then in~, we present our hierarchical rank pooling method. In~, we provide extensive experiments evaluating various aspects of our proposed methods. We conclude the paper in~ with a summary of our main contributions and discussion of future directions.