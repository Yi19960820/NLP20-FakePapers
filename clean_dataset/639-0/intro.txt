Perceiving the physical properties of a scene undoubtedly plays a fundamental role in understanding real-world imagery. Such inherent properties include the N-D geometric configuration, the illumination or shading, and the reflectance or albedo of each scene surface. Depth prediction and intrinsic image decomposition, which aims to recover shading and albedo, are thus two fundamental yet challenging tasks in computer vision. While they address different aspects of scene understanding, there exist strong consistencies among depth and intrinsic images, such that information about one provides valuable prior knowledge for recovering the other. In the intrinsic image decomposition literature, several works have exploited measured depth information to make the decomposition problem more tractable _cite_ . These techniques have all demonstrated better performance than using RGB images alone. On the other hand, in the literature for single-image depth prediction, illumination-invariant features have been utilized for greater robustness in depth inference _cite_, and shading discontinuities have been used to detect surface boundaries _cite_, suggesting that intrinsic images can be employed to enhance depth prediction performance. Although the two tasks are mutually beneficial, previous research have solved for them only in sequence, by using estimated intrinsic images to constrain depth prediction _cite_, or vice versa _cite_ . We propose in this paper to instead jointly predict depth and intrinsic images in a manner where the two complementary tasks can assist each other. We address this joint prediction problem using convolutional neural networks (CNNs), which have yielded state-of-the-art performance for the individual problems of single-image depth prediction _cite_ and intrinsic image decomposition _cite_, but are hampered by ambiguity issues that arise from limited training sets. In our work, the two tasks are formulated synergistically in a joint conditional random field (CRF) that is solved using a novel CNN architecture, called the joint convolutional neural field (JCNF) model. This architecture differs from previous CNNs in several ways tailored to our particular problem. One is the sharing of convolutional activations and layers between networks for each task, which allows each network to account for inferences made in other networks. Another is to perform learning in the gradient domain, where there exist stronger correlations between depth and intrinsic images than in the image value domain, which helps to deal with the ambiguity problem from limited training sets. A third is the incorporation of a gradient scale network which jointly learns the confidence of the estimated gradients, to more robustly balance them in the solution. These networks of the JCNF model are jointly learned using a unified energy function in a joint CRF. Within this system, depth, shading and albedo are predicted in a coarse-to-fine manner that yields more globally consistent results. Our experiments show that this joint prediction outperforms existing depth prediction methods and intrinsic image decomposition techniques on various benchmarks.