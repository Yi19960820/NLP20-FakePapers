Visual saliency attempts to determine the amount of attention steered towards various regions in an image by the human visual and cognitive systems~ _cite_ . It is thus a fundamental problem in psychology, neural science, and computer vision. Computer vision researchers focus on developing computational models for either simulating the human visual attention process or predicting visual saliency results. Visual saliency has been incorporated in a variety of computer vision and image processing tasks to improve their performance. Such tasks include image cropping~ _cite_, retargeting~ _cite_, and summarization~ _cite_ . Recently, visual saliency has also been increasingly used by visual recognition tasks~ _cite_, such as image classification~ _cite_ and person re-identification~ _cite_ . Human visual and cognitive systems involved in the visual attention process are composed of layers of interconnected neurons. For example, the human visual system has layers of simple and complex cells whose activations are determined by the magnitude of input signals falling into their receptive fields. Since deep artificial neural networks were originally inspired by biological neural networks, it is thus a natural choice to build a computational model of visual saliency using deep artificial neural networks. Specifically, recently popular convolutional neural networks (CNN) are particularly well suited for this task because convolutional layers in a CNN resemble simple and complex cells in the human visual system _cite_ while fully connected layers in a CNN resemble higher-level inference and decision making in the human cognitive system. In this paper, we develop a new computational model for visual saliency using multiscale deep features computed by convolutional neural networks. Deep neural networks, such as CNNs, have recently achieved many successes in visual recognition tasks~ _cite_ . Such deep networks are capable of extracting feature hierarchies from raw pixels automatically. Further, features extracted using such networks are highly versatile and often more effective than traditional handcrafted features. Inspired by this, we perform feature extraction using a CNN originally trained over the ImageNet dataset~ _cite_ . Since ImageNet contains images of a large number of object categories, our features contain rich semantic information, which is useful for visual saliency because humans pay varying degrees of attention to objects from different semantic categories. For example, viewers of an image likely pay more attention to objects like cars than the sky or grass. In the rest of this paper, we call such features {\em CNN features} . By definition, saliency is resulted from visual contrast as it intuitively characterizes certain parts of an image that appear to stand out relative to their neighboring regions or the rest of the image. Thus, to compute the saliency of an image region, our model should be able to evaluate the contrast between the considered region and its surrounding area as well as the rest of the image. Therefore, we extract multiscale CNN features for every image region from three nested and increasingly larger rectangular windows, which respectively encloses the considered region, its immediate neighboring regions, and the entire image. On top of the multiscale CNN features, our method further trains fully connected neural network layers. Concatenated multiscale CNN features are fed into these layers trained using a collection of labeled saliency maps. Thus, these fully connected layers play the role of a regressor that is capable of inferring the saliency score of every image region from the multiscale CNN features extracted from nested windows surrounding the image region. It is well known that deep neural networks with at least one fully connected layers can be trained to achieve a very high level of regression accuracy. We have extensively evaluated our CNN-based visual saliency model over existing datasets, and meanwhile noticed a lack of large and challenging datasets for training and testing saliency models. At present, the only large dataset that can be used for training a deep neural network based model was derived from the MSRA-B dataset~ _cite_ . This dataset has become less challenging over the years because images there typically include a single salient object located away from the image boundary. To facilitate research and evaluation of advanced saliency models, we have created a large dataset where an image likely contains multiple salient objects, which have a more general spatial distribution in the image. Our proposed saliency model has significantly outperformed all existing saliency models over this new dataset as well as all existing datasets. In summary, this paper has the following contributions: Visual saliency computation can be categorized into bottom-up and top-down methods or a hybrid of the two. Bottom-up models are primarily based on a center-surround scheme, computing a master saliency map by a linear or non-linear combination of low-level visual attributes such as color, intensity, texture and orientation~ _cite_ . Top-down methods generally require the incorporation of high-level knowledge, such as objectness and face detector in the computation process~ _cite_ . Recently, much effort has been made to design discriminative features and saliency priors. Most methods essentially follow the region contrast framework, aiming to design features that better characterize the distinctiveness of an image region with respect to its surrounding area. In _cite_, three novel features are integrated with a conditional random field. A model based on low-rank matrix recovery is presented in _cite_ to integrate low-level visual features with higher-level priors. Saliency priors, such as the center prior~ _cite_ and the boundary prior~ _cite_, are widely used to heuristically combine low-level cues and improve saliency estimation. These saliency priors are either directly combined with other saliency cues as weights~ _cite_ or used as features in learning based algorithms~ _cite_ . While these empirical priors can improve saliency results for many images, they can fail when a salient object is off-center or significantly overlaps with the image boundary. Note that object location cues and boundary-based background modeling are not neglected in our framework, but have been implicitly incorporated into our model through multiscale CNN feature extraction and neural network training. Convolutional neural networks have recently achieved many successes in visual recognition tasks, including image classification~ _cite_, object detection~ _cite_, and scene parsing~ _cite_ . Donahue et al. _cite_ pointed out that features extracted from Krizhevsky's CNN trained on the ImageNet dataset~ _cite_ can be repurposed to generic tasks. Razavian et al. _cite_ extended their results and concluded that deep learning with CNNs can be a strong candidate for any visual recognition task. Nevertheless, CNN features have not yet been explored in visual saliency research primarily because saliency cannot be solved using the same framework considered in _cite_ . It is the contrast against the surrounding area rather than the content inside an image region that should be learned for saliency prediction. This paper proposes a simple but very effective neural network architecture to make deep CNN features applicable to saliency modeling and salient object detection.