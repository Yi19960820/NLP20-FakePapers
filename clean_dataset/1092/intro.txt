Learning to see by moving is central to humans, starting very early during their perceptual and cognitive development _cite_ . Human observers are able to continuously improve the ability to understand their dynamic visual environment and accurately identify object categories, scene structures, actions or interactions. Supervision is necessary during early cognitive development but is practically impossible (and apparently unnecessary) at frame level. In contrast, current computer vision systems are not yet good at processing large amounts of complex video data, or at leveraging partial supervision within temporally coherent visual streams, in order to build increasingly more accurate semantic models of the environment. Successful semantic video segmentation models would be widely useful for indexing digital content, robotics, navigation or manufacturing. The problem of semantic image segmentation has received increasing attention lately, with some of the most successful methods being based on deep, fully trainable convolutional neural network (CNN) architectures. Data for training and refining single frame, static models is now quite diverse _cite_ . In contrast, fully trainable approaches to semantic video segmentation would face the difficulty of obtaining detailed annotations for the individual video frames, although datasets are emerging for the (unsupervised) video segmentation problem _cite_ . Therefore, for now some of the existing, pioneering approaches to semantic video segmentation _cite_ rely on single frame models with corresponding variables connected in time using random fields with higher-order potentials, and mostly pre-specified parameters. Fully trainable approaches to video are rare. The computational complexity of video processing further complicated matters. One possible practical approach to designing semantic video segmentation models in the long run can be to only label frames, sparsely, in video, as it was done for static datasets _cite_ . Then one should be able to leverage temporal dependencies in order to propagate information, then aggregate in order to decrease uncertainty during learning and inference. This would require a model that can integrate spatio-temporal warping across video frames. Approaches based on CNNs seem right--one option could be to construct a fully trainable convolutional video network. Designing it naively could require e.g. matching edges in every possible motion direction as features. This would require a number of filters that is the number of filters for a single image CNN multiplied by possible motion directions. This holds for two frames. Extending it to even more frames would further amplify the issue. The problem here is how to choose a network architecture and how to connect different pixels temporally. Instead of building a spatio-temporal CNN directly, we will rely on existing single-frame CNNs but augment them with spatial transformer structures that implement warping along optical flow fields. These will be combined with adaptive recurrent units in order to learn to optimally fuse estimates from single (unlabeled) frames with temporal information from nearby ones, properly grated based on their uncertainty. The proposed model is differentiable and end-to-end trainable.