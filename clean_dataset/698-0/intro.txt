Human beings are able to describe what they see, and image captioning is a task that can make a computer have this ability. To achieve this goal requires at least three models: (N) vision model---to extract visual features from images, (N) language model---to generate captions, (N) the connection between vision and language models. Image captioning combines two fields---computer vision and natural language processing (NLP) Recently, increasing research has been devoted to image captioning, and a variety of methods have been proposed _cite_ . Almost all of the current proposed methods are under the framework of CNN _inline_eq_ RNN, in which a CNN is used for the vision model, and an RNN is employed to generate sentences. Moreover, there are different ways to connect the CNN and RNN. A naive way is directly feeding the output of the CNN into the RNN _cite_ . However, this naive approach treats objects in an image the same and ignores the salient objects when generating one word. To imitate the visual attention mechanism of humans, attention modules have been introduced into the CNN _inline_eq_ RNN framework _cite_ . Although the CNN _inline_eq_ RNN framework is popular and provides satisfying results, there are disadvantages of sequential RNNs: (N) RNNs have to be calculated step-by-step, which is not amenable to parallel computing ; (N) there is a long path between the start and end of the sentence using RNNs (), . Tree structures can make a shorter path between the start and end words (), but trees require special processing . Also trees are defined by NLP (e.g., noun phrase, adjectives, verb phrase, etc), which is a hand-crafted structure that may not be optimal for the captioning task. An alternative language model to RNNs and trees are CNNs applied to the sentence to merge words layer by layer _cite_, thus learning a tree structure of the sentence () . can be implemented in parallel, and have a larger receptive field size () using less layers. E.g., given a sentence composed of N words, a RNN needs to iterate N times to get a representation of the sentence (equivalent to N layers), while a CNN with kernel size N only needs N layers. Using wider kernels, CNNs are able to tackle longer sentences with less layers. CNN have been widely applied to the field of NLP, leading to improved performances in many tasks, such as language modeling, machine translation and text classification _cite_ . Inspired by the applications of CNNs in the field of NLP, we develop a framework that only employ CNNs for image captioning. The main contributions of this paper are: