Semantic segmentation plays an important role in understanding visual content of an image as it assigns pre-defined object or scene labels to each pixel and thus translates the image into a segmentation map. When dealing with video content, a human can easily segment an object in the whole video without knowing its semantic meaning, which inspired a research topic named semi-supervised video segmentation. In a typical scenario of semi-supervised video segmentation, one is given the first frame of a video along with an annotated object mask, and the task is to accurately locate the object in all following frames~ _cite_ . The ability of performing accurate pixel-level video segmentation with minimum supervision (e.g., one annotated frame) can foster a large amount of applications, such as accurate object tracking for video understanding, interactive video editing, augmented reality, and video-based advertisement. When the supervision is limited to only one annotated frame, researchers refer to this scenario as one-shot learning . In the recent years, we have witnessed a rising amount of interests in developing one-shot learning techniques for video segmentation~ _cite_ . Most of these work share a similar two-stage paradigm: first, train a general-purpose Fully Convolutional Network (FCN) ~ _cite_ to segment the foreground object; Second, fine-tune this network based on the first frame of the video for several hundred forward-backward iterations to adapt the model to the specific video sequence. Despite the high accuracies achieved by these approaches, the fine-tuning process is arguably time consuming, which makes it prohibited for real-time applications. Some of these approaches~ _cite_ ~ _cite_ also utilize optical flow information, which is computationally heavy for state-of-the-art algorithms _cite_ ~ _cite_ . In order to alleviate the computational cost of semi-supervised segmentation, we propose a novel approach to adapt the generic segmentation network to the appearance of a specific object instance in one single feed-forward pass. We propose to employ another meta neural network called to to adjust the intermediate layers of the generic segmentation network given an arbitrary target object instance. Fig.~ _ref_ shows an illustration of our approach. By extracting information from the image of the annotated object and the spatial prior of the object, the modulator produces a list of parameters, which are injected into the segmentation model for layer-wise feature manipulation. Without one-shot fine-tuning, our model is able to change the behavior of the segmentation network with minimum extracted information from the target object. We name this process . Our proposed model is efficient, requiring only one forward pass from the modulator to produce all parameters needed for the segmentation model to adapt to the specific object instance. Network modulation guided by the spatial prior facilitates the model to track the object even with the presence of multiple similar instances. The whole pipeline is differentiable and can be learned end-to-end using the standard stochastic gradient descent. The experiments show that our approach outperforms previous approaches without one-shot fine-tuning by a large margin, and achieves comparable performance with these approaches after one-shot fine-tuning with a N _inline_eq_ speed up.