Recent interest in augmented reality wearables, home-automation devices, and self-driving vehicles has created a strong need for semantic-segmentation (or visual scene-understanding) algorithms that can operate in real-time on low-power mobile devices. These algorithms label each and every pixel in the image with one of the object classes. In recent years, the availability of larger datasets and computationally-powerful machines have helped deep convolutional neural networks (CNNs) _cite_ surpass the performance of many conventional computer vision algorithms _cite_ . Even though CNNs are increasingly successful at classification and categorization tasks, they provide coarse spatial results when applied to pixel-wise labeling of images. Therefore, they are often cascaded with other algorithms to refine the results, such as color based segmentation _cite_ or conditional random fields _cite_, to name a few. In order to both spatially classify and finely segment images, several neural network architectures have been proposed, such as SegNet _cite_ or fully convolutional networks _cite_ . All these works are based on a VGGN _cite_ architecture, which is a very large model designed for multi-class classification. These references propose networks with huge numbers of parameters, and long inference times. In these conditions, they become unusable for many mobile or battery-powered applications, which require processing images at rates higher than N fps. In this paper, we propose a new neural network architecture optimized for fast inference and high accuracy. Examples of images segmented using ENet are shown in Figure _ref_ . In our work, we chose not to use any post-processing steps, which can of course be combined with our method, but would worsen the performance of an end-to-end CNN approach. In Section _ref_ we propose a fast and compact encoder-decoder architecture named ENet. It has been designed according to rules and ideas that have appeared in the literature recently, all of which we discuss in Section _ref_ . Proposed network has been evaluated on Cityscapes _cite_ and CamVid _cite_ for driving scenario, whereas SUN dataset _cite_ has been used for testing our network in an indoor situation. We benchmark it on NVIDIA Jetson TXN Embedded Systems Module as well as on an NVIDIA Titan X GPU. The results can be found in Section _ref_ .