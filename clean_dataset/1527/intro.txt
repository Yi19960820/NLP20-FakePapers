In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification~ _cite_ . With advances in training, there is a growing trend towards understanding the inner working of these deep networks. By training on a very large image data set, DNNs develop a representation of images that makes object information increasingly explicit at various levels of the hierarchical architecture. Significant visualization techniques have been developed to understand the deep image representations on trained networks~ _cite_ . Inversion techniques have been developed to create synthetic images with feature representations similar to the representations of an original image in one or several layers of the network. Feature representations are a function _inline_eq_ of the source image _inline_eq_ An approximate inverse _inline_eq_ is used to construct a new image _inline_eq_ from the code _inline_eq_ by reducing some statistical discrepancy between _inline_eq_ and _inline_eq_ . Mahendran et al. _cite_ use the pretrained CNN AlexNet~ _cite_ and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image. Gatys et al.~ _cite_ define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG~ _cite_ . Gatys et al.~ _cite_ then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups~ _cite_ . Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features~ _cite_ or the class prediction scores~ _cite_ . Nguyen et al. further try multifaceted visualization to separate and visualize different features that a neuron learns~ _cite_ . Feature inversion and neural activation maximization both start from a white noise image and calculate the gradient via backpropagation to morph the white noise image and output a natural image. In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including _inline_eq_ norm~ _cite_, total variation _cite_, jitter _cite_, Gaussian blur~ _cite_, data-driven patch priors~ _cite_, etc. The method of visualizing the feature representation on the intermediate layers sheds light on the information represented at each layer of the pretrained CNN. A third set of researchers trains a separate feed-forward CNN with upconvolutional layers using representations or correlations of the feature maps produced in the original network as the input and the source image as the target to learn the inversion of the original network. The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction _cite_, texture synthesis _cite_ or even style transfer _cite_ . Instead of designing a natural prior, some researchers incorporate adversarial training _cite_ to improve the realism of the generated images _cite_ . Their trained upconvolutional network could give similar qualitative results as the inversion technique does and is two or three orders of magnitude faster, as the previous inversion technique needs a forward and backward pass through the pretrained network. This technique is slightly different from the previous two in that it does not focus on understanding the original CNN but on the visualization task. It is well recognized that deep visualization techniques conduct a direct analysis of the visual information contained in image representations, and help us understand the representation encoded at the intermediate layers of the well trained DNNs. In this paper, we raise a fundamental issue that other researchers rarely address: Could we do deep visualization using untrained, random weight DNNs? This would allow us to separate the contribution of training from the contribution of the network structure. It might even give us a method to evaluate deep network architectures without spending days and significant computing resources in training networks so that we could compare them. Though Gray et al. demonstrated that the VGG architecture with random weights failed in generating textures and resulted in white noise images in an experiment indicating the trained filters might be crucial for texture generation~ _cite_, we conjecture the success of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep network hierarchical structure rather than from the training, and that the architecture itself may cause the inversion invariant to the original image. Gatys et al.'s unsuccessful attempt on the texture synthesis using the VGG architecture with random weights may be due to their inappropriate scale of the weighting factors. To verify our hypothesis, we try three popular inversion tasks for visualization using the CNN architecture with random weights. Our results strongly suggest that this is true. Applying inversion techniques on the VGG with random weights, we reconstruct high perceptual quality images. The results are qualitatively better than the reconstructed images produced on the pretrained VGG with the same architecture. Then, we try to synthesize natural textures using the random weight VGG. With automatic normalization to scale the squared correlation loss for different activation layers, we succeed in generating similar textures as the prior work of Gatys et al.~ _cite_ on well-trained VGG. Furthermore, we continue the experiments on style transfer, combining the content of an image and the style of an artwork, and create artistic imagery using random weight CNN. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the perception and representation of deep network architecture, and shed light on new understandings on deep visualization. Our work will inspire more possibilities of using the generative power of CNNs with random weights, which do not need long training time on multi-GPUs. Furthermore, it is very hard to prove why trained deep neural networks work so well. Based on the networks with random weights, we might be able to prove some properties of the deep networks. Our work using random weights shows a possible way to start developing a theory of deep learning since with well-trained weights, theorems might be impossible.