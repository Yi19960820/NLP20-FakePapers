Deep neural network-based methods have recently achieved dramatic accuracy improvements in many areas of computer vision, including image classification _cite_, object detection _cite_, face recognition _cite_, and text recognition _cite_ . These high-performing methods rely on deep networks containing millions or even billions of parameters. For example, the work by Krizhevsky _cite_ achieved breakthrough results on the N ImageNet challenge using a network containing N million parameters with five convolutional layers and three fully-connected layers. The top face verification results on the Labeled Faces in the Wild (LFW) dataset were obtained with networks containing hundreds of millions of parameters, using a mix of convolutional, locally-connected, and fully-connected layers _cite_ . In architectures that rely only on fully-connected layers, the number of parameters can grow to billions _cite_ . As larger neural networks are considered, with more layers and also more nodes in each layer, reducing their storage and computational costs becomes critical to meeting the requirements of practical applications. Current efforts towards this goal focus mostly on the optimization of convolutional layers _cite_, which consume the bulk of computational processing in modern convolutional architectures. We instead explore the redundancy of parameters in fully-connected layers, which are often the bottleneck in terms of memory consumption. Our solution relies on a simple and efficient approach based on to significantly reduce the storage and computational costs of fully-connected neural network layers, while mantaining competitive error rates. Our work brings the following advantages: A basic computation in a fully-connected neural network layer is where _inline_eq_, and _inline_eq_ is a element-wise nonlinear activation function. The above operation connects a layer with _inline_eq_ nodes, and a layer with _inline_eq_ nodes. In convolutional neural networks, the fully connected layers are often used before the final softmax output layer, in order to capture global properties of the image. The computational complexity and space complexity of this linear projection are _inline_eq_ . In practice, _inline_eq_ is usually comparable or even larger than _inline_eq_ . This leads to computation and space complexity at least _inline_eq_, creating a bottleneck for many neural network architectures. In this work, we propose to impose a on the projection matrix _inline_eq_ in (_ref_) . This special structure dramatically reduces the number of parameters. It also allows us to use the Fast Fourier Transform (FFT) to speed up the computation. Considering a neural network layer with _inline_eq_ input nodes, and _inline_eq_ output nodes, the proposed method reduces the space complexity from _inline_eq_ to _inline_eq_, and the time complexity from _inline_eq_ to _inline_eq_ . Table _ref_ compares the time and space complexity of the proposed approach with the conventional method. Surprisingly, although the circulant matrix is highly structured with a very small number of parameters (_inline_eq_), it captures the global information well, and does not impact the final performance much. We show empirically that our method can provide significant reduction of storage and computational costs while achieving very competitive error rates. Our work is organized as follows. We propose imposing the circulant structure on the linear projection matrix of fully-connected layers of neural networks to speed up computations and reduce storage costs in Section _ref_ . We show a method which can efficiently optimize the neural network while keeping the circulant structure in Section _ref_ . We demonstrate with experiments on visual data that the proposed method can speed up the computation and reduce memory needs while maintaining competitive error rates in Section _ref_ . We begin by reviewing related work in the following section.