The performance of object detection has been significantly improved recently with the emergence of deep neural networks. Novel neural network structures, such as GoogLeNet _cite_, VGG _cite_ and ResNet _cite_, were proposed to improve the learning capability on large-scale computer vision datasets for various computer vision tasks, such as object detection _cite_, semantic segmentation _cite_, tracking _cite_, scene understanding _cite_, person search _cite_, \etc. State-of-the-art object detection frameworks for static images are based on these networks and consist of three main stages _cite_ . Bounding box proposals are first generated from the input image based on how likely each location contains an object of interest. The appearance features are then extracted from each box proposal to classify them as one of the object classes. Such bounding boxes and their associated class scores are refined by post-processing techniques (\eg, Non-Maximal Suppression) to obtain the final detection results. Multiple frameworks, such as Fast R-CNN _cite_ and Faster R-CNN _cite_, followed this research direction and eventually formulated the object detection problem as training end-to-end deep neural networks. Although great success has been achieved in detecting objects on static images, video object detection remains a challenging problem. Several factors contribute to the difficulty of this problem, which include the drastic appearance and scale changes of the same object over time, object-to-object occlusions, motion blur, and the mismatch between the static-image data and video data. The new task of detecting objects in videos (VID) introduced by the ImageNet challenge in N provides a large-scale video dataset, which requires labeling every object of N classes in each frame of the videos. Driven by this new dataset, multiple systems _cite_ were proposed to extend static-image object detectors for videos. Similar to the bounding box proposals in the static object detection, the counterpart in videos are called tubelets, which are essentially sequences of bounding boxes proposals. State-of-the-art algorithms for video object detection utilize the tubelets to some extend to incorporate temporal information for obtaining detection results. However, the tubelet generation is usually based on the frame-by-frame detection results, which is extremely time consuming. For instance, the tracking algorithm used by _cite_ needs _inline_eq_ second to process each detection box in each frame, which prevents the systems to generate enough tubelet proposals for classification in an allowable amount of time, since the video usually contains hundreds of frames with hundreds of detection boxes on each frame. Motion-based methods, such as optical-flow-guided propagation~ _cite_, can generate dense tubelets efficiently, but the lengths are usually limited to only several frames (\eg, _inline_eq_ frames in _cite_) because of their inconsistent performance for long-term tracking. The ideal tubelets for video object detection should be long enough to incorporate temporal information while diverse enough to ensure high recall rates (Figure~ _ref_) . To mitigate the problems, we propose a framework for object detection in videos. It consists of a Tubelet Proposal Network (TPN) that simultaneously obtains hundreds of diverse tubelets starting from static proposals, and a Long Short-Term Memory (LSTM) sub-network for estimating object confidences based on temporal information from the tubelets. Our TPN can efficiently generate tubelet proposals via feature map pooling. Given a static box proposal at a starting frame, we pool features from the same box locations across multiple frames to train an efficient multi-frame regression neural network as the TPN. It is able to learn complex motion patterns of the foreground objects to generate robust tubelet proposals. Hundreds of proposals in a video can be tracked simultaneously. Such tubelet proposals are shown to be of better quality than the ones obtained on each frame independently, which demonstrates the importance of temporal information in videos. The visual features extracted from the tubelet boxes are automatically aligned into feature sequences and are suitable for learning temporal features with the following LSTM network, which is able to capture long-term temporal dependency for accurate proposal classification. The contribution of this paper is that we propose a new deep learning framework that combines tubelet proposal generation and temporal classification with visual-temporal features. An efficient tubelet proposal generation algorithm is developed to generate tubelet proposals that capture spatiotemporal locations of objects in videos. A temporal LSTM model is adopted for classifying tubelet proposals with both visual features and temporal features. Such high-level temporal features are generally ignored by existing detection systems but are crucial for object detection in videos.