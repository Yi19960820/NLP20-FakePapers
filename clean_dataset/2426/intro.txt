Binary image segmentation aims to separate an image into an object of interest (foreground) and the other parts (background) . It has a wide range of applications, ~ e.g., medical image analysis, image editing, object retrieval, ~ etc . However, since the object of interest varies highly in different contexts, most fully automatic methods are tailored and optimized to seek the particular object of interest in a certain application. It is difficult to develop a fully automatic method which is guaranteed to work in general applications. To improve the flexibility and generality of image segmentation methods, many algorithms adopt interactive frameworks. These algorithms allow users to interact with a system to specify the object of interest by labeling some foreground/background pixels. Most traditional algorithms of interactive image segmentation~ _cite_ rely on low-level features to estimate the foreground/background distributions from user-labeled pixels, to predict the category of unlabeled pixels. A problem relating to these methods is that low level features may not be effective to distinguish between foreground and background in many situations, ~ e.g., the foreground and the background have similar color and texture; or the foreground includes several parts with very different appearance. Consequently, low-level feature-based algorithms may require a large number of user interactions to obtain reliable segmentation, increasing the burden on the end user. Recently, deep features produced by deep neural networks (DNNs) have shown their power in many computer vision tasks including image classification~ _cite_ and semantic segmentation~ _cite_ . Thus, several researchers~ _cite_ have used DNNs to extract deep features with higher-level understanding for image and user interactions to improve interactive image segmentation. Most of these DNN-based methods can be viewed as an early fusion of features using DNN. They concatenate features from image and user interaction as the input to DNN; generally, a DNN is used to combine the concatenated features to predict the foreground and background. However, such early-fusion schemes may not fully exploit the information in user interactions to predict foreground/background. Specifically, considering that state-of-the-art DNNs usually consist of a large number of layers, an early-fusion of user interactions with image features may weaken the influence of user interactions on the final prediction results. In contrast to existing networks performing early fusion, we argue that better performance can be achieved with a late-fusion structure that uses two individual deep streams to learn and extract deep features from the image and the user interactions individually, then fusing the features from the two streams. Our intuition is that such a late fusion structure allows the user interactions to have a more direct impact on the prediction result, as it has a smaller number of layers between pure user interaction features and the prediction results. We expect this will lead to improved performance, as user interactions are more direct information on the location of foreground/background than the image itself. At the same time, deep features are still produced from the two individual deep streams, so the whole network still preserves the representative advantage of deep features. This allows the network to accurately understand image content and predict the object of interest. In this paper, we propose a novel fully convolutional two-stream fusion network (FCTSFN) for interactive image segmentation. As shown in Fig.~ _ref_, the proposed network starts with two-stream late fusion network (TSLFN) . The TSLFN extracts deep features from the image and the user interaction individually using two separate streams, and it applies a fusion net to fuse the features from the two streams to predict foreground and background. Since this two-stream late fusion structure reduces the number of layers between pure user interaction features and the network output, we expect it is able to improve the impact of user interactions on the prediction results to achieve better segmentation performance. Furthermore, to handle the loss of resolution in the TSLFN, we use a multi-scale refining network (MSRN) to refine the result of TSLFN at full resolution. The MSRN fuses the features from different layers of the TSLFN with different scales. It is expected that the fusion result includes the local to global structural information on the object specified by user interactions, and hence the MSRN can utilize the fusion result to refine the ouput of TSLFN at full resolution. The contributions of this paper are as follows: The rest of the paper is organized as follows. In section~ _ref_, we review related works. In section~ _ref_, we detail the proposed FCTSFN for interactive image segmentation. In section~ _ref_, we report the experimental results of the analysis and of the comparisons. Section~ _ref_ concludes the paper.