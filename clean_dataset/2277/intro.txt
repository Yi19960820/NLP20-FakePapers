Nowadays person re-identification (re-id) is emerging as a key problem in intelligent surveillance system, which deals with maintaining identities of individuals at physically different locations through non-overlapping camera views. Cross-view person re-id enables automated discovery and analysis of person specific long-term structural activities over wide areas, and is fundamental to many surveillance applications such as multi-camera people tracking and forensic search. More recently, deep learning methods gradually gain the popularity in person re-id, which are developed to incorporate two aspects of feature extraction and metric learning into an integrated framework _cite_ . The basic idea is to feed-forward a pair of input images into two CNNs with shared weights to extract features, and a subsequent metric learning part compares the features to measure the similarity. This process is carried out essentially by a classification on cross-image representation whereby images are coupled to extract their features, after which a parameterized classifier based on some distance measure (\eg Euclidean distance) performs an ordinary binary classification task to predict whether the two pedestrian images are from the same person. The cross-image representation is effective in capturing the relationship across pairs of images, and several approaches have been suggested to address horizontal displacement by local patch matching. For instance, the FPNN _cite_ algorithm introduced a patch matching layer for the CNN part at early layers. An improved deep learning architecture is proposed in _cite_ with cross-input neighborhood differences and patch summary features.These two methods are both dedicated to improve the CNN architecture with a purpose to evaluate the pair similarity early in the CNN stage, so that it could make use of spatial correspondence of feature maps. Adding on, in _cite_, a matching gate is embedded into CNN to extract more locally similar patterns in horizontal correspondence across viewpoints. As for the metric learning part, with the aim to reduce the distance of matched images while enlarging the distance of mismatched images, common choices are pairwise and/or triplet comparison constraints. For example, _cite_ use the logistic loss to directly form a binary classification problem of whether the input image pair belongs to the same identity. In some other works, _cite_ adopts the contrastive loss based on pairwise comparison. _cite_ uses Euclidean distance and triplet loss while _cite_ optimizes the combination loss function based on pairwise and triplet constraints. However, these deep learning methods are inherently limited due to two presumable assumptions: the availability of large numbered labeled samples across views and the two fixed camera views are supposed to exhibit a unimodal inter-camera transform. In practice, building a training dataset with tuples of labeled corresponding images is impossible for every pair of camera views in the context of a large camera network in video surveillance. Thus, this correspondence dependency greatly limits the applicability of the existing approaches with training samples in correspondence. Secondly, the practical configurations (which are the combinations of view points, poses, lightings, and photometric settings) of pedestrian images are multi-modal and view-specific _cite_ even if they are observed under the same camera. Therefore, the complex yet multi-modal inter-camera variations cannot be well learned with a generic metric which is incapable of handling multiple types of transforms across views. Last but not the least, existing deep learning methodologies directly compute the difference between intermediate CNN features and propagate only distance/similarity value to a ultimate scalar. This would lose important information since they did not consider feature alignment in cross-view. To overcome these limitations, we propose the crossing net based on a couple of generative adversarial networks (GANs) _cite_ to seek effective cross-view representations for person re-id. To combat the first issue of relying on supervision, as shown in Fig. _ref_, we observe some patterns that appear commonly across image pairs are distinct to discriminate positive pairs from negatives. Thus, these co-occurrence patterns should be mined out automatically to facilitate the task of re-id. Specifically, as shown in Fig. _ref_, the proposed network starts from a tuple of variational auto-encoder (VAE) _cite_, each for one image from a camera view, to encode the input images into their respective latent variables without any region-level annotations on person images. The technique of VAE has been established a viable solution for image distribution learning tasks while in this paper, we employ VAE to statistically generate latent variables for paired images without correspondence labeling. We remark that we don't use the Siamese Convolutional Neural Networks (CNNs) _cite_ to encode the input pair because CNNs are composed of fixed receptive fields which may not flexible to capture the varied local patterns. Also, the Siamese architecture enforces the weight sharing across CNN layers which are not suited for multi-modal view-specific variations. To address the view disparity, we propose a cross-view alignment which is bridged over VAE outputs to allow the comparable matching. This alignment operation is to derive a shared latent space by modeling the statistical relationships between generative variables, and we empirically demonstrate this explicit alignment is crucial for cross-view representation learning (see Section _ref_) . Then, the crossing net is coupled with adversarial networks to produce joint view-invariant distribution which gives a probability function to each joint occurrence of cross-view person images. The major contributions of this paper can be summarized as follows: