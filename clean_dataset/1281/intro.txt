A neural network (NN) _cite_ _cite_ is a computational model inspired by the way we believe our brain operates: the data that comes from our sensors, e.g., eyes, is processed by multiple simple computational units called neurons. The neurons are interconnected through a complex network of connections (axons), and after several transformations, the input is translated into a conclusion such as ``there is a chair in the picture.'' Similarly, artificial NNs use vast amounts of simple computational elements that are organized in interconnected layers. Modern NNs usually have multiple layers (sometimes N _cite_ or more) and thus are called deep neural networks (DNNs) . These networks are widely used in image processing, medicine, autonomous driving, translation and other fields. In order to better interpret local features of multidimensional inputs such as images, convolutional neural networks (CNNs) are commonly used. This type of NNs has been shown to be efficient in image-related problems such as classification or scene parsing. To achieve these results, CNNs need many parameters (over NM parameters reported in _cite_) and require huge amounts of computational resources and memory. As a result, expensive and power hungry computers are needed to efficiently process these networks, which has led researchers to seek ways to reduce the computational, memory, and bandwidth requirements _cite_ _cite_ _cite_ _cite_ _cite_ . Using binarized neural networks (BNNs) _cite_ _cite_ _cite_ is one proposed solution to the problem. In BNNs, each parameter is represented by only one bit, which saves memory, communication time and energy, and enables the use of bitwise operations, which are simpler and faster than multiplications. For this reason, FPGAs seem to be the most appropriate architecture for BNN execution. Programming FPGAs, however, is non-trivial, especially in comparison to modern scripting languages that are being used for NN development. In order to simplify development, major FPGA manufacturers have invested heavily in high-level synthesis tools that can translate a program written in a high level language such as OpenSPL _cite_ and C-to-VHDL (presented as part of Vivado HLS _cite_), or frameworks such as OpenCL _cite_ _cite_ . Today, HLS-based tools provide a decent trade-off between resource utilization, compared to custom-written HDL code, and development time. In this paper, we focus on architectural and optimization techniques for implementing QNNs on FPGAs using high level programming languages. The main objective of this work is to investigate architectural features of reduced-precision NNs without focusing on low-level optimizations, and accordingly we used an HLS-based platform to model our architecture. We propose a streaming model based on functional decomposition of the computations, which are embedded in data flow engines (DFEs) based on FPGAs. For this purpose, we used the OpenSPL programming environment and the Maxeler's hardware platform since the latter allowed us to implement the desired processor model using high level languages. The paper indicates that QNNs scale well both on input and network sizes, showing only a minor increase in resource usage on larger inputs. In addition, our system can easily be divided into a couple of FPGAs, almost without a performance drop. All this allows us to run a full-sized ResNet-N and AlexNet on two and three FPGAs, respectively, achieving runtime comparable with the latest GPUs, consuming less power and energy. Moreover, in contrast to previous work, we implemented multiple-bit activations, which improves accuracy of the network by up to _inline_eq_ _cite_ _cite_ . We also analyze skip connections and their impact on resource utilization and runtime, concluding that streaming architecture allows us to add skip connections for a relatively small price. The paper is organized as follows: Section _ref_ explains the platform on which we built our network. Section _ref_ describes our model architecture and optimizations. Section _ref_ presents our experimental evaluation, Section _ref_ presents our conclusions.