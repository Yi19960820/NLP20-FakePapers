Action classification in still images has been a popular research topic in computer vision. Traditional fully-supervised learning methods for action classification rely on large amount of fully-labelled data (each image is labelled with one or more action categories) to learn action classifiers. However, labelling image data with action categories requires tremendous manual work, which is time-consuming and hard to scale-up. Another drawback of traditional supervised learning framework is that the action categories are pre-defined and limited, while humans may describe the same action with different phrases, for example, take out the chopping board and fetch out the wooden board. This drawback leads to the difficulty of vocabulary expansion, as CNN _cite_ _cite_ models or SVM classifiers just assign a label to the test image. Hence, CNN or SVM models would fail to classify the categories that are not in the training set. We observe that large amounts of images with sentence descriptions are readily available on the Internet, such as videos with captions and social media, such as Flickr and Instagram. Such sentence descriptions can be regarded as weak labels of the images. Sentence descriptions are generated by humans and contain rich information about actions, which could be used to learn an expanding vocabulary of actions. Some example are shown in Fig. N. Another observation we make is that action concepts are naturally represented as a hierarchy; for example, ``play guitar" and ``play violin" are subcategories of ``play instrument". If such hierarchical structure of action categories is available, classification methods can choose to use detailed knowledge if necessary or generalized knowledge when details are unavailable or irrelevant. In this paper, we propose a method to tackle the problem of: Given a set of image-description data (assuming descriptions containing human action information), learning to recognize human actions. Our method supports hierarchical clustering of action concepts and vocabulary expansion for action classification. Specifically, our method learns an Action Concept Tree (ACT) and an Action Semantic Alignment (ASA) model for classification via a two-stage learning process. ASA model contains a CNN to extract image-level features, an LSTM to extract text embeddings and a multi-layer neural network to align these two modalities. In the first stage, (a) we design a Hierarchical Action Concept Discovery (H-ACD) method to automatically discover action concepts from image-description data and cluster them into a hierarchical structure (ACT) ; (b) ASA is initialized by the image-description mapping task in stage-N. In the second stage, the target action categories are matched to the nodes in ACT and the associated image data are used to fine-tune ASA for this action classification task to improve the performance. Note that no image data from test domain are used for training. To facilitate research on this task, we constructed a dataset based on Visual Genome _cite_, called Visual Genome Action (VGA) . Although Visual Genome contains well-annotated region descriptions, we do not use the region information and treat the descriptions as image-level. There are N image-description pairs in the training set and N images of N categories in test set. More details of this dataset are given in Section N later. In summary, our main contributions are: (N) A Hierarchical Action Concept Discovery (H-ACD) algorithm to automatically discover an Action Concept Tree (ACT) from image-description data and gather samples for each action node in ACT. (N) An end-to-end CNN-LSTM Action Semantic Alignment (ASA) network which aligns semantic and visual representation to classify actions with expanding vocabulary. (N) A dataset for the problem of, which is built on Visual Genome, containing N image-description pairs for training and N action categories for testing. The paper is organized as follows. Section N discusses the related works. In section N, we will introduce our two-stage framework to learn actions from image-description data. We evaluate our model in section N and give our conclusions in section N.