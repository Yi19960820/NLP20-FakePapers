Style transfer is a process of migrating a style from a ``style image'' to a ``content image''. The goal is to be able to generate different renditions of the same scene according to different style images. Image style transfer has become a popular problem in computer vision and graphics, and can generate impressive results covering a wide variety of styles for both images~ _cite_ and videos~ _cite_ . It has also been widely employed to solve problems such as texture synthesis _cite_, inpainting _cite_, head portraits _cite_ and super-resolution _cite_ . When existing neural style transfer methods are applied to images with complex structures, visual elements from the style image are often transferred to semantically irrelevant areas of the content image. In order to achieve good results, users must pay attention to the composition and/or the selection of the style image, because for example the background colours or textures will often ruin the style transfer results, especially for portraits where the artifacts can be particularly off-putting. Addressing this problem, _cite_ (and subsequently~ _cite_) recently proposed a method which uses a manually generated semantic map to help control the style transfer, and can achieve better results than some common methods. In this paper, we specifically consider the problem of image style transfer guided by extracted semantic masks. To achieve this, we adapt various semantic segmentation and labelling techniques to extract soft masks associated with specific semantics. By deploying the semantic masks to control the transfer, it is possible to avoid errors such as those shown in figure~ _ref_ (c) generated using the CNNMRF method~ _cite_ in which stylised foreground objects are contaminated by the background texture, and vice versa. The main contributions of the paper are as follows: We adapt a state-of-the-art semantic segmentation method~ _cite_ to generate semantic masks automatically. Instead of using hard segmentation as~ _cite_, we propose to use soft masks containing the probabilities of occurrence of different objects in the image, since they preserve more information and is more robust when image regions have similar chances of belonging to multiple object categories. They are used to capture elements of the styles for objects in the style image and to preserve the structure of the content image. For the human face in particular we use a more detailed segmentation, in which different facial parts such as the nose, eyes and mouth are also automatically segmented, providing fine-grained control in perceptually crucial areas; these are also treated as semantic masks. We augment a trained deep convolutional neural network by concatenating _inline_eq_ soft mask channels and _inline_eq_ channels of regular filters. This is further combined with a generative Markov random field (MRF) model~ _cite_ for image style transfer. Both the style and content images and their semantic maps are input into the augmented deep convolutional neural network. Extensive experiments show that such higher-level semantic information improves the quality of style transfer.