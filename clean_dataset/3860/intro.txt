Gathering huge datasets and organizing competitions _cite_ _cite_ has driven forward machine learning model development. In fact, some of the key breakthroughs were achieved as part of these competitions _cite_ _cite_ . However, large scale video predicting remains to date a non-trivial problem. To tackle this problem Google Cloud has hosted a Kaggle competition with a dataset consisting of N million and N thousand labeled and unlabeled videos, respectively. The labels vocabulary consisted of N labels. Participants were asked to develop a machine learning model that would predict what labels were assigned to each of the videos from the test dataset. The organizers provided two datasets; one on video level features and another on frame level features. The frame level consisted of N RGB features and N audio features for each frame of the video sampled every second up to N frames. Those RGB features were extracted from the last ReLu activation of the hidden layer, just before the classification layer (layer named poolN/ \_reshape) of the Inception-vN network. After extraction, these RGB features were dimension reduced by PCA and whitened. The video level features are simply an average of the PCA and whitened frame level features. For more details please refer the original YouTube-NM dataset paper _cite_ . The dataset is available on the project's website, https: //research.google.com/youtubeNm/. In addition to supplying the video data, the dataset authors also provided a starter code base for the competition. The starter code is well structured and provided the whole pipeline from data input, model training to prediction output. It included several base models and proved to be a robust baseline for further creative model work. The Global Average Precision was picked as evaluation metric. It is defined as defined as _eq Where _inline_eq_ is the number of final predictions (if there are _inline_eq_ predictions for each video, then _inline_eq_), _inline_eq_ is the precision score, and _inline_eq_ is the recall score. In the following three sections we describe key elements of our work. First we describe data augmentation, followed by descriptions of used models and their assembly. Finally, we describe model training techniques that we tested and used. Our final solution includes both frame level and video level models. Hence, the further description with cover both cases, but not everything is applicable for all models.