Pedestrian trajectory prediction is still an open problem, especially for real-world deployments of autonomous mobile robots. Most of the existing work focuses on the modeling of social interactions between people or human groups in large indoor public areas _cite_ . However, these approaches typically do not consider the spatial and temporal context of human activities, which can help trajectory prediction, especially over longer durations (e.g.~more than N seconds) . For instance, in a large hospital, pedestrians are likely to queue up in a reception area during specific periods in the daytime or at an emergency desk at night-time, and are likely to walk through corridors and stand near coffee machines. In order to capture such contextual cues, long-term sensory data from actual robotic deployments can be used to learn predictive models. Conventional approaches for pedestrian trajectory prediction are based on the learning of ND trajectories from manually annotated data~ _cite_ . Data-driven methods such as Social-LSTM _cite_ achieve the state-of-the-art performance in ND trajectory prediction. The emerging ND LiDAR devices are able to provide long-range and wide-angle laser scans, and are very accurate and not affected by lighting conditions. With the continuing reduction in hardware prices, ND LiDAR is becoming a popular choice for pedestrian detection and tracking for mobile robot applications. Most of the existing works on human trajectory prediction have the following limitations. Firstly, the existing datasets were recorded with monocular cameras, which are generally used to predict pedestrian positions in image frames rather than in real-world coordinates. Though in these datasets~ _cite_ the x-y coordinates are converted to the camera's coordinate system (in meters) using the intrinsic camera parameters, the depth is missing. Secondly, the existing work does not consider the spatial and temporal context of the human activities in the environment. In this research, we use range-finder sensors for human trajectory observation and prediction. Our model is trained using two datasets collected by mobile service robots (one using ND laser and RGB-D sensors, and the other ND laser/LiDAR), including a N-month autonomous deployment in a care home~ _cite_ . The contributions of this paper are as follows. We propose a novel approach for learning to predict NDOF pedestrian trajectories (including both ND position and orientation) from the real-world robot data, using a new LSTM-based architecture to incorporate spatial and temporal context information from the deployment environment. We also publish our data, comprising a novel ND pedestrian trajectory dataset based on ND LIDAR data. The dataset and demo are available online at: https: //lcas.lincoln.ac.uk/wp/Ndof-pedestrian-trajectory-dataset/. The remainder of this paper is organized as follows: Section~ _ref_ gives an overview of the related literature; Section~ _ref_ describes our approach based on the LSTM model; Section~ _ref_ presents the experimental results on two real-world datasets; and the paper is concluded with contributions and suggestions for future research.