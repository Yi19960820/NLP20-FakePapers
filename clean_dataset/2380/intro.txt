The ability to automatically monitor and infer human behaviors in naturalistic environments is essential for a wide range of applications in areas such as context-aware personal assistance, healthcare, and energy management. Recently, wearable egocentric cameras such as the GoPro and Narrative have become ubiquitous, enabling a new form of capturing human experience. The first-person perspective photos taken by these cameras can provide rich and objective evidence of a person's everyday activities. As a result, this data collection approach has been extensively used in a variety of research domains, particularly in healthcare. Health-related applications that have leveraged first-person photos include, but are not limited to, retrospective memory support _cite_, dietary assessment _cite_, autism support _cite_, travel and sedentary behavior assessment _cite_, and recognition of physical activities _cite_ . Besides the ever important issue of privacy, first-person photo capture with wearable cameras has one additional and serious challenge. Once photographs have been taken, in many cases, it is necessary to review them to identify moments and activities of interest, and possibly to remove privacy-sensitive images. This is particularly challenging when wearable cameras are programmed to take snapshots periodically, for example: every N or N seconds. At this rate, thousands of images are captured every week, making it imperative to automate and personalize the process of image analysis and categorization. We describe a computational method leveraging state-of-the-art methodologies in machine learning to automatically learn a person's behavioral routines and predict daily activities from first-person photos and contextual metadata such as day of the week and time. Example of daily activities include cooking, eating, watching TV, working, spending time with family, and driving (see Table~ _ref_ for a full list) . The ability to objectively track such daily activities and lifestyle behaviors is extremely valuable since behavioral choices have strong links to chronic diseases _cite_ . To test and evaluate our method, we compiled a dataset of N, N images representing everyday human activities. The dataset has N categories of activities and were collected by one individual over a period of six months ``in the wild". Given the egocentric image and the contextual date-time information, our method achieves an overall accuracy of N \% at determining which one of these N activities the user is performing at any moment. Our classification method uses a combination of a Convolutional Neural Network (CNN) and a Random Decision Forest (RDF), using what we refer to as a CNN late-fusion ensemble. It is designed to work on single images captured over a regular interval as opposed to video clips. Capturing hours of egocentric video footage would require tethered power and large storage bandwidth, which still remains impractical. An example of our input egocentric image and the output class prediction probabilities is shown in Figure _ref_ . In brief, our contributions are: