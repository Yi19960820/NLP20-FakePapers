Deep convolutional neural networks (ConvNets) _cite_ have recently become increasingly important for computer vision applications. However, standard deep ConvNets suffer from high computational cost due to their high numbers of parameters and mult-add operations. MobileNets _cite_ uses depthwise separable convolutions, which factorize a standard convolution into a depthwise convolution and a pointwise (_inline_eq_) convolution, to effectively reduce the numbers of both parameters and mult-add operations. Xception _cite_ leverages depthwise separable convolutions to improve its classification performance. However, as reported in _cite_ and _cite_, depthwise convolutions have a low computation vs. memory access rate, which means memory access takes more execution time than computation and it is more difficult to implement depthwise convolutions as efficient as computation-intensive layers like standard convolutions. This makes training depthwise convolution layers with GPUs very slow in current deep learning frameworks such as Caffe _cite_, PyTorch _cite_, MXNet _cite_ and TensorFlow _cite_, mainly because their implementations of depthwise convolutions cannot fully utilize the GPU capacity. Caffe, PyTorch and MXNet implement depthwise convolutions by performing the standard convolution channel-by-channel . This method simply launches a CUDA kernel or cuDNN function for each of the input channels, and applies no inter-channel optimizations such as filter combination. Consequently, the number of threads launched for each standard convolution is small and the utilization of GPU cores is very low. For example, although depthwise convolutions have only about N \% of the mult-adds and N \% of the parameters when training a MobileNet _cite_, they spend over N \% of the overall training time on Caffe which is much higher than any other layer types, as shown in our evaluation (Table _ref_) . Different from the channel-by-channel method, TensorFlow adopts the specialized kernel method which implements depthwise convolutions by designing a specialized CUDA kernel and computing all the input channels in the single kernel. This method is more efficient in training depthwise convolution layers because it exploits the inter-channel parallelism. However, the specialized kernel method prevents TensorFlow from leveraging the cuDNN library _cite_ with the algorithm-level and microarchitecture-level optimizations, which are vital for high-performance GPU computations. This paper presents diagonalwise refactorization, an efficient method for accelerating the training of depthwise convolution layers. First, the weight vectors (filters) of the input channels are rearranged into a diagonal matrix to construct one single large filter. Then, the depthwise convolution is computed as a standard convolution with the large filter, which supports to leverage the cuDNN library to accelerate the computation. We further adopt a grouping mechanism for convolutions with large numbers of input channels, where the channels are divided into several groups and the diagonalwise refactorization is performed for each group. By combining all filters into a large one, our method could exploit the inter-channel parallelism to utilize the GPU capability more efficiently. By supporting the cuDNN library, our method could directly enjoy its algorithm-level and microarchitecture-level optimizations. We have implemented our method on five popular frameworks including Darknet _cite_, Caffe, PyTorch, MXNet, and TensorFlow. Evaluation results show that our method gains _inline_eq_ speedup on Darknet, _inline_eq_ on Caffe, _inline_eq_ on PyTorch, _inline_eq_ on MXNet, and _inline_eq_ on TensorFlow, when training a standard MobileNet, compared to their original implementations of depthwise convolutions. We conduct extensive experiments on different MobileNet hyper-parameters including shallow models, width multiplier and resolution multiplier, and perform detailed analysis on the layer-by-layer training time. Code has been made publicly available at _url_ . The contribution of this paper is summarized as follows.