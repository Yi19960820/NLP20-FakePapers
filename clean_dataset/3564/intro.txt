In recent years, universities, high-tech companies, and the automotive industry have invested vast resources into developing the technology necessary to enable fully autonomous driving. There are several reasons behind this collective effort; to name a few, it is argued that with autonomous vehicles there will be fewer accidents, less pollution, and a more efficient use of the infrastructure. Currently, two main paradigms exist to address the problem~ _cite_: mediated perception and behavior reflex . Approaches utilizing mediated perception divide autonomous driving into subtasks, such as lane marking detection, vehicle detection, free road estimation, and traffic sign recognition. The results of each subtask are then combined to produce a world model that is afterwards analyzed to decide what driving action should be carried out. This paradigm is currently the leading one in both industry and academia thanks to its important strengths of being modular and interpretable. Modularity makes it possible to substitute a subcomponent (e.g., a lane marking detector) for a better one whenever it becomes available, without having to redesign the entire system. Interpretability means that the output of each subcomponent can be easily understood and debugged: For example, one could look at the lane marking detections and immediately notice if the system is misunderstanding road barriers or shadows for lane markings. The behavior reflex paradigm was first successfully demonstrated in N by Pomerlau~ _cite_ who trained a simple three-layer fully connected artificial neural network, ALVINN, to predict the vehicle heading direction from camera and laser range finder data. More recently, Bojarski et al.~ _cite_ have used modern deep learning~ _cite_ techniques and trained a convolutional neural network (CNN) to infer appropriate steering angles given as input only forward-looking camera images. In~ _cite_, the authors proposed a more sophisticated architecture to perform driving action prediction that combines a fully convolutional neural network (FCN) for visual feature extraction with a long-short-term-memory (LSTM) recurrent neural network for temporal fusion of visual features and past sensory information. While mediated perception approaches require time-consuming and expensive hand-labeling of training examples (e.g., selecting all the pixels belonging to lane markings in a given image), a behavior reflex system in its simplest form may only require on-board steering angle logs and their corresponding time-stamped camera images, both of which are easily obtainable. However, this kind of approach works as a black-box and modularity is lost in favor of a monolithic system that maps raw input information to control actions. It is therefore difficult to understand why a system is choosing one action over another, and consequently how to correct undesired behaviors. Chen et al.~ _cite_ have recently proposed an alternative approach, called direct perception, that takes an intermediate position between mediated perception and behavior reflex. Their main idea is to train a CNN to map camera images to a predefined set of descriptors such as heading angle and position in the ego-lane. It is argued that such a set provides a compact but complete representation of the vehicle's surrounding that can then be used for choosing appropriate control actions. Their approach, however, was developed within a simple driving simulator and it would probably not generalize well when applied to much more complex real-world scenarios. The approach described in this work also occupies an intermediate position between the two main paradigms previously described. By taking as input LIDAR point clouds, past GPS-IMU information, and driving directions, our system generates as output driving paths in the vehicle reference frame. This is accomplished by implicitly learning, from real-world driving sequences, which regions of a point cloud are drivable and how a driver would navigate them. One of the novelties of our approach is that GPS-IMU data and driving directions are transformed into a spatial format that makes possible the direct information fusion with LIDAR point clouds. In comparison with behavior reflex methods, the proposed approach preserves the decoupling between perception and control while, at the same time, producing interpretable results. Whereas mediated perception methods carry out low-level scene parsing, our system generates a more abstract output that is closer to vehicle control and its training data is obtained automatically without the need of time-consuming hand-labeling. The paper is organized as follows: In Section~ _ref_, an overview of the proposed system is presented and it is followed by a description of the preprocessing steps applied to the raw input data. The FCN architecture is presented in Section~ _ref_ . The data set and details about the training procedure are described in Section~ _ref_ . The results are presented in Section~ _ref_ and are followed by the conclusions and future work in Section~ _ref_ .