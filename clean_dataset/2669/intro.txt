Handwritten Text Recognition (HTR) has been a major research problem for several decades _cite_ _cite_ and has gained recent impetus due to the potential value that can be unlocked from extracting the data stored in handwritten documents and exploiting it via modern AI systems. Traditionally, HTR is divided into two categories: offline and online recognition. In this paper, we consider the offline recognition problem which is considerably more challenging as, unlike the online mode which exploits attributes like stroke information and trajectory in addition to the text image, offline mode has only the image available for feature extraction. Historically, HTR has been formulated as a sequence matching problem: a sequence of features extracted from the input data is matched to an output sequence composed of characters from the text, primarily using Hidden Markov Models (HMM) _cite_ _cite_ . However, HMMs fail to make use of the context information in a text sequence, due to the Markovian assumption that each observation depends only on the current state. This limitation was addressed by the use of Recurrent Neural Networks (RNN) which encode the context information in the hidden states. Nevertheless, the use of RNN was limited to scenarios in which the individual characters in a sequence could be segmented, as the RNN objective functions require a separate training signal at each timestep. Improvements were proposed in form of models that have a hybrid architecture combining HMM with RNN _cite_ _cite_, but major breakthrough came in _cite_ which proposed the use of Connectionist Temporal Classification (CTC) _cite_ in combination with RNN. CTC allows the network to map the input sequence directly to a sequence of output labels, thereby doing away with the need of segmented input. The performance of RNN-CTC model was still limited as it used handcrafted features from the image to construct the input sequence to the RNN. Multi-Dimensional Recurrent Neural Network (MDRNN) _cite_ was proposed as the first end-to-end model for HTR. It uses a hierarchy of multi-dimensional RNN layers that process the input text image along both axes thereby learning long term dependencies in both directions. The idea is to capture the spatial structure of the characters along the vertical axis while encoding the sequence information along the horizontal axis. Such a formulation is computationally expensive as compared to standard convolution operations which extract the same visual features as shown in _cite_, which proposed a composite architecture that combines a Convolutional Neural Network (CNN) with a deep one-dimensional RNN-CTC model and holds the current state-of-the-art performance on standard HTR benchmarks. In this paper, we propose an alternative approach which combines a convolutional network as a feature extractor with two recurrent networks on top for sequence matching. We use the RNN based Encoder-Decoder network _cite_ _cite_, that essentially performs the task of generating a target sequence from a source sequence and has been extensively employed for Neural Machine Translation (NMT) . Our model incorporates a set of improvements in architecture, training and inference process in the form of Batch \& Layer Normalization, Focal Loss and Beam Search to name a few. Random distortions were introduced in the inputs as a regularizing step while training. Particularly, we make the following key contributions: