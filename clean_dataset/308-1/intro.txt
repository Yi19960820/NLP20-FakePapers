Despite the recent advancements in deep learning which resulted in huge performance improvements in computer vision tasks such as image recognition _cite_, object detection _cite_, semantic segmentation _cite_, etc., video action recognition still remains a challenging task. This can be attributed mainly to two major reasons, one being the lack of large scale video datasets to enable deep networks with millions of parameters to be tuned effectively to the given task, which can be partly solved by making use of large image datasets such as imagenet for pre-training the network. The second and the most important challenge is present in the nature of the data itself, i.e., the varying duration of action instances and the huge variability of action-specific spatio-temporal patterns in videos. The former can be addressed by sampling operations such as max pooling or average pooling while the latter requires a careful design of network structure capable of encoding the spatial information present in each frame in relation to how it evolves in subsequent frames with each action instance. Several techniques have been proposed for encoding spatio-temporal information present in videos. Simonyan and Zisserman _cite_ propose to use stacked optical flow along with video frames for encoding both appearance and motion information present in the video. A huge performance improvement was observed after incorporating the optical flow stream to the image based convolutional neural network (CNN) which confirms the fact that a simple CNN has limited capability in capturing spatio-temporal information. Wang _cite_ propose to combine improved dense trajectory method with the two stream network of _cite_ to perform an effective pooling of the convolutional feature maps. The original two stream network _cite_ is further improved by adding residual connections from the motion stream to the appearance stream in _cite_ . Wang _cite_ propose to use several segments of the video on a two stream network and predict the action class of each segment followed by a segment consensus function for predicting the action class of the video. This enables the network to model long term temporal changes. A ND convolutional network is proposed by Tran _cite_ to encode spatio-temporal information from RGB images. Carreira and Zisserman _cite_ propose to combine this model with the two stream model to further improve the spatio-temporal information captured by the network. Several techniques that use recurrent neural networks such as LSTM _cite_, ConvLSTM _cite_, have also been proposed for encoding long term temporal changes. Girdhar _cite_ propose an end-to-end trainable CNN with a learnable spatio-temporal aggregation technique. The majority of the methods mentioned above use optical flow images along with RGB frames for improving the recognition performance. The major drawback associated with this is the huge amount of computations required for optical flow image generation. An interesting observation that we noticed is that the performance improvement brought about by the addition of the optical flow stream to these methods were almost the same (_inline_eq_ N \% in the state of the art techniques that use a flow stream in addition to the RGB stream) _cite_ . This emphasizes the importance in improving the appearance stream, which uses RGB frames instead of optical flow, in order to further improve the performance of action recognition methods. In this paper, we propose to use spatial attention during the feature encoding process to address this problem. Spatial attention have been proved to be useful in several applications such as image captioning _cite_, object localization _cite_, saliency prediction _cite_, action recognition _cite_ . Majority of these works use bottom-up attention whereas we propose to use top-down attention. Both these attention mechanisms are used by the human brain for processing visual information _cite_ . Bottom-up attention is based on the salient features of regions in the scene such as how one region differs from another, while top-down attention uses internally guided information based on prior knowledge such as the presence of objects and how they are spatially arranged. Since majority of the actions are correlated with the objects being handled, we propose to make use of the prior information embedded in a CNN trained for image classification task for identifying the location of the objects present in the scene. The location information is decoded from the raw video frames in the form of attention maps for weighting the spatial regions that provide discriminant information in differentiating one class from another. We leverage on class activation mapping _cite_ which was originally proposed for fine-grained image recognition for generating the attention map. Once the pertinent objects in the frames are identified and located, we aggregate the features from the regions into a fixed-length descriptor of the video. This is illustrated in figure _ref_ in which the object that is representative of the action class, the bow, is changing its position in subsequent frames. We use recurrent memory cells with parameters to learn such spatio-temporal aggregation for video classification. In summary, The paper is organized as follows. TA-VLAD is presented in section _ref_, followed by our analysis of experimental results in section _ref_ . In Section _ref_ we present our conclusions.