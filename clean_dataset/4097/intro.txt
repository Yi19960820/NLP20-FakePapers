Stochastic gradient descent (SGD) remains the dominant optimization algorithm of deep learning. However while SGD finds minima that generalize well, each parameter update only takes a small step towards the objective. Increasing interest has focused on large batch training, in an attempt to increase the step size and reduce the number of parameter updates required to train a model. Large batches can be parallelized across many machines, reducing training time. Unfortunately, when we increase the batch size the test set accuracy often falls . To understand this surprising observation, argued one should interpret SGD as integrating a stochastic differential equation. They showed that the scale of random fluctuations in the SGD dynamics, _inline_eq_, where _inline_eq_ is the learning rate, _inline_eq_ training set size and _inline_eq_ batch size. Furthermore, they found that there is an optimum fluctuation scale _inline_eq_ which maximizes the test set accuracy (at constant learning rate), and this introduces an optimal batch size proportional to the learning rate when _inline_eq_ . already observed this scaling rule empirically and exploited it to train ResNet-N to N _inline_eq_ ImageNet validation accuracy in one hour. Here we show, We note that a number of recent works have discussed increasing the batch size during training, but to our knowledge no paper has shown empirically that increasing the batch size and decaying the learning rate are quantitatively equivalent. A key contribution of our work is to demonstrate that decaying learning rate schedules can be directly converted into increasing batch size schedules, and vice versa; providing a straightforward pathway towards large batch training. In section N we discuss the convergence criteria for SGD in strongly convex minima, in section N we interpret decaying learning rates as simulated annealing, and in section N we discuss the difficulties of training with large momentum coefficients. Finally in section N we present conclusive experimental evidence that the empirical benefits of decaying learning rates in deep learning can be obtained by instead increasing the batch size during training. We exploit this observation and other tricks to achieve efficient large batch training on CIFAR-N and ImageNet.