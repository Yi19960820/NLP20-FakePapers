Visual information retrieval (VIR) is the information delivery mechanism that enables users to post their queries and then obtain the answers from visual contents~ _cite_ . As an emerging kind of recommender system, visual question answering is an important problem for VIR sites, which automatically returns the relevant answer from the referenced visual contents according to users' posted question~ _cite_ . Currently, most of the existing visual question answering methods mainly focus on the problem of static image question answering~ _cite_ . Although existing methods have achieved promising performance in image question answering task, they may still be ineffective applied to the problem of video question answering due to the lack of modeling the temporal dynamics of video contents~ _cite_ . The video content often contains the evolving complex interactions and the simple extension of image question answering is thus ineffectively to provide the satisfactory answers. This is because the relevant video information is usually scattered among the entire frames. Furthermore, a number of frames in video are redundant and irrelevant to the question. We give a simple example of video question answering in Figure~ _ref_ . We demonstrate that the answering for question "What is a woman boiling in a pot of water?" requires the collective information from multiple video frames. Recently, temporal attention mechanisms have been shown to its effectiveness on critical frame extraction for video representation learning~ _cite_ . Thus, we then employ the temporal attention mechanisms to model the temporal dynamics of video contents. On the other hand, the utilization of high-level semantic attributes has demonstrated the effectiveness in visual understanding tasks~ _cite_ . Furthermore, we observe that the detected attributes are able to enhance the performance of video question answering in Figure~ _ref_ . Thus, leveraging both temporal dynamic modeling and semantic attributes is critical for learning effective video representation in video question answering. In this paper, we study the problem of video question answering by modeling its temporal dynamics and semantic attributes. Specifically, we propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attribute-augmented attention network to further improve the performance, named as r-ANL. When a certain question is issued, r-ANL can return the relevant answer for it based on the referenced video content. The main contributions of this paper are as follows: