Kernel methods _cite_ _cite_ represent a well-established learning paradigm that is able to capture the nonlinear complex patterns underlying data. In kernel methods, the learning is implicitly performed in a high-dimensional (even infinite-dimensional) nonlinear feature space (called Reproducing Kernel Hilbert Space, RKHS) _cite_ via the kernel trick: for a vector _inline_eq_ in the input space, it is projected into the RKHS using a nonlinear mapping _inline_eq_, then linear learning is performed on the nonlinear features _inline_eq_ . _inline_eq_ is implicitly characterized by a kernel function associated with the RKHS, which known as kernel trick _cite_ . Classical kernel methods perform single-layer feature learning: the input _inline_eq_ is transformed into _inline_eq_ which are used as the final features on which learning is carried out. Motivated by recent success of deep neural networks which perform hierarchical (``deep") representation learning _cite_ that is able to capture low-level, middle-level and high-level features, we aim to study ``deep" kernel methods that learn several layers of stacked nonlinear transformations. Specifically, we propose a Stacked Kernel Network (SKN) that interleaves several layers of nonlinear transformations and linear mappings. Starting from the input _inline_eq_, a nonlinear feature map _inline_eq_ is applied to project _inline_eq_ into a _inline_eq_-dimensional (_inline_eq_ could be infinite) RKHS _inline_eq_ . Then we use a linear mapping _inline_eq_ to project _inline_eq_ into a _inline_eq_-dimensional linear space _inline_eq_: _inline_eq_ . Let _inline_eq_ be the _inline_eq_-th row vector of _inline_eq_, according to the definition of RKHS, _inline_eq_ can be computed as _inline_eq_ where _inline_eq_ is a function in this RKHS. To this end, the representation of _inline_eq_ in _inline_eq_ can be written as _inline_eq_, where the _inline_eq_-th element of _inline_eq_ is _inline_eq_ . Then treating _inline_eq_ as input, we apply the above procedure again: projecting _inline_eq_ into another RKHS _inline_eq_, followed by a linear projection into another linear space _inline_eq_, getting the representation _inline_eq_ . Repeating this process _inline_eq_ times, we obtain a SKN with _inline_eq_ hidden layers. SKN contains multiple layers of nonlinear representations which could be infinite-dimensional. This grants SKN vast representation power to capture the complex patterns behind data. On the other hand, after each nonlinear layer, a linear mapping is applied to confine the size of the model, so that the model capacity does not get out of control. Figure _ref_ shows the architecture of SKN. Similar to a deep neural network, it contains multiple hidden layers. The striking difference is in SKN each hidden unit is parameterized by a RKHS function while in DNN the units are parametrized by vectors. The RKHS functions could be infinite-dimensional, which are arguably more expressive than finite-dimensional vectors. As a result, SKN could possess more representational power than DNN. Given the multiple layers of RKHS functions in SKN, how to learn them is very challenging. At first, we need to seek explicit representations of these functions. We propose three ways. First, motivated by the representer theorem _cite_, we parameterize a RKHS function _inline_eq_ as a linear combination of kernel functions _inline_eq_ anchored over training data _inline_eq_: _inline_eq_ . Second, we shrink the domain of the functions from the entire RKHS into the image of the nonlinear feature map _cite_: _inline_eq_, where each function is parameterized by a learnable vector _inline_eq_ . Third, gaining insight from random Fourier features _cite_, a RKHS function _inline_eq_ can be approximated with _inline_eq_ where _inline_eq_ is the random Fourier feature transformation of _inline_eq_, and _inline_eq_ is a parameter vector. We use backpropogation algorithm to learn the SKNs under these three representations. Evaluations on various datasets demonstrate the effectiveness of SKN. The major contributions of this paper are: The rest of the paper is organized as follows. Section _ref_ introduces the Stack Kernel Network and Section _ref_ presents experimental results. Section _ref_ reviews related works and Section _ref_ concludes the paper.