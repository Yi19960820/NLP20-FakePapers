Sound conveys important information about the world around us--the bustle of a caf tells us that there are many people nearby, while the low-pitched roar of engine noise tells us to watch for fast-moving cars . Although sound is in some cases complementary to visual information, such as when we listen to something out of view, vision and hearing are often informative about the same structures in the world. Here we propose that as a consequence of these correlations, concurrent visual and sound information provide a rich training signal that can be used to learn useful representations of the visual world. In particular, an algorithm trained to predict the sounds that occur within a visual scene might be expected to learn about objects and scene elements that are associated with salient and distinctive noises, such as people, cars, and flowing water () . Such an algorithm might also learn to associate visual scenes with the ambient sound textures that occur within them. It might, for example, associate the sound of wind with outdoor scenes, and the buzz of refrigerators with indoor scenes. Although human annotations are indisputably useful for learning, they are expensive to collect. The correspondence between ambient sounds and video is, by contrast, ubiquitous and free. While there has been much work on learning from unlabeled image data, an audio signal may provide information that is largely orthogonal to that available in images alone--information about semantics, events, and mechanics are all readily available from sound . One challenge in utilizing audio-visual input is that the sounds we hear are only loosely associated with what we see. Sound-producing objects often lie outside of our visual field, and objects that are capable of producing characteristic sounds--barking dogs, ringing phones--do not always do so. A priori it is thus not obvious what might be achieved by predicting sound from images. In this work, we show that a model trained to predict held-out sound from video frames learns a visual representation that conveys semantically meaningful information. We formulate our sound-prediction task as a classification problem, in which we train a convolutional neural network (CNN) to predict a statistical summary of the sound that occurred at the time a video frame was recorded. We then validate that the learned representation contains significant information about objects and scenes. We do this in two ways: first, we show that the image features that we learn through our sound-prediction task can be used for object and scene recognition. On these tasks, our features obtain performance that is competitive with that of state-of-the-art unsupervised and self-supervised learning methods. Second, we show that the intermediate layers of our CNN are highly selective for objects. This augments recent work showing that object detectors ``emerge'' in a CNN's internal representation when it is trained to recognize scenes. As in the scene recognition task, object detectors emerge inside of our sound-prediction network. However, our model learns these detectors from an unlabeled audio-visual signal, without any explicit human annotation. In this paper, we: (N) present a model based on visual CNNs and sound textures that predicts a video frame's held-out sound; (N) demonstrate that the CNN learns units in its convolutional layers that are selective for objects, extending the methodology of _cite_ ; (N) validate the effectiveness of sound-based supervision by using the learned representation for object-and scene-recognition tasks. These results suggest that sound data, which is available in abundance from consumer videos, provides a useful training signal for visual learning.