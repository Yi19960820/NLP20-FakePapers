data including images and videos have been growing explosively on the Internet in recent years, and retrieving similar data from a large scale database has become an urgent need. Hashing methods can map similar data to similar binary codes that have small Hamming distance, due to the low storage cost and fast retrieval speed, hashing methods have been receiving broad attention~ _cite_ . Hashing methods have also been used in many applications, such as image retrieval~ _cite_ and video retrieval~ _cite_ . Traditional hashing methods~ _cite_ take pre-extracted image features as input, and then learn hash functions by exploiting the data structures or applying the similarity preserving regularizations. These methods can be divided into two categories: unsupervised and supervised methods. Recently, inspired by the remarkable progress of deep networks, some deep hashing methods have also been proposed~ _cite_ . \IEEEpubidadjcol Unsupervised methods design hash functions by using unlabeled data to generate binary codes. Locality Sensitive Hashing (LSH) ~ _cite_ is a representative unsupervised method, which is proposed to use random linear projections to map data into binary codes. Instead of using randomly generated hash functions, some data-dependent methods~ _cite_ have been proposed to capture the data properties, such as data distributions and manifold structures. For example, Weiss et al. ~ _cite_ propose Spectral Hashing (SH), which tries to keep hash functions balanced and uncorrelated. Liu et al. ~ _cite_ propose Anchor Graph Hashing (AGH) to preserve the neighborhood structures by anchor graphs, and Locally Linear Hashing (LLH) ~ _cite_ is proposed to use locality sensitive sparse coding to capture the local linear structures and then recover these structures in Hamming space. Gong et al. ~ _cite_ propose Iterative Quantization (ITQ) to generate hash codes by minimizing the quantization error of mapping data to the vertices of a binary hypercube. Topology Preserving Hashing (TPH) ~ _cite_ is proposed to perform hash function learning by preserving consistent neighborhood rankings of data points in Hamming space. Shen et al. ~ _cite_ propose Asymmetric Inner-product Binary Coding (AIBC) method, where two asymmetric coding functions are learned such that the inner products between original data pairs are approximated by the produced binary code vectors. While unsupervised methods are promising to retrieve the neighbors under some kind of distance metrics (such as _inline_eq_ distance), the neighbors in the feature space can not optimally reflect the semantic similarity. Therefore, supervised hashing methods~ _cite_ are proposed to utilize the semantic information such as image labels to generate effective hash codes. For example, Kulis et al. ~ _cite_ propose Binary Reconstruction Embedding (BRE) to learn hash functions by minimizing the reconstruction error between original distances and reconstructed distances in the Hamming space. Liu et al. ~ _cite_ propose Supervised Hashing with Kernels (KSH) to learn hash functions by preserving the pairwise relations between data samples provided by labels. Wang et al. ~ _cite_ propose Semi-supervised Hashing (SSH) to learn hash functions by minimizing the empirical error over labeled data while maximizing the information entropy of the generated hash codes over both labeled and unlabeled data. Norouzi et al. ~ _cite_ propose to learn hash functions based on a triplet ranking loss that can preserve relative semantic similarity. Ranking Preserving Hashing (RPH) ~ _cite_ and Order Preserving Hashing (OPH) ~ _cite_ are proposed to learn hash functions by preserving the ranking information, which is obtained based on the number of shared semantic labels between data examples. Shen et al. ~ _cite_ propose Supervised Discrete Hashing (SDH) to leverage label information to obtain hash codes by integrating hash code generation and classifier training. Shen et al. ~ _cite_ further improve SDH by a discrete proximal linearized minimization method, which directly handles the discrete constraints during the learning process and improves the performance. For most of the unsupervised and supervised hashing methods, input images are represented by hand-crafted features (e.g. GIST~ _cite_), which can not optimally represent the semantic information of images. Inspired by the fast progress of deep networks on image classification~ _cite_, some deep hashing methods have been proposed~ _cite_ to take advantage of the superior feature representation power of deep networks. Convolutional Neural Network Hashing (CNNH) ~ _cite_ is a two-stage framework based on the convolutional networks, which learns fixed hash codes in the first stage, and learns hash functions and image representations in the second stage. Although the learned hash codes can guide feature learning, the learned image features cannot provide feedback for learning better hash codes. To overcome the shortcomings of the two-stage learning scheme, some approaches have been proposed to perform simultaneously feature learning and hash code learning. Network in Network Hashing (NINH) ~ _cite_ is proposed to design a triplet ranking loss to capture the relative similarities of images. NINH is a one-stage supervised method, thus image representation learning and hash code learning can benefit each other in the deep architecture. Some similar ranking-based deep hashing methods~ _cite_ have also been proposed recently, which also design hash functions to preserve the ranking information obtained by labels. However, existing deep hashing methods~ _cite_ are mainly supervised methods. On one hand, they heavily rely on labeled images and require a large amount of labeled data to achieve better performance. But labeling images consumes large amounts of time and human labors, which is not practical in real world applications. One the other hand, they only consider semantic information while ignore the underlying data structures of unlabeled data. Thus it is necessary to make full use of unlabeled images to improve the deep hashing performance. In this paper, we propose the semi-supervised deep hashing (SSDH) approach, which learns hash functions by preserving the semantic similarity and underlying data structures simultaneously. To the best of our knowledge, the proposed SSDH is the first deep hashing method that can perform hash code learning and feature learning simultaneously in a semi-supervised fashion. The main contributions of this paper can be concluded as follows: Experiments on N widely-used datasets show that our proposed SSDH approach achieves the best search accuracy comparing with N state-of-art methods. The rest of this paper is organized as follows. Section II presents a brief review of related deep hashing methods. Section II introduces our proposed SSDH approach. Section IV shows the experiments on the N widely-used image datasets. Finally Section V concludes this paper.