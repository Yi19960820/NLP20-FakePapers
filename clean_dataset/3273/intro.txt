Understanding how a trained model derives its output, as well as the factors responsible, is a central challenge in machine learning . A local explanation identifies what dimensions of a single input was most responsible for a DNN's output. As DNNs get deployed in areas like medical diagnosis and imaging, reliance on explanations has grown. Given increasing reliance on local model explanations for decision making, it is important to assess explanation quality, and characterize their fidelity to the model being explained . Towards this end, we seek to assess the fidelity of local explanations to the parameter settings of a DNN model . We use random initialization of the layers of a DNN to help assess parameter sensitivity. Main Contributions