Person Re-identification (ReID) is the problem of associating different tracklets of a person across non-overlapping cameras. It has become increasingly popular for its crucial applications in visual surveillance and human computer interaction. Benefited from tremendous success of deep learning, the computer vision field has witnessed the prominent progresses in image-based person re-ID~ _cite_, which only utilizes the spatial information. However, single-shot appearance features of people are intrinsically limited to the inherent visual ambiguity. More recently, many attentions have been shifted to the video re-ID since its natural setting and some benefits with sequential information. Video re-ID faces several significant challenges, like cluttered backgrounds, out-of-focus targets, misalignments and large appearance changes as a person moves between cameras. In a meanwhile, how to extract more comprehensive representations, particularly incorporate spatial and temporal information available in videos, is still under-studied. To overcome the these issues, recent video-based methods have tended to utilize RNNs~ _cite_ (or CNN-RNNs) to take consecutive frames as inputs, and adaptively incorporate temporal information~ _cite_ . For instance, ~ _cite_ focus on considering the mutual influence between video sequences. On the other hand, another frequently used strategy is the multi-shot matching~ _cite_, where they only utilize convolution-based representations. Pooling operation in these methods aggregates frame-level features into a global vector, which has demonstrated its simplicity but effectiveness. However, the residual learning in CNN or RNN is rarely studied in the literature related to person ReID. To tackle with aforementioned issues, we propose Spatial-Temporal Synergic Residual Network (STSRN), a novel method aiming at solving the coherent representation learning bottlenecks on existing spatial-temporal models. This is achieved by exploring three different modules: a spatial residual extractor, a temporal residual processor and a spatial-temporal smoother. Particularly, the spatial residual extractor can extract discriminative frame-level spatial features while temporal processor could further improve the ability of modelling long-range dependencies and eliminate redundancy in videos with the help of residual learning. The spatial-temporal smoother makes a gentle transition between spatial domain and temporal domain. Moreover, our proposed model provides an end-to-end and parameter-efficient solution. Instead of extracting frame-level representations by using GoogLeNet~ _cite_ such as in~ _cite_ and~ _cite_, or time-consuming in inference stage~ _cite_, we employ a much smaller architecture in that STSRN could be more likely to apply to real-time video surveillance system. In summary, our main contributions are in three-folds: