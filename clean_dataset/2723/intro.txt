Ensemble learning methods train several baseline models, and use some rules to combine them together to make predictions. The ensemble learning methods have gained popularity because of their superior prediction performance in practice. Consider a prediction task with some fixed data generating mechanism. The performance of a particular learner depends on how effective its searching strategy is in approximating the optimal predictor defined by the true data generating distribution . In theory, the relative performance of various learners will depend on the model assumptions and the true data-generating distribution. In practice, the performance of the learners will depend on the sample size, dimensionality, and the bias-variance trade-off of the model. Thus it is generally impossible to know a priori which learner would perform best given the finite sample data set and prediction problem . One widely used method is to use cross-validation to give an ``objective'' and ``honest'' assessment of each learners, and then select the single algorithm that achieves best validation-performance. This is known as the discrete Super Learner selector, which asymptotically performs as well as the best base learner in the library, even as the number of candidates grows polynomial in sample size. Instead of selecting one algorithm, another approach to guarantee the predictive performance is to compute the optimal convex combination of the base learners. The idea of ensemble learning, which combines predictors instead of selecting a single predictor, is well studied in the literature: summarized and referred several related studies about the theoretical properties of ensemble learning. Two widely used ensemble techniques are bagging and boosting . Bagging uses bootstrap aggregation to reduce the variance for the strong learners, while boosting algorithms ``boost'' the capacity of the weak learners. proposed a linear combination strategy called stacking to ensemble the models. further extended stacked generalization with a cross-validation based optimization framework called Super Learner, which finds the optimal combination of a collection of prediction algorithms by minimizing the cross-validated risk. Recently, the super learner have showed great success in variety of areas, including precision medicine, mortality prediction, online learning, and spatial prediction . In recent years, deep artificial neural networks (ANNs) have led to a series of breakthroughs in a variety of tasks. ANNs have shown great success in almost all machine learning related challenges across different areas, like computer vision, machine translation, and social network analysis . Due to their high capacity/flexibility, deep neural networks usually have high variance and low bias. In practice, model averaging with multiple stochastically trained networks is commonly used to improve the predictive performance. won the first place in the image classification challenge of ILSVRC N, by averaging N CNNs with same structure. won the first place in classification and localization challenge in ILSVRC N with averaging of multiple deep CNNs. won the first place using six models of Residual Network with different depth to form an ensemble in ILSVRC N. In addition, also won the ImageNet detection task in ILSVRC N with the ensemble of N residual network models. However, the behavior of ensemble learning with deep networks is still not well studied and understood. First, most of the neural networks literature focuses mainly on the design of the network structure, and only applies naive averaging ensemble to enhance the performance. To the best of our knowledge, no detailed work investigates, compares and discusses ensemble methods for deep neural networks. Naive unweighted averaging, which is largely used, is not data-adaptive and thus vulnerable to a ``bad'' library of base learners: it works well for networks with similar structure and comparable performance, but it is sensitive to the presence of excessively biased base learners. This issue could be easily addressed by a cross-validation based data-adaptive ensemble like Bayes Optimal Classifier and Super Learner. In later sections, we investigate and compare the performance of four commonly used ensemble methods on an image classification task, with deep convolutional neural networks (CNNs) as base learners. This study mainly focuses on the comparison of ensemble methods of CNNs for image recognition. For readers who are not familiar with deep learning, each CNN could be just treated as a black-box estimator, with an image as input, and outputs the probability vector for each possible class. We refer the interested reader to for more details about deep learning.