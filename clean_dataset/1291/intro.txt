Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics _cite_, speech _cite_, and computer vision _cite_ . Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data _cite_ . In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., Figure _ref_) . Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation _inline_eq_ that is sufficient to change the estimated label _inline_eq_: where _inline_eq_ is an image and _inline_eq_ is the estimated label. We call _inline_eq_ the robustness of _inline_eq_ at point _inline_eq_ . The robustness of classifier _inline_eq_ is then defined as where _inline_eq_ is the expectation over the distribution of data. The study of adversarial perturbations helps us understand what features are used by a classifier. The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples. Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models _cite_ . This can actually become a real concern from a security point of view. An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper. \noindent Our main contributions are the following: We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in _cite_ . The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in _cite_ is time-consuming and therefore does not scale to large datasets. In _cite_, the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on PascalND + annotations. Recently, Tsai et al. _cite_ provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. _cite_ generated synthetic unrecognizable images, which are classified with high confidence. The authors of _cite_ also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of _cite_ introduced the ``fast gradient sign" method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, _cite_ introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in _cite_ was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in _cite_ that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon. The rest of paper is organized as follows. In Section N, we introduce an efficient algorithm to find adversarial perturbations in a binary classifier. The extension to the multiclass problem is provided in Section N. In Section N, we propose extensive experiments that confirm the accuracy of our method and outline its benefits in building more robust classifiers.