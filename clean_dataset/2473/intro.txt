in small, low-power CMOS image sensors and related optics have revolutionized consumer photography _cite_ . These technologies have improved dramatically the spatial resolution, dynamic range, and low light sensitivity of digital photography. In addition to improving conventional photography, these technologies open up many possibilities for novel image systems architectures. The new optics and CMOS sensors capabilities have already motivated novel camera architectures that extend the original Bayer RGB design. For example, in recent years a new generation of architectures have been produced to increase spatial resolution _cite_, control depth of field through light field camera designs _cite_, extend dynamic range and sensitivity by the use of novel arrangements of color filters _cite_ and mixed pixel architectures _cite_ . To develop these opportunities requires that we innovate on the third fundamental component of image systems, the image processing pipeline. The pipeline is the set of algorithms, including demosaicking, noise reduction, color management, and display nonlinear transforms (gamma curves), that convert the sensor data into a rendered image. Even modest changes to the camera architecture, such as more color pixels into the mosaic _cite_ or including near infrared detectors _cite_ can require substantial rethinking of the image processing pipeline. New image processing pipelines, specialized for the new types of cameras, are slow to develop. Consequently, the design of new imaging sensors is far outpacing the development of algorithms that can take advantage of these new designs _cite_, and the vast majority of image processing algorithms are still designed for sensors that use the classic single plane Bayer RGB spatial sampling mosaic. In this paper, we describe a new framework that enables image systems engineers to rapidly design image processing pipelines that are optimized for novel camera architectures. The general idea of the framework was first proposed by Lansel et al. in N _cite_ . Here, we introduce the framework in the form of a set of software tools that use simulation and learning methods to design and optimize image processing pipelines for these new camera systems. This paper is organized into three sections that define our contributions. First, we explain the image processing architecture: the input data are grouped by their local features into one of a set of local classes, where locality refers to both position on the sensor array (space), pixel type (color) and response level. The optimal affine transform in each class is learned using camera simulation technology. We refer to this framework as _inline_eq_ to emphasize its key principles: Local, Linear and Learned. Second, we assess the performance of _inline_eq_ method by comparing the rendered quality with the ones from high-end modern digital cameras. We specifically show that such a collection of affine transforms accurately approximates the complex, nonlinear pipelines implemented in modern consumer photography systems. Third, we illustrate how _inline_eq_ method can learn image processing pipelines for new camera architectures. There are a number of related efforts that incorporate system joint optimization and data-driven learning methods in designing camera image processing pipeline. We discuss the relationship between _inline_eq_ and these contributions more fully in Discussion section after introducing _inline_eq_ framework.