With the remarkable improvements of Convolutional Neural Networks (CNNs), varied excellent performance has been achieved in a wide range of pattern recognition tasks, such as image classification~ _cite_, object detection~ _cite_ and semantic segmentation~ _cite_, etc. A well-performed CNN based systems usually need considerable storage and computation power to store and calculate millions of parameters in tens or even hundreds of CNN layers. Therefore, the deployment of CNNs to some resource limited scenarios is hindered, especially low-power embedded devices in the emerging Internet-of-Things (IoT) domain. Many efforts have been devoted to optimizing the inference resource requirement of CNNs, which can be roughly divided into three categories according to the life cycle of deep models. First, design-time network optimization considers designing efficient network structures from scratch in a handcraft way such as MobileNet~ _cite_, interlacing/shuffle networks~ _cite_, or even automatic search way such as NASNet~ _cite_, PNASNet~ _cite_ . Second, training-time network optimization tries to simplify the pre-defined network structures on neural connections _cite_, filter structures _cite_, and even weight precisions _cite_ through regularized retraining or fine-tuning or even knowledge distilling _cite_ . Third, deploy-time network optimization tries to replace heavy/redundant components/structures in pre-trained CNN models with efficient/lightweight ones in a training-free way. Typical works include low-rank decomposition _cite_, spatial decomposition _cite_, channel decomposition _cite_ and network decoupling _cite_ . To produce desired outputs, it is obvious that the first two categories of methods require a time-consuming training procedure with full training-set available, while methods of the third category may not require training-set, or in some cases require a small dataset (e.g., N images) to calibrate some parameters. The optimization process can be typically done within dozens of minutes. Therefore, in case that the customers can't provide training data due to privacy or confidential issues, it is of great value when software/hardware vendors help their customers optimize CNN based solutions. It also opens the possibility for on-device learning to compression, and online learning with new ingress data. In consequence, there is a strong demand for modern deep learning frameworks or hardware (GPU/ASIC/FPGA) vendors to provide deploy-time model optimizing tools. However, current deploy-time optimization methods can only provide very limited optimization (N _inline_eq_ N _inline_eq_ in compression/speedup) over original models. Meanwhile, binary neural networks~ _cite_, which aim for training CNNs with binary weights or even binary activations, attract much more attention due to their high compression rate and computing efficiency. However, binary networks generally suffer much from a long training procedure and non-negligible accuracy drops, when comparing to the full-precision (FPN) counterparts. Many efforts have been spent to alleviate this problem in training-time optimization _cite_ . This paper considers the problem from a different perspective via raising the question: is it possible to directly transfer full-precision networks into binary networks at deploy-time in a training-free way? We study this problem, and give a positive answer by proposing a solution named composite binary decomposition networks (CBDNet) . Figure _ref_ illustrates the overall framework of the proposed method. The main contributions of this paper are summarized as below: