We first briefly describe the original algorithm of style transfer presented in _cite_ . In the following we denote _inline_eq_ the ND output (activation / feature volume) of a convolutional layer _inline_eq_ of size _inline_eq_ with _inline_eq_ being the number of convolutional filters (features, kernels) in layer _inline_eq_, _inline_eq_ and _inline_eq_ being the spatial dimensions. Using a pre-trained object recognition deep convolutional neural network the content of an image is defined as an activation volume _inline_eq_ of a fixed (usually relatively deep) layer _inline_eq_ of the network given the image as an input. In _cite_ the VGG _cite_ network was used and the output of layer was considered as content. Style is defined on top of activation volumes as a set of Gram correlation matrices _inline_eq_ with the entries _inline_eq_, where _inline_eq_ is the network layer, _inline_eq_ and _inline_eq_ are two filters of the layer _inline_eq_ and _inline_eq_ is the activation map (indexed by spatial coordinates) of filter _inline_eq_ in layer _inline_eq_ . In other words, _inline_eq_ value says how often features _inline_eq_ and _inline_eq_ in the layer _inline_eq_ appear together in the input image. Once defined, the content and style spaces can be used to find a synthesized image _inline_eq_ matching both given content _inline_eq_ and style _inline_eq_ images, resulting in high-quality repainting of the content image with preserved semantic information but covered with textures extracted from the style image. The resulting image _inline_eq_ is synthesized by back-propagating the loss _eq into pixels of _inline_eq_ (_inline_eq_ is the style/content trade-off hyper-parameter and _inline_eq_ is a style weight assigned to a specific layer, set to N for all layers by default) . The suggested synthesizing algorithm generally produces great results in transferring repetitive artistic styles. Unfortunately, otherwise generated images often don't meet human expectations of how the style should be transferred. This is the problem we attempt to tackle in this work. Further, when visually inspecting style transfer results and commenting on the quality, we use two complimentary criteria that we expect a good style transfer algorithm to meet: As it turns out, it is often difficult to satisfy both simultaneously. The rest of the work is structured as follows: section _ref_ gives an overview of other research built on top of the original style transfer algorithm. Section _ref_ presents our suggested improvements to the style transfer algorithm. Section _ref_ describes and presents the results of experiments conducted with a visual comparison of different approaches to style transfer. We give concluding remarks in section _ref_ .