Numbers speak for themselves: more than N hours of video content are added to YouTube every minute . A need that certainly arises from such huge source of on-line videos is the ability to automatically learn from it, ideally without requiring any annotation or form of supervision. State of the art approaches learning from videos are, \eg . This means they all rely on large-scale video datasets heavily annotated, an aspect that has not been a problem so far: every year a novel fully annotated video dataset appears (\eg HMDBN, UCFN, THUMOS, ActivityNet) . So, the question is: why unsupervised learning? The first answer is obvious: the use of a supervised model implies a tremendous effort annotating data, a time-consuming task, with its associated costs, and that is irremediably prone to user-specific biases and errors. But there are more reasons that justify the need of a truly unsupervised learning approach, especially for videos. Fundamentally, videos are much higher dimensional entities compared to images. Therefore, to learn long range structures generally implies to do a lot of feature engineering, \eg computing the right kind of flow and trajectories features, or using elaborated feature pooling methods . Furthermore, like we pointed above, every minute hundreds of videos are freely available without annotations, and none of these heavily supervised solutions can benefit from this invaluable source of information. Current unsupervised methods use the temporal coherence of contiguous video frames as regularization for the learning process. This way, the spatio-temporal continuity of close frames is used as a free form of annotation or supervision, which is used to learn representations that exhibit small differences between these frames. However, these approaches fail to capture the dissimilarity between videos with different content, hence learning less discriminative features. In this paper, we also choose to work in a fully unsupervised learning setting, where our models only have access to unlabeled videos. The key contributions of our work can be summarized as follows: