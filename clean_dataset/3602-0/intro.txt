Many cities are being instrumented with hundreds of surveillance cameras mounted on streets and intersections _cite_ . They capture traffic N hours a day, N days a week, generating large scale video data. Citycam videos can be regarded as highly versatile, being an untapped potential to develop many vision-based techniques for applications like traffic flow analysis and crowd counting. This paper aims to extract vehicle counts from streaming real-time video captured by citycams. Vehicle count is the number of vehicles in a given region of the road _cite_ . As shown in Figure _ref_, we select a region of fixed length in a video and count the number of vehicles in that region. Vehicle counting is of great importance for many real-world applications, such as urban traffic management. Important as it is, Counting vehicles from city cameras is an extremely difficult problem faced with severe challenges (illustrated in Figure _ref_) due to network bandwidth limitations, lack of persistent storage, and privacy concerns. Publicly available citycam video is limited by: All these challenges make vehicle counting from citycam data very difficult. The challenges of citycam videos preclude existing approaches to vehicle counting, which can be grouped into five categories: frame differencing based _cite_, detection based _cite_, motion based _cite_, density estimation based _cite_, and deep learning based _cite_ methods. Frame differencing, detection, and motion based methods are sensitive to environment conditions and tend to fail in high occlusion, low resolution, and low frame rate videos. While density estimation approaches avoid detecting or tracking individual vehicles, they perform poorly in videos with large perspective and oversized vehicles. Though the low frame rate citycam video lacks motion information, vehicle counts of sequential frames are still correlated. Existing methods fail to account for such temporal correlation _cite_ . Work _cite_ and _cite_ achieve state-of-the-art performance on animal counting and traffic counting, respectively, yet they fail to model the temporal correlation as an intrinsic feature of the surveillance video. To overcome these limitations, we propose a deep spatio-temporal network architecture to sequentially estimate vehicle count by combining FCN _cite_ with LSTM _cite_ in a residual learning framework (FCN-rLSTM) . The FCN maps pixel-level features into vehicle density to avoid individual vehicle detection or tracking. LSTM layers learn complex temporal dynamics by incorporating nonlinearities into the network state updates. The residual connection of FCN and LSTM reformulates global count regression as learning residual functions with reference to the sum of densities in each frame, avoiding learning unreferenced functions and significantly accelerating the network training. FCN-rLSTM enables refined feature representation and a novel end-to-end optimizable mapping from image pixels to vehicle count. The framework is shown in Figure _ref_ . Video frames are input into FCN, and the output density maps are fed into LSTMs to learn the vehicle count residual for each frame. The global vehicle count is finally generated by summing the learned residual and the densities. The proposed FCN-rLSTM has the following novelties and contributions: The rest of paper is outlined as follows. Section N briefly reviews the related work for vehicle counting. Section N details the proposed FCN-rLSTM. Section N presents experimental results, and Section N concludes the paper.