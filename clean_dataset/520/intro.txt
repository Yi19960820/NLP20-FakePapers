Metric learning aims to learn a transformation from the image space to a feature embedding space, in which distance is a measure of semantic similarity. Feature embeddings from semantically similar images will be located nearby, while those of semantically dissimilar images will be located far apart. Applications for such effective feature embeddings include transfer learning, retrieval, zero-and few-shot learning, clustering and weakly or self supervised learning. Image classification is the task of categorising an image into one of a set of classes. Applications include object and scene recognition. Classification and metric learning are generally treated as separate problems. As such, metric learning approaches have struggled to reach the classification performance of state-of-the-art classifiers. Likewise, classification approaches fail to learn feature spaces that represent inter-and intra-class similarities to the standard of metric learning approaches. Outside of zero-and few-shot learning, some metric learning algorithms have been applied to classification _cite_, although approaches that perform well in both domains remain uncommon. We propose a novel loss function and training algorithm for convolutional neural networks that can be applied to both metric learning and image classification problems, outperforming conventional approaches in both domains. Our approach defines training set feature embeddings as Gaussian kernel centres, which are used to push or pull features in a local neighbourhood, depending on the labels of the associated training examples. Fast approximate nearest neighbour search is used to provide an efficient and scalable solution. Our approach differs from kernel or radial basis function neurons _cite_, as our kernel centres are not learned network parameters, but are defined to be the locations of the training set features in the embedding space. Additionally, we use kernels only in the loss function and classifier, not as activation functions throughout the network. Beyond activation functions, kernels have also been used in neural network classifiers as support vector machines _cite_ . Our approach is related to NCA _cite_, but introduces per exemplar weights, makes training feasible through the introduction of periodic asynchronous updates of the kernel centres, and is made scalable for a large number of training examples and a high embedding dimension. Additionally, we explore the importance of the embedding space dimensionality. The best success on embedding learning tasks has been achieved by deep metric learning methods _cite_, which make use of deep neural networks. The majority of these approaches use or generalise a triplet architecture with hinge loss _cite_, although including global loss terms can also be beneficial _cite_ . Triplet networks take a trio of inputs; an anchor image, an image of the same class as the anchor and an image of a different class. Triplet approaches aim to map the anchor nearer the positive example than the negative example, in the feature space. Such approaches may indiscriminately pull examples of the same class together, regardless of the local structure of the space. In other words, these methods aim to form a single cluster per class, limiting the intra-and inter-class similarities that can be represented. In contrast, our approach considers only the local neighbourhood of a feature, allowing multiple clusters to form for a single class, if that is appropriate. Our approach outperforms state-of-the-art deep metric learning approaches on embedding learning challenges. The most common approach to image classification is a convolutional neural network trained with softmax loss, which transforms activations into a distribution across class labels _cite_ . However, softmax is inflexible as classes must be axis-aligned and the number of classes is baked into the network. Our approach is free to position clusters such that the intrinsic structure of the data can be better represented. Our metric learning approach to classification outperforms softmax on several datasets, while simultaneously representing the intra-and inter-class similarities sought by metric learning approaches. Metric learning also allows for new classes to be added on-the-fly, with no updates to the network weights required to obtain reasonable results. The advantages of our approach are as follows: