the rapid growth of multimedia data in search engines and social networks, how to store these data and make a fast search when an novel one such as the image is given, plays a fundamental role in machine learning. Due to the low storage cost and fast retrieval speed, hashing techniques have attracted much attention and are widely applied in nearest neighbor search _cite_ for information retrieval on large scale datasets. Hashing learning aims to project the data from the original space into a Hamming space by generating compact codes. These codes can not only dramatically reduce the storage overhead and achieve a constant or sub-linear time complexity in information search, but also preserve the semantic affinity existing in the original space. Many hashing methods have been studied _cite_ _cite_ _cite_ _cite_ _cite_ _cite_ _cite_ _cite_ _cite_ . Generally, these approaches can be roughly classified into two categories: data-independent and data-dependent hashing methods. Locality Sensitive Hashing (LSH) _cite_ and its extension Kernelized LSH (KLSH) _cite_, as the most typical data-independent hashing methods, were proposed to obtain the hashing function by using random projections. Although the designation of these data-independent methods is quite simple, they often meet a performance degradation when the length of the binary codes is relatively low. By contrary, instead of randomly generating the hashing function like LSH does, data-dependent methods aims to learn a data-specific hashing function by using the training data, being capable of generating shorter binary codes but achieving more remarkable performance. Therefore, various data-dependent hashing approaches containing both unsupervised and supervised have been proposed. Unsupervised hashing, e.g. Spectral Hashing _cite_, Anchor Graph Hashing (AGH) _cite_, and Discrete Graph Hashing (DGH) _cite_ etc., only try to utilize the data structure to learn compact binary codes to improve the performance. By taking the label information into account, supervised hashing methods attempt to map the original data into a compact Hamming space to preserve the similarity between each pair samples. Many representative works including Fast Supervised Hashing (FastH) _cite_, Kernel Supervised Hashing (KSH) _cite_, and Supervised Discrete Hashing (SDH) _cite_ etc., demonstrate that supervised hashing methods often obtain an outstanding performance compared with unsupervised hashing methods. Thus, we focus on studying the supervised hashing method in this paper. Although some traditional supervised hashing methods achieve a good performance in some applications, most of them only linearly map the original data into a Hamming space by using the hand-crafted features, limited their application for large-scale datasets which have complex distributions. Fortunately, due to the powerful capability of data representation, deep learning _cite_ _cite_ provides a promising way to jointly represent the data and learn hash codes. Some existing deep learning based hashing methods have been studied, such as Deep Supervised Hashing (DSH) _cite_ and Deep Pairwise Supervised Hashing (DPSH) _cite_, etc. These approaches demonstrate the effectiveness of the end-to-end deep learning architecture for hashing learning. Despite the wide applications of deep neural network on hashing learning, most of them are symmetric structures in which the similarity between each pair points are estimated by the Hamming distance between the outputs of the same hash function _cite_ . As described in _cite_, a crucial problem is that this symmetric scheme would result in the difficulty of optimizing the discrete constraint. Thus in this paper, we propose a novel asymmetric hashing method to address aforementioned problem. Note that a similar work was described by Shen et al. _cite_, named deep asymmetric pairwise hashing (DAPH) . However, our study is quite distinctive from DAPH. Shen et al. tried to approximate the similarity affinity by exploiting two different hashing functions, which can preserve more similarity information among the real-value features. However, DAPH only exploits a simple Euclidean distance, but ignores the semantic structure between the learned real-value features and binary codes _cite_ _cite_ . One major deficiency is that it is difficult to efficiently preserve the similarity in the learned hash functions and discrete codes. Furthermore, in DAPH, two different types of discrete hash codes corresponding to two hash functions are estimated in the training time. However this strategy would enlarge the gap between two schemes, resulting in a performance degradation. By contrast, we not only propose a novel asymmetric structure to learn two different hash functions and one consistent binary code for each sample at the training phase, but also asymmetrically exploit real-value and multiple integer values, which permits the better preservation of similarity between the learned features and hash codes. Experiments show that this novel asymmetric structure can get a better performance in image retrieval and quicker convergence at the training stage. The main contributions of the proposed method are shown as follows: (N) A novel asymmetric deep structure are proposed. Two streams of deep neural networks are trained to asymmetrically learn two different hash functions. The similarity between each pair images are utilized through a pairwise loss according to their semantic/label information. (N) The similarity between the learned features and binary codes are also revealed through an additional asymmetric loss. Real-value features and binary codes are bridged through an inner product, which alleviates the binary limitation, better preserves the similarity, and speeds up convergence at the training phase. (N) By taking advantage of these two asymmetric properties, an alternative algorithm is designed to efficiently optimize the real values and discrete values. (N) Experimental results on three large-scale datasets substantiate the effectiveness and superiority of our approach as compared with some existing state-of-the-art hashing methods in image retrieval. The rest of this paper is organized as follows. In Section N, the related works including data-independent and data-dependent hashing methods are briefly reviewed. In Section N, the proposed D ual A symmetric D eep H ashing Learning (DADH) is then analyzed, followed by its optimization. In Section N, experiments are conducted on three real-world datasets, and some comparisons, parameter sensitivity analysis and convergence analysis are discussed. This paper is finally concluded in Section N.