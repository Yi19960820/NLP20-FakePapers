Humans are instantly aware of their surroundings, being able to estimate their approximate position based on a handful of visual cues. With the advent of deep convolutional networks (_cite_, _cite_, _cite_), solutions to such vision problems are now in sight, having the potential to impact technology in the growing fields of autonomous ground or aerial vehicles. Thus, the ability to interpret a scene and tell its location is of growing interest in the domain of aerial images. While the task of semantic segmentation has already attracted many solid approaches, the task of vision-based localization in aerial images is still in its infancy. In this paper we proposed a novel and competitive approach, a multi-stage, multi-task convolutional neural network, that can solve both semantic segmentation and localization in a single forward pass, using only RGB input. The structure has a singular encoder module, termed UniEncoder, which we prove experimentally to be efficient in learning meaningful descriptors for both pixelwise predictions and localization. Our system is able to produce state-of-the-art segmentation results on several public datasets. The localization precision within a city-wide area is comparable to commercial GPS _cite_, making our system suitable for deployment when the airborne sensors malfunction.