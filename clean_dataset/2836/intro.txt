Although convolutional neural networks (CNNs) have been largely successful in various applications, they have been shown to be quite vulnerable to additive adversarial perturbations~ _cite_ which can negatively affect their applicability in sensitive applications such as autonomous driving ~ _cite_ . Deep networks have also been shown to be vulnerable to rigid geometric transformations _cite_, which are more natural than additive perturbations: they can simply represent the change of the viewpoint of an image. Therefore, invariance to such transformations is certainly a key feature in practical vision systems. In this paper, we focus on studying the robustness of deep networks to geometric transformations in the worst-case regime as these can be quite problematic for sensitive applications. We approach this problem by searching for minimal 'fooling' transformations, \ie, transformations that change the decision of image classifiers, and we use these transformed examples to measure the invariance of a deep network. We further show that fine-tuning on such worst-case transformed examples can improve the invariance properties of deep image classifiers. Our main contributions are as follows: The adversarial examples are first introduced in _cite_ . Since then, many methods to find additive adversarial perturbations have been proposed such as _cite_ . Other types of adversarial examples are later found in _cite_ . The work _cite_ also introduces the concept of adversarial training to increase the accuracy of networks. The authors in _cite_ later show that adversarial training can also be used for increasing the robustness of networks against adversarial examples constructed by additive perturbations. The vulnerability of CNNs against geometric transformations, on the other hand, has been studied in~ _cite_ and _cite_ that analyze image and visual representations to find theoretical foundations of transformation invariant features. The work in _cite_ uses the information about human visual system to understand and improve the transformation invariance. In addition, several practical solutions have been suggested for improving the invariance characteristics. One approach is to modify the layers of the networks; \eg, pooling layer _cite_ or convolutional layers _cite_ . Another method is to add modules to the network, like the spatial transformer networks _cite_ . Even though these works focus on improving the invariance, they do not offer methods for measuring invariance properties of classification architectures. This problem is the main focus of _cite_, where the invariance is measured by using the firing rates of neurons in the network for one dimensional transformations. On the other hand, the authors of _cite_ propose a probabilistic framework for estimating the robustness of a classifier by using a Metropolis algorithm to sample the set of transformations. Lastly, another approach is given by Manitest _cite_, where the invariance is measured using the geodesic distances on the manifold of transformed images. In this work, we also use a manifold-based definition of invariance and propose a new scalable algorithm for evaluating invariance in more complex networks and improving it by fine-tuning.