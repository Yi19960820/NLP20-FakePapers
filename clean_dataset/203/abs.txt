This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs) . Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of N _inline_eq_ is demonstrated on a large network trained for ImageNet, while the top-N error rate is only increased by N \%. Our accelerated model has a comparably fast speed as the ``AlexNet'' _cite_, but is N \% more accurate.