Most image understanding and computer vision methods build on image representations such as textons~ _cite_, histogram of oriented gradients (SIFT~ _cite_ and HOG~ _cite_), bag of visual words~ _cite_ _cite_, sparse~ _cite_ and local coding~ _cite_, super vector coding~ _cite_, VLAD~ _cite_, Fisher Vectors~ _cite_, and, lately, deep neural networks, particularly of the convolutional variety~ _cite_ . However, despite the progress in the development of visual representations, their design is still driven empirically and a good understanding of their properties is lacking. While this is true of shallower hand-crafted features, it is even more so for the latest generation of deep representations, where millions of parameters are learned from data. In this paper we conduct a direct analysis of representations by characterising the image information that they retain (Fig.~ _ref_) . We do so by modeling a representation as a function _inline_eq_ of the image _inline_eq_ and then computing an approximated inverse _inline_eq_, . A common hypothesis is that representations collapse irrelevant differences in images (e.g. illumination or viewpoint), so that _inline_eq_ should not be uniquely invertible. Hence, we pose this as a reconstruction problem and find a number of possible reconstructions rather than a single one. By doing so, we obtain insights into the invariances captured by the representation. Our contributions are as follows. First, we propose a general method to invert representations, including SIFT, HOG, and CNNs (Sect.~ _ref_) . Crucially, this method {\bf uses only information from the image representation} and a generic natural image prior, starting from random noise as initial solution, and hence captures only the information contained in the representation itself. We discuss and evaluate different regularization penalties as natural image priors. Second, we show that, despite its simplicity and generality, this method recovers significantly better reconstructions from DSIFT and HOG compared to recent alternatives~ _cite_ . As we do so, we emphasise a number of subtle differences between these representations and their effect on invertibility. Third, we apply the inversion technique to the analysis of recent deep CNNs, exploring their invariance by sampling possible approximate reconstructions. We relate this to the depth of the representation, showing that the CNN gradually builds an increasing amount of invariance, layer after layer. Fourth, we study the locality of the information stored in the representations by reconstructing images from selected groups of neurons, either spatially or by channel. The rest of the paper is organised as follows. Sect.~ _ref_ introduces the inversion method, posing this as a regularised regression problem and proposing a number of image priors to aid the reconstruction. Sect.~ _ref_ introduces various representations: HOG and DSIFT as examples of shallow representations, and state-of-the-art CNNs as an example of deep representations. It also shows how HOG and DSIFT can be implemented as CNNs, simplifying the computation of their derivatives. Sect.~ _ref_ and~ _ref_ apply the inversion technique to the analysis of respectively shallow (HOG and DSIFT) and deep (CNNs) representations. Finally, Sect.~ _ref_ summarises our findings. We use the matconvnet toolbox~ _cite_ for implementing convolutional neural networks. There is a significant amount of work in understanding representations by means of visualisations. The works most related to ours are Weinzaepfel~ \etal~ _cite_ and Vondrick~ \etal~ _cite_ which invert sparse DSIFT and HOG features respectively. While our goal is similar to theirs, our method is substantially different from a technical viewpoint, being based on the direct solution of a regularised regression problem. The benefit is that our technique applies equally to shallow (SIFT, HOG) and deep (CNN) representations. Compared to existing inversion techniques for dense shallow representations~ _cite_, it is also shown to achieve superior results, both quantitatively and qualitatively. An interesting conclusion of~ _cite_ is that, while HOG and SIFT may not be exactly invertible, they capture a significant amount of information about the image. This is in apparent contradiction with the results of Tatu~ \etal~ _cite_ who show that it is possible to make any two images look nearly identical in SIFT space up to the injection of adversarial noise. A symmetric effect was demonstrated for CNNs by Szegedy~ \etal~ _cite_, where an imperceptible amount of adversarial noise suffices to change the predicted class of an image. The apparent inconsistency is easily resolved, however, as the methods of~ _cite_ require the injection of high-pass structured noise which is very unlikely to occur in natural images. Our work is also related to the DeConvNet method of Zeiler and Fergus~ _cite_, who backtrack the network computations to identify which image patches are responsible for certain neural activations. Simonyan~ \etal~ _cite_, however, demonstrated that DeConvNets can be interpreted as a sensitivity analysis of the network input/output relation. A consequence is that DeConvNets do not study the problem of representation inversion in the sense adopted here, which has significant methodological consequences; for example, DeConvNets require about the activations in several intermediate layers, while our inversion uses only the final image code. In other words, DeConvNets look at certain network outputs are obtained, whereas we look for information is preserved by the network output. The problem of inverting representations, particularly CNN-based ones, is related to the problem of inverting neural networks, which received significant attention in the past. Algorithms similar to the back-propagation technique developed here were proposed by~ _cite_, along with alternative optimisation strategies based on sampling. However, these methods did not use natural image priors as we do, nor were applied to the current generation of deep networks. Other works~ _cite_ specialised on inverting networks in the context of dynamical systems and will not be discussed further here. Others~ _cite_ proposed to learn a second neural network to act as the inverse of the original one, but this is complicated by the fact that the inverse is usually not unique. Finally, auto-encoder architectures~ _cite_ train networks together with their inverses as a form of supervision; here we are interested instead in visualising feed-forward and discriminatively-trained CNNs now popular in computer vision.