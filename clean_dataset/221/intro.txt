Despite remarkable successes in various applications, deep neural networks (DNNs) usually suffer following two problems that stem from their inherent huge parameter space. First, most of state-of-the-art deep architectures are prone to over-fitting even when trained on large datasets~ . Secondly, DNNs usually consume large amount of storage memory and energy~, which makes it difficult to use them in devices with limited memory and power (such as portable devices or chips) . Different from most existing works~ on model compression and acceleration that ignore the strong dependencies among weights and learn filters independently based on existing network architectures, this paper proposes to explicitly enforce the parameter sharing among filters to more effectively learn compact and efficient deep networks. In this paper, we propose a W eight S ampling deep neural network (i.e. \WSNet) to significantly reduce both the model size and computation cost, achieving more than N _inline_eq_ smaller size and up to N _inline_eq_ speedup at negligible performance drop or even achieving better performance than the baseline ({i.e. conventional networks that learn filters independently}) . Specifically, WSNet is parameterized by layer-wise from which each filter participating in actual convolutions can be directly sampled, in both spatial and channel dimensions. Since condensed filters have significantly fewer parameters than independently trained filters as in conventional CNNs, learning by sampling from them makes WSNet a more compact model compared to conventional CNNs. In addition, to reduce the ubiquitous computational redundancy in convolving the overlapped filters and input patches, we propose an integral image based method to dramatically reduce the computation cost of WSNet in both training and inference. The integral image method is also advantageous because it enables weight sampling with different filter size and minimizes computational overhead to enhance the learning capability of WSNet. In order to demonstrate the efficacy of WSNet, we conduct extensive experiments on challenging audio classification tasks. On each test dataset, including ESC-N, UrbanSoundNK, DCASE and MusicDetNK (a self-collected dataset, as detailed in Section~ _ref_), WSNet significantly reduces the model size of the baseline by N _inline_eq_ with comparable or even higher classification accuracy. When compressing more than N _inline_eq_, WSNet is only subject to negligible accuracy drop. {At the same time, WSNet significantly reduces the computation cost (up to N _inline_eq_) .} Such results strongly establish the capability of WSNet to learn compact and efficient networks. Last but not the least, we provide an intuitive method to extend WSNet from ND CNNs to ND CNNs. Experimental results on MNIST and CIFARN strongly evidence the potential capability of WSNet to learn efficient networks on ND CNNs.