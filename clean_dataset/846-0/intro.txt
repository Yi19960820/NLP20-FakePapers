Convolutional Neural Network (CNN) is a multilayer representation learning architecture which has received immense success in multiple applications such as object classification, image segmentation, and natural language processing. From LeNet~ _cite_ to AlexNet~ _cite_, GoogleNet~ _cite_, VGG-Net~ _cite_, ResNet~ _cite_, and now DenseNet~ _cite_, given large training data, CNNs have shown state-of-the-art performance for several applications. However, large training data is also a limiting requirement for applications with small sample size and many of these architectures easily overfit on small training samples. For example, as shown in Figure _ref_, a face recognition model trained on large training data of adult faces (e.g. CelebA or LFW databases) may not provide good performance when tested for newborn face recognition _cite_ . In newborn face recognition, the available training data may be small and therefore, even after fine-tuning, standard deep learning based face recognition models may not yield high performance. To address the challenge of small sample size, researchers have proposed algorithms focusing on CNN initialization tricks and modifications to CNN architecture. Erhan~ et al. _cite_ have investigated the importance of unsupervised pre-training of deep architecture and empirically shown that pre-trained weights of the network generalize better than randomly initialized weights. Similarly, Mishkin and Matas _cite_ have proposed Layer-Sequential Unit-Variance (LSUV) initialization that utilizes the orthonormal matrices to initialize the weights of each convolutional layer and normalize the weight to the unit variance. Along the same lines, pre-defined handcrafted filters are also proposed to handle the small sample size problem. For example, And {\'e} n and Mallat _cite_ propose Scattering network (ScatNet) which is a CNN like architecture where pre-defined Morlet filter bank is utilized to extract features. However, these handcrafted filters may not represent the true distribution of the data and hence extract not-so-meaningful features. To overcome this limitation, Oyallon et. al. _cite_ have proposed hybrid network, where they have utilized ScatNet feature followed by CNN architecture. Similarly, Chan~ et. al. ~ _cite_ propose PCANet architecture that utilizes Principal Component Analysis (PCA) to learn the filter banks. They also present an extension, termed as LDANet, in which the selection of the cascade filters are trained from Linear Discriminant Analysis (LDA) . Gan~ et al. ~ _cite_ propose a PCA-based Convolutional Network (PCN) which has the influence of both CNN~ _cite_ and PCANet~ _cite_ . Dan~ et al. ~ _cite_ utilize the concept of kernel PCA to further improve the PCANet architecture. Zeng~ et al. ~ _cite_ propose a multilinear discriminant analysis network (MLDANet) which is a variant of PCANet and LDANet. Feng~ et al. ~ _cite_ propose Discriminative Locality Alignment Network (DLANet) which is based on manifold learning. These architectures learn filters in stack-wise manner, and once the network (filters) is trained, generally, it is not allowed to fine-tune the filters on other databases. In other research directions for small sample size training, Mao~ et al. ~ _cite_ propose a neural network learning method based on posterior probability (PPNN) to improve the accuracy. Ngiam~ et al. ~ _cite_ propose tied weights in a filter using tiling parameter which handles the total number of learning parameters. In another work, Indian Buffet Process (IBP) priors are utilized to propose semi-supervised ibpCNN which shows better generalizability _cite_ . Xiong~ et al. ~ _cite_ propose Structured Decorrelation Constrained (SDC) for hidden layers. The authors have also proposed a novel approach termed as Regularized Convolutional Layers (Reg-Conv) that can help SDC to regularize the complex convolutional layers. Similarly, Cogswell~ et al. ~ _cite_ propose DeConv loss for CNN architecture that helps in training small databases. One of the major problems with adapting pre-trained CNN models for small sample size problems, as mentioned previously, is large amount of parameters; therefore, insufficient training samples may cause overfitting. If we reduce these parameters to a significantly small number, then the problem can be addressed in a better way. This paper focuses on two novel ways to develop CNN based feature representation algorithm for small sample size problems: (i) associating ``strength'' parameter to control the effect of each pre-trained filter, and (ii) utilizing a generalizable approach that pre-learns the ``structure" of the filters using small training samples. The proposed architecture is motivated from ScatNet but in place of pre-defined filters, we utilize dictionary learning model to pre-learn the filters. Further, unlike CNN approaches where we update the weights in every iteration, we introduce strength of the filter and update only the strength parameter not the filters. The introduction of `` strength '' of filters significantly reduces the number of parameters to learn (detailed calculations shown later) and therefore avoids overfitting with limited training. Experiments are performed on object classification databases, MNIST _cite_, CIFAR-N _cite_, NORB _cite_, Omniglot _cite_, and a challenging small sample size database of newborn faces _cite_ . Comparison with existing algorithms show that the proposed approach achieves state-of-the-art performance for small sample size problems and significantly reduces the number of parameters to learn/fine-tune.