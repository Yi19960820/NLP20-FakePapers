Structured labeling is a key machine learning problem: structured inputs and 	outputs are common in a wide range of machine learning and computer vision applications _cite_ . The goal of structured labeling is to simultaneously assign labels (from some fixed label set) to individual elements in a structured input. Markov random fields (MRFs) _cite_ and conditional random fields (CRFs) _cite_ have been widely used to model the correlations between the structured labels. However, due to the heavy computational burden in their training and testing/inference stages, MRFs and CRFs are often limited to capturing a few neighborhood interactions with consequent restrictions of their modeling capabilities. Structural SVM methods _cite_ and maximum margin Markov networks (_inline_eq_) _cite_ capture correlations in a way similar to CRFs, but they try to specifically maximize the prediction margin; these approaches are likewise limited in the range of contexts, again due to associated high computational costs. When long range contexts are used, approximations are typically used to trade between accuracy and efficiency _cite_ . Other approaches to capture output variable dependencies have been proposed by introducing classifier cascades. For example, cascade models _cite_ in the spirit of stacking _cite_, are proposed to take the outputs of classifiers of the current layer as additional features for the next classifiers in the cascade. Since these approaches perform direct label prediction (in the form of functions) instead of inference as in MRFs or CRFs, the cascade models _cite_ are able to model complex and long-range contexts. Despite the efforts in algorithmic development with very encouraging results produced in the past, the problem of structured labeling remains a challenge. To capture high-order configurations of the interacting labels, top-down information, or prior offers assistance in both training and testing/inference. The demonstrated role of top-down information in human perception _cite_ provides a suggestive indication of the form that top-down information could play in structured visual inference. Systems trying to explicitly incorporate top-down information under the Bayesian formulation point to a promising direction _cite_ but in the absence of a clear solution. Conditional random fields family models that learn the posterior directly _cite_ alleviates some burdens on learning the labeling configuration, but still with many limitations and constraints. The main difficulty is contributed by the level of complexity in building high-order statistics to capture a large number of interacting components within both short-and long-range contexts. From a different angle, building convolutional neural networks for structured labeling _cite_ has resulted in systems that greatly outperform many previous algorithms. Recent efforts in combining CNN with CRF and RNN models _cite_ have also shed light onto the solution of extending CNN to structured prediction. However, these approaches still rely on CRF-like graph structure with limited neighborhood connections and heavy manual specification. More importantly, the explosive development in modeling data using layers of convolution has not been successfully echoed in modeling the prior in the label space. In this paper, we propose a new structured labeling method by developing convolutional pseudoprior (ConvPP) on the ground-truth labels, which is infeasible by directly learning convolutional kernels using the existing CNN structure. We accomplish our task by developing a novel end-to-end fixed-point network structure using pseudo-likelihood approximation _cite_ to the prior that learns convolutional kernels and captures both the short-and the long-range contextual labeling information. We show state-of-the-art results on benchmark datasets in sequential labeling and popular image labeling.