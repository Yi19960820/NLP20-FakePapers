mage recognition _cite_ is a fundamental problem in image content analysis. It aims to assign a single class label to a given image which typically corresponds to a specific type of visual concept, e.g., the (single) object contained in the image or the visual scene depicted by the image. In the past five years, significant advances have been achieved on large-scale image recognition tasks _cite_, with the latest models being capable of beating humans on these visual recognition tasks. These advances are mainly attributed to the existence of large-scale benchmarks such as ImageNet _cite_ and the deployment of a deep representation learning paradigm based on deep neural networks (DNNs) which learn the optimal representation and classifier jointly in an end-to-end fashion. As a closely related task, image annotation _cite_ aims to describe (rather than merely recognise) an image by annotating all visual concepts that appear in the image. This brings about a number of differences and new challenges compared to the image recognition problem. First, image annotation is a multi-label multi-class classification problem _cite_, instead of a single-label multi-class classification problem as in image recognition. The model thus needs to predict multiple labels for a given images with richer contents than those used in image recognition. The second difference, which is more subtle yet more significant, is on the label types: labels in image annotation can refer to a much wider and more diverse range of visual concepts including scene properties, objects, attributes, actions, aesthetics etc. This is because image annotation benchmark datasets have become larger and larger _cite_ by crawling images and associated noisy user-provided tags from social media websites such as Flickr. These images are uploaded by a large variety of users capturing an extremely diverse set of visual scenes containing a wide range of visual concepts. Finally, in image annotation, the task is to predict not only more than one label, but also a variable number of labels--an image with simple content may require only one or two labels to describe, whilst more complicated content necessitates more labels. In summary, these three differences make image annotation a much harder problem far from being solved. Existing methods on image annotation focus on solving the multi-label classification problem. Two lines of research exist. In the first line, the focus is on modeling the correlation of different labels for a given image (e.g. cow typically co-exists with grass) _cite_ to make the multi-label prediction more robust and accurate. In the second line, side information such as the noisy user-provided tags are used as additional model input from a different modality (i.e., text) _cite_ . These models ignore the variable label number problem and simply predict the top-k most probable labels per image _cite_ . This is clearly sub-optimal as illustrated in Fig.~ _ref_ . More recently the variable label number problem has been identified and a number of solutions have been proposed _cite_ . These solutions treat the image annotation problem as an image to text translation problem and solve it using an encoder-decoder model. Specifically, the image content is encoded using a convolutional neural network (CNN) and then fed into a recurrent neural network (RNN) _cite_ which outputs a label sequence. The RNN model is able to automatically determine the length of the label sequence thus producing variable numbers of labels for different images. However, one fundamental limitation of these existing approaches is that the original training labels are orderless whilst the RNN model requires a certain output label order for training. Existing methods have to introduce artificial orders such as rarest or more frequent label first. This indirect approach thus leads to sub-optimal estimation of the label quantity. Nevertheless the most important difference and challenge, i.e. the rich and diverse label space problem, has never been tackled explicitly. In particular, recent image annotation models use deep learning in various ways: they either directly apply a CNN pretrained on the ImageNet image recognition task to extract features followed by a non-deep multi-label classification model _cite_ ; or fine-tune a pretrained CNN on the image annotation benchmark datasets and obtain both the feature representation and classifier jointly _cite_ . However, all the CNN models used were designed originally for the image recognition task. Specifically, only the final layer feature output is used as input to the classifier. It has been demonstrated _cite_ that features computed by a deep CNN correspond to visual concepts of higher levels of abstraction when progressing from bottom to top layer feature maps, e.g., filters learned in bottom layers could represent colour and texture whilst those in top layers object parts. This means that only the most abstract features were used for the classifier in the existing models. However, as mentioned earlier, for image annotation, the visual concepts to be annotated/labeled have drastically different levels of abstraction; for instance, `grass' and `sand' can be sufficiently described by colour and texture oriented features computed at the bottom layers of a CNN, whist `person', `flower' and `reflection' are more abstract thus requiring features learned from top layers. These image recognition-oriented CNNs, with or without fine-tuning, are thus unsuitable for the image annotation task because they fail to provide rich representations at different abstraction scales. In this paper, we propose a novel multi-modal multi-scale deep neural network (see Fig.~ _ref_) to address both the diverse label space problem and variable label number problem. First, to extract and fuse features learned at different scales, a novel deep CNN architecture is proposed. It comprises two network branches: a main CNN branch which can be any contemporary CNN such as GoogleNet _cite_ or ResNet _cite_ ; and a companion multi-scale feature fusion branch which fuses features extracted at different layers of the main branch whilst maintaining the feature dimension as well as performing automated feature selection. Second, to estimate the optimal number of labels that is needed to annotate a given image, we explicitly formulate an optimum label quantity estimation task and optimize it jointly with the label prediction main task. Finally, to further improve the label prediction accuracy, the proposed model is made multi-modal by taking both the image and noisy user-provided tags as input. Our main contributions are as follows: (N) A novel multi-scale deep CNN architecture is proposed which is capable of effectively extracting and fusing features at different scales corresponding to visual concepts of different levels of abstraction. (N) The variable label number problem is solved by explicitly estimating the optimum label quantity in a given image. (N) We also formulate a multi-modal extension of the model to utilize the noisy user-provided tags. Extensive experiments are carried out on two large-scale benchmark datasets: NUS-WIDE _cite_ and MSCOCO _cite_ . The experimental results demonstrate that our model outperforms the state-of-the-art alternatives, often by a significant margin.