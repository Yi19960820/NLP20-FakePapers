years have witnessed the marked progress of deep learning. Since the breakthrough in N ImageNet competition~ _cite_ achieved by AlexNet~ _cite_ using five convolutional layers and three fully connected layers, a series of more advanced deep neural networks have been developed to keep rewriting the record,, VGGNet~ _cite_, GoogLeNet~ _cite_, and ResNet~ _cite_ . However, their excellent performance requires the support from a huge amount of computation. For instance, AlexNet~ _cite_ contains about N million parameters and needs _inline_eq_ multiplications to process an image with resolution of _inline_eq_ . Hence, the potential power of deep neural networks can only be fully unlocked on high performance GPU servers or clusters. In contrast, majority of the mobile devices used in our daily life usually have rigorous constraints on the storage and computational resource, which prevents them from fully taking advantages of deep neural network. As a result, networks with smaller hardware demanding while still maintaining similar accuracies are of great interests to image processing and computer vision community. Compressing convolutional neural networks can be achieved by vector quantization~ _cite_, decomposing weight matrices~ _cite_, and encoding with hashing tricks~ _cite_ . Unimportant weights can be pruned to achieve the same goal by removing the subtle weights~ _cite_, reducing the redundancy between weights in the frequency domain~ _cite_, and using the binary networks~ _cite_ . Another straightforward approach is to design a compact network directly,, ResNeXt~ _cite_, Xception network~ _cite_, and MobileNets~ _cite_ . These networks are often deep and thin with fewer parameters in each layer, and the non-linearity of these networks are strengthened by increasing the number of layers, which guarantees the performance of the network. Student-teacher learning framework, introduced in knowledge distillation (KD) ~ _cite_, is one of the most popular approaches to realize model compression and acceleration~ _cite_ . Taking a heavy neural network, such as GoogleNet~ _cite_ or ResNet~ _cite_, that has already been well trained with massive data and computing resources as the teacher network, a student network of light architecture can be better learned under teacher's guidance. To inherit the advantages of teacher networks, different methods have been proposed to encourage the consistency between teacher and student network. For example, Ba and Caruana~ _cite_ minimized the Euclidean distance between features extracted from these two networks, Hinton ~ _cite_ encouraged the student to mimic a softened version of the teacher's output, and FitNet~ _cite_ introduced intermediate-level hints from teacher's hidden layers to guide the training process of student. Patrick and Nikolaus~ _cite_ proposed to keep the pairwise distance of examples between student network and teacher. You ~ _cite_ utilized multiple teacher networks to guide the training process of student network. Wang ~ _cite_ introduced a teaching assistant to encourage the similarity between distributions of features maps extracted from teacher and student networks. These aforementioned algorithms have achieved impressive experimental results, however, they were mainly developed in ideal scenarios, where all data are implicitly assumed to be clean. In practice, given examples with perturbation, the training process of the network can be seriously influenced, and the resulting network would not be confident as before to make predictions of examples. Teacher network might make some mistakes, since it is difficult for teacher network to be familiar with all examples fed into the student network. This is consistent with student-teacher learning in the real world. An excellent student is expected to solve practical problems in changeable circumstances, where there might be questions even not known by teachers. To solve this problem, in this paper, we introduce a robust teacher-student learning algorithm. The framework of the proposed method is illustrated in . We enable student network to be more confident on its prediction with the help of teacher network. Perturbations on examples might seriously influence the learning of student network. We derive the lower bound of the perturbations that can make student be more vulnerable than teacher through a rigorous theoretical analysis. New objectives in terms of prediction scores and gradients of examples are further developed to maximize the lower bound of the required perturbation. Hence, the overall robustness of the student network to resist perturbations on examples can be improved. Experimental results on benchmark datasets demonstrate the superiority of the proposed method for learning compact and robust deep neural networks. We organized the rest of the paper as follows. In Section II, we summarize related works on learning convolutional neural networks with fewer parameters by different methods. Section III introduces the previous work we based on. In Section IV, we formally introduce our robust student network learning method in detail, including mathematical proof to the proposed theorem, the calculation method of loss function, and the training strategy. Section V provides results of our algorithm obtained on various benchmark datasets to prove the effectiveness of the proposed method. Section VI concludes this paper.