Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition~ and computer vision, notably object recognition from images~ . More recently, deep learning is making important strides in natural language processing, especially statistical machine translation~ . Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of N to N-fold, starting with~, and similar improvements with distributed training~ . Indeed, the ability to train larger models on more data has enabled the kind of breakthroughs observed in the last few years. Today, researchers and developers designing new deep learning algorithms and applications often find themselves limited by computational capability. This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks~ . Most of the computation performed during training and application of deep networks regards the multiplication of a real-valued weight by a real-valued activation (in the recognition or forward propagation phase of the back-propagation algorithm) or gradient (in the backward propagation phase of the back-propagation algorithm) . This paper proposes an approach called BinaryConnect to eliminate the need for these multiplications by forcing the weights used in these forward and backward propagations to be binary, i.e. constrained to only two values (not necessarily N and N) . We show that state-of-the-art results can be achieved with BinaryConnect on the permutation-invariant MNIST, CIFAR-N and SVHN. What makes this workable are two ingredients: The main contributions of this article are the following.