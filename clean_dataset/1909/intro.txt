excel at tasks in the realm of image classification (e.g. _cite_) . However, from a probability theory perspective, it is unjustifiable to use single point-estimates as weights to base any classification on. with frequentist inference require substantial amounts of data examples to train on and are prone to overfitting on datasets with few examples per class. \newline In this work, we apply Bayesian methods to in order to add a measure for uncertainty and regularization in their predictions, respectively their training. This approach allows the network to express uncertainty via its parameters in form of probability distributions (see Figure _ref_) . At the same time, by using a prior probability distribution to integrate out the parameters, we compute the average across many models during training, which gives a regularization effect to the network, thus preventing overfitting. \newline We build our Bayesian upon Bayes by Backprop _cite_, and approximate the intractable true posterior probability distributions _inline_eq_ with variational probability distributions _inline_eq_, which comprise the properties of Gaussian distributions _inline_eq_ and _inline_eq_, denoted _inline_eq_, where _inline_eq_ is the total number of parameters defining a probability distribution. The shape of these Gaussian variational posterior probability distributions, determined by their variance _inline_eq_, expresses an uncertainty estimation of every model parameter. The main contributions of our work are as follows: This paper is structured as subsequently outlined: after we have introduced our work here, we secondly review Bayesian neural networks with variational inference, including previous works, an explanation of Bayes by Backprop and its implementation in . Thirdly, we examine aleatoric and epistemic uncertainty estimations with an outline of previous works and how our proposed method directly connects to those. Fourthly, we present our results and findings through experimental evaluation of the proposed method on various architectures and datasets before we finally conclude our work.