With the rapid development of the Internet, the amount of images grows rapidly. The large-scale image retrieval has attracted increasing interest. Hashing methods that encode images into binary codes have been widely studied since the compact binary codes are suitable for fast search and efficient storage. There are a multitude of hashing methods in the literature~ _cite_ . Among these methods, the supervised information is given with triplet labels, which have been shown to be most effective since hashing is actually a ranking problem~ _cite_ . In these works, the triplet ranking loss is defined to learn binary codes that preserve relative similarity relations. In _cite_, an architecture based on deep convolutional neural networks (CNNs) with triplet ranking loss is proposed for image retrieval. In _cite_, it presents a deep semantic ranking based method to learn hash functions that preserve multi-level semantic similarity between multi-label images. The FaceNet~ _cite_ also uses the triplet ranking loss for face recognition and clustering. Due to the huge number of triplets, a collaborative two-stage approach~ _cite_ is employed to reduce the training complexity of the triplet-based deep binary embedding networks. Not all triplets are of equal importance. In _cite_, it finds that the triplet loss relatively quickly learns to correctly map most trivial triplets, which makes a large fraction of all triplets uninformative after some point. Thus, the loss decreases quickly at the beginning and slows down drastically after some point~ _cite_ . For instance, the triplet _inline_eq_ is easier than the triplet _inline_eq_ in which three images are from the fine-grained bird database. Intuitively, the hash model which was told over and over again that bird and dog are dissimilar cannot further improve the performance. Hence, up-weighting the informative triplets and down-weighting the uninformative triplets become a crucial problem. However, most of the existing hashing methods treat the triplets equally~ _cite_ . Few works select the hard triplets based on the loss~ _cite_ . For instance, semi-hard negative mining~ _cite_ is proposed in FaceNet. It uses all anchor-positive pairs in a mini-batch and selects the negative examplars that are further away from the anchor than the positive examplar. ~ _cite_ investigated to select top _inline_eq_ hard negative triplets with highest losses and the other triplets are ignored. In summary, all existing methods use the loss to select the hard triplets or treat them equally, and totally ignore the order relations in the rank list, which is important in retrieval task . Since hashing problem is a ranking problem, the losses in the rank lists might not be sufficiently accurate than the order relations. Inspired by that, we propose an order-aware reweighting method for triplet-based deep binary embedding networks, which up-weights the informative triplets and down-weights the uninformative triplets. We firstly introduce a weighting factor for each triplet. In practice, the weighting factor can be set to the value that indicates how the triplet is misranked by the current hash model. Hence, we use the MAP (mean average precision), which is a widely used evaluation measure, to calculate the weights. Specifically, for each mini-batch in the training phase, we encode the images into binary codes via deep CNNs. For an arbitrary triplet with an anchor, a positive code and a negative code, we rank all binary codes, including the positive code and the negative code, in the mini-batch according to their Hamming distances to the anchor. The weight of this triplet is defined as the change of MAP by only swapping the rank positions of the positive code and the negative code. Besides this order-aware weighting factor, we further use the squared triplet loss instead of the linear form, which up-weights hard triplets and down-weights easy ones from the perspective of the order relation of binary codes in triplets themselves. We conduct extensive evaluations on four benchmark datasets for image retrieval. The empirical results show that the proposed method achieves significant performance over the baseline methods.