Text recognition in natural scene is an important problem in computer vision. However, due to the enormous appearance variations in natural images, e.g. different fonts, scales, rotations, illumination conditions, it is still quite challenging. Identifying the position of a character and recognizing it are two interdependent problems. Straight-forward methods treat the task as separate character segmentation and recognition _cite_ . This paradigm is fragile in unconstrained natural images for it's difficult to deal with low resolution, low contrast, blurring, large diversity of text fonts and highly complicated background clutters. Due to the shortcoming of these methods, algorithms combining segmentation and recognition were proposed. GMM-HMMs are the mostly used models, especially in speech and handwriting communities _cite_ . In this paradigm, scene text images are transformed to frame sequences by sliding window. GMMs are used for modeling frame appearance and HMMs are used to infer the target labels of the whole sequence. _cite_ The merit of this method is avoiding the need of fragile character segmentation. However HMMs have several obvious shortcomings, e.g. lacking of long context consideration, improper independent hypothesis etc. As the developments of deep neural networks (DNNs) flourishing, convolutional neural networks (CNNs) have been used to form the hybrid CNN-HMM model _cite_, replacing GMMs as the observation model. The model generally performs better than the GMM-HMM model thanks to the strong representation capacity of CNN, however still doesn't eliminate the issues with HMM. \IEEEpubidadjcol In this work, we address the issues of HMMs while keeping the algorithm free of segmentation. The novalty of our method is using Recurrent Neural Network (RNN), which has the ability of adaptively retaining long-term dynamic memory, as the sequence model. We combine CNN with RNN to utilize their representation abilities on different aspects. RNN is a powerful connectionist model for sequences. Comparing with static feed-forward networks, it introduces recurrent connections enabling the network to maintain an internal state. It doesn't make any hypothesis on the independence of inputs, so each hidden unit can take into account more input information. Specifically, LSTM memory blocks are used enabling RNN to retain longer range of inter-dependences of input frames. Another virtue of using RNN as the sequence model is the ease of build an end-to-end trainable system directly trained on the whole image without needing explicit segmentation. The main weakness of RNN is its feature extraction capability. To alleviate the shortcomings of both HMM and RNN, we propose a novel end-to-end sequence recognition model named Convolutional Recurrent Neural Network (CRNN) . The model is composed with hierarchical convolutional feature extraction layers and recurrent sequence modeling layers. CNN is good at appearance modeling and RNNs have strong capacity for modeling sequences. The model is trained with Connectionist Temporal Classification (CTC) _cite_ object which enables the model directly learned from images without segmentation information. Our idea is motivated by observing the complementary modeling capacities of CNN and RNN, and inspired by recent success applications of LSTM architectures to various sequential problems, such as handwriting _cite_, and speech recognition _cite_, image description _cite_ . The whole architecture of our model is illustrated in _ref_ .