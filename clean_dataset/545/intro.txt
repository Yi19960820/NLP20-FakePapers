In order for autonomous systems to move out of the controlled confines of labs, they must acquire the ability to understand the cluttered indoor environments they will inevitably encounter. While many researchers have addressed the problems of pose estimation, object detection, semantic segmentation, and object classification separately, comprehensive understanding of scenes remains an elusive goal. To this end, in this work we propose an architecture which is able to perform all of the above tasks in concert using a single artificial neural network. Classification in cluttered indoor scenes can be extremely challenging, especially when trying to classify instances of objects which have never been observed before. Considering only ND color information only compounds this problem, as clutter can easily cause vast changes in the visible signature of otherwise distinguishable items. ND geometric features, on the other hand, tend to be less susceptible to clutter and have (especially for furniture) geometric features which generalize well across the class. As such, in this work we use ND geometric features in addition to standard RGB channels. Pose estimation in-the-wild is another difficult problem, as it requires estimating pose for object instances which have never been observed before. For example, consider the task of helping a human to sit down in a chair-to be of any help, one must be able to determine pose of the back-rest, the seat area, and the supporting legs-even on types of chairs that one has never seen before. In this work we will show that just such a task is possible, to a surprising degree of accuracy, using a wide, deep, multi-stage CNN trained on synthetic models. In fact, it is possible to do so even with wholly unobserved types of chairs-for example, in Fig.~ _ref_, none of the chair models were seen in training. Moreover, we shall demonstrate that it is possible to estimate such poses even in complex cluttered scenes containing many classes of furniture (\eg see Fig. _ref_) . Our approach, outlined in Fig. _ref_, uses a relatively complex CNN architecture to solve our three sub-tasks; class-, pose-, and position-estimation of objects, concurrently. One unusual aspect of our network is that it recombines class output back into the network layers which calculate pose and position, allowing the network to accurately determine pose for multiple classes within a single architecture. Furthermore, we are able to train this large network by using synthetic rendered RGB-D scenes consisting of randomly placed instances from a dataset of thousands of ND object models. Our training scenes are generated on the fly on the CPU and a secondary GPU as we train on the primary GPU, allowing us to have a training set of virtually unlimited size at a completely hidden computational cost. Finally, we use a small number of transfer learning iterations using a small set of real annotated images to adapt our network to the modality of real indoor RGB-D scenes. To demonstrate the effectiveness of our approach, we perform a variety of experiments on both synthetic and real scenes. Our pose estimation and classification results outperform existing methods on a difficult real dataset. We also present qualitative and quantitative results on both real and synthetic data which demonstrate the capability of our system to distill semantic understanding of scenes. Moreover, we do these tasks jointly in a single forward pass through our network, allowing us to produce results significantly faster than existing methods. As we propose to solve multiple problems in tandem in this work, there is a substantial body of work which could be considered related. We will restrict ourselves to those recent works which deal exclusively with RGB-D data and/or use CNNs to accomplish one or more of our sub-tasks. As a first step in a pipeline to parse full scenes, the image is typically broken down into small ``object proposals'' to be considered by other methods. For example, in Silberman \etal _cite_ they perform an over-segmentation, and then iteratively merge regions using classifiers which predict whether regions belong to the same object instance. These are then classified using an ensemble of features with a logistic regression classifier. Couprie \etal _cite_ take a different approach, instead using a multi-scale CNN to classify the full image, and then use superpixels to aggregate and smooth prediction outputs. While this allows them to extract a per-pixel semantic segmentation, they fail to achieve very high scores in important classes, such as table and chair. Hariharan \etal _cite_ also predict pixel-level class associations, but classify region proposals instead of the full image. They also use a CNN as a feature extractor on these regions, before classifying into categories with an SVM and aggregating onto a coarse mask. They then use a second classifier stage on this coarse mask projected on to superpixels to extract a detailed segmentation. While these results are interesting, we question the overall utility of such a fine grained segmentation, as it does not provide pose with respect to a class-level representation. Song and Xiao _cite_ use renderings of ND models from many viewpoints to obtain synthetic depth maps for training an ensemble of Exemplar-SVM classifiers. They use a ND sliding window to obtain proposals during testing and perform non-maximum suppression to obtain bounding boxes. While this ND sliding window approach is able to handle occlusions and cluttered scenes well, it is very expensive (tens of minutes per image), requiring testing of many windows on many separate detector classifiers. Guo and Hoiem _cite_ predict support surfaces (such as tables and desks) in single view RGB-D images using a bottom up approach which aggregates low-level features (\eg edges, voxel occupancy) . These features are used to propose planar surfaces, which are then classified using a linear SVM. While they provide object-class pose annotations for the NYUvN set which we use in this paper, they do not classify objects or their pose themselves. Object detection in RGB-D is addressed directly by Gupta \etal _cite_ using a CNN which classifies bounding-box proposals in a room-centric embedding. As with other approaches, they use superpixels to aggregate their classifier results in order to get class instance segmentations. Lin \etal _cite_ use candidate cuboids, rather than bounding boxes, and classify them using a CRF approach. While they achieve good overall classification performance, they merge similar classes (such as table and desk), and while their cuboids give them spatial extent of objects, they do not give pose. In contrast to the above methods, we do not need expensive and difficult to obtain annotated ground truth data for training. Instead, we use synthetic renderings of scenes containing ND models pulled from the Internet. While these models need to be aligned to a common pose, this is a relatively inexpensive operation which has already been performed in the ModelNetN database _cite_ . The only other work to address pose directly, that of Gupta \etal~ _cite_, suffers from using unrealistic training data-training instances are single objects rendered in empty space. In contrast, our synthetic data is cluttered and contains realistic noise, as we use a camera model which closely replicates Kinect-like sensors. Because of this, our trained networks are far more effective on real data-we test on the full NYU dataset, while they must leave out instances that have many (_inline_eq_ N \%) missing depth pixels. Additionally, since we work with full scenes rather than single object instances, our model is trained on and can thus handle inter-object occlusions, rather than only self-occlusions. Moreover, their network contains separate top-level layers for each object class, while we only need a single output network for pose for all classes. Their method is also computationally demanding, requiring about a minute per image per class, while ours runs in a few seconds for all classes.