Dictionary learning and sparse representation (DLSR) is one of the most successful mathematical models, which has led to state-of-the-art results in various applications such as face recognition~ _cite_, image denoising~ _cite_, texture classification~ _cite_, and emotion recognition~ _cite_ . DLSR, however, was originally proposed in an unsupervised setting~ _cite_ . The main objective function in the optimization problem related to DLSR is to minimize the reconstruction error between the original signal and the reconstructed one in the space of learned dictionary without including the information on class labels into the learning process. To formally describe the original DLSR formulation, we suppose that there is a finite set of data samples denoted as _inline_eq_, where _inline_eq_ is the dimensionality of the data and _inline_eq_ is the number of data samples. In original DLSR, the data is decomposed using a few dictionary atoms by optimizing the empirical cost function where _inline_eq_ is a dictionary of _inline_eq_ atoms, _inline_eq_ are the sparse coefficients and _inline_eq_ are loss functions. In the literature of the DLSR, the reconstruction error, in mean-squared sense, between the original signal and the reconstructed signal is the most common loss function, which is usually regularized by the _inline_eq_ norm to induce sparsity into the coefficients. Thus, the formulation in~ (_ref_) can be written as where _inline_eq_ is the th column of _inline_eq_ . In order to avoid arbitrarily large values for _inline_eq_ and consequently, arbitrarily small values for _inline_eq_, we need an additional constraint on the dictionary atoms to limit their _inline_eq_ norm to be smaller than or equal to one. The complete optimization problem in~ (_ref_) after adding this constraint is as follows: The original DLSR formulation given in~ (_ref_) is unsupervised as the category information has not been taken into consideration in the optimization problem. However, in a supervised learning paradigm, where the ultimate goal is the classification of the data, this setting may not lead to an optimal discriminative dictionary nor coefficients. A more recent attempt in the literature was to incorporate the class labels into the learning of the dictionary and/or coefficients (refer to~ _cite_ for a review) . This modification resulted in a new category of DLSR, namely called supervised dictionary learning and sparse representation (S-DLSR) . Improvements (some significant) over unsupervised DLSR have been reported in the literature for the classification tasks~ _cite_ . Although S-DLSR benefits from the side information available from category information to learn a more discriminative dictionary, unfortunately, gathering labeled data is often very expensive and time consuming. Most data available is unlabeled and the sample size of the labeled data is often very small, which has a hindering effect on the discriminative quality of the learned dictionary. Semi-supervised learning (SSL) methods can potentially boost the performance of a machine learning system by utilizing both supervisory information and global data distribution. Using a large amount of unlabeled data, which is usually easily accessible, can improve revealing the manifold global distribution~ _cite_, and compensate for the small sample size of labeled data~ _cite_ . In this paper, a semi-supervised dictionary learning and sparse representation (SS-DLSR) based on Hilbert-Schmidt independence criterion (HSIC) is proposed. The proposed SS-DLSR approach finds a dictionary based on two criteria: first, the maximization of the dependency between the labeled data and the corresponding category information, and second, minimization of the distances between the unlabeled data and their nearest labeled data. The first criterion guarantees finding the space of maximum discrimination based on the information in the category information and labeled data, whereas the second criterion, guarantees that the unlabeled data remain as close as possible to their nearest-neighbor labeled data. Therefore, the learned dictionary (the projection directions computed by using the aforementioned criteria) benefits from the discriminative power of the category information in the labeled data and proximity information of the unlabeled data as an indication of global manifold distribution. the sparse coefficients are subsequently computed in the space of learned dictionary using the formulation given in~ (_ref_) .