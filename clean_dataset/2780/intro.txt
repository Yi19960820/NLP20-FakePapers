Multi-stage visual pipelines for learning feature representations of images have recently proven valuable for classifying small objects from a variety of lighting conditions, scales, and poses. An effective variant of these was explored in experiments by _cite_ that compared features learned and encoded by stages that originate in encoder-decoder networks, deep learning _cite_, and bags of features models _cite_ . This architecture partitions images into patches to perform local learning of an over-complete codebook and uses this codebook to form global representations of the images for classification. Intermediate or mid-level representations are of high-dimensionality and retain the spatial structure of the image. Pooling is an integral late stage that performs the same role as the sub-sampling layer of a convolutional neural network (CNN): reduction in the final number of features (passed to the classifier or next layer) by an aggregation operation meant to improve invariance to small changes _cite_ . Unfortunately, each additional stage of the architecture often adds hyper-parameters for model selection that must be explored. For the pooling layer the number of pools, their structure or spatial layout, weights within this region, and operator (often max, average, or _inline_eq_-norm) are hyper-parameters that are frequently chosen by rules of thumb. Recently, _cite_ explored pool selection by optimization over a full training set with an over-complete number of pools and achieved excellent improvements over standard pyramid models, although their method uses a feature selection stage with retraining to tractably create a classifier. In this paper we present a method for learning pooling maps with weight parameters that may optimize or tune the feature space representation for better discrimination through stochastic gradient descent. This converts two of the model choices above to parameters which may be learned from a limited stream of labeled training data. Back-propogation through the architecture in _cite_ is used to obtain the appropriate weight updates. This technique stems back at least to the inception of convolutional neural networks and graph transformer networks _cite_, where each module or layer of the network may be utilized in a forward pass output calculation and backward pass parameter update.