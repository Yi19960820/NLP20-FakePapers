A style transfer method takes a content and a style image as inputs to synthesize an image with the look from the former and feel from the latter. In recent years, numerous style transfer methods have been developed. The method by Gatys et al.~ _cite_ iteratively optimizes content and style reconstruction losses between the target image and input images. To reduce the computational cost, a few approaches have since been developed based on feed-forward networks~ _cite_ . However, these approaches cannot generalize to arbitrary style images with a single network. For universal style transfer, a number of methods explore the second order statistical transformation from reference image onto content image via a linear multiplication between content image features and a transformation matrix~ _cite_ . The AdaIn method is developed~ _cite_ by matching the mean and variance of intermediate features between content and style images. The WCT~ _cite_ algorithm further explores the covariance instead of the variance, by embedding a whitening and coloring process within a pre-trained encoder-decoder module. However, these approaches directly compute these matrices from features. On one hand, they do not explore the general solutions to this problem. On the other hand, such matrix computation based methods can be expensive when the feature has a large dimension. Generalized from these two methods~ _cite_, in this work we present theoretical analysis of such linear style transfer frameworks. We derive the form of transformation matrix and draw connections between this matrix to the style reconstruction objective (squared Frobenius norm of the difference between Gram matrices) widely used in style transfer~ _cite_ . Based on the analysis, we learn the transformation matrix with two light-weighted CNNs. We show that the learning-based transformation matrix can be controlled by different levels of style losses, and is highly efficient. The contributions of this work are: