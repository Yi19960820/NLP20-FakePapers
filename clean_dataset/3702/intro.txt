Whereas off-the-shelf deep models keep finding promising applications, it has been gradually recognized to incorporate the problem structure into the design of deep architectures. Such customized deep architectures can benefit from their problem-specific regularizations, and improve the performance. In particular, there has been a blooming interest in bridging sparse coding _cite_ and deep models. Starting from _cite_, many work _cite_, _cite_, _cite_, _cite_ leveraged similar ideas on fast trainable regressors, and constructed feed-forward network approximations to solve the variants of sparse coding models. Lately, _cite_ demonstrated both theoretically and empirically that a trained deep network is potentially able to recover _inline_eq_-based sparse representations under milder conditions. The paper proceeds along this direction to embed sparsity regularization into the target deep model, and simultaneously exploits the structure of model parameters into the design of the model architecture. Up to our best knowledge, it is the first principled and unified framework, that jointly sparsifies both learned features and model parameters. The resulting deep feed-forward network, called deep double sparsity encoder (DDSE), enjoys a compact structure, a clear interpretation, an efficient implementation, and competitive performance, as verified by various comparison experiments.