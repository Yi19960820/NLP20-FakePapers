Many real-world applications require the estimation of human body joints for higher-level tasks, \eg, human behaviour understanding, medical physical therapies or human-computer interaction . This problem is known in the literature as human pose estimation (HPE) . The goal of HPE is to estimate the position and orientation of body limbs in single images or video sequences. Note that a body limb (\eg, lower-arm) is usually defined by two joints (\eg, wrist and elbow) . So, detecting body joints is equivalent to estimate the pose of their respective body limbs. Although great effort has been put into solving this problem in previous years, it is far from solved. The main challenge associated with this task is that human body is highly deformable and suffers from self-occlusions (\ie~one body limb may occlude partially or completely other) . In addition, the vast variety of people clothing and camera viewpoints make this problem even more difficult. HPE has been classically addressed by using either single RGB images~ or multiple cameras . In recent years, depth sensors, like the Microsoft Kinect device, have become affordable and, therefore, popular. These devices provide for each image point its distance to the sensor, \ie~its depth . Depth can be used as a rough estimation of the ND position of the image points, thus, it can help to disambiguate relative positions between body parts. In this work, we propose the use of depth maps for ND human pose estimation, using either single or multiple cameras. Our model, named Deep Depth Pose (DDP), receives as input a depth map containing a person and a set of predefined ND prototype poses and returns the ND position of the body joints of the person. In particular, DDP computes the specific weights needed to linearly combine the prototypes for the given input (see Fig.~ _ref_) . DDP is defined as a Convolutional Neural Network (ConvNet) that computes the specific weights needed to linearly combine the prototypes for the given input depth map. For that purpose, a suitable architecture and loss function have been defined. If multiple camera viewpoints are available, our system is able to fuse their data in order to provide a more accurate estimation of the body joint locations. We have thoroughly evaluated our model on `ITOP' _cite_ and `UBCNV' datasets, establishing a new state-of-the-art on both datasets. For `ITOP' of _inline_eq_ of ` accuracy at Ncm ' for both frontal and top views, versus the previous _inline_eq_ and _inline_eq_ reported by the authors of the dataset. And, a _inline_eq_ of ` accuracy at Ncm ' in `UBCNV' versus the previous _inline_eq_ reported by the authors of the dataset. The rest of this paper is organized as follows. After presenting the related work, the proposed model is described in Sec.~ _ref_ . Then, in Sec.~ _ref_ the dataset used for evaluating our model is described. The experimental results are presented in Sec.~ _ref_ . Finally, Sec.~ _ref_ concludes the paper.