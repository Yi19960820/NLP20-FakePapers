In a social photo sharing service such as Flickr, Pinterest or Instagram, a new word can emerge at any moment, and even the same word can change its semantics and transforms our vocabulary set at any time. For instance, the word wicked (literally means evil or morally wrong) is often used as a synonym of really among teenagers in these recent years-``Wow, that game is wicked awesome!" . In such a dynamic environment, how can we discover emerging visual concepts and build a visual classifier for each concept without a concrete dataset? It is unrealistic to manually build a high-quality dataset for learning every visual concepts for every application domains, even if some of the difficulty can be mitigated by the human-in-the-loop approach~ _cite_ . All we have are the observations but not definitions, provided in the form of co-occurring words and images. In this paper, we consider an automatic approach to learn visual attributes from the open-world vocabulary on the Web. There have been numerous attempts of learning novel concepts from the Web in the past~ _cite_ . What distinguishes our work from the previous efforts is in that we try to understand potentially-attribute words in terms of perception inside deep neural networks. Deep networks have demonstrated outstanding performance in object recognition~ _cite_, and successfully applied to a wide range of tasks including learning from noisy data~ _cite_ or sentiment analysis~ _cite_ . In this paper, we focus on the analysis of neural activations to identify the degree of being visually perceptible, namely visualness of a given attribute, and take advantage of the layered structure of the deep model to determine the semantic depth of the attribute. We collect two domain-specific datasets from online e-commerce and social networking websites. We study in domain-specific data rather than trying to learn general concept on the Web~ _cite_ to isolate contextual dependency of attributes to object categories. For example, the term red eye can refer to an overnight airline flight or an eye that appears red due to illness or injury. This contextual dependency can cause an ambiguity for the visual classifier (red classifier) ; i.e., sense disambiguation. In this paper, we use a single-domain dataset to reduce such a semantic shift to study consistent meaning of attributes under the fixed context~ _cite_ . We show that, using a trained neural network, we are able to characterize a visual attribute word by the divergence of neural activations in the weakly-annotated data. Figure _ref_ illustrates our framework. Our approach starts by cleaning the noisy Web data to find potentially visual attributes in the dataset, then splits the data into positive and negative sets. Using a pre-trained neural network, we identify highly activating neurons by KL divergence of activations. We show that we can use the identified neurons (prime units) for N) learning a novel attribute classifier that is close to human perception, N) understanding perceptual depth of the attribute, and N) identifying attribute-specific saliency in the image. We summarize our contributions in the following.