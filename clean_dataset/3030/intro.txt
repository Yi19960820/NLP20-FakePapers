The safe and effective operation of an autonomous vehicle depends on its ability to interpret its surroundings and track and predict the state of the environment over time. Many tracking systems employ multiple hand-engineered stages (e.g. object detection, semantic classification, data association, state estimation and motion modelling, occupancy grid generation) in order to represent the state and evolution of the world (_cite_) . However, as the tasks assigned to robots become more complex, this approach becomes increasingly infeasible. Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications (_cite_) . Such approaches would however typically require large, task-specific corpora of annotated ground-truth labels to master the desired task. This becomes difficult when learning a model of the environment without access to corresponding ground truth, as is often the case for object tracking in crowded urban environments. In recent work, ~ _cite_ took an alternative approach and presented an end-to-end fully and efficiently trainable framework for learning a model of the world dynamics, building on the original work by~ _cite_ . We considered the specific problem of learning to track and classify moving objects in a complex and only partially-observable real-world scenario, as viewed from a sensor. Here, we advance this work and address the problem of tracking from a platform. We extend the neural network architecture proposed in~ _cite_ to account for the egomotion of the sensor frame as it moves in the world frame, and demonstrate improved tracking accuracy as compared to previous work. We demonstrate the system on laser point cloud data collected in a busy urban environment, with an array of static and dynamic objects, including cars, buses, pedestrians and cyclists. The model not only bypasses the sequence of hand-engineered steps typical of traditional tracking approaches, but is empirically shown to successfully predict the future evolution of objects in the environment, even when they are completely occluded. The rest of the paper is structured as follows. Section~ _ref_ highlights related work and Section~ _ref_ summarises the problem definition and framework first presented in~ (_cite_) . Section~ _ref_ describes the models used to perform tracking in real-world scenarios considering both static and dynamic sensors. Section~ _ref_ presents an empirical evaluation of our methods, and Section~ _ref_ concludes the paper and discusses the future implications of our findings.