\noindent Deep learning has been highly successful in tackling many machine learning problems in vision, speech, text and many other areas. The Convolutional Neural Networks~ (CNNs) represent a deep neural network architecture that is the de facto standard for many image recognition tasks today. While CNNs are very effective in many applications, training of CNNs requires a large number of labelled examples as well as a large amount of computing resources. Transfer Learning is a popular approach suitable for improving the efficiency of learning~ _cite_ . A simple transfer learning protocol for CNNs would be as follows. We start with a CNN that was previously trained on a related task and then finetune the weights using the training examples for the new target task. Finetuning of weights involves learning weights using the same learning algorithm. However, this way of initializing the weights of the target CNN using the weights of a previously learnt CNN (rather than initializing the weights randomly) results in more efficient learning both in terms of number of training epochs and the number of training examples needed. There are many ways in which this basic idea of transfer learning can be improved. The transfer learning as outlined above essentially amounts to only a good heuristic for initializing the weights. It is more interesting to see if some aspects of what is learnt earlier can be directly used in the new target task. We may have learnt many CNNs previously on different tasks, all of which are somewhat related to the new target task. Hence it would be nice to have a generic mechanism whereby one can transfer knowledge (weights) from multiple learnt CNNs to a new target net. In the normal transfer learning method, the architecture of the target net has to be same as (or very similar to) that of the previously learnt CNN. Transfer learning can be much more effective if this constraint can be removed. One of the main motivations for deep networks, in general, and CNNs in particular, is that we automatically learn the relevant features from the data. The filters in the convolutional layers of a CNN essentially represent such learnt features. It is reasonable to expect that there would be a generic set of features that should be useful in a host of visual pattern recognition tasks. Given a set of already learnt CNNs, the learnt features should be useful in new tasks also. Thus, ideally, transfer learning should be a mechanism for reusing previously learnt features for a new task (rather than being a heuristic for initializing weights) . In this paper we propose a mechanism that we call bank of filter-trees ~ (BFT) which can potentially address all the issues outlined above. The term filter-tree, refers to a tree or a subnetwork that we associate with a filter in any convolutional layer of a CNN. The filter-tree of a filter in layer-_inline_eq_ of a CNN contains the filter itself along with all the filters that effectively connect to it from all the preceding _inline_eq_ layers. Given any (learnt) CNN, we can create a set of such filter-trees corresponding to individual filters in any convolutional layer. If we have a number of pre-learnt CNNs, we can do the same for each CNN and this ensemble of all filter-trees is what we call bank of filter-trees ~ (BFT) . In our method of transfer learning, the elements of this BFT are what are considered the exchangeable elements. Suppose we have a BFT containing each of the layer-k filters of all the learnt CNNs. We can now create a new CNN where layer-k (and hence all the earlier layers) are populated by some selected (or randomly sampled) elements from the BFT and all higher layers would have randomly initialized weights which need to be learnt. (The weights in the subnetwork populated by elements from the BFT are fixed and they are not finetuned for the target task) . This is a mechanism of transfer learning where individual features that we learnt are what are being transferred. The transfer can be effected from several pre-learnt CNNs in a seamless manner. The architecture of the new or target CNN can be different from earlier learnt CNNs. Thus, this idea has the potential to address all issues we outlined earlier. In this paper we formulate the idea of BFT and explain how it can be implemented. We also present empirical investigations which clearly demonstrate the effectiveness and efficiency of this type of transfer learning. To the best of our knowledge, this is the first instance of a transfer learning method where transfer is at a subnetwork level; transfer can be effected from multiple source networks; and, with no finetuning of the transferred weights, the performance is on par with that of normally learnt networks. Transfer learning can be viewed as any method that uses pre-learnt networks for improving efficiency of learning a new task. A straight-forward method is to start with a pre-learnt network (that is, take the learnt weights as the initial weights) and finetune all weights using the training data of the target task, resulting in more efficient learning~ _cite_ . One can also allow for some variation in the architecture. For example, one can start with a learnt network and either add one or more layers on top of it or expand a layer by adding a few randomly initialized nodes to it~ _cite_ . Unlike these, where we start with a single pretrained network, in the method proposed here an ensemble of filter-trees is created by pooling together any number of pre-trained networks. The architecture of the target network can be very different from that of any of the source networks. Also, in our method, the weights transferred are fixed and are not finetuned. There are transfer learning methods reported in literature where previously learnt features are directly used. In~ _cite_, the convolutional layers of a learnt CNN are used as fixed feature extractors for the new task and the training data of the new task, represented as feature vectors like this, is then used for training a new classifier, e.g., an SVM. In our method, the filter-trees represent pre-learnt features; but they come from multiple source networks. Also, since the target network can have one or more convolutional layers on top of the filter-trees, in the new task we can learn useful ways of combining previously learnt features. Some earlier works in transfer learning in neural networks such as ~ _cite_ also use multiple source networks for transfer. However, here, each of these networks is previously trained on a subtask of the target task and then these networks are simply fused together to learn the whole of the target task. Our method based on an ensemble of filter-trees is a much more general method of effecting transfer learning from multiple source networks. Transfer learning has also been used as method to augment the data set of the target task. If the target task has less number of training examples, one can use pre-learnt CNNs as feature extractors to decide which of the patterns from some other task are similar to the training data of the target task and thus increase the number of training examples. (See, for example, _cite_) . Though the previously learnt feature generators are used here to aid in the learning of a new task, it is quite different from the type of transfer learning that is explored in this paper. Another similar transfer learning method, that relies on determining which sets of examples are similar, is reported in~ _cite_ . This paper also talks about a tree structure for transfer learning though it is quite different from our use of filter-tree here. The tree in~ _cite_ refers to tree of object categories which is an externally supplied information which allows one to decide on the distances between different sub-categories of examples. The question of the level or granularity at which learnt weights can be transferred in CNNs has also been investigated. Transfer learning where an entire convolutional layer can be transferred from a source network to a target network is explored in~ _cite_ . The method presented in~ _cite_ is similar to our method and it also uses a bank of filter weights from multiple source networks to effect transfer and they explore transfer learning both at convolutional layer level as well as at the level of individual filters of learnt networks. However, these methods need finetuning of all weights to reach the performance achieved by networks trained from scratch. This is because individual weight vectors of filters in the pre-learnt networks, by themselves, do not contain the full context of the features learnt earlier. The filter-trees that we propose here seem to have the correct granularity for transferring the earlier learnt knowledge of useful features as demonstrated by the results we present here. Our method involves sampling from an ensemble of pre-learnt filters to populate the convolutional layers of a target network and hence it may seem similar to classifier ensemble methods such as bagging or boosting~ _cite_ . However in such methods all networks in the ensemble are trained on the same task whereas in our method, the ensemble of filter trees are created from networks trained on different tasks and the transfer learning is for a new task. This work presented here can be viewed as a step in the direction of creating an {\em universal bank of feature generators} which can be used as a dictionary to realize CNNs across tasks. Several studies in neuroscience have pointed out that representations that brain seems to be using for visual or text stimuli are highly similar across individuals~ _cite_ . This lends support to the existence of some universal set of feature generators that should be useful for classification of sensory signals. The bank of filter-trees proposed here explores this idea in the limited context of transfer learning in CNNs. The rest of the paper is organized as follows. Section~ _ref_ introduces the concept of Bank of filter-trees. Section~ _ref_ describes our experimental setup. Section~ _ref_ discusses the experimental results showing the performance of the BFT based transfer learning and discusses why filter-trees as an unit of transfer (as opposed to individual filters as a unit of transfer) is better. We conclude the paper in Section~ _ref_ .