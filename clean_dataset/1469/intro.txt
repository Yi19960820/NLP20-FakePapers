the breakthrough in image classification using Convolutional Neural Networks (CNN) ~ _cite_, pre-trained CNN models trained for one task (e.g., recognition or detection) have also been applied to domains different from their original purposes (e.g., for describing texture~ _cite_ or finding object proposals~ _cite_) . Such adaptations of pre-trained CNN models, however, still require further annotations in the new domain (e.g., image labels) . In this paper, we show that for fine-grained images which contain only subtle differences among categories (e.g., varieties of dogs), pre-trained CNN models can both localize the main object and find images in the same variety. Since no supervision is used, we call this novel and challenging task . In fine-grained image classification~ _cite_, categories correspond to varieties in the same species. The categories are all similar to each other, only distinguished by slight and subtle differences. Therefore, an accurate system usually requires strong annotations, e.g., bounding boxes for object or even object parts. Such annotations are expensive and unrealistic in many real applications. In answer to this difficulty, there are attempts to categorize fine-grained images with only image-level labels, e.g., ~ _cite_ . In this paper, we handle a more challenging but more realistic task, i.e., Fine-Grained Image Retrieval (FGIR) . In FGIR, given database images of the same species (e.g., birds, flowers or dogs) and a query, we should return images which are in the same variety as the query, without resorting to any other supervision signal. FGIR is useful in applications such as biological research and bio-diversity protection. As illustrated in Fig.~ _ref_, FGIR is also different from general-purpose image retrieval. General image retrieval focuses on retrieving near-duplicate images based on similarities in their contents (e.g., textures, colors and shapes), while FGIR focuses on retrieving the images of the same types (e.g., the same species for the animals and the same model for the cars) . Meanwhile, objects in fine-grained images have only subtle differences, and vary in poses, scales and rotations. To meet these challenges, we propose the Selective Convolutional Descriptor Aggregation (SCDA) method, which automatically localizes the main object in fine-grained images and extracts discriminative representations for them. In SCDA, only a pre-trained CNN model (from ImageNet which is not fine-grained) is used and we use absolutely no supervision. As shown in Fig.~ _ref_, the pre-trained CNN model first extracts convolution activations for an input image. We propose a novel approach to determine which part of the activations are useful (i.e., to localize the object) . These useful descriptors are then aggregated and dimensionality reduced to form a vector representation using practices we propose in SCDA. Finally, a nearest neighbor search ends the FGIR process. We conducted extensive experiments on six popular fine-grained datasets (~ _cite_, ~ _cite_, ~ _cite_, ~ _cite_, ~ _cite_ and ~ _cite_) for image retrieval. Moreover, we also tested the proposed SCDA method on standard general-purpose retrieval datasets (~ _cite_ and ~ _cite_) . In addition, we report the classification accuracy of the SCDA method, which only uses the image labels. Both retrieval and classification experiments verify the effectiveness of SCDA. The key advantages and major contributions of our method are: Moreover, beyond the specific fine-grained image retrieval task, our proposed method could be treated as one kind of transfer learning, i.e., a model trained for one task (image classification on ImageNet) is used to solve another different task (fine-grained image retrieval) . It indeed reveals the reusability of deep convolutional neural networks. The rest of this paper is organized as follows. Sec.~ _ref_ introduces the related work about general deep image retrieval and fine-grained image tasks. The details of the proposed SCDA method are presented in Sec.~ _ref_ . In Sec.~ _ref_, for fine-grained image retrieval, we compare our method with several baseline approaches and three state-of-the-art general deep image retrieval approaches. Moreover, discussion on the quality of the SCDA feature is illustrated. Sec.~ _ref_ concludes the paper.