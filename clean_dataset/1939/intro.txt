Thanks to recent advances in computer vision and natural language processing, computers are expected to be able to automatically understand the semantics of images and natural languages in the near future. Such advances have also stimulated new research topics like image-text retrieval _cite_, image captioning _cite_, and visual question answering _cite_ . Compared with image-text retrieval and image captioning (which just require the underlying algorithms to {search} or {generate} a free-form text description for a given image), (VQA) is a more challenging task that requires fine-grained understanding of the semantics of both the images and the questions as well as supports complex reasoning to predict the best-matching answer correctly. In some aspects, the VQA task can be treated as a generalization of image captioning and image-text retrieval. Thus building effective VQA algorithms, which can achieve close performance like human beings, is an important step towards enabling artificial intelligence in general. Existing VQA approaches usually have three stages: (N) representing the images as visual features and questions as textual features; (N) combining these multi-modal features to obtain fused image-question features; (N) using the integrated image-question features to learn a multi-class classifier and to predict the best-matching answer. Deep neural networks (DNNs) are effective and flexible, many existing approaches model the three stages in one DNN model and train the model in an end-to-end fashion through back-propagation. In the three stages, {feature representation} and {multi-modal feature fusion} particular affect VQA performance. With respect to multi-modal feature fusion, most existing approaches simply use linear models for multi-modal feature fusion (e.g., concatenation or element-wise addition) to integrate the image's visual feature with the question's textual feature _cite_ . Since multi-modal feature distributions may vary dramatically, the integrated image-question representations obtained by such linear models may not be sufficiently expressive to fully capture complex associations between the visual features from images and the textual features from questions. In contrast to linear pooling, bilinear pooling _cite_ has recently been used to integrate different CNN features for fine-grained image recognition _cite_ . However, the high dimensionality of the output features and the huge number of model parameters may seriously limit the applicability of bilinear pooling. Fukui proposed the Multi-modal Compact Bilinear (MCB) pooling model to effectively and simultaneously reduce the number of parameters and computation time using the Tensor Sketch algorithm _cite_ . Using the MCB model, the group proposed a network architecture for the VQA task and won the VQA challenge N. Nevertheless, the MCB model lies on a high-dimensional output feature to guarantee robust performance, which may limit its applicability due to huge memory usage. To overcome this problem, Kim proposed the Multi-modal Low-rank Bilinear (MLB) pooling model based on the Hadamard product of two feature vectors _cite_ . Since MLB generate output features with lower dimensions and models with fewer parameters, it is highly competitive with MCB. However, MLB has a slow convergence rate and is sensitive to the learned hyper-parameters. To address these issues, here we develop the Multi-modal Factorized Bilinear pooling (MFB) method, which enjoys the dual benefits of compact output features of MLB and robust expressive capacity of MCB. With respect to feature representation, directly using global features for image representation may introduce noisy information that is irrelevant to the given question. Therefore, it is intuitive to introduce mechanism _cite_ into the VQA task to adaptively learn the most relevant image regions for a given question. Modeling visual attention may significantly improve performance _cite_ . However, most existing approaches only model image attention without considering question attention, even though question attention is also very important since the questions interpreted in natural languages may also contain colloquialisms that can be regarded as noise. Therefore, based on our MFB approach, we design a deep network architecture for the VQA task using a module to jointly learn both image and question attentions. To summarize, the main contributions of this study are as follows: First, we develop a simple but effective Multi-modal Factorized Bilinear pooling (MFB) approach to fuse the visual features from images with the textual features from questions. MFB significantly outperforms existing multi-modal bilinear pooling approaches such as MCB _cite_ and MLB _cite_ . Second, based on the MFB module, a learning architecture is designed to jointly learn both image and question attention. Our MFB approach with co-attention model achieves the state-of-the-art performance on the VQA dataset. We also conduct detailed and extensive experiments to show why our MFB approach is effective. Our experimental results demonstrate that normalization techniques are extremely important in bilinear models.