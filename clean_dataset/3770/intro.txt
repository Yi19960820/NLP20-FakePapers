Sparse coding is a popular data representation method _cite_ . It tries to reconstruct a given data point as a linear combination of some basic elements in a dictionary, which are referred to as codewords. The linear combination coefficients are imposed to be sparse, e.g., most of the combination coefficients are zeros. The linear combination coefficient vector of a data point can be used as its new representation, and we call it the sparse code due to its sparsity. Because of its ability to explore the latent part-based nature of the data, it has been widely used to represent data in pattern classification, image understanding, and database retrieval problems. Many sparse coding algorithms were proposed to learn the dictionary and sparse codes _cite_ . Meanwhile, in nearest neighbor-based classification and content-based database retrieval problems, the data points are usually ranked according to their similarity measures to the queries. The similarity measures are referred to as the ranking scores. Recently, methods to learn the ranking scores from the data points were proposed and showed their power in retrieval problems _cite_ . By considering both the query information provided by the users and the distribution of the data points, efficient algorithms were developed to learn the ranking scores _cite_ . It is possible to use both sparse coding and ranking score learning techniques to boost the performance of nearest neighbor searching. One may firstly map the data points to the sparse codes using a sparse coding algorithm, and then learn the ranking scores in the sparse code space. However, this strategy uses sparse coding and ranking methods independently, and assumes that they are two irrelevant problems. In this paper, we ask the following two questions about sparse coding and ranking score learning: To answer these two questions, we propose to learn the sparse codes and ranking scores jointly to explore their internal relationship. Actually, in _cite_, Mairal et al. proposed to learn sparse codes, a dictionary and a classifier jointly to explore the internal relationship between sparse coding and classification. However, up to now, there is no existing work considering both sparse coding and ranking problems simultaneously. To this end, we propose to perform sparse coding to all the data points and use the query information provided by the user to regularize the learning of the ranking scores. More importantly, to bridge the learning of sparse codes and ranking scores, and also to utilize the local distribution of the data points, we assume that in a local neighborhood of each data point, the ranking scores can be approximated from the sparse codes using a local linear function. By considering the reconstruction error and sparsity of the sparse coding problem, the local approximation error and the complexity of local ranking score approximation, and the query information regularization problems simultaneously, we construct a unified objective function for learning of the sparse codes, the dictionary and ranking scores. By optimizing this objective function, sparse codes and ranking scores can regularize the learning of each other, and thus the internal relationship can be explored. An iterative algorithm is developed to optimize the objective function with regard to the sparse codes, the dictionary and ranking scores, using the alternate optimization strategy. The rest parts of this paper are organized as follows: in Section _ref_, we briefly introduce related works on sparse coding and ranking score learning. In Sections _ref_ and _ref_, we introduce the proposed joint sparse coding and ranking score learning method. In Section _ref_, we show the performance of the proposed algorithm on nearest neighbor retrieval problems using six benchmark data sets. In Section _ref_, the paper is concluded with some future work.