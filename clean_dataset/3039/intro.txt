Problems with a high degree of symmetry create large inefficiencies for machine learning algorithms which do not take those symmetries into account. When the problem has symmetries (such as translation or rotation invariance, or in the case of this paper, permutation invariance), then the algorithm must independently learn the same pattern multiple times over, leading to a massive increase in the amount of training data and time required, and creating many opportunities for spurious biases to creep in. One approach is to handle this at the level of the data, removing the redundant symmetries from the input and aligning things into a canonical form. Examples of this range from things like data augmentation _cite_, SIFT features _cite_ and spatial transformer networks _cite_ . Attentional models also do this implicitly, by aligning the location of attention to features rather than coordinates in the input data _cite_ . The other possibility is to build the desired symmetry into the functional form of the network at some level. For example, convolutional neural networks do this with respect to translation invariance. Because the same shared filter is applied at each point in the image space, and then (eventually) aggregated through a hierarchy of pooling operations, the final classification result behaves in a locally translation-invariant way at each scale. This kind of convolutional approach has been extended to other domains and other symmetries. For example, graph convolutional networks allow for convolution operations to be performed on local neighborhoods of arbitrary graphs _cite_ (this kind of approach can be used for both invariance and, see later, equivariance) . A general framework for constructing these kinds of specified invariances was proposed in _cite_, where the authors generalized convolutional operations to affine invariant computations and showed a way to construct similar things for arbitrary symmetry spaces. For permutation symmetry, work along this line has been done in specifying an expansion in terms of a neural net applied to specific symmetry functions for estimation of potentials in ab initio quantum mechanics calculations _cite_ . In addition, _cite_ uses pooling over instances to construct permutation-invariant summary statistics for different data distributions. Instead of invariance, which generates a result that does not change under transformations of the input, one might want to obtain a property called `equivariance'. In this case, the goal is not to make the final outcome constant despite the application of some set of transformations, but rather to create a structure such that the input and output of the network both behave the same way under transformation. That is to say, if invariance targets a function of the form _inline_eq_, equivariance targets a function of the form _inline_eq_ . This is useful for things such as image segmentation, where applying a rotation to the input image should not change the pixel classes, but should change the positions at which those classes occur (in the same way as the input was rotated) . A general recipe for constructing equivariant convolutions was proposed in _cite_, where this technique was used to generate a convolutional layer equivariant to N degree rotations and to mirror symmetry. In this paper, we're interested in making networks which predict the future dynamics of sets of similar, interacting objects---for example, learning to predict and synthesize the future dynamics of a set of particles just by observing their trajectories. This could be something along the line of deriving effective equations of motions for complex particles such as animals in a swarm or players of a sport, or apply at a more abstract level to something such as relationships between variations in different sectors of the economy or in the interaction between specific stocks. The associated symmetry to this sort of data is permutation symmetry, and we are looking for methods which let us build permutation equivariant neural networks. We present numerical experiments assessing the feasibility and performance of various approaches to this problem, and show that a combination of multi-layer embedded functions inside a max pooling permutation invariant wrapper can achieve significantly higher accuracy on predicting the dynamics of interacting hard discs. Furthermore, networks using this combination demonstrate the ability to generalize to a different number of particles than seen during training, and can also handle cases in which the particles are not identical by way of including an auxiliary random label feature.