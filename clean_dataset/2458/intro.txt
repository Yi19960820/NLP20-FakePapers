Image understanding is becoming a popular topic in the field of multimedia and computer vision, especially accompanying with recent great successes on the tasks of object detection~ _cite_ and recognition~ _cite_ . Over past few years, the deep learning techniques have contributed a milestone progress to such basic tasks. In contrast, as a mid-level understanding of visual content, visual relationship detection (VRD) still needs more concerns, since it may effectively underpin high-level image understanding tasks, such as image captioning~ _cite_ and question answering~ _cite_ . Visual relationship detection aims to find object pairs of interest and estimate their relations from a given image. We denote a visual relationship with a triplet of subject-predicate-object, where the predicate may be spatial, verb, preposition and comparative. Given _inline_eq_ objects and _inline_eq_ predicates, the combinatorial number of possible relationships is _inline_eq_ . As a straightforward solution to visual relationship detection, the joint models~ _cite_ treat each type of triplet as a unique class. However, the long-tailed effect heavily influences the scalability and generalization ability of learned models. To address this problem, the separate models on objects and predicates are often employed in most methods~ _cite_ . In this way, different relation triplets (\eg, person behind cat, house behind car) are merged into the same category if they share the same predicate. But the extreme diversity of samples often overwhelms the learning. To tackle this problem, some recent methods attempt to employ language priors~ _cite_ or structural learning~ _cite_ to reduce unreasonable relationship combinations. However, the relationship triplets are often recognized independently of global contexts, whereas context cues may reduce vagueness of relationships as well as better generalize new relationships. In this paper, we propose a context-dependent diffusion network (CDDN) framework to deal with visual relationship detection. Our motivation comes from this observation that the subjects/objects are correlated to each other under semantic relationships, as shown in Fig.~ _ref_ . Under the shared predicates, some subjects or objects forms some tight cliques, in which connections are with high possibilities. Intuitively, even if some triplets (\eg, girl-eat-banana) are not observed never, we can infer them from person-eat-orange, according to these two cliques  and  . To this end, we encapsule semantic priors of subjects/objects into semantic graphs by using training data. Given a pair of objects, the information of those related objects is probabilistically transmited to them as an object-related aggregation on semantic graph. Besides, we construct another graph (\ie, visual scence graph) of visual objects based on image scene such that the surrounding context can be captured to promote the performance. Thus, semantic graph is built through language priors to model semantic correlations across objects, whilst visual scene graph defines connections of scene objects by utilizing surrounding scene information. After creating structured graphs, we design a graph diffusion network to learn latent representations of objects by adaptively gathering context information. The diffusion strategy may be well-catered to visual relationship detection in view of high flexibility of isomorphic invariance. In experiments, we explore different ways to model each input cue and conduct experiments to validate their effectiveness. Experimental results indicate that our proposed method can achieve the competitive performance compared with the state-of-the-art methods on two widely-used datasets: Visual Relationship Dataset (VRD) ~ _cite_ and Visual Genome (VG) ~ _cite_ .