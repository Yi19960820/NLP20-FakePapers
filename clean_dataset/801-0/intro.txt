Humans can distinguish N, N basic object classes _cite_ and many more subordinate ones (e.g.~breeds of dogs) . They can also create new categories dynamically from few examples or solely based on high-level description. In contrast, most existing computer vision techniques require hundreds of labelled samples for each object class in order to learn a recognition model. Inspired by humans' ability to recognise without seeing samples, and motivated by the prohibitive cost of training sample collection and annotation, the research area of or _cite_ has received increasing interests. These studies aim to intelligently apply previously learned knowledge to help future recognition tasks. In particular, a major and topical challenge in this area is to build recognition models capable of recognising novel visual categories without labelled training samples, i.e.~zero-shot learning (ZSL) . The key idea underpinning ZSL approaches is to exploit knowledge transfer via an intermediate-level semantic representation. Common semantic representations include binary vectors of visual attributes _cite_ (e.g. 'hasTail' in Fig.~ _ref_) and continuous word vectors _cite_ encoding linguistic context. In ZSL, two datasets with disjoint classes are considered: a labelled auxiliary set where a semantic representation is given for each data point, and a target dataset to be classified without any labelled samples. The semantic representation is assumed to be shared between the auxiliary/source and target/test dataset. It can thus be re-used for knowledge transfer between the source and target sets: a projection function mapping low-level features to the semantic representation is learned from the auxiliary data by classifier or regressor. This projection is then applied to map each unlabelled target class instance into the same semantic space. In this space, a `prototype' of each target class is specified, and each projected target instance is classified by measuring similarity to the class prototypes. Depending on the semantic space, the class prototype could be a binary attribute vector listing class properties (e.g., 'hasTail') _cite_ or a word vector describing the linguistic context of the textual class name _cite_ . Two inherent problems exist in this conventional zero-shot learning approach. The first problem is the projection domain shift problem . Since the two datasets have different and potentially unrelated classes, the underlying data distributions of the classes differ, so do the `ideal' projection functions between the low-level feature space and the semantic spaces. Therefore, using the projection functions learned from the auxiliary dataset/domain without any adaptation to the target dataset/domain causes an unknown shift/bias. We call it the projection domain shift problem. This is illustrated in Fig.~ _ref_, which shows two object classes from the Animals with Attributes (AwA) dataset _cite_: Zebra is one of the N auxiliary classes while Pig is one of N target classes. Both of them share the same `hasTail' semantic attribute, but the visual appearance of their tails differs greatly (Fig.~ _ref_ (a)) . Similarly, many other attributes of Pig are visually different from the corresponding attributes in the auxiliary classes. Figure _ref_ (b) illustrates the projection domain shift problem by plotting (in ND using t-SNE _cite_) an ND attribute space representation of the image feature projections and class prototypes (ND binary attribute vectors) . A large discrepancy can be seen between the Pig prototype in the semantic attribute space and the projections of its class member instances, but not for the auxiliary Zebra class. This discrepancy is caused when the projections learned from the N auxiliary classes are applied directly to project the Pig instances--what `hasTail' (as well as the other N attributes) visually means is different now. Such a discrepancy will inherently degrade the effectiveness of zero-shot recognition of the Pig class because the target class instances are classified according to their similarities/distances to those prototypes. To our knowledge, this problem has neither been identified nor addressed in the zero-shot learning literature. The second problem is the prototype sparsity problem: for each target class, we only have a single prototype which is insufficient to fully represent what that class looks like. As shown in Figs.~ _ref_ (b) and (c), there often exist large intra-class variations and inter-class similarities. Consequently, even if the single prototype is centred among its class instances in the semantic representation space, existing zero-shot classifiers will still struggle to assign correct class labels--one prototype per class is not enough to represent the intra-class variability or help disambiguate class overlap _cite_ . In addition to these two problems, conventional approaches to zero-shot learning are also limited in exploiting multiple intermediate semantic representations . Each representation (or semantic `view') may contain complementary information--useful for distinguishing different classes in different ways. While both visual attributes _cite_ and linguistic semantic representations such as word vectors _cite_ have been independently exploited successfully, it remains unattempted and non-trivial to synergistically exploit multiple semantic views. This is because they are often of very different dimensions and types and each suffers from different domain shift effects discussed above. In this paper, we propose to solve the projection domain shift problem using transductive multi-view embedding. The transductive setting means using the unlabelled test data to improve generalisation accuracy. In our framework, each unlabelled target class instance is represented by multiple views: its low-level feature view and its (biased) projections in multiple semantic spaces (visual attribute space and word space in this work) . To rectify the projection domain shift between auxiliary and target datasets, we introduce a multi-view semantic space alignment process to correlate different semantic views and the low-level feature view by projecting them onto a common latent embedding space learned using multi-view Canonical Correlation Analysis (CCA) _cite_ . The intuition is that when the biased target data projections (semantic representations) are correlated/aligned with their (unbiased) low-level feature representations, the bias/projection domain shift is alleviated. The effects of this process on projection domain shift are illustrated by Fig.~ _ref_ (c), where after alignment, the target Pig class prototype is much closer to its member points in this embedding space. Furthermore, after exploiting the complementarity of different low-level feature and semantic views synergistically in the common embedding space, different target classes become more compact and more separable (see Fig.~ _ref_ (d) for an example), making the subsequent zero-shot recognition a much easier task. Even with the proposed transductive multi-view embedding framework, the prototype sparsity problem remains--instead of one prototype per class, a handful are now available depending on how many views are embedded, which are still sparse. Our solution is to pose this as a semi-supervised learning _cite_ problem: prototypes in each view are treated as labelled `instances', and we exploit the manifold structure of the unlabelled data distribution in each view in the embedding space via label propagation on a graph. To this end, we introduce a novel transductive multi-view hypergraph label propagation (TMV-HLP) algorithm for recognition. The core in our TMV-HLP algorithm is a new {of graph structure termed heterogeneous hypergraph which allows us to exploit the complementarity of different semantic and low-level feature views, as well as the manifold structure of the target data to compensate for the impoverished supervision available from the sparse prototypes. Zero-shot learning is then performed by semi-supervised label propagation from the prototypes to the target data points within and across the graphs. The whole framework is illustrated in Fig.~ _ref_ . By combining our transductive embedding framework and the TMV-HLP zero-shot recognition algorithm, our approach generalises seamlessly when none (zero-shot), or few (N-shot) samples of the target classes are available. Uniquely it can also synergistically exploit zero + N-shot (i.e., both prototypes and labelled samples) learning. Furthermore, the proposed method enables a number of novel cross-view annotation tasks including zero-shot class description and zero prototype learning . \noindent Our contributions Our contributions are as follows: (N) To our knowledge, this is the first attempt to investigate and provide a solution to the projection domain shift problem in zero-shot learning. (N) We propose a transductive multi-view embedding space that not only rectifies the projection shift, but also exploits the complementarity of multiple semantic representations of visual data. (N) A novel transductive multi-view heterogeneous hypergraph label propagation algorithm is developed to improve both zero-shot and N-shot learning tasks in the embedding space and overcome the prototype sparsity problem. (N) The learned embedding space enables a number of novel cross-view annotation tasks. Extensive experiments are carried out and the results show that our approach significantly outperforms existing methods for both zero-shot and N-shot recognition on three image and video benchmark datasets.