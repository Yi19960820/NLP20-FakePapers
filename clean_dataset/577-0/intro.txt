The brain is a computational device for information processing and its flexible and adaptive behaviors emerge from a system of interacting neurons depicting very complex networks _cite_ . Many biological evidences suggest that the neocortex implements a common set of algorithms to perform ``intelligent'' behaviors like learning and prediction. In particular, two important related aspects seem to represent the crucial core for learning in biological neural networks: the hierarchical information processing and the abstraction process _cite_ . The hierarchical architecture emerges from anatomical considerations and is fundamental for associative learning (e.g. multisensory integration) . The abstraction instead leads the inference of concepts from senses and perceptions (Fig. _ref_ D) . Specifically, information from sensory receptors (eyes, skin, ears, etc.) travels into the human cortical circuits following subsequent abstraction processes. For instance, elementary sound features (e.g. frequency, intensity, etc.) are first processed in the primary stages of human auditory system (choclea) . Subsequently sound information gets all the stages of the auditory pathway up to the cortex where higher level features are extracted (Fig. _ref_ E-F) . In this way information passes from raw data to objects, following an abstraction process in a hierarchical layout. Thus, biological neural networks perform generalization and association of sensory information. For instance, we can associate sounds, images or other sensory objects that present together as it happens in many natural and experimental settings like during Pavlovian conditioning. Biological networks process these inputs following a hierarchical order. In a first stations inputs from distinct senses are separately processed accomplishing data abstraction. This process is repeated in each subsequent higher hierarchical layer. Doing so, in some hierarchical layer, inputs from several senses converge showing associations among sensory inputs. Recent findings indicate that neurons can perform invariant recognitions of their input activity patterns producing specific modulations of their synaptic releases _cite_ . Although the comphrension of such neuronal mechanisms is still elusive, these hints can drive the development of algorithms closer to biology than spiking networks or other brain-inspired models appear to be. In this work, we propose a learning framework based on these biological considerations, called Inductive Conceptual Network (ICN), and we tested the accuracy of this network on the MNIST and USPS datasets. The ICN represents a general biological plausible model of the learning mechanisms in neuronal networks. The invariant pattern recognition that occurs in the hierarchy nodes is achieved by modeling node inputs by Variable-order Markov Models (VMMs) _cite_ .