Processing ND data as obtained from ND scanners or depth cameras is fundamental to a wealth of applications in the field of ND computer vision, scene understanding, augmented/mixed reality, robotics and autonomous driving. The extraction of reliable semantic information from a ND scene is useful, for example, to appropriately add virtual content to the ND space around us or describe it to a visually-impaired person. Analogously, in robotics, processing of ND data acquired from a depth camera allows the robot to perform sophisticated tasks, beyond path-planning and collision avoidance, that require intelligent interaction in real world environments. A recent research trend has focused on designing effective learning architectures for processing common ND data representations such as point clouds, meshes and voxel maps, to be employed in tasks such as voxel-based semantic scene segmentation _cite_, part-based segmentation of ND objects _cite_ and ND correspondence matching _cite_ . A primary objective of these models is robustness against typical issues present when working with real world data such as, noise, holes, occlusion and partial scans, as well as viewpoint changes and ND transformations (rotation and translation) . Another challenge more related to semantic inference relates to dealing with the large number of classes that characterize real world scenarios and their typically large intra-class variance. In pursuit of a versatile ND architecture, applicable in small-and large-scale tasks, it is not only necessary to extract meaningful features from ND data at several scales, but also desirable to operate on a large spatial region at once. For this, fully-convolutional networks (FCN) _cite_ have recently grown to prominence due to their drastic reduction in parameters and flexibility to variable input sizes. However, learning these hierarchical statistical distributions starting at the lowest level requires a huge amount of data. To achieve this, some methods train on synthetic data, but suffer from the domain gap when applied to the real-world _cite_ . A big step toward closing this gap is ScanNet, a large-scale dataset of indoor scans _cite_ . Methods that achieve state-of-the-art performance on these challenging tasks quickly reach the memory limits of current GPUs due to the additional dimension present in ND data _cite_ . While the aforementioned FCN architecture reduces the number of parameters, it requires the input to be in an ordered (dense) form. To bypass the need to convert the raw, unordered data into an ordered representation, PointNet _cite_ proposes an architecture that directly operates on sets of unordered points. Since PointNet only learns a global descriptor of the point cloud, Qi et. al later introduced a hierarchical point-based network with PointNet + + _cite_ . While achieving impressive results in several tasks, PointNet + + cannot take advantage of the memory and performance benefits that ND convolutions offer due to its fully point-based nature. This requires PointNet + + to redundantly compute and store the context of every point even when they spatially overlap. We present a general-purpose, fully-convolutional network architecture for processing ND data: Fully-Convolutional Point Network (FCPN) . Our network is hybrid, i.e. designed to take as input unorganized ND representations such as point clouds while processing them internally in an organized fashion through ND convolutions. This is different from other approaches, which require both input and internal data representation to be either unorganized point sets _cite_ or organized data _cite_ . The advantage of our hybrid approach is to take the benefits of both representations. Unlike _cite_, our network operates on memory efficient input representations that scale well with the scene/object size and transforms it to organized internal representations that can be processed via convolutions. A benefit of our method is that it can scale to large volumes while processing point clouds in a single pass. It can also be trained on small regions, e.g. _inline_eq_ meters and later applied to larger point clouds during inference. A visualization of the output at three different scales of our network trained on semantic voxel labeling is given in Figure _ref_ . While the proposed method is primarily intended for large-scale, real-world scene understanding applications, demonstrated by the semantic voxel labeling and ND scene captioning tasks, the method is also evaluated on semantic part segmentation to demonstrate its versatility as a generic feature descriptor capable of operating on a range of spatial scales. Our main contributions are (N) a network based on a hybrid unorganized input/organized internal representation; and, (N) the first fully-convolutional network operating on raw point sets. We demonstrate its scalability by running it on full ScanNet scenes regions of up to _inline_eq_ in a single pass. In addition, we show the versatility of our learned feature descriptor by evaluating it on both large-scale semantic voxel labeling as well as on ND part segmentation. In addition, as a third contribution, we explore the suitability of our approach to a novel task which we dub ND captioning, that addresses the extraction of meaningful text descriptions out of partial ND reconstructions of indoor spaces. We demonstrate how our approach is well suited to deal with this task leveraging its ability to embed contextual information, producing a spatially ordered output descriptor from the unordered input, necessary for captioning. For this task, we also publish a novel dataset with human-annotated captions.