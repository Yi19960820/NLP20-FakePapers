The problem of tracking an arbitrary object in video, where an object is identified by a single bounding-box in the first frame, requires both a robust similarity function and an efficient method for querying plausible locations of the object in subsequent frames. Early video tracking approaches have included feature-based approaches and template matching algorithms [N] that attempt to track specific features of an object or even the object as a whole. Feature-based approaches use local features, including points and edges, keypoints [N], SIFT features [N], HOG features [N] and deformable parts [N] . Conversely, template-based methods take the object as a whole – offering the potential advantage that they treat complex templates or patterns that cannot be modeled by local features alone. \indent Through the course of a video, an object can potentially undergo a variety of different visual transformations, including rotation, occlusion, changes in scale, illumination changes, etc., that pose significant challenges for tracking. In order to obtain a robust template matching for video tracking, researchers have developed a host of methods, including mean-shift [N] and cross-correlation filtering which entails convolving a template over a search region; significant advances to cross-correlation filtering for video tracking include MOSSE [N] adaptive correlation filter and the MUSTer algorithm [N] which draws influence from cognitive psychology in the design of a flexible object representation using long and short-term memory stored by means of an integrated correlation filter. \indent More recently, deep learning models have been applied to video tracking to leverage the benefits of learning complex functions from large data sets. While deep models offer the potential of improved robustness for tracking, they have nevertheless presented two significant challenges to tracking research to date. First, many deep tracking models are too slow for practical use due to the fact that they require online training, and, second, many deep trackers, when trained offline, are based on classification approaches, so that they are limited to class-specific searches and frequently require the aggregation of many image patches (and thus many passes through the network) in order to locate the object [N] . In light of these difficulties, several contemporary state of the art deep learning-based tracking models have been developed as generic object trackers in an effort to obviate the need for online training and to also improve the generalizability of the tracker. [N] applies a regression-based approach to train a generic tracker, GOTURN, offline to learn a generic relationship between appearance and motion; several deep techniques additionally incorporate motion and occlusion models, including particle filtering methods [N] and optical flow [N] . \indent demonstrated the power of deep Siamese networks (see section N) based on [N], achieving a new state of the art for generic object matching for video tracking. Remarkably, the SINT algorithm delivered state of the art performance despite the fact that it was not equipped with any model updating, no occlusion detection, and no explicit geometric or feature matching components. [N, N] extended this work to achieve state of the art Siamese-based tracking while operating at frame rates beyond real-time by exploiting a fully-convolutional network structure. Even with these recent successes in video object tracking, there nevertheless exists a void in state of the art video tracking workflows that fully integrate deep learning models with classical statistics and machine learning approaches. Most state of the art video trackers lack – for instance – a capacity to generate systematic “belief” states (e.g. through explicit error and uncertainty measures), or ways to seamlessly incorporate contextual and scene structure, or to adaptively encode temporal information (e.g. by imposing intelligent search stopping conditions and bounds) and the ability to otherwise directly and inferentially control region proposal generation or sampling methods in a precise and principled way. To this end, we believe that the fusion of deep models with classical approaches can provide a necessary incubation for “intelligent” computer vision systems capable of high-level vision tasks in the future (e.g. scene and behavior understanding) . In the current work we present the first integrated dynamic Bayesian optimization framework in conjunction with deep learning for object tracking in video.