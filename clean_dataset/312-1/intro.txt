Advances in a wide range of medical imaging technologies have revolutionized how we view functional and pathological events in the body and define anatomical structures in which these events take place. X-ray, CAT, MRI, Ultrasound, nuclear medicine, among other medical imaging technologies, enable ND or tomographic ND images to capture in-vivo structural and functional information inside the body for diagnosis, prognosis, treatment planning and other purposes. One fundamental problem in medical image analysis is image segmentation, which identifies the boundaries of objects such as organs or abnormal regions (e.g. tumors) in images. Since manually annotation can be very time-consuming and subjective, an accurate and reliable automatic segmentation method is valuable for both clinical and research purpose. Having the segmentation result makes it possible for shape analysis, detecting volume change, and making a precise radiation therapy treatment plan. In the literature of image processing and computer vision, various theoretical frameworks have been proposed for automatic segmentation. Traditional unsupervised methods such as thresholding~ _cite_, region growing~ _cite_, edge detection and grouping~ _cite_, Markov Random Fields (MRFs) ~ _cite_, active contour models~ _cite_, Mumford-Shah functional based frame partition~ _cite_, level sets~ _cite_, graph cut~ _cite_, mean shift~ _cite_, and their extensions and integrations~ _cite_ usually utilize constraints about image intensity or object appearance. Supervised methods~ _cite_, on the other hand, directly learn from labeled training samples, extract features and context information in order to perform a dense pixel (or voxel)-wise classification. Convolutional Neural Networks (CNNs) have been widely applied to visual recognition problems in recent years, and they are shown effective in learning a hierarchy of features at multiple scales from data. For pixel-wise semantic segmentation, CNNs have also achieved remarkable success. In~ _cite_, Long first proposed a fully convolutional networks (FCNs) for semantic segmentation. The authors replaced conventional fully connected layers in CNNs with convolutional layers to obtain a coarse label map, and then upsampled the label map with deconvolutional layers to get per pixel classification results. Noh ~ _cite_ used an encoder-decoder structure to get more fine details about segmented objects. With multiple unpooling and deconvolutional layers in their architecture, they avoided the coarse-to-fine stage in~ _cite_ . However, they still needed to ensemble with FCNs in their method to capture local dependencies between labels. Lin ~ _cite_ combined Conditional Random Fields (CRFs) and CNNs to better explore spatial correlations between pixels, but they also needed to implement a dense CRF to refine their CNN output. In the field of medical image segmentation, deep CNNs have also been applied with promising results. Ronneberger ~ _cite_ presented a FCN, namely U-net, for segmenting neuronal structures in electron microscopic stacks. With the idea of skip-connection from~ _cite_, the U-net achieved very good performance and has since been applied to many different tasks such as image translation~ _cite_ . In addition, Havaei ~ _cite_ obtained good performance for medical image segmentation with their InputCascadeCNN. The InputCascadeCNN has image patches as inputs and uses a cascade of CNNs in which the output probabilities of a first-stage CNN are taken as additional inputs to a second-stage CNN. Pereira ~ _cite_ applied deep CNNs with small kernels for brain tumor segmentation. They proposed different architectures for segmenting high grade and low grade tumors, respectively. Kamnitsas ~ _cite_ proposed a ND CNN using two pathways with inputs of different resolutions. ND CRFs were also needed to refine their results. Although these previous approaches using CNNs for segmentation have achieved promising results, they still have limitations. All above methods utilize a pixel-wise loss, such as softmax, in the last layer of their networks, which is insufficient to learn both local and global contextual relations between pixels. Hence they always need models such as CRFs ~ _cite_ as a refinement to enforce spatial contiguity in the output label maps. Many previous methods _cite_ address this issue by training CNNs on image patches and using multi-scale, multi-path CNNs with different input resolutions or different CNN architectures. Using patches and multi-scale inputs could capture spatial context information to some extent. Nevertheless, as described in U-net~ _cite_, the computational cost for patch training is very high and there is a trade-off between localization accuracy and the patch size. Instead of training on small image patches, current state-of-the-art CNN architectures such as U-net are trained on whole images or large image patches and use skip connections to combine hierarchical features for generating the label map. They have shown potential to implicitly learn some local dependencies between pixels. However, these methods are still limited by their pixel-wise loss function, which lacks the ability to enforce the learning of multi-scale spatial constraints directly in the end-to-end training process. Compared with patch training, an issue for CNNs trained on entire images is label or class imbalance. While patch training methods can sample a balanced number of patches from each class, the numbers of pixels belonging to different classes in full-image training methods are usually imbalanced. To mitigate this problem, U-net uses a weighted cross-entropy loss to balance the class frequencies. However, the choice of weights in their loss function is task-specific and is hard to optimize. In contract to the weighted loss in U-net, a general loss that could avoid class imbalance as well as extra hyper-parameters would be more desirable. In this paper, we propose a novel end-to-end Adversarial Network architecture, called SegAN, with a multi-scale _inline_eq_ loss function, for semantic segmentation. Inspired by the original GAN~ _cite_, the training procedure for SegAN is similar to a two-player min-max game in which a segmentor network (_inline_eq_) and a critic network (_inline_eq_) are trained in an alternating fashion to respectively minimize and maximize an objective function. However, there are several major differences between our SegAN and the original GAN that make SegAN significantly better for the task of image segmentation. By training the entire system end-to-end with back propagation and alternating the optimization of _inline_eq_ and _inline_eq_, SegAN can directly learn spatial pixel dependencies at multiple scales. Compared with previous methods that learn hierarchical features with multi-scale multi-path CNNs~ _cite_, our SegAN network applies a novel multi-scale loss to enforce the learning of hierarchical features in a more straightforward and efficient manner. Extensive experimental results demonstrate that the proposed SegAN achieves comparable or better results than the state-of-the-art CNN-based architectures including U-net. The rest of this paper is organized as follows. Section _ref_ introduces our SegAN architecture and methodology. Experimental results are presented in Section N. Finally, we conclude this paper in Section N.