Deep learning has many recent successes; for example, deep learning approaches have made strides in automatic speech recognition, in visual object recognition, and in machine translation . While these successes demonstrate the wide-ranging effectiveness of deep learning approaches, there yet remains useful information that current deep learning is less able to bring to bear. To take a specific example, consider that much of current deep learning practice is dominated by approaches that proceed from input to output in a fundamentally bottom-up fashion. While current performance is extremely impressive, these strongly bottom-up characteristics leave room for one to ask whether providing deep learning with the ability to also incorporate top-down information might open a path to even better performance. The demonstrated role of top-down information in human perception provides a suggestive indication of the role that top-down information could play in deep learning. Visual illusions (such as the ``Chaplin mask'') provide the clearest examples of the strong effect that top-down/prior information can have on human perception; the benefits of top-down information in human perception are widespread but subtler to notice: prominent examples include color constancy and the interpretation of visual scenes that would otherwise be relatively meaningless (e.g. the ``Dalmatian'' image) . Another particularly common experience is the human ability to focus on some specific conversation in a noisy room, distinguishing the relevant audio component among potentially overwhelming interference. Motivated by the importance of top-down information in human perception, as well as by the successful incorporation of top-down information in non-deep approaches to computer vision, we pursue an approach to bringing top-down information into current deep network practice. The potential benefits from incorporating top-down information in deep networks include improved prediction accuracy in settings where bottom-up information is misleading or insufficiently distinctive as well as generally improved agreement when multiple classification predictions are made in a single image (such as in images containing multiple objects) . A particularly appealing direction for future work is the use of top-down information to improve resistance to ``adversarial examples'' . The incorporation of top-down information in visual tasks stands at the intersection of three fields: cognitive science, computer vision, and deep learning. Succinctly, we find our inspiration in cognitive science, our prior examples in computer vision, and our actual instantiation in deep learning. We consider these each in turn. Even before Stroop's work it has been noted that human perception of the world is not a simple direct path from, e.g., photons reaching the retina to an interpretation of the world around us. Researchers have established a pervasive and important role for top-down information in human perception . The most striking demonstrations of the role of top-down information in human perception come in the form of ``visual illusions'', such as incorrectly perceiving the concave side of a plastic Chaplin mask to be convex . The benefits of top-down information are easy to overlook, simply because top-down information is often playing a role in the smooth functioning of perception. To get a sense for these benefits, consider that in the absence of top-down information, human perception would have trouble with such useful abilities as the establishment of color constancy across widely varying illumination conditions or the interpretation of images that might otherwise resemble an unstructured jumble of dots (e.g., the ``Dalmatian'' image) . Observations of the role of top-down information in human perception have inspired many researchers in computer vision. A widely-cited work on this topic that considers both human perception and machine perception is . The chain of research stretches back even to the early days of computer vision research, but more recent works demonstrating the performance benefits of top-down information in tasks such as object perception include . Two recent related works in computer vision are . There are distinct differences in goal and approach, however. Whereas spatial transformer networks pursue an architectural addition in the form of what one might describe as ``learned standardizing preprocessing'' inside the network, our primary focus is on exploring the effects (within an existing CNN) of the types of transformations that we consider. We also investigate a method of using the explored effects (in the form of learned generators) to improve vanilla AlexNet performance on ImageNet. On the other hand, state that their goal is ``to directly impose good transformation properties of a representation space'' which they pursue via a group theoretic approach; this is in contrast to our approach centered on effects on representations in an existing CNN, namely AlexNet. They also point out that their approach is not suitable for dealing with images much larger than NxN, while we are able to pursue an application involving the entire ImageNet dataset. Another recent work is, modeling random fields in convolutional layers; however, they do not perform image synthesis, nor do they study explicit top-down transformations. As part of our exploration, we make use of recent work on generating images corresponding to internal activations of a CNN. A special purpose (albeit highly intriguing) method is presented in . The method of is generally applicable, but the specific formulation of their inversion problem leads to generated images that significantly differ from the images the network was trained with. We find the technique of to be most suited to our purposes and use it in our subsequent visualizations. One of the intermediate steps of our process is the computation of ``feature flows''---vector fields computed using the SIFTFlow approach, but with CNN features used in place of SIFT features. Some existing work has touched on the usefulness of vector fields derived from ``feature flows''. A related but much more theoretical diffeomorphism-based perspective is . Another early reference touching on flows is ; however, the flows here are computed from image pixels rather than from CNN features. uses feature flow fields as a means of visualizing spatio-temporal features learned by a convolutional gated RBM that is also tasked with an image analogy problem. The ``image analogy'' problem is also present in the work focusing on gated Boltzmann machines; here the image analogy is performed by a ``field of gated experts'' and the flow-fields are again used for visualizations. Rather than pursue a special purpose re-architecting to enable the performance of such ``zero-shot learning''-type ``image analogy'' tasks, we pursue an approach that works with an existing CNN trained for object classification: specifically, AlexNet .