With the recent advancements in deep learning, many image processing tasks that were once thought impossible are now possible. One such task is that of generating completely random textures that visually look similar to a given sample texture. Recently Gatys et al. [N] experimentally showed that by utilizing a specific energy function that uses the first few layers of a pretrained CNN, natural textures can be effectively generated. Later Champandard [N] used a different energy function and random weights to generate textures. He et al. [N] used the same method as in Gatys et al. [N] with random weights, to generate nice textures. Around the same time, Ustyuzhaninov et al. [N] experimentally showed that less variable natural textures can be generated using just one layer of a CNN with random filters along with the same energy function. Although these methods work quite well there is no theoretical analysis why these methods (in particular the choice of energy functions and also use of random weights) would be helpful for texture generation even in the most basic settings such as in [N] . This leads us to ask a natural question. What theoretical properties lead to the fact that we can generate random textures just using a one layer CNN with random filters? In section N with a slight adjustment to the one layer CNN architecture, we show rigorously why random weights in one layer CNNs without a nonlinearity, can be used to generate random textures with the same performance as with pre-trained weights, while also drawing connections to previous work in texture generation. In section N, we show how the behavior of generated images changes when one adds a ReLu non-linearity.We show that in the case of a ReLu non-linearity there are often cases where there is only one solution that gives the minimum energy (the input itself), whereas in the case with no nonlinearity there are infinite inputs that give the minimum energy.