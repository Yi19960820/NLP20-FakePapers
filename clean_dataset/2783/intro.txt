We are witnessing a revolution in computer vision with the advent of Deep Learning (DL) architectures~ _cite_ . Computer vision problems have been traditionally solved using hand-crafted features specifically designed to tackle particular problems~ _cite_, where the main challenge was to find the right descriptors for certain image contents. DL introduced a general way to proceed via supervised learning. Fukushima \etal~ _cite_ were pioneers developing a hierarchical architecture for handwritten character recognition and other pattern recognition, which we may consider the inspiration for Convolutional Neural Networks (CNNs) . In N, LeCun \etal _cite_ introduced one of the first and most popular architectures for handwritten character recognition, and a decade later, Serre \etal~ _cite_ contributed with a new general framework for the recognition of complex visual scenes. Those first steps were based on a small number of layers and limited datasets due to the modest computational power available, so researchers often moved to less demanding approaches like SVM~ _cite_ . In N, Krizhevsky \etal _cite_ released `AlexNet', a CNN composed of N layers and around N million parameters. GPUs were capable to train the model with CUDA in a reasonable amount of time using four GPUs, and since then, the fascinating evolution of GPU performance and its recent emphasis on DL has propelled those models to gain extraordinary popularity. Meanwhile, new datasets~ _cite_ containing millions of samples were released to train models with even more parameters without overfitting, promoting CNN models to be established as the state-of-art in computer vision. The challenge for researchers to tune computer vision applications at this point is no longer based on low-level features, but on general neural network components like number of layers, set of parameters or batch size. Within this trend, the last couple of years have been prolific in assorted areas like image recognition~ _cite_, action recognition~ _cite_, object detection~ _cite_, and biometric identification~ _cite_, just to mention a few akin to that of this work. This trend has been lately fortified with the arrival of deep learning frameworks publicly available, like Caffe~ _cite_, TensorFlow~ _cite_, CNTK~ _cite_, MatConvNet~ _cite_ and PyTorch~ _cite_ . Most of these frameworks are optimized for GPUs and still require large execution times, so energy consumption on GPUs becomes critical. That way, the flagship metric is no longer GFLOPS (Giga Floating-Point Operations Per Second), but GFLOPS/w (GFLOPS per watt) . This paper emphasizes energy over speed, choosing representative CNN instances to shed some light about the way energy is spent within CNN depending on its architecture (ResNet/CaffeNet/ND-CNN), input dataset (images/videos) and batch size (N/N/N) . Finally, a correlation with performance and accuracy is performed to complete our analysis. On the hardware side, latest generations of Nvidia GPUs, namely Maxwell (N) and Pascal (N), have been used for our experimental setup. Those two generations have contributed like no other before to optimize the GFLOPS/w ratio, and the advantage amplifies in supercomputers to populate the greenN list _cite_ . Our work gathers results combining the best GeForce model for those two generations, Titan X, and a multi-GPU server endowed with up to four GPUs, returning somehow to the departure point where AlexNet emerged five years ago. Previous works have contributed with performance analysis of DL networks in GPU architectures~ _cite_ . We extend those results to energy for a more complete study using a probe plugged to the GPU that measures power consumption at real-time for every stage a deep learning algorithm consists of. Major contributions of this paper on DL algorithms are the following: The rest of this paper is structured as follows. Section _ref_ introduces some related work. Section _ref_ provides a general overview of CNNs, and Section _ref_ particularizes our selection of CNNs for the experimental study. Section _ref_ outlines our CNN implementation on multi-GPU environments. Section _ref_ describes the infrastructure we have used for measuring energy on GPUs. Section _ref_ introduces the input datasets. Section _ref_ presents and discusses the experimental results, and finally, Section _ref_ summarizes the conclusions drawn from this work.