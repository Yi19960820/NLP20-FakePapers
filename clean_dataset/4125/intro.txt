Continual learning, or lifelong learning~ _cite_, the ability to learn consecutive tasks without forgetting how to perform previously trained tasks, is an important topic for developing artificial intelligence. The primary goal of continual learning is to overcome the forgetting of learned tasks and to leverage the earlier knowledge for obtaining better performance or faster convergence/training speed on the newly coming tasks. In deep learning community, two groups of strategies have been developed to alleviate the problem of forgetting the previously trained tasks, distinguished by whether the network architecture changes during learning. The first category of approaches maintain a fixed network architecture with large capacity. When training the network for consecutive tasks, some regularization term is enforced to prevent the model parameters from deviating too much from the previous learned parameters according to their significance to old tasks~ _cite_ . In~ _cite_, the authors proposed to incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. Alternatively, an episodic memory~ _cite_ is budgeted to store the subsets of previous datasets, and then trained together with the new task. Fernando et al. _cite_ proposed PathNet, in which a neural network has ten or twenty modules in each layer, and three or four modules are picked for one task in each layer by an evolutionary approach. However, these methods typically require unnecessarily large-capacity networks, particularly when the number of tasks is large, since the network architecture is never dynamically adjusted during training. The other group of methods for overcoming catastrophic forgetting dynamically expand the network to accommodate the new coming task while keeping the parameters of previous architecture unchanging. Progressive networks~ _cite_ expand the architectures with a fixed size of nodes or layers, leading to an extremely large network structure particularly faced with a large number of sequential tasks. The resultant complex architecture might be expensive to store and even unnecessary due to its high redundancy. Dynamically Expandable Network (DEN, ~ _cite_ alleviated this issue slightly by introducing group sparsity regularization when adding new parameters to the original network; unfortunately, there involves many hyperparameters in DEN, including various regularization and thresholding ones, which need to be tuned carefully due to the high sensitivity to the model performance. In this work, in order to better facilitate knowledge transfer and avoid catastrophic forgetting, we provide a novel framework to adaptively expand the network. Faced with a new task, deciding optimal number of nodes/filters to add for each layer is posed as a combinatorial optimization problem. We provide a sophisticatedly designed reinforcement learning method to solve this problem. Thus, we name it as Reinforced Continual Learning (RCL) . In RCL, a controller implemented as a recurrent neural network is adopted to determine the best architectural hyper-parameters of neural networks for each task. We train the controller by an actor-critic strategy guided by a reward signal deriving from both validation accuracy and network complexity. This can maintain the prediction accuracy on older tasks as much as possible while reducing the overall model complexity. To the best of our knowledge, the proposal is the first attempt that employs the reinforcement learning for solving the continual learning problems. RCL not only differs from adding a fixed number of units to the old network for solving a new task~ _cite_, which might be suboptimal and computationally expensive, but also distinguishes from~ _cite_ as well that performs group sparsity regularization on the added parameters. We validate the effectiveness of RCL on various sequential tasks. And the results show that RCL can obtain better performance than existing methods even with adding much less units. The rest of this paper is organized as follows. In Section~ _ref_, we introduce the preliminary knowledge on reinforcement learning. We propose the new method RCL in Section~ _ref_, a model to learn a sequence of tasks dynamically based on reinforcement learning. In Section~ _ref_, we implement various experiments to demonstrate the superiority of RCL over other state-of-the-art methods. Finally, we conclude our paper in Section~ _ref_ and provide some directions for future research.