ow to design a feature representation to fully exploit the spatial-temporal information in videos constitutes a cornerstone in video based human action recognition. Current state-of-the-art methods usually generate video-level representations by adopting hand-crafted features such as spatial-temporal interest points _cite_ or trajectories _cite_ and unsupervised feature encoding methods such as Fisher vector encoding _cite_ . Recently, deep Convolutional Neural Networks has been established as the state-of-the-art method in image classification _cite_ and it has been demonstrated that a CNN pretrained on a large image dataset, such as ImageNet _cite_, can be used to initialize networks built for other visual recognition tasks. Inspired by the success of CNNs in image recognition, some studies attempt to apply CNNs to video based action recognition. However, most existing deep models are designed to work with single image input. It is non-trivial to extend these models to videos since video clips often contain a varying number of frames. To handle this problem, the work in _cite_ samples a fixed number of frames and reshapes them into a compatible input format of an image-based CNN. However, sampling may risk missing important frames for action recognition, especially in videos with uncontrolled scene variation. Another strategy is to bypass this issue by directly using videos as input and replacing the ND convolution with ND convolution which is operated on the spatial-temporal domain. However, the above strategy sacrifices the possibility of leveraging the powerful off-the-shelf image-based CNN to initialize model parameters or extract mid-level features. Thus, it has to rely on a huge number of training videos to avoid the risk of over-fitting. For example, the authors in _cite_ collect a dataset of N million YouTube videos for network training which takes weeks to train with modern GPUs. In this work, we propose a novel network structure which allows an arbitrary number of video frames as input. This is achieved by designing a module which consists of an encoding layer and a temporal pyramid pooling layer. The encoding layer maps the activations from previous layer to a feature vector suitable for pooling, which is akin to the encoding operation in the traditional bag-of-features model. The temporal pyramid pooling layer converts multiple frame-level activations into a fixed-length video-level representation. At the same time, the temporal pyramid pooling layer explicitly considers the weak temporal structure within videos. In addition, we also introduce a feature concatenation layer into our network to combine motion and appearance information.