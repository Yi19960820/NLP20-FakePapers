Convolutional neural networks (CNNs) demonstrate great success in various computer vision tasks, such as image classification~ _cite_, object detection _cite_, image segmentation~ _cite_, etc. However, they suffer from high computation cost when deployed to resource-constrained devices. Many efforts have been devoted to optimize/accelerate the inference speed of CNNs, which could be roughly divided into three categories. First, design-time network optimization considers designing efficient network structures from scratch in a handcraft way or automatic search way. Typical handcraft based works include Xception~ _cite_, MobileNet~ _cite_, and networks with channel interleaving/shuffle~ _cite_, while typical works on automatic network architecture search are NASNet~ _cite_, PNASNet~ _cite_ . Second, training-time network optimization takes pre-defined network structures as input, and refines the structures through regularized retraining or fine-tuning or even knowledge distilling _cite_ . Typical works involve weight pruning _cite_, structure (filters/channels) pruning _cite_, weight hashing/quantization _cite_, low-bit networks _cite_ . Third, deploy-time network optimization takes pre-trained CNN models as input, and replaces some redundant and less-efficient CNN structures with efficient ones in a training-free way. Low-rank decomposition _cite_, spatial decomposition _cite_, and channel decomposition _cite_ fall into this category. Methods in the first two categories require time-consuming training procedure to produce desired outputs, with full training-set available. On the contrary, methods in the third category may not require training-set at all, or in some cases require a small calibration-set (e.g., N, N images) to tune some parameters. The optimization procedure can typically be done within dozens of minutes. Hence, it is of great value when software/hardware vendors assist their customers to optimize CNN based solutions in case that either the time budget is so tight that training based solutions are not feasible, or the customer data are unavailable due to privacy or confidential issues. Therefore, there is a strong demand for modern deep learning frameworks or hardware (GPU/ASIC/FPGA, etc) vendors to provide deploy-time model optimization tools. Meanwhile, handcraft designed structures such as depthwise separable convolution _cite_ have shown great efficiency over regular convolution, while still keeping high accuracy. To the best of our knowledge, the mathematical relationship between regular convolutions and depthwise separable convolutions is not yet studied and unknown to the public. Our motivation is to show their relationship, and present a solution to decouple the regular convolutions into depthwise separable convolutions in a training-free way for deploy-time network optimization/acceleration. Our main contributions are summarized as below: