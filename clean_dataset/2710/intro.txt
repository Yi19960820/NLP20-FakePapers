In the big data era, large-scale and high-dimensional media data has been pervasive in search engines and social networks. To guarantee retrieval quality and computation efficiency, approximate nearest neighbors (ANN) search has attracted increasing attention. Parallel to the traditional indexing methods _cite_, another advantageous solution is hashing methods _cite_, which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items. In this paper, we will focus on learning to hash methods _cite_ that build data-dependent hash encoding schemes for efficient image retrieval, which have shown better performance than data-independent hashing methods, e.g. Locality-Sensitive Hashing (LSH) _cite_ . Many learning to hash methods have been proposed to enable efficient ANN search by Hamming ranking of compact binary hash codes _cite_ . Recently, deep learning to hash methods _cite_ have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks _cite_, which can naturally encode any nonlinear hash functions. These deep learning to hash methods have shown state-of-the-art performance on many benchmarks. In particular, it proves crucial to jointly learn similarity-preserving representations and control quantization error of binarizing continuous representations to binary codes _cite_ . However, a key disadvantage of these deep learning to hash methods is that they need to first learn continuous deep representations, which are binarized into hash codes in a separated post-step of sign thresholding. By, i.e. solving the discrete optimization of hash codes with continuous optimization, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn hash codes in their optimization procedure. Hence, existing deep hashing methods may fail to generate compact binary hash codes for efficient similarity retrieval. There are two key challenges to enabling deep learning to hash truly end-to-end. First, converting deep representations, which are in nature, to hash codes, we need to adopt the function _inline_eq_ as activation function when generating binary hash codes using similarity-preserving learning in deep neural networks. However, the gradient of the sign function is zero for all nonzero inputs, which will make standard back-propagation infeasible. This is known as the problem, which is the key difficulty in training deep neural networks via back-propagation _cite_ . Second, the similarity information is usually very sparse in real retrieval systems, i.e., the number of similar pairs is much smaller than the number of dissimilar pairs. This will result in the problem, making similarity-preserving learning ineffective. Optimizing deep networks with activation remains an open problem and a key challenge for deep learning to hash. This work presents HashNet, a new architecture for deep learning to hash by continuation with convergence guarantees, which addresses the ill-posed gradient and data imbalance problems in an end-to-end framework of deep feature learning and binary hash encoding. Specifically, we attack the problem in the non-convex optimization of the deep networks with non-smooth sign activation by the methods _cite_, which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize. By gradually reducing the amount of smoothing during the training, it results in a sequence of optimization problems converging to the original optimization problem. A novel weighted pairwise cross-entropy loss function is designed for similarity-preserving learning from imbalanced similarity relationships. Comprehensive experiments testify that HashNet can generate exactly binary hash codes and yield state-of-the-art retrieval performance on standard datasets.