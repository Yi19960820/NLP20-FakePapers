\noindent Convolutional Neural Networks (\cnn) have become tremendously popular for a huge number of applications _cite_ since the success of AlexNet _cite_ in N. AlexNet, VGGN _cite_ and ResNet _cite_, are some of the famous architectures designed for image classification which have shown incredible results. While image classification aims at predicting a single class per image (presence or not of an object in an image) we tackle the problem of full scene labelling. Full scene labelling or semantic segmentation from RGB images aims at segmenting an image into semantically meaningful regions, i.e. at providing a class label for each pixel of an image. Based on the success of classical \cnn, new networks designed especially for semantic segmentation, named fully convolutional networks have been developed. The main advantage of these networks is that they produce ND matrices as output, allowing the network to label an entire image directly. Because they are fully convolutional, they can be fed with images of various sizes. In order to construct fully convolutional networks, two strategies have been developed: conv-deconv networks and dilated convolution-based networks (see Section~ _ref_ for more details) . Conv-deconv networks are composed of two parts: the first one is a classical convolutional network with subsampling operations which decrease the feature maps sizes and the second part is a deconvolutional network with upsampling operations which increase the feature maps sizes back to the original input resolution. Dilated convolution-based networks~ _cite_ do not use subsampling operations but a " \`a trous" algorithm on dilated convolutions to increase the receptive field of the network. If increasing the depth of the network has often gone hand in hand with increasing the performance on many data rich applications, it has also been observed that the deeper the network, the more difficult its training is, due to vanishing gradient problems during the back-propagation steps. Residual networks _cite_ (ResNet) solve this problem by using identity residual connections to allow the gradient to back-propagate more easily. As a consequence, they are often faster to train than classical neural networks. The residual connections are thus now commonly used in all new architectures. Lots of pre-trained (usually on Imagenet _cite_) ResNet are available for the community. They can be fine-tuned for a new task. However, the structure of a pre-trained network cannot be changed radically which is a problem when a new architecture, such as ours, comes out. In this paper we present \gridnet, a new architecture especially designed for full scene labelling. \gridnet is composed of multiple paths from the input image to the output prediction, that we call streams, working at different image resolutions. High resolution streams allow the network to give an accurate prediction in combination with low resolution streams which carry more context thanks to bigger receptive fields. The streams are interconnected with convolutional and deconvolutional units to form the columns of our grid. With these connections, information from low and high resolutions can be shared. In Section~ _ref_, we review the network architectures used for full scene labelling from which \gridnet takes inspiration and we show how our approach generalises existing methods. In Section~ _ref_, we present the core components of the proposed \gridnet architecture. Finally, Section~ _ref_ shows results on the Cityscapes dataset.