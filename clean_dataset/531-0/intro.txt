Supervised training of deep convolution networks _cite_ is clearly highly effective for image classification, as shown by results on ImageNet _cite_ . The first layers of the networks trained on ImageNet also perform very well to classify images in very different databases, which indicates that these layers capture generic image information _cite_ . This paper shows that such generic properties can be captured by a scattering transform, which has the capability to build invariants to affine transformations. Scattering transforms compute hierarchical invariants along groups of transformations by cascading wavelet convolutions and modulus non-linearities, along the group variables _cite_ . Invariant scattering transforms to translations _cite_ and rotation-translations _cite_ have previously been applied to digit recognition and texture discrimination. The main source of variability of these images are due to deformations and stationary stochastic variability. This paper applies a scattering transform to CalTech-N and CalTech-N datasets, which include much more complex structural variability of objects and clutter, with good classification results. The scattering transform is adapted to the important source of variability of these images, by applying wavelet transforms along spatial, rotation, and scaling variables, with separable convolutions which is computationally efficient and by considering YUV color channels independently. It differs to most deep network in two respects. No ad-hoc renormalization is added within the network and no max pooling is performed. All poolings are average pooling, which guarantees the mathematical stability of the representation. It does not affect the quality of results when applied to wavelets, even in cluttered environments. This study concentrates on two layers because adding a third layer of wavelet coefficients did not reduce classification errors. Beyond the first two layers, which take care of translation, rotation and scaling variability, seems necessary to learn the filters involved in the third and next layers to improve classification performances.