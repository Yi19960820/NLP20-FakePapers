With an increasing number of digital images everyday, it is a need to display the images in any given space efficiently for easy browsing. This is facilitated by the use of thumbnail images which are a smaller version of the original images that effectively capture and represent the content of the original images. Thumbnails are widely used in social media platforms and for applications such as image gallery where there is a need to show the images within a fixed resolution display and in an organized manner. Earlier methods on thumbnail generation _cite_ employ a two-stage framework which first predict a salient map specifying the importance of various regions and objects of the input image and then use this for generating a crop for the thumbnail through a region search algorithm. Although _cite_ could crop images with a limited set of aspect ratios, it was mentioned it could be infeasible for a given overall saliency threshold value. Other approaches like _cite_ attempt to crop the most aesthetic part of the image. Huang et al. _cite_ were the first to address this problem with a direct solution. They used manually selected features and SVM to score a large set of candidate crops. The crop with largest score was finally selected as the thumbnail. But their method was slow and only considered thumbnails of fixed size. Esmaeili et al. _cite_, proposed a fast thumbnail generation algorithm inspired from a object detection framework (F-RCN) _cite_ to predict the bounding box coordinates of the thumbnail image. They utilize the aspect-ratio information by maintaining a set of filter banks for various aspect-ratio values. In this work, we propose a deep neural framework which generates a thumbnail of any given size and aspect-ratio for an input image in real time. We use Global Context Aggregation (GCA) and a Region Proposal Network (RPN) with adaptive convolutions to generate thumbnails of varying sizes with high accuracy and precision. GCA selectively attends and aggregates the contextual information from the entire image at each context location, increasing the receptive field of the model to the entire image and enhancing the feature representability of the CNNs. A RPN is then used to generate candidate bounding boxes for the thumbnail image. The RPN uses adaptive convolutions where the convolution filter weights are generated dynamically according to the aspect ratio of the thumbnail image. The adaptive convolution layer provides a smooth manifold for the convolution filters to vary and is used to disentangle the variations in shape of the bounding box predictions with respect to aspect ratio. The smooth manifold for convolution filters also allows us to interpolate and use unseen but similar values of aspect ratio for thumbnails during inference. In summary, we make the following two contributions: \noindent _inline_eq_ Our method can generate thumbnails of any size and aspect ratio efficiently, even for unseen values during training. \noindent _inline_eq_ Our method is fast and can generate thumbnails in real time. It can process around N images per second on a GPU.