\noindent Deep Learning using convolution neural networks (CNNs) is a hot topic in machine learning research and is the basis for a staggering number of consumer-facing data-driven applications, including those based on object recognition, voice recognition, and search~ _cite_ . Deep Learning is likely to be a major workload for future data analytics applications. Given the recent resurgence of CNNs, there have been few studies of CNNs from a data-systems perspective. Database systems have a role here, as efficiency in runtime and cost are chief concerns for owners of these systems. In contrast to many analytics that are memory-bound~ _cite_, CNN calculations are often compute-bound. Thus, processor technology plays a key role in these systems. GPUs are a popular choice to support CNNs, as modern GPUs offer between N TFLOPS (NVIDIA GRID KN) and N TFLOPS (NVIDIA KN) . However, GPUs are connected to host memory by a slow PCI-e interconnect. On the other hand, Microsoft's Project Adam argues that CPUs can deliver more cost-effective performance~ _cite_ . This debate is only going to get more interesting: the next generation of GPUs promise high-speed interconnection with host memory, while Intel's current Haswell CPU can achieve N FLOPS on a single chip. Moreover, SIMD parallelism has doubled in each of the last four Intel CPU generations and is likely to continue. For users who cannot control the footprint of the data center, another issue is that Amazon's ECN provides GPUs, but neither Azure nor Google Compute do. This motivates our study of CNN-based systems across different architectures. To conduct our study, we forked Caffe, the most popular open-source CNN system, and rebuilt its internals to produce a system we call {\em Caffe con Troll} (\cct) . \cct is a fully compatible end-to-end version of Caffe that matches Caffe's output on each layer, which is the unit of computation. As reported in the literature and confirmed by our experiments, the bottleneck layers are the so-called {\em convolutional layers}, which consume between N-N \% of execution time. Although we optimize all layers in \cct using essentially the same techniques, we focus on the tradeoff space for the convolutional layer on CPUs and GPUs. The convolutional layer operates on batches of tensors. Currently, \cct studies one method of performing the convolution called {\it lowering}, which remaps the high-dimensional input tensors into a series of standard matrix multiplications. In turn, these matrix multiplications are executed using a BLAS-compatible library, such as OpenBLAS or Intel's MKL. Lowering is used in many state-of-the-art systems, including Caffe and CuDNN. Previous approaches picked a single lowering, but we find that there are at least three different ways to lay out (or block) the matrices in the lowering operation. Our study reveals that the optimal strategy depends on the ratio of input to output channels of the convolution, and that while this means that one lowering usually dominates the others, we offer experimental evidence of this fact and propose a simple automatic optimizer to pick the best lowering in the tradeoff space automatically. On popular networks, we find that the optimal lowering contributes around N \% of the execution time for a single layer, and N \% performance improvement for end-to-end execution. More significantly, with some standard batching optimizations that are not employed in other systems, our study reveals that CPU systems are much faster than is often reported in the literature. Using a simple batching strategy, we achieve a N _inline_eq_ end-to-end speed improvement over Caffe on popular networks like CaffeNet, and up to an order of magnitude speedup for convolutional layers. Moreover, the end-to-end time is {\em proportional} to the FLOPS delivered by the CPU. We build on this proportionality of the devices to create a hybrid CPU-GPU system. Typically, CNN systems are either GPU-based or CPU-based--but not both. And the debate has reached almost religious levels. Using \cct, we argue that one should use both CPUs and GPUs, simultaneously. \cct is the first hybrid system that uses both CPUs and GPUs on a single layer. We show that on the ECN GPU instance, even with an underpowered, older N-core CPU, we can achieve N \% higher throughput on a single convolutional layer. Thus these hybrid solutions may become more effective than homogeneous systems and open new questions in provisioning such CNN systems. Finally, on the newly announced Amazon ECN instance with N GPUs we also show end-to-end speedups for N GPU + CPU of _inline_eq_ and speedups of _inline_eq_ using N GPUs.