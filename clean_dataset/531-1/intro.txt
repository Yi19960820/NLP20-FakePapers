Identifying conspicuous stimuli in the visual field is a key attentional mechanism in humans. While free viewing, our eyes tend to fixate on regions of the scene which have distinctive variations in visual stimuli such as a bright colour, unique texture or more complex semantic aspects such as presence of a familiar face or any sudden movements. This mechanism guides our eye gaze to the salient and informative regions in the scene. The human visual system is dictated by two kinds of attentional mechanisms: bottom-up and top-down~ _cite_ . Bottom-up factors, which are derived entirely from the visual scene, are responsible for the automatic deployment of attention towards discriminative regions in the scene. The involuntary detection of a red coloured STOP sign on the road, while driving, is an example of this attentional mechanism. This kind of attention is automatic, reflexive and stimulus-driven. On the contrary, the top-down attention mechanism is driven by internal factors such as subject's prior knowledge, expectations and the task at hand, making it situational and highly subjective~ _cite_ . It uses information available in the working memory, thereby biasing attention towards areas of the scene important to the current behavioral goals~ _cite_ . The selective attention exhibited by a hungry animal while searching for its camouflaged prey is an example of the top-down mechanism. In this work, we propose an approach for modeling the bottom-up visual attentional mechanism by predicting human eye fixations on images. This modeling, commonly referred to as visual saliency prediction, is a classic research area in the field of computer vision and neuroscience~ _cite_ . This modeling has applications in vision-related tasks such as video compression~ _cite_, object and action recognition~ _cite_, image retargetting~ _cite_ and surveillance systems~ _cite_ . In the past, many computational models have been developed to predict human eye fixations in the form of a saliency map-``a topographically arranged two dimensional map that represents the visual saliency of a scene"~ _cite_ . Saliency map predictions for two example images are shown in Fig.~ _ref_ . Saliency in a visual scene can arise from a spectrum of stimuli, both low-level (color/intensity, orientation, size etc.) and high-level (faces, text etc.) . Most of the classic saliency models~ _cite_ are biologically inspired and use multi-scale low-level visual features such as color and texture. However, these methods do not adequately capture the high level semantic aspects of a scene that can contribute to visual saliency. The wide variety of possible causes, both low-level and high-level, make it difficult to hand-craft good features for predicting saliency. This makes deep networks, which are capable of learning features from data in a task dependent manner, a natural choice for this problem. Recently, deep networks have shown impressive results on a diverse set of perceptual tasks such as speech recognition~ _cite_, natural language processing~ _cite_ and object recognition~ _cite_ . The ability of deep neural networks to automatically learn complex patterns from data in a hierarchical fashion makes them applicable to a wide range of problems with different modalities of data. Though neural networks are being used in the field of artificial intelligence since many decades, their recent wide applicability can be attributed to the increased computational power of GPUs, efficient techniques for training~ _cite_ and the availability of very large datasets~ _cite_ . In this work, we propose a fully convolutional neural network-DeepFix, for predicting human eye fixations on images in the form of a saliency map. Our model, inspired from VGG net~ _cite_, is a very deep network with N convolutional layers, each of a small kernel size, operating in succession on an image. The network is designed to capture object-level semantics, which can occur at multiple scales, efficiently through inception style~ _cite_ convolution blocks. Each inception module consists of a set of convolution layers with different kernel sizes operating in parallel. The global context of the scene, which is crucial for saliency prediction, is captured using convolutional layers with very large receptive fields. These layers are placed towards the end of the network and replace the densely connected inner product layers commonly present in convolutional nets. Fully Convolutional Nets (FCNs), in general, are location invariant i.e., a spatial shift in the input results only in a corresponding spatial shift of the output without affecting its values. This property prevents FCNs from learning location specific patterns such as the centre-bias. The proposed DeepFix model is designed to handle this through a novel Location Biased Convolutional (LBC) layer. Overall, our model predicts the saliency map from the image in an end-to-end manner, without relying on any hand-crafted features. Here, we summarize the key aspects of our DeepFix network: We evaluate the proposed network on two challenging eye fixation datasets--MITN~ _cite_, CATN~ _cite_ and show that it achieves state-of-the-art results on both these datasets.