Generalized zero-shot learning (GZSL) is a challenging task especially for unbalanced and large datasets such as ImageNet~ _cite_ . Although at training time no visual data of some classes, i.e. unseen classes, are provided the classifier must learn to differentiate between all classes, i.e. seen and unseen classes. As visual data of unseen classes is not available at training time, typically knowledge transfer from seen to unseen classes is achieved via some form of side information that encode semantic relationship between classes, i.e. class embeddings. Most approaches to GZSL~ _cite_ learn a mapping between images and their class embeddings. An orthogonal approach is to augment data by generating artificial images~ _cite_ . However, due to the level of detail missing in the synthetic images, CNN features extracted from them do not improve classification accuracy. To alleviate this issue, _cite_ proposed to generate image features via a conditional WGAN, which simplifies the task of the generative model and directly optimizes the loss on image features. Although the features generated by~ _cite_ improved GZSL significantly, GAN-based loss functions suffer from instability in training. Hence, recently conditional variational autoencoders (VAE) ~ _cite_ have been employed for this purpose. As GZSL is inherently a multi-modal learning task, _cite_ proposed to transform both modalities to the latent spaces of autoencoders and match the corresponding distributions by minimizing the Maximum Mean Discrepancy (MMD) . Learning such cross-modal embeddings is beneficial for potential downstream tasks that require multimodal fusion, e.g. visual question answering. In this domain, _cite_ recently used a cross-modal autoencoder to extend visual question answering to previously unseen objects. In this work, we train VAEs to encode and decode features from different modalities, e.g. images and class attributes, and use the learned latent features to train a generalized zero-shot learning classifier. Our latent representations are aligned by matching their parametrized distributions and by enforcing a cross-modal reconstruction criterion. Consequently, by explicitly enforcing alignment both in the latent features and in the distributions of latent features learned using different modalities, the VAEs enable knowledge transfer to unseen classes without forgetting the previously seen classes. Our contributions are as follows. (N) We propose the CADA-VAE model that learns shared cross-modal latent representations of multiple data modalities using VAEs via distribution alignment and cross alignment objectives. (N) We extensively evaluate our model using conventional benchmark datasets, i.e. CUB, SUN, AWAN and AWAN, on zero-shot and few-shot learning settings. Our model establishes the new state-of-the-art performance on generalized zero-shot and few-shot learning settings on all these datasets. Furthermore, we show that our model can be extended easily to more than two modalities that are trained simultaneously. (N) Finally, we show that the latent features learned by our model improve the state of the art in the truly large-scale ImageNet dataset in all splits for the generalized zero-shot learning task.