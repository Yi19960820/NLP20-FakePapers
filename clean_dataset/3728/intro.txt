Methods based on deep neural networks (\DNNs) have achieved impressive results for several computer vision tasks, such as image classification, object detection and image generation. Combined with the general tendency in the Computer Vision community of developing methods with a focus on high quantitative performance, this has motivated the adoption of \DNN-based methods, despite their black-box characteristics. In this work, we aim for more visually-descriptive predictions and propose means to improve the quality of the visual feedback capabilities of \DNN-based methods. Our goal is to bridge the gap between methods aiming at model interpretation, i.e., ~understanding what a given trained model has actually learned, and methods aiming at model explanation, i.e., ~justifying the decisions made by a model. Model interpretation of \DNNs ~is commonly achieved in two ways: either by a) manually inspecting visualizations of every single filter~ (or a random subset thereof) from every layer of the network~ (_cite_) or, more recently, by b) exhaustively comparing the internal activations produced by a given model w.r.t.~a dataset with pixel-wise annotations of possibly relevant concepts~ (_cite_) . These two paths have provided useful insights into the internal representations learned by \DNNs. However, they both have their own weaknesses. For the first case, the manual inspection of filter responses introduces a subjective bias, as was evidenced by _cite_ . In addition, the inspection of every filter from every layer becomes a cognitive-expensive practice for deeper models, which makes it a noisy process. For the second case, as stated by _cite_, the interpretation capabilities over the network are limited by the concepts for which annotation is available. Moreover, the cost of adding annotations for new concepts is quite high due to its pixel-wise nature. A third weakness, shared by both cases, is inherited by the way in which they generate spatial filter-wise responses, i.e., either through deconvolution-based heatmaps~ (_cite_) or by up-scaling the activation maps at a given layer/filter to the image space~ (_cite_) . On the one hand, deconvolution methods are able to produce heatmaps with high level of detail from any filter in the network. However, as can be seen in Fig.~ _ref_ \, (right), they suffer from artifacts introduced by strided operations in the back-propagation process. Up-scaled activation maps, on the other hand, can significantly lose details when displaying the response of filters with large receptive field from deeper layers. Moreover, they have the weakness of only being computable for convolutional layers. In order to alleviate these issues, we start from the hypothesis proven by _cite_, that a small subset of the internal filters of a network encode features that are important for the task that the network addresses. Based on that assumption, we propose a method which, given a trained \DNN ~model, automatically identifies a set of relevant internal filters whose encoded features serve as indicators for the class of interest to be predicted~ (Fig.~ _ref_ \, left) . These filters can originate from any type of internal layer of the network, i.e., ~ convolutional, fully connected, etc. is formulated as a _inline_eq_-lasso optimization problem in which a sparse set of filter-wise responses are linearly combined in order to predict the class of interest. At test time, . Given an image, a set of identified relevant filters, and a class prediction, we accompany the predicted class label with heatmap visualizations of the top-responding relevant filters for the predicted class, see Fig.~ _ref_ \, (center) . In addition, by improving the resampling operations within deconvnet-based methods, we are able to address the artifacts introduced in the back-propagation process, see Fig.~ _ref_ \, (right) . The code and models used to generate our visual explanations can be found in the following link~ . Overall, the proposed method removes the requirement of additional expensive pixel-wise annotation, by relying on the same annotations used to train the initial model. Moreover, by using our own variant of a deconvolution-based method, our method is able to consider the spatial response from any filter at any layer while still providing visually pleasant feedback. This allows our method to reach some level of explanation by interpretation. Finally, recent approaches to evaluate explanation methods measure the validity of an explanation either via user studies~ (_cite_) or by measuring its effect on a proxy task, e.g. object detection/segmentation~ (_cite_) . While user studies inherently add subjectivity, benchmarking through a proxy task steers the optimization of the explanation method towards such task. Here we propose an objective evaluation via , a synthetic dataset where the discriminative feature between the classes of interest is controlled. This allows us to produce ground-truth masks for the regions to be highlighted by the explanation. Furthermore, it allows us to quantitatively measure the performance of methods for model explanation. The main contributions of this work are four-fold. First, we propose an automatic method based on feature selection to identify the network-encoded features that are important for the prediction of a given class. This alleviates the requirement of exhaustive manual inspection or additional expensive pixel-wise annotations required by existing methods. Second, the proposed method is able to provide visual feedback with higher-level of detail over up-scaled raw activation maps and improved quality over recent deconvolution + guided back-propagation methods. Third, the proposed method is general enough to be applied to any type of network, independently of the type of layers that compose it. Fourth, we release a dataset and protocol specifically designed for the evaluation of methods for model explanation. To the best of our knowledge this is the first dataset aimed at such task. This paper is organized as follows: in Sec.~ _ref_ we position our work w.r.t. existing work. Sec.~ _ref_ presents the pipeline and inner-workings of the proposed method. In Sec.~ _ref_, we conduct a series of experiments evaluating different aspects of the proposed method. We draw conclusions in Sec.~ _ref_ .