Recent years have seen the breakthrough of mobile robotics into the consumer market. Domestic robots have become increasingly common, as well as vehicles making use of cameras, radar and other sensors to assist the driver. An important aspect of human-robot interaction, is the ability of artificial agents to understand the way humans think and talk about abstract spatial concepts. For example, a domestic robot may be asked to “clean the bathroom”, while a car may be asked to “stop at the parking area”. Hence, a robot's definition of “bathroom”, or “parking area” should point to the same set of places that a human would recognize as such. The problem of assigning a semantic spatial label to an image has been extensively studied in the computer and robot vision literature _cite_ . The most important challenges in identifying places come from the complexity of the concepts to be recognized and from the variability of the conditions in which the images are captured. Scenes from the same category may differ significantly, while images corresponding to different places may look similar. The historical take on these issues has been to model the visual appearance of scenes considering a large variety of both global and local descriptors _cite_ and several (shallow) learning models (e.g. SVMs, Random Forests) . Since the (re-) emergence of Convolutional Neural Networks (CNNs), approaches based on learning deep representations have become mainstream. Several works exploited deep models for visual-based scene classification and place recognition tasks, showing improved accuracy over traditional methods based on hand-crafted descriptors _cite_ . Some of these studies _cite_ demonstrated the benefit of adopting a region-based approach (i.e. considering only specific image parts) in combination with descriptors derived from CNNs, such as to obtain models which are robust to viewpoint changes and occlusions. With a similar motivation, lately several works in computer vision have attempted to bring back the notion of localities into deep networks, e.g. by designing appropriate pooling strategies _cite_ or by casting the problem within the Image-N-Class (INC) recognition framework _cite_, with a high degree of success. All these works decouple the choice of the significant localities from the learning of deep representations, as the CNN feature extraction and the classifier learning are implemented as two separate modules. This leads to two drawbacks: first, choosing heuristically the relevant localities means concretely cropping parts of the images before feeding them to the chosen features extractor. This is clearly sub-optimal, and might turn out to be computationally expensive. Second, it would be desirable to fully exploit the power of deep networks by directly learning the best representations for the task at hand, rather than re-use architectures trained on general-purpose databases like ImageNet and passively processing patches from the input images without adapting its weights. Ideally, a fully-unified approach would guarantee more discriminative representations, resulting in higher recognition accuracy. This paper contributes to this last research thread by addressing these two issues. We propose an approach for semantic place categorization which exploits local representations within a deep learning framework. Our method is inspired by the recent work~ _cite_, which demonstrates that, by dividing images into regions and representing them with CNN-based features, state-of-the-art scene recognition accuracy can be achieved by exploiting an INC approach, namely a parametric extension of the Na ve Bayes Nearest Neighbor (NBNN) model. Following this intuition, we propose a deep architecture for semantic scene classification which seamlessly integrates the NBNN and CNN frameworks (Fig.~ _ref_) . We automatize the multi-scale patch extraction process by adopting a fully-convolutional network _cite_, guaranteeing a significant advantage in terms of computational cost over two-steps methods. Furthermore, a differentiable counterpart of the traditional NBNN loss is considered to obtain an error that can be back-propagated to the underlying CNN layers, thus enabling end-to-end training. To the best of our knowledge, this is the first attempt to fully unify NBNN and CNN, building a deep version of Na ve Bayes Nearest Neighbor. We extensively evaluate our approach on several publicly-available benchmarks. Our results demonstrate the advantage of the proposed end-to-end learning scheme over previous works based on a two-step pipeline and the effectiveness of our deep network over state-of-the-art methods on challenging robot place categorization tasks.