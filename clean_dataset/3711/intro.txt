A standard formulation of supervised learning starts with a parametrized class of mappings, a training set of desired input-output pairs, and a loss function measuring deviation of actual output from desired output. The goal of learning is to minimize the average loss over the training set. A popular minimization method is stochastic gradient descent. For each input in sequence, the parameters of the mapping are updated in minus the direction of the gradient of the loss with respect to the parameters. Here we are concerned with a class of mappings known as convolutional networks (ConvNets) . Significant effort has been put into parallelizing ConvNet learning on GPUs, as in the popular software packages Caffe~ _cite_, Torch~ _cite_ and Theano _cite_ . ConvNet learning has also been distributed over multiple machines ~ _cite_ . However, there has been relatively little work on parallelizing ConvNet learning for single shared memory CPU machines. Here we introduce a software package called ZNN, which implements a novel parallel algorithm for ConvNet learning on multi-core and many-core CPU machines. ZNN implements ND ConvNets, with ND as a special case. ZNN can employ either direct or FFT convolution, and chooses between the two methods by autotuning each layer of the network. FFT convolution was previously applied to ND ConvNets running on GPUs~ _cite_, and is even more advantageous for ND ConvNets on CPUs. As far as we know, ZNN is the first publicly available software that supports efficient training of sliding window max-pooling ConvNets, which have been studied by~ _cite_ . There is related work on using Xeon Phi \texttrademark for supervised deep learning ~ _cite_ . and unsupervised deep learning~ _cite_ .