Recently, temporal action localization has drawn considerable interest in the computer vision community _cite_ . This task involves two components: (N) determining whether a video contains specific actions (such as diving, jump, \etc) and (N) identifying temporal boundaries (start time and end time) of each action instance. A typical framework used by many state-of-the-art systems _cite_ is fusing a large set of features and training classifiers that operate on sliding windows or segment proposals. Recently, an end-to-end deep learning framework called Segment-CNN (S-CNN) _cite_ based on ND ConvNets _cite_ demonstrated superior performances both in efficiency and accuracy on standard benchmarks such as THUMOS'N _cite_ . S-CNN consists of a proposal network for generating candidate video segments and a localization network for predicting segment-level scores of action classes. Although the localization network can be optimized to select segments with high overlaps with ground truth action instances, the detected action boundaries are still retained and thus are restricted to the pre-determined boundaries of a fixed set of proposal segments. As illustrated in Figure _ref_, our goal is to refine temporal boundaries from proposal segments to precisely localize boundaries of action instances. This motivates us to move beyond existing practices based on segment-level predictions, and explicitly focus on the issue of fine-grained, dense predictions in time. To achieve this goal, some existing techniques can be adapted: (N) Single-frame classifiers operate on each frame individually; (N) Recurrent Neural Networks (RNN) further take into account temporal dependencies across frames. But both of them fail to explicitly model the spatio-temporal information in raw videos. ND CNN _cite_ has been shown that it can learn spatio-temporal abstraction of high-level semantics directly from raw videos but loses granularity in time, which is important for precise localization, as mentioned above. For example, layers from _inline_eq_ to _inline_eq_ in the well-known CND architecture _cite_ reduce the temporal length of an input video by a factor of N. In pixel-level semantic segmentation, de-convolution proves to be an effective upsampling method in both image _cite_ and video _cite_ for producing output of the same resolution as the input. In our temporal localization problem, the temporal length of the output should be the same as the input video, but the spatial size should be reduced to NxN. Therefore, we not only need to upsample in time but also need to downsample in space. To this end, we propose a novel Convolutional-De-Convolutional (CDC) filter, which performs convolution in space (for semantic abstraction) and de-convolution in time (for frame-level resolution) simultaneously. It is unique in jointly modeling the spatio-temporal interactions between summarizing high-level semantics in space and inferring fine-grained action dynamics in time. On top of ND ConvNets, we stack multiple CDC layers to form our CDC network, which can achieve the aforementioned goal of temporal upsampling and spatial downsampling, and thereby can determine action categories and can refine boundaries of proposal segments to precisely localize action instances. In summary, this paper makes three novel contributions: (N) To the best of our knowledge, this is the first work to combine two reverse operations (\ie convolution and de-convolution) into a joint CDC filter, which simultaneously conducts downsampling in space and upsampling in time to infer both high-level action semantics and temporal dynamics at a fine granularity in time. (N) We build a CDC network using the proposed CDC filter to specifically address precise temporal action localization. The CDC network can be efficiently trained end-to-end from raw videos to produce dense scores that are used to predict action instances with precise boundaries. (N) Our model outperforms state-of-the-art methods in video per-frame action labeling and significantly boosts the precision of temporal action localization over a wide range of detection thresholds.