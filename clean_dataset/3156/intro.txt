Actions in videos arise from motions of objects with respect to other objects and/or the background. To understand an action, an effective architecture should recognize not only the appearance of the target object associated with the action, but also how it relates to other objects in the scene, in both space and time. Figure _ref_ shows four different categories of actions. Each column shown an action where, in temporal order, the figures above occur before the figures below. Recognizing the hand and the object is not enough. To distinguish _inline_eq_ motion from _inline_eq_ motion, the model must know how the hand moves against the background. It is more complicated to classify _inline_eq_ and _inline_eq_ since it is an XOR operation on the relative positions of the hand and the object resulting from the hand's movements. The key point here is the need for recognizing patterns in spatiotemporal context. Even the same hand-iron-background combination has different meanings in different spatiotemporal contexts. The number of combinations increases sharply as scenes become more complicated and the number of objects involved increases. Conventional convolution recognizes fixed patterns determined by the fixed filter parameters, so it is difficult to capture various variations that distinguish the action categories. To recognize {\em every} object-in-context pattern, the model needs to have more detailed filters, potentially leading to a blow up of the number parameters. On the other hand, although the object-in-context patterns can vary, they are related through a higher-order structure: pushing an iron, pulling an iron, pushing a pen, and so on. We hypothesize that the structure of object-in-context patterns can be learned, i.e., the model can learn to conclude object-in-context pattern given the context, and propose a corresponding feature extractor. Explicitly, let _inline_eq_ and _inline_eq_ respectively represent the input and output of a convolution. Let _inline_eq_ and _inline_eq_ represent a specific position of _inline_eq_ and the set of positions of _inline_eq_ where _inline_eq_ is computed, respectively. Denote conventional convolution operation (first-order) as _inline_eq_ where _inline_eq_ is the shared parameters at different positions. The parameters act as determined feature extractors as _inline_eq_ for different positions. As we analyze, the visual pattern of the target object can vary in different contexts, and feature extractors (filters) that ignore this dependence are not optimal. We replace the fixed filters with context-dependent filters _inline_eq_ where the filters _inline_eq_ are in turn obtained as _inline_eq_ . The mapping _inline_eq_ is the structure of object-in-context patterns and _inline_eq_ are the learned parameters as we hypothesize. The entire relation between _inline_eq_ and _inline_eq_ can be represented through _inline_eq_ . We define this as a higher-order operation since the function _inline_eq_ takes function _inline_eq_ as as an argument. The proposed model is able to capture spatiotemporal contexts effectively. We test our method on four benchmark datasets for action recognition: Kinetics-N _cite_, Something-Something VN _cite_, Something-Something VN, and Charades datasets _cite_ . Specifically, we make comprehensive ablation studies on Something-Something VN datasets and further evaluate on the other three datasets to demonstrate the generality of our proposed method. The experiments establish significant advantages of the proposed models over existing algorithms, achieving results on par with or better than the current state-of-the-art methods.