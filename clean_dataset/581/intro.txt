Many applications for sensor networks, cyber-physical system or Internet-of-Things require a low power consumption for long-term autonomous operation. Local preprocessing and classifying data at the edge nodes can be a solution to reduce data transmission and therefore, to reduce energy consumption. In addition, such an approach can avoid that enormous amounts of data need to be communicated to and processed by a centralized data analysis infrastructure. The detection of acoustic events is a typical example: Instead of streaming audio through the network to perform server-side event detection, the acoustic events of interest can be pre-detected directly at the sensing device reducing the network's data throughput significantly. The accurate detection and classification of individual acoustic events from a sound-emitting environment is of interest for many application such as surveillance _cite_ or environmental monitoring _cite_ . The low-power embedded devices used for such systems, however, come with very stringent memory and throughput limitations. These resource constraints impose severe limits to the complexity of suitable event detection algorithms. Recently, a convolutional neural network (CNN)-based approach has been proposed for acoustic event detection _cite_ using a network design adapted from image classification _cite_ . It has been shown that this approach outperforms previous state-of-the-art approaches by a large margin. Unfortunately, CNNs are computationally expensive, and the proposed algorithm also comes at the expense of having a huge memory requirement due to the huge number of parameters. For image classification a way to reduce the complexity of CNN-based classification systems has been presented _cite_ . These two approaches are joined in this paper to build a highly-accurate CNN capable of running on embedded platforms with limited resources. In this context, the present paper contains the following contributions: