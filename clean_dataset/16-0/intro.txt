Human action recognition _cite_ has become an active area in computer vision and there are many important research problems, such as event recognition _cite_, group based activities recognition _cite_, human object interactions _cite_ and activities in egocentric videos _cite_ . Most approaches have been proposed to recognize actions in RGB videos recorded by ND cameras. However, it still remains a challenging problem for three reasons. First, it is hard to well extract useful information from the high dimensional and low quality input data. Second, the RGB video is highly sensitive to some factors like illumination changes, occlusion and background clutter. Third, the identification of actions is related to high-level visual clues such as human poses and objects, which are very difficult to obtain from RGB videos directly. Humans can recognize actions with a few spots describing motions of the main joints of skeletons _cite_, and experiments show that a large set of actions can be recognized solely from skeletons _cite_ . In contrast to RGB based action recognition, skeleton based action recognition can avoid the awful task of feature extraction from videos and explicitly model the dynamics of actions. There are three ways to obtain skeletons: motion capture systems, RGB images and depth maps. Sophisticated motion capture systems are very expensive and require the user to wear a motion capture suit with markers. Extracting reliable skeletons from monocular RGB images or videos, i.e., pose estimation, is still an unsolved problem. Fortunately, with the recent advent of affordable depth sensors, it is much easier and cheaper to obtain ND skeletons from depth maps. For example, Shotton et al. _cite_ propose a method to quickly and accurately predict ND positions of body joints from a single depth image. These advances excite considerable interest for skeleton based action recognition and various algorithms have been proposed recently. Traditional skeleton based action recognition approaches are mainly divided into two categories: joint based approaches and body part based approaches. Joint based approaches consider the human skeleton as a set of points and use various positions based features such as joint positions _cite_ and pairwise relative joint positions _cite_ to characterize actions. While body part based approaches regard the human skeleton as a connected set of segments, and then focus on individual or connected pairs of body parts _cite_ and joint angles _cite_ . Based on handcrafted low-level features, both approaches employ relatively simple time series models, e.g., hidden Markov model _cite_, to recognize actions. However, human-engineered features are limited to represent the complexity of the intrinsic characteristics of actions and the subsequent time series models do not unleash the full potential of the sequential data. Inspired by the great success of deep learning for RGB based action recognition _cite_, there is a growing trend of using deep neural networks for skeleton based action recognition. Different structures of Recurrent Neural Networks (RNN), e.g., hierarchical RNN _cite_, RNN with regularizations _cite_, differential RNN _cite_ and part-aware Long Short-Term Memory (LSTM) _cite_, have been used to learn motion representations from raw skeletons. However, considering an action is a continuous evolution of articulated rigid segments connected by joints _cite_, these RNN-based methods only model the contextual information in the temporal domain by concatenating skeletons for each frame. In fact, different actions are performed with different spatial configurations of joints of skeletons. The dependency in the spatial domain also reflects the characteristics of actions and should not be neglected for skeleton based action recognition. To this end, we introduce a novel two-stream RNN architecture which incorporates both spatial and temporal networks for skeleton based action recognition. Figure _ref_ shows the pipeline of our method. The temporal stream uses a RNN based model to learn the temporal dynamics from the coordinates of joints at different time steps. We employ two different RNN models, and . Compared with, is designed according to human body kinematics and has fewer parameters. At the same time, the spatial stream learns the spatial dependency of joints. We propose a simple and effective method to model the spatial structure that first casts the spatial graph of articulated skeletons into a sequence of joints, then feeds this resulting sequence into a RNN structure. Different methods are explored to turn the graph structure into a sequence for the purpose of better maintaining the spatial relationships. The two channels are then combined by late fusion and the whole network is end-to-end trainable. Finally, to avoid overfitting and improve generalization, we exploit data augmentation techniques by using ND transformation, i.e., rotation transformation, scaling transformation and shear transformation to transform the ND coordinates of skeletons during training. In summary, the main contributions of this paper are listed as follows. First, we propose a two-stream RNN architecture to utilize both spatial and temporal relations of joints of skeletons. Second, we exploit and compare different architectures of both streams. Third, we propose data augmentation techniques based on ND transformation and demonstrate the effectiveness for skeleton based action recognition. Finally, our method obtains the state-of-the-art results on three important benchmarks for a variety of actions, i.e., generic actions (NTU RGB + D), interaction activities (SBU Interaction) and gestures (ChaLearn) .