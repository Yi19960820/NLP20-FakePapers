Nowadays, millions of videos are being uploaded to the Internet every day. These videos capture all aspects of multimedia content about their uploader's daily life. These explosively growing user generated content videos online are becoming an crucial source of video data. Automatically categorizing videos into concepts, such as people actions, objects, etc., has become an important research topic. Recently many work have been proposed to tackle with building concept detectors both in image domain and video domain _cite_ . However, the need for manual labels by human annotators has become one of the major important limitations for large-scale concept learning. It is even more so in video domain, since training concept detectors on videos is more challenging than on still images. Many image datasets such as ImageNet~ _cite_, CIFAR~ _cite_, PASCAL VOC _cite_, MS COCO~ _cite_ and Caltech~ _cite_ have been collected and manually labeled. In video domain, some largest datasets such as UCF-N _cite_, MCG-WEBV~ _cite_, TRECVID MED~ _cite_ and FCVID~ _cite_ are popular benchmark datasets for video classification. Collecting such datasets requires a large amount of human effort that can take thousands of man hours. In addition, manually labeling video requires playing back the video, which is more time consuming and expensive than labeling still images. As a result, the largest labeled video collection, FCVID~ _cite_, only contains about N million labels with N concept classes, much less than the N million labels with over N, N classes in the image collection ImageNet~ _cite_ . Many state-of-the-art models in visual classification are based on the neural networks~ _cite_ . As the architecture gets deeper, the neural network would need more data to train in order to get better performance. However, more data needs more human supervision which are more expensive to acquire in the video domain. Videos are available on the web and contain rich contextual information with a weak annotation about their content, such as their titles, descriptions and surrounding text. These webly-labeled data are orders of magnitude larger than that of any manually-labeled collections. Moreover, automatically extracted features from multiple modalities such as existing still image classification models, automatic speech recognition and optical character recognition tools can be useful additional information for the content of the video. Figure _ref_ shows an example of webly-labeled video for walking with a dog. As we see, the textual metadata we get from the web videos contain useful but very noisy information. The multi-modal prior information we get is correlated across modalities, as the image classification results and speech transcript show high probability of dog appearance, while the textual metadata indicates the same content. Some of the videos (about N \% in the FCVID dataset) have very little textual metadata and we can only obtain web labels via other modalities. To address the problem of learning detectors from the big web data, in this paper, we utilize multi-modal information to harness prior knowledge from the web video without any manual efforts. Existing methods on learning from noisy webly-labeled data has mainly focused on the image domain _cite_ . Existing studies demonstrated promising results in this direction. However, these methods are primarily based on some heuristic methods. It is not clear what objective is being optimized and where or even whether the learning process will converge. Moreover, these methods only utilize a single text modality in the image domain. It is unclear how to exploit the multi-modal prior knowledge for concept learning from the rich context of Internet video. To utilize the large amount of webly-labeled video data for concept learning, we propose a learning framework called WEbly-Labeled Learning (WELL) . It is established on the theories called curriculum learning ~ _cite_ and self-paced learning ~ _cite_ . The learning framework is motivated by human learning, where people generally start learning easier aspects of a concept, and then gradually take more complex examples into the learning process _cite_ . Following this idea, WELL learns a concept detector iteratively from first using a few samples with more confident labels (more related to the concept), then gradually incorporate more video samples with noisier labels. The algorithm combines the prior knowledge, called learning curriculum, extracted from the webly-labeled data with the dynamic information learned from the statistical model (self-paced) to determine which video samples to learn in the next iteration. This idea of easy-to-hard learning paradigm has been adopted for learning in noisy web data~ _cite_ and has been proved to be efficient to deal with noise and outliers. Our proposed method generalizes such learning paradigm using a clear objective function. It is proved to be convex and is a also general framework that can incorporate state-of-the-art deep learning methods to learn robust detectors from noisy data. Our framework fundamentally changes self-paced learning and allows learning for video concept detectors at unlimited scale. Figure~ _ref_ shows the architecture of the proposed method. We extract keyframe-level convolutional neural network features and feed them into WELL layer with average pooling and iterative learning process. We have also tried using other video features such as motion features and audio MFCC features. Our contributions are threefold. First, we address the problem of learning robust video concept detectors from noisy web data through a general framework with solid theoretical justifications. We show that WELL not only outperforms state-of-the-art learning methods on noisy labels, but also, notably, achieves comparable results with state-of-the-art models trained using manual annotation on one of the largest video dataset. Second, we provide detailed comparison of different approaches to exploit multi-modal curriculum from noisy labels and verify that our method is robust against certain level of noisiness in the video data. Finally, the efficacy and the scalability have been empirically demonstrated on two public benchmarks, including by far the largest manually-labeled video set called FCVID~ _cite_ and the largest multimedia dataset called YFCCNM~ _cite_ . The promising results suggest that detectors trained on sufficient webly-labeled videos may outperform detectors trained on any existing manually-labeled datasets.