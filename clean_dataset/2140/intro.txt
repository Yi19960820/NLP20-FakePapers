the field of multi-sensor image fusion, a common task is to combine infrared and visible images. It arises in many applications, such as surveillance _cite_, object detection and target recognition _cite_ _cite_ _cite_ . The key assumption is that different modalities convey complementary information. The main purpose of image fusion is to generate a single image, which combines the complementary sources of information from multiple images of the same scene _cite_ . The key problem is to extract the salient image content from both modalities and then fuse it to produce enhanced output. Many fusion methods have been proposed for this purpose. The methods most commonly used for image fusion are multi-scale transforms _cite_ _cite_ and representation learning based methods _cite_ _cite_ . In the multi-scale transform domain, there are many fusion tools, such as the discrete wavelet transform (DWT) _cite_, contourlet transform _cite_, shift-invariant shearlet transform _cite_, quaternion wavelet transform _cite_ and nonsubsampled shearlet transform _cite_ . These decomposition methods project source images to the frequency domain, which increases the computational complexity. Hence, researchers try to find other methods to process source images without a transform, such as representation learning based methods. In the representation learning domain, the most common methods are based on sparse representation (SR) _cite_ and dictionary learning _cite_ _cite_ . For instance, Zong et al. _cite_ proposed a novel medical image fusion method based on SR and Histogram of Oriented Gradients (HOG) . There are also many algorithms based on combining SR and other tools including pulse coupled neural network (PCNN) _cite_, low-rank representation (LRR) _cite_ and shearlet transform _cite_ . Other options include the joint sparse representation _cite_ and cosparse representation _cite_ . Although SR-based fusion methods achieve good fusion performance, these methods are complex, and dictionary learning is a very time-consuming operation, especially online learning. These problems have motivated a growing interest in deep learning to replace the dictionary learning phase in SR. In deep learning-based fusion methods, deep features of the source images are used to generate the fused image. In _cite_, Liu et al. proposed a fusion method based on convolutional sparse representation (CSR), in which multi-scale and multi-layer features are used to construct the fused output. A follow up work was presented in Liu et al. _cite_ . Image patches which contain different blur versions are used to train a network to produce a decision map. The fused image is obtained by using the decision map and source images. In ICCV N, Prabhakar et.al. _cite_ proposed a novel CNN-based fusion framework for a multi-exposure image fusion task. However, these deep learning-based methods still have drawbacks. The network is difficult to train when the training data is insufficient, especially in infrared and visible image fusion tasks. CNN-based methods tend to work for specific image fusion tasks. To overcome this drawback, Li et.al. _cite_ _cite_ proposed two novel fusion frameworks based on a pretrained network (VGG-N _cite_ and ResNet-N _cite_) . The pretrained network is used as a feature extraction tool. They compared this approach with a strategy involving training a novel network architecture (DenseFuse) _cite_ . Ma et.al. _cite_ _cite_, on the other hand, applied Generative Adversarial Networks to the image fusion task, which also achieves impressive fusion performance. A simple, yet effictive end-to-end fusion framework was proposed by Zhang et. al _cite_ . Interestingly, in these deep learning based methods, a very little attention is paid to image decomposition. The latent low rank representation (LatLRR) _cite_ is commonly used in clustering analysis tasks. The authors _cite_ also mentioned that LatLRR can be used to extract salient features from the input data. In this paper we follow this suggestion and propose a novel multi-level decomposition method (MDLatLRR) as a basis of a fusion framework for infrared and visible image fusion. In our fusion framework, MDLatLRR is used to decompose source images, so as to extract salient detail parts and the base parts. The detail parts are reconstructed by an adaptive fusion strategy and by a reshape operator. The base parts are fused by averaging strategy. Finally, the fused image is constructed by combining the detail parts and the base part. Compared with the state-of-the-art fusion methods, our fusion framework achieves better fusion performance in both subjective and objective evaluations. The main contributions are summarized as follows, (N) A LatLRR based multi-level decomposition method (MDLatLRR) is proposed for the image fusion task. In contrast to another LRR-based fusion method~ _cite_, which calculates the low-rank coefficients for each image, in our method, the LatLRR problem is only solved once in the training phase to learn a projection matrix _inline_eq_ . As the size of _inline_eq_ is only related to the image patch size, the fusion method can process the source images of an arbitrary size. (N) With the multi-level decomposition operation, the proposed fusion method captures more detail information from the source images. As the base part contains preponderance of contour features, it makes the weight-average strategy more effective. (N) Contrasting it with the _inline_eq_-norm and _inline_eq_-norm, the nuclear-norm (the fusion strategy for detail parts) calculates the sum of singular values, which preserves ND information from the source images. It will generate a larger weight when an input patch contains more texture and structural information. This behaviour is desirable in image processing tasks. This paper is structured as follows. In Section _ref_, we introduce preliminaries. In Section _ref_, we present our multi-level decomposition method (MDLatLRR) in detail. In Section _ref_, the proposed fusion framework based on MDLatLRR is introducted. The experimental results are presented in Section _ref_ . Finally, the conclusions are drawn in Section _ref_ .