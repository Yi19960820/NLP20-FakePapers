Compelling advantages of exploiting temporal rather than merely spatial cues for video classification have been shown lately~ _cite_ . In recent works, researchers have focused on improving modeling of spatio-temporal correlations. Like ND CNNs, ND CNNs try to learn local correlation along input channels. Therefore, ND CNNs neglect the hidden information in between channels correlations in both directions: space and time, which limits the performance of these architectures. Another major problem in using ND CNNs is training the video architectures calls for extra large labeled datasets. All of these issues negatively influence their computational cost and performance. To avoid these limitations, we propose (i) a new network architecture block that efficiently captures both spatial-channels and temporal-channels correlation information throughout network layers; and (ii) an effective supervision transfer that bridges the knowledge transfer between different architectures, such that training the networks from scratch is no longer needed. Motivated by the above observations, we introduce the spatio-temporal channel correlation (STC) block. The aim of this block is considering the information of inter channels correlations over the spatial and temporal features simultaneously. For any set of transformation in the network (e.g. convolutional layers) a STC block can be used for performing spatio-temporal channel correlation feature learning.The STC block has two branches: a spatial correlation branch (SCB) and a temporal correlation branch (TCB) . The SCB considers spatial channel-wise information while TCB considers the temporal channel-wise information. The input features _inline_eq_ are fed to SCB and TCB. In SCB a spatial global pooling operation is done to generate a representation of the global receptive field which plays two vital roles in network: (i) considering global correlations in _inline_eq_ by aggregating the global features over the input, (ii) providing a channel-wise descriptor for analyzing the between channels correlations. This channel-wise feature vector is then fed to two bottleneck fully connected layers which learn the dependencies between channels. The same procedure happens in TCB, however, for the first step a temporal global pooling is used instead of the spatial global pooling. Output features of these two branches are then combined and returned as the output of the STC block. These output features can be combined with the output features of the corresponding layer (s) . By employing such features along-side traditional features available inside a ND CNN, we enrich the representation capability of ND CNNs. Therefore, the STC block equipped ND CNNs are capable of learning channel wise dependencies which enables them to learn better representations of videos. We have added the STC block to the current state-of-the-art ND CNN architectures such as ND-ResNext and ND-ResNet _cite_ . The STC block is inserted after each residual block of these networks. As mentioned before, training ND CNNs from scratch need a large labeled dataset. It has been shown that training ND Convolution Networks ~ _cite_ from scratch takes two months~ _cite_ for them to learn a good feature representation from a large scale dataset like Sports-NM, which is then finetuned on target datasets to improve performance. Another major contribution of our work therefore is to achieve supervision transfer across architectures, thus avoiding the need to train ND CNNs from scratch. Specifically, we show that a ND CNN pre-trained on ImageNet can act as ` a teacher ' for supervision transfer to a randomly initialized ND CNN for a stable weight initialization. In this way we avoid the excessive computational workload and training time. Through this transfer learning, we outperform the performance of generic ND CNNs (CND~ _cite_) which was trained on Sports-NM and finetuned on the target datasets HMDBN and UCFN. The rest of the paper is organized as follows. In Section~ _ref_, we discuss related work. Section~ _ref_ describes our proposed approaches. The implementation details, experimental results and their analysis are presented in Section~ _ref_ . Finally, conclusions are drawn in Section~ _ref_ .