Since the breakthrough work of Krizhevsky et al.~ _cite_ on ImageNet _cite_, researches on convolutional neural networks (CNN) have been exploding. Among them, a lot of researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection _cite_, object recognition _cite_, image retrieval _cite_, etc. Features are usually from different CNN layer activations or outputs. To achieve advanced and robust performance, people either fine-tune the pre-trained CNN models on their own tasks, or make extensively data augmentation to get robust classifiers. These developed techniques have shown promising results in comparison to conventional methods using standard feature representations like bag-of-words _cite_, sparse-coding _cite_, etc. However, there are two limitations of these kind of methods. First, neural codes from middle-layer are difficult to explain. Second, neural code extraction from whole image definitely loss many context information. These two are summarized to be the well-known {\em semantic gap} ~ _cite_ . Meanwhile, region features are appealing on recognition task. For instance, Gu \etal~ _cite_ employ mid-level features like contour shape, edge shape, color and texture to describe each region for visual recognition tasks. It is well known that region features can naturally preserve more mid-level semantic information like materials, textures, shapes, etc of objects~ _cite_ . However, traditional region representations either highly depend on segmentation algorithm~ _cite_, or lack of a generic semantic representation for regions for various visual recognition tasks. To address these challenges and leverage the power from both richness and semantics of these two types of feature representations, in this paper, we propose to integrate the semantic output, i.e., the output from the soft-max layer of CNN models, as well as region proposals to achieve compact yet effective visual representations, namely {\em deep attribute} (DA) . Since the soft-max layer neural codes are the probability response to the categories on which CNNs are trained, it is fairly compact, semantic, and sparse due to insignificant responses to most categories. Briefly, the proposed method contains four key components. The major difference to traditional off-the-shelf CNN methods are illustrated in Figure~ _ref_, while the proposed approach is further illustrated in details in Figure~ _ref_ . To capture more context information related to scale, we also study different pooling layout schemes like multi-scale-pooling and spatial pyramid-pooling extensions. To demonstrate the capacity and robustness of the proposed technique, we employ deep attributes on three visual recognition tasks: image classification, fine-grained object recognition, and visual instance retrieval. Experimental results on several standard benchmark datasets show that deep attributes can achieve state-of-the-arts performance. Especially, the context-aware region refining (CARR) algorithm clearly outperforms peer methods with a significant margin. In summary, the major contributions of this paper include: In the rest of the paper, we will first give a brief survey on related works in Section N, and then present the details of the proposed deep attribute approach in Section N. In Section N, we conduct experiments to study different aspects of setting which will impact the performance of the algorithm. We show experimental results on three visual recognition tasks on Section N. Conclusions and discussions are given in Section N.