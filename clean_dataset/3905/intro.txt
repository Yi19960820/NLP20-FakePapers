The capability of detecting objects in challenging environments is a key component for many computer vision and robotics task. Current leading object detectors---Faster-RCNNs~ _cite_, SSD~ _cite_, RFCN~ _cite_, YoloN~ _cite_---all rely on convolutional neural networks. However, to perform at their best, they require huge amounts of labeled training data, which is usually time consuming and expensive to create. Using synthetic images is therefore very attractive to train object detectors, as the labeling comes for free. Unfortunately, synthetic rendering pipelines are usually unable to reproduce the statistics produced by their real-world counterparts. This is often referred to as the 'domain gap' between synthetic and real data and the transfer from one to another usually results in deteriorated performance, as observed in _cite_ for example. Several approaches have tried to overcome this domain gap. For instance, _cite_ use synthetic images in addition to real ones to boost performance. While this usually results in good performance, it is still dependent on real world labeled data. Transfer learning approaches are also possible~ _cite_, however they also require real images of the objects to detect. _cite_ create photo-realistic graphics renderings and _cite_ compose realistic scenes which both shows to improve performance. Unfortunately, these strategies are usually difficult to engineer, need domain specific expertise and require some additional data such as illumination information and scene labeling to create realistic scenes. _cite_ uses 'domain randomization' to narrow the gap. While this has shown very promising results, it has mainly been demonstrated to work with simple objects and scenarios. Other works~ _cite_ use Generative Adversarial Networks~ (GANs) to remove the domain gap, however, GANs are still very brittle and hard to train, and to our knowledge they have not been used for detection tasks yet. In this paper we consider a simple alternative solution. As shown by~ _cite_ and illustrated in Fig.~ _ref_, many of today's modern feature extractors can be split into a feature extractor and some remaining layers that depend on the meta-architecture of the detector. Our claim is twofold: a) the pre-trained feature extractors are already rich enough and do not need to be retrained when considering new objects to detect; b) when applied to an image synthetically generated using simple rendering techniques, the feature extractors work as a ``projector'' and output image features that are close to real image features. Therefore, by freezing the weights of feature extractor pre-trained on real data and by only adapting the weights of the remaining layers during training, we are able to train state-of-the-art object detectors purely on synthetic data. While using pre-trained layers for feature extraction is not new (for example, VGG~ _cite_ has been used extensively for this purpose), our contribution is therefore to show that this approach is also advantageous when training with synthetic data. We also show in this paper that this observation is fairly general and we give both qualitative and quantitative experiments for different detectors---Faster-RCNN~ _cite_, RFCN~ _cite_ and Mask-RCNN~ _cite_---and different feature extraction networks---InceptionResnet~ _cite_ and ResnetN~ _cite_ . Furthermore, we show that different cameras have different image statistics that allow different levels of performance when re-trained on synthetic data. We will demonstrate that performance is significantly boosted for these cameras if our simple approach is applied. In the remainder of the paper we first discuss related work, describe how we generate synthetic data, demonstrate the domain gap between synthetic and real data, and show how to boost object detection by freezing the pre-trained feature extractor during training. Finally, we show qualitative and quantitative experiments.