Face recognition is a major field in computer vision. In recent years, the community shifted from engineering features by hand to using deep learning approaches. As a result of this paradigm shift, the general face recognition performance increased massively. Among others, work by Facebook _cite_ and Google _cite_ demonstrated that a training with large-scale datasets leads to a close-to-maximum performance on standard benchmarks, such as LFW _cite_, YTF _cite_ or IJB-A _cite_ . Hence, it became apparent that deep learning approaches strongly rely on large and complex training sets to generalize well in unconstrained settings. However, the common approach of collecting and maintaining large-scale datasets has two severe disadvantages, which we aim to overcome in this work: N) It is practically unfeasible to collect large-scale training datasets for advanced face recognition tasks, such as e.g. disguised or occluded identities in difficult illumination conditions or strong pose variations. However, with the introduction of more complex benchmark datasets such as MegaFace _cite_, IJB-B _cite_, IJB-C _cite_ and the recently proposed disguised face recognition challenge _cite_, it becomes apparent that in the future more complex and thus rarer data is needed for training. This data is difficult to obtain, since it is time-intensive to acquire and annotate. We aim to overcome this problem by using a parametric face image generator to synthesize arbitrary amounts of face images. N) Large data sets are not just difficult to collect but are also hard to control in terms of dataset variability. It is not scalable to annotate all the variations of interest such as e.g. pose, expression or illumination. In addition, a labor-intensive ground truth annotation process is also prone to errors. However, controlling the distribution of these variables is of critical importance for face-recognition applications since these are often deployed in security systems. A bias or a blind spot in these systems could have far-reaching implications. Synthetic data offers full control and detailed reliable annotations for free. In our experiments, we show that synthetic face images are complementary to biased real-world datasets resulting in an increased recognition performance across several recognition benchmarks. Observing these challenges and the fact that statistical models of faces in ND can be combined with computer graphics to synthesize face images motivates the major research question that guides this work: ND Morphable Face Models (NDMMs) _cite_ are an active field of research since decades. Rendering synthetic ND images of faces is a basic capability of NDMMs. We will leverage this capability in order to generate large scale datasets, which will then be used to train deep face recognition systems. The parametric nature of the NDMM thereby provides full control over the facial identities in the terms of shape and albedo texture in an image, but also over nuisance parameters such as pose, illumination and the facial expression. In addition, NDMMs enable us to create possibly infinite data sets in arbitrary depth (number of samples per identity) and width (number of identities) . The synthetic images can easily be released publicly for research purposes as the NDMM is publicly available. However, the major advantage of our approach is that it opens an alternative approach to the acquisition and labelling of ground truth data for new face recognition problems. Namely, the possibility to model the problem generatively, which is much simpler for many facial appearance variations e.g. due to changing illumination conditions or head pose variations. A critical part of the research question is whether deep face recognition systems can transfer the knowledge acquired from synthetic data and combine it with real data. Because although computer graphics and ND facial modelling have made great progress, some variations of face images currently cannot be represented realistically in a parametric model, such as the skin texture and wrinkles or facial occluders such as beards or scarfs. In this work, we analyze this research question in in detail. Thereby, we use synthetically generated data to train a vanilla OpenFace _cite_ implementation of the FaceNet architecture _cite_ . Our analysis documents that: The paper is structured as follows: We discuss related work in Section~ _ref_ and introduce our face image generator in Section~ _ref_ . We study in detail how synthetic data can support the training and reliability of face recognition systems in Section~ _ref_ . We conclude our work and discuss caveats in Section~ _ref_ .