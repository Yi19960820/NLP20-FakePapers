Deep learning has rapidly become the state of the art in modern machine learning, and has surpassed the human level in several classical problems. Majority of research concentrates on the accuracy of the model, but also the computational load has been studied. This line of research attempts to reduce the execution time of the forward pass of the network. Well-known techniques for constructing lightweight yet accurate networks include decomposing the convolutions either in horizontal and vertical dimensions _cite_, or in spatial and channel dimensions _cite_, or using a fully convolutional architecture _cite_ . In non-static environments, the computational requirements may have a large variation. For example, in the use case of this paper---real-time age estimation---the number of faces seen by the camera directly influences the amount of computation. Namely, each detected face is sent to the GPU for age assessment. Thus, the workload for N faces in front of the camera is N-fold compared to just a single face. Additionally, the same network should be deployable over a number of architectures: For example, in N there were already over N, N different hardware architectures running the Android operating system. Thus, there is a demand for an elastic network structure, able to adjust the computational complexity on the fly. The network should be monolithic (as opposed to just training several networks with different complexities), since the storage and computational overhead of deploying a large number of separate networks would be prohibitive in many applications. Recently, Huang et al. _cite_ proposed a network architecture addressing these problems. Their Multi-Scale Dense Network (MSDNet) adds early-exits to the computational graph, and stores a representation of the input at several resolutions throughout the network. This way, the computational time can be adjusted even to the extent of anytime prediction, where the network almost always has some prediction at hand. In this work, we take a simpler approach: Instead of proposing a new network topology, we study the effect of adding early-exits to any network . Namely, there exist a number of successful network topologies pretrained with large image databases and the progress of deep learning is so rapid, that new network topologies appear daily. We propose a general framework that systematically ``elastifies'' an arbitrary network to provide novel trade-offs between execution time and accuracy, while leveraging key properties of the original network.