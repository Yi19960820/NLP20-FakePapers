Convolutional neural networks (CNNs) have recently achieved the state-of-the-art performance in many image analysis tasks _cite_ . It has also been shown to be very effective in extracting features for action recognition and other tasks involving temporal data _cite_ . In most (if not all) of the existing CNN architectures such as the ``AlexNet'' _cite_, ``VGGNet'' _cite_ and ``InceptionNet'' _cite_, pooling is an important component for aggregating local features and reducing computational burden. In some networks _cite_, convolution with strides (larger than _inline_eq_) is used to reduce the dimension of features to achieve a function similar to pooling. Noticeably, pooling is the only component in a typical CNN architecture (without considering normalization layers which are mostly for fast training and convergence) that is completely engineered with prior knowledge (such as max pooling and average pooling) instead of learning from data. Since the power of CNNs comes from their ability to adapt to the data through learning, the natural question to ask is ``Would pooling become the bottleneck of the network performance, and could pooling be learned in a similar way as other components from data?''. To answer this question, this paper proposes a learnable pooling function based on recurrent neural units. Together with the convolutional layers and fully connected layers in CNNs, such a learnable pooling leads to a fully trainable network (FTN) . Compared with the traditional CNNs, it has the benefit of being fully adapted to data and task. Experimental results have demonstrated that FTN can improve the performance, especially on small networks. The main contributions of this paper are summarized as follows. The rest of the paper is organized as follows. The related work is discussed in Section _ref_ . Section _ref_ presents the proposed method and experimental results are shown in Section _ref_ . Conclusion is drawn in Section _ref_ .