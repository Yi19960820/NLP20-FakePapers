Action recognition methods based on skeleton data have been widely investigated and attracted considerable attention due to their strong adaptability to the dynamic circumstance and complicated background~ _cite_ . Conventional deep-learning-based methods manually structure the skeleton as a sequence of joint-coordinate vectors~ _cite_ or as a pseudo-image~ _cite_, which is fed into RNNs or CNNs to generate the prediction. However, representing the skeleton data as a vector sequence or a ND grid cannot fully express the dependency between correlated joints. The skeleton is naturally structured as a graph in a non-Euclidean space with the joints as vertexes and their natural connections in the human body as edges. The previous methods cannot exploit the graph structure of the skeleton data and are difficult to generalize to skeletons with arbitrary forms. Recently, graph convolutional networks (GCNs), which generalize convolution from image to graph, have been successfully adopted in many applications _cite_ . For the skeleton-based action recognition task, Yan et al.~ _cite_ first apply GCNs to model the skeleton data. They construct a spatial graph based on the natural connections of joints in the human body and add the temporal edges between corresponding joints in consecutive frames. A distance-based sampling function is proposed for constructing the graph convolutional layer, which is then employed as a basic module to build the final spatiotemporal graph convolutional network (ST-GCN) . However, there are three disadvantages for the process of the graph construction in ST-GCN~ _cite_: (N) The skeleton graph employed in ST-GCN is heuristically predefined and represents only the physical structure of the human body. Thus it is not guaranteed to be optimal for the action recognition task. For example, the relationship between the two hands is important for recognizing classes such as ``clapping" and ``reading.'' However, it is difficult for ST-GCN to capture the dependency between the two hands since they are located far away from each other in the predefined human-body-based graphs. (N) The structure of GCNs is hierarchical where different layers contain multilevel semantic information. However, the topology of the graph applied in ST-GCN is fixed over all the layers, which lacks the flexibility and capacity to model the multilevel semantic information contained in all of the layers; (N) One fixed graph structure may not be optimal for all the samples of different action classes. For classes such as ``wiping face" and ``touching head", the connection between the hands and head should be stronger, but it is not true for some other classes, such as ``jumping up" and ``sitting down". This fact suggests that the graph structure should be data dependent, which, however, is not supported in ST-GCN. To solve the above problems, a novel adaptive graph convolutional network is proposed in this work. It parameterizes two types of graphs, the structure of which are trained and updated jointly with convolutional parameters of the model. One type is a global graph, which represents the common pattern for all the data. Another type is an individual graph, which represents the unique pattern for each data. Both of the two types of graphs are optimized individually for different layers, which can better fit the hierarchical structure of the model. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Another notable problem in ST-GCN is that the feature vector attached to each vertex only contains ND or ND coordinates of the joints, which can be regarded as the first-order information of the skeleton data. However, the second-order information, which represents the feature of bones between two joints, is not exploited. Typically, the lengths and directions of bones are naturally more informative and discriminative for action recognition. In order to exploit the second-order information of the skeleton data, the lengths and directions of bones are formulated as a vector pointing from its source joint to its target joint. Similar to the first-order information, the vector is fed into an adaptive graph convolutional network to predict the action label. Moreover, a two-stream framework is proposed to fuse the first-order and second-order information to further improve the performance. To verify the superiority of the proposed model, namely, the two-stream adaptive graph convolutional network (Ns-AGCN), extensive experiments are performed on two large-scale datasets: NTU-RGBD~ _cite_ and Kinetics-Skeleton~ _cite_ . Our model achieves state-of-the-art performance on both of the datasets. The main contributions of our work lie in three folds: (N) An adaptive graph convolutional network is proposed to adaptively learn the topology of the graph for different GCN layers and skeleton samples in an end-to-end manner, which can better suit the action recognition task and the hierarchical structure of the GCNs. (N) The second-order information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance. (N) On two large-scale datasets for skeleton-based action recognition, the proposed Ns-AGCN exceeds the state-of-the-art by a significant margin. The code will be released for future work and to facilitate communication .