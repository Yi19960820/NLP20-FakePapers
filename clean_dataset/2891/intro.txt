How do humans come to understand their world? Consider the learning of mathematics-we learn basic mathematics before proceeding to advanced mathematics, the understanding of basic math serving as the foundation on which to develop an understanding of advanced math. Once an advanced understanding of math is acquired, it in turn is used to better understand basic math, reinforcing an understanding of mathematics as a whole. Similarly in perception-higher levels of abstraction within our perceptual hierarchies are built upon the lower levels of abstraction. Additionally, using higher level interpretations to reinforce lower level interpretations can provide benefits to perception as a whole, in the same way that using a more expansive understanding of mathematics to understand elementary mathematics reinforces an understanding of mathematics as a whole. Disregarding the interplay between levels of abstraction in perception, the recent trend in deep neural networks is to simply make networks deeper. Since networks have become so deep, researchers have developed a now standard design that divides network layers into blocks of layers~ _cite_ . Each block consists of layers of transforms that produce feature maps of the same shape. Cross-block transforms recombine the previous features and incorporate strided convolutions or pooling operations to reshape the feature maps. This strategy is an extension of the traditional design of the multilayer perceptron or fully-connected network~ _cite_ . The introduction of blocks has allowed a better organization of the evolving layers of abstraction within those networks. Overall, more high level features are abstracted through these transforms (Fig.~ _ref_, ~ _ref_) . However, the unstructured recombination of features in existing networks has made the investigation of deep neural networks nontrivial~ _cite_ . Aside from deviating from basic intuitions about learning and perception, the now standard design has other apparent shortcomings. Since strided convolution or pooling are used to resize the feature maps by integer scales, researchers are compelled to create ad hoc network shapes, ones which are subject to awkward constraints of integer pooling and strides. Some networks assign more layers in later stages, where the feature maps are very small~ _cite_ . Some other networks are wider but have fewer layers~ _cite_ . The specification of feature map dimensions or distribution of computational resources is highly engineered and uneven in these designs. It is difficult to determine which designs outperform others, given that the overall shape is different. To make things worse, when the task changes, the network shape needs to be handcrafted again. Arguably the biggest shortcoming of existing designs is that they afford little intuition on the training process, and consequently make deep networks notoriously hard to train. Researchers have expended significant effort in developing better methods to train these networks. In this work we introduce an architecture more closely in line with intuitions of biological learning and perception: Evenly Cascaded convolutional Network (ECN) . ECN is N) easier than existing architectures to adapt to new tasks, N) produces internal representations which are more humanly interpretable, N) performs robustly as parameter count is restricted, and N) produces competitive performance when compared to other state-of-the-art methods. ECN is structured around the insights that N) maintaining low-level features through to the upper layers of a network is beneficial, N) allowing multiple levels of features to interact with each other within the network is beneficial, N) abrupt changes in feature map dimension could be less ideal than gradual changes, and N) the manner in which features are conventionally combined within network blocks hampers the preservation of low-level features. Our architecture instantiates the first two of these insights through a ``cascade'' architecture-a two stream architecture, one stream for low-level features, one stream for successively higher level features, where these two streams interact at every layer. This differs from the existing method of using skip connections to introduce low-level features into upper level network layers in that low level features are maintained, and modulated appropriately, rather than simply jumping layers~ _cite_ . This approach differs from a conventional two-stream approach~ _cite_ in that both streams interact with each other. For the third insight, in both streams of our cascade architecture bilinear interpolation is employed for fractional pooling, allowing a gradual rather than abrupt decrease of feature map dimensions. Different from existing architectures, in ECN a scaling factor is introduced which simplifies the design of the shape of the network removing the need for handcrafting network shape. For the fourth insight we remove the conventional combination of features across blocks in neural networks in order to preserve the low-level signal in ECN's two-stream cascade architecture. We demonstrate that preserving multilevel features enhances training by providing easy-to-train shortcuts. Our evaluation of ECN resulted in several intriguing results: N) With ECN's principled structuring, shallow networks (Fig. _ref_) seem to perform competitively well when compared to extremely deep networks. N) Complex high level tasks such as image classification can be approached through evenly downsampling and adapting a set of highly structured features (Fig. _ref_) . N) Low level features may be of critical importance in high level tasks such as image classification: not only do these features remain similar in the deeper adaptation process, but they may have provided major convenience for the training of high level features. Finally, we evaluate ECN using multiple convolution block designs and find that recurrent and recursive designs lead to improved efficiency and accuracy. Without additional treatment, a standard convolution block design combined with recurrent connections leads to state-of-the-art accuracy in benchmark image classification tasks. Another block design using a recursive filter gives rise to state-of-the-art efficiency. Our N-cascading-layer design with under Nk parameters achieves N \% and N \% accuracy on CIFAR-N and CIFAR-N datasets, respectively, outperforming the current state-of-the-art on small parameter networks, and our N million parameter version is competitive to the state-of-the-art.