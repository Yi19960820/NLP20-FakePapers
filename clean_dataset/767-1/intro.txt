Deep Neural Networks (DNNs) are powerful methods for solving large scale real world problems such as automated image classification _cite_, natural language processing _cite_, human action recognition _cite_, or physics _cite_ ; see also _cite_ . Since DNN training methodologies (unsupervised pretraining, dropout, parallelization, GPUs etc.) have been improved _cite_, DNNs are recently able to harvest extremely large amounts of training data and can thus achieve record performances in many research fields. At the same time, DNNs are generally conceived as black box methods, and users might consider this lack of transparency a drawback in practice. Namely, it is difficult to intuitively and quantitatively understand the result of DNN inference, i.e. for an {\em individual} novel input data point, {\it what} made the trained DNN model arrive at a particular response. Note that this aspect differs from feature selection _cite_, where the question is: which features are on average salient for the {\em ensemble} of training data. Only recently, the transparency problem has been receiving more attention for general nonlinear estimators _cite_ . Several methods have been developed to understand what a DNN has learned _cite_ . While in DNN a large body of work is dedicated to visualize particular neurons or neuron layers _cite_, we focus here on methods which visualize the impact of particular regions of a given and fixed single image for a prediction of this image. Zeiler and Fergus _cite_ have proposed in their work a network propagation technique to identify patterns in a given input image that are linked to a particular DNN prediction. This method runs a backward algorithm that reuses the weights at each layer to propagate the prediction from the output down to the input layer, leading to the creation of meaningful patterns in input space. This approach was designed for a particular type of neural network, namely convolutional nets with max-pooling and rectified linear units. A limitation of the deconvolution method is the absence of a particular theoretical criterion that would directly connect the predicted output to the produced pattern in a quantifiable way. Furthermore, the usage of image-specific information for generating the backprojections in this method is limited to max-pooling layers alone. Further previous work has focused on understanding non-linear learning methods such as DNNs or kernel methods _cite_ essentially by sensitivity analysis in the sense of scores based on partial derivatives at the given sample. Partial derivatives look at local sensitivities detached from the decision boundary of the classifier. Simonyan et al. cite {DBLP: journals/corr/SimonyanVZN} applied partial derivatives for visualizing input sensitivities in images classified by a deep neural network. Note that although _cite_ describes a Taylor series, it relies on partial derivatives at the given image for computation of results. In a strict sense partial derivatives do not explain a classifier's decision ({\it ``what speaks for the presence of a car in the image''}), but rather tell us {\it what change would make the image more or less belong to the category car} . As shown later these two types of explanations lead to very different results in practice. An approach, Layer-wise Relevance Propagation (LRP), which is applicable to arbitrary types of neural unit activities (even if they are non-continuous) and to general DNN architectures has been proposed by Bach et al.~ _cite_ . This work aims at explaining the difference of a prediction _inline_eq_ relative to the neutral state _inline_eq_ . The LRP method relies on a conservation principle to propagate the prediction back without using gradients. This principle ensures that the network output activity is fully redistributed through the layers of a DNN onto the input variables, i.e., neither positive nor negative evidence is lost. In the following we will denote the visualizations produced by the above methods as heatmaps. While per se a heatmap is an interesting and intuitive tool that can already allow to achieve transparency, it is difficult to quantitatively evaluate the quality of a heatmap. In other words we may ask: what exactly makes a ``good'' heatmap. A human may be able to intuitively assess the quality of a heatmap, e.g., by matching with a prior of what is regarded as being relevant (see Figure _ref_) . For practical applications, however, an automated objective and quantitative measure for assessing heatmap quality becomes necessary. Note that the validation of heatmap quality is important if we want to use it as input for further analysis. For example we could run computationally more expensive algorithms only on relevant regions in the image, where relevance is detected by a heatmap. \noindent In this paper we contribute by The next section briefly introduces three existing methods for computing heatmaps. Section _ref_ discusses the heatmap evaluation problem and presents a generic framework for this task. Two experimental results are presented in Section _ref_: The first experiment compares different heatmapping algorithms on SUNN _cite_, ILSVRCN _cite_ and MIT Places _cite_ data sets and the second experiment investigates the correlation between heatmap quality and neural network performance on the CIFAR-N data set _cite_ . We conclude the paper in Section _ref_ and give an outlook.