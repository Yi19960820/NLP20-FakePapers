Deep convolutional neural networks (CNNs) are good at accurately mapping inputs to outputs from large amounts of labeled data. For example, deep residual learning _cite_ already achieves superhuman performance at recognizing objects on the ImageNet challenge _cite_ . However, these successful models rely on large amounts of annotated data to achieve such performances. For tasks and domains that do not have sufficient training data, deep models may have inferior performance to traditional classification algorithms that utilize hand crafted features. Transfer learning is helpful to deal with these novel scenarios that go beyond tasks where labeled data is abundant. The essence is to transfer the knowledge learned in the pre-trained models to new domains. Transfer learning is a broad research topic, and is studied in several forms such as domain adaptation, multi-task learning and model fine-tuning. In this work, we focus on model fine-tuning and try to answer the question of how to best adapt a pre-trained CNN model to new tasks. We start with the observations of existing model fine-tuning approaches for classification: The common approach is to pre-train a CNN architecture on a source domain with sufficient training data (e.g., ImageNet, which contains N million images in N categories), and then employ the pre-trained CNN as a feature extraction module, combined with a classification module for the task domain. When the task domain contains limited training data (e.g., CUB-N _cite_, which contains N thousand images in N categories), two types of prevalent classifier models are employed in transfer learning. The first one is to remove the classification layer from the pre-trained CNN, treat the remaining CNN as a fixed feature extractor, and train a linear classifier for classification. The second type is to replace the pre-trained classification layer with a new classification layer (e.g., a new N-way classification layer for CUB-N classification), fine-tuning all (or a portion of) the layers of the network by continuing the back-propagation with a relatively small learning rate. For both scenarios, the underlying thesis is that the well trained CNNs appear to learn cross domain features. However, both scenarios have their own drawbacks. When we regard a pre-trained CNN as a fixed feature extractor, the features are not tailored for the target domain, which limits their performance. When we fine-tune the pre-trained CNN to learn task specific features in an end-to-end manner, we will encounter the over-fitting problem quite often. How can we best leverage the information from both the source and target domains? A recent study _cite_ shows that there exists a similarity ratio between the source and target domains, termed domain similarity. Inspired by this, we argue that features extracted from pre-trained CNNs also have a similarity ratio: some features share more similarity between the source and target domains than other features. The features with a higher similarity ratio should get more attention. Hence, a feature selection mechanism is expected to be helpful during model fine-tuning. In this paper, we consider some novel questions. Should the features extracted from the pre-trained CNN be treated equally? Are some features more important and thus be weighted more than others in the novel target domain? Will a feature selection mechanism be helpful for better knowledge transfer? We aim to answer these questions by introducing a gating architecture, acting like a weight assignment, to the extracted features before classification. We refer to it as a transfer module and illustrate the details in later sections. Specifically, our contributions include the following: