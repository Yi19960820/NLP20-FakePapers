Deep neural networks are often trained for recognition problems over very many labels. This is partially to ensure wide applicability of the network and partially because networks are known to benefit from multi-label data (additional training examples from one class can increase performance of another class because they share features among several layers) . At testing time, however, one might want to apply the neural network to a collection of examples which are highly correlated. They only contain a limited subset of the original labels and consequently will result in sparse node activations in the network. In these cases, application of the full neural network to the whole collection results in a considerable amount of wasted computation. In this paper we describe a method for pruning of neural networks based on analysis of internal unit activations with the objective of constructing more efficient networks. In computer vision many problems have the structure described above. We briefly mention two here. Imagine you want to classify the semantic content in each frame (an example) of a video (the collection) . A fast assessment of the video might reveal that it is an indoor birthday party. This knowledge might exclude many of the nodes in the neural network--those which correspond to 'snow', 'leopards', and 'rivers', for example, will be unlikely to be needed in any of the thousands of frames in this video. Another example is object detection, where we extract thousands of bounding boxes (examples) from a single image (the collection) with the aim of locating all semantic objects in the image. Given an assessment of the image, we have knowledge of the node activations for the entire collection, and based on this we can propose a smaller network which is subsequently applied to the thousands of bounding boxes. We will here only consider the latter example in more detail. Reducing the size and complexity of neural networks (or network) enjoys a long history in the learning community. The authors of~ _cite_ train a simpler neural network to mimic the output of a complex one, and in~ _cite_ the authors compress deep and wide (i.e. with many feature maps) networks to shallow but wider ones. The technique of Knowledge Distillation was introduced in~ _cite_ as a model compression framework. The framework compresses an ensemble of deep networks (teacher) into a student network of similar depth. More recently, the FitNets approach leverages the Knowledge Distillation framework to exploit depth and train student networks that are but remain (~ _cite_) . Another network compression strategy was proposed in~ _cite_ that uses singular value decomposition to reduce the rank of weight matrices in fully connected layers in order to improve efficiency. In this paper we are not interested in mimicking the operation of a deep neural network over all examples and all classes (as in the student-teacher compression paradigm common in the literature) . Rather, our approach is to make a quick assessment of image content and then, based on analysis of unit activation on entire image, to modify the network to use only those units likely to contribute to correct classification of labels of interest when applied to each candidate bounding box.