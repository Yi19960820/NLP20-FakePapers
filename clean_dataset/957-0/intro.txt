The proper choice of kernel function and its hyperparameters are crucial to the success of applying kernel methods to practical applications. These selections span a number of different problems: from choosing a single width parameter in radial basis kernels, scaling different feature dimensions with different weights _cite_, to learning a linear or a non-linear combination of multiple kernels (MKL) _cite_ . In complicated practical problems such as computer vision, sometimes the need of multiple kernels arises naturally _cite_ . Images can be represented using descriptors based on shape, color and texture, and these descriptors have different roles in classifying different categories. In such situations it is in principle easy to design a kernel classifier where each kernel represents one of the descriptors and the classifier would be based on a weighted combination of kernels with category dependent learnt weights. A natural difficulty in kernel learning is scalability. Kernel methods scale with an already mediocre time complexity of at least _inline_eq_ with respect to the size _inline_eq_ of the training set, but combining multiple kernels or learning the hyperparameters of a single kernel significantly slows down training. Some speed-ups apply for specific kernels, but only in limited scenarios _cite_ . In consequence, most of the kernel learning approaches so far are only capable to handle a few thousand training examples at most. This is insufficient for the current age of massive datasets, such as a N million images ImageNet, or the N million articles within Wikipedia. An emerging technique that can in principle speed up the costly kernel method while at the same time preserving its non-linear predictive power is the random Fourier feature methodology (RFF) _cite_ . By sampling components from the frequency space of the kernel using Monte Carlo methods, RFF obtains a bounded, approximate representation of a kernel embedding that may initially span an infinite-dimensional space. Many operations are simplified once such representation is available, the most notable being that any kernel learning algorithm would now scale as _inline_eq_, where _inline_eq_ is the number of examples. This opens the path for applying kernel methods to the realm of massive datasets. In the seminal work on random Fourier features _cite_, the methodology was developped primarily for radial basis kernels. Recent work _cite_ focused on extending this technique to a number of other useful kernels defined on histogram features (empirical estimates of multinomial distributions), such as the _inline_eq_ and histogram intersection measures. However, the potential of the linear random Fourier methodology for kernel learning remains largely unexplored. In this paper we develop the methodology for learning both single kernel and for multiple kernel combinations in the Fourier domain and show that these produce accurate and efficient models. We conduct experiments in visual object recognition, using the difficult PASCAL Visual Object Challenges N dataset, in order to demonstrate the performance of the proposed Fourier kernel learning methodology and compare against non-linear learning algorithms, designed to operate in the original kernel space.