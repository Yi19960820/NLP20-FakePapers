Temporal action detection requires not only to determine whether an action occurs in a video but also to locate the temporal extent of when it occurs, which is a challenging problem for real-life long untrimmed videos. Most of modern approaches usually solve the problem via a two-step pipeline: firstly generate a set of class independent action proposals, which are obtained via running a action/background classifier over a video at multiple temporal scales; then the proposals are classified by the pre-trained action detector, and post processing such as non-maximum suppression is applied. However, such extensive search for action localization is unsatisfying in terms of both accuracy and computational efficiency. Like the human detects the action through successively altering the span of attended region to narrow down the difference between the bounds of current window and that of true action region, the optimal algorithm should be the process of sequential, iterative observation and refinement consuming search steps as less as possible. In this paper, we propose a class-specific action detection model that learns to continuously adjust the current region to cover the groundtruth more precisely in a self-adapted way. This is achieved by applying a sequence of transformations to a temporal window that is initially placed in the video at random and finally finds and covers action region as large as possible. The sequence of transformation is decided by an agent that analyzes the content of the current attended region and select the next best action according to a learned policy, which is trained via reinforcement learning based on Deep Q-Learning algorithm . Different from existing approaches that locate the action following a fixed path, our method generates various search trajectories for different action instances, depending on the video scenarios, the starting search position and the sequences of actions adopted. As a result, the trained agent will locate a single instance of an action in about N steps, which means that the model only processes N successive regions of an image to explore an uncover video segment, thus it is of great computational efficiency to compare with sliding window based approaches. Our model draws the inspiration from works that have used reinforcement learning to build active models for object localization in image . However, we can not handle the video in a top-down way that is proved to perform effectively for image object localization, as the duration of the video is usually too long (from hundreds to thousands frames) . We start the search from a position randomly selected from the video, which will terminate until a instance of action has been found or the maximum transformation steps has been reached, and then a new search begins from the position away from current attended region. We incorporate temporal pooling operation with feature extraction process to better represent the long video segment and design a " " action to avoid the agent trapping itself in the region where no action occurs. We conducted a comprehensive experimental evaluation in the challenging THUMOS'N dataset, and the results demonstrate that the proposed method can achieve competitive performance in terms of precision and recall via a small number of action proposals.