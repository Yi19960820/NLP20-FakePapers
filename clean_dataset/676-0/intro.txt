One of the key factors in revitalizing deep neural networks~ _cite_ and their tremendous success is the significant growth in computational power. The proliferation of massively parallel computing devices such as graphics processing units (GPUs) and distributed computing has revolutionized the training and inference of deep neural networks due to their highly parallelizable nature. This incredible rise in the computational power enables researchers to design and build increasingly larger and deeper neural networks to boost modeling accuracy. This on-going growth in architectural complexity, however, has become a bottleneck in the widespread adoption of such deep neural networks in many operational scenarios. For example, in many applications such as self-driving cars, smart-phone applications, and surveillance cameras, the computational resources are limited to low-power embedded GPUs, CPUs and deep learning accelerator chips with strong constraints on the memory usage. Furthermore, there are many situations where the use of cloud computing is intractable due to transmission cost, bandwidth issues, as well as privacy concerns. Considering the obstacles associated with the architectural complexity of deep neural networks, we have witnessed a growing attention towards learning highly efficient deep neural network architectures that are able to provide strong modeling power for operational scenarios where limited memory and computational resources are available. One of the first approaches in this area is the optimal brain damage method~ _cite_ in which synapses are pruned based on their strength. ~ _cite_ proposed a network compression framework where vector quantization is used to shrink the storage requirements of deep neural networks. Han {\it et al.} ~ _cite_ utilized Huffman coding in addition to pruning and vector quantization to further reduce the memory requirements. Hashing is another related trick employed by Chen {\it et al.} ~ _cite_ for network compression. Low-rank approximation~ _cite_ and sparsity learning~ _cite_ are other strategies used to build smaller and more efficient deep neural networks. Recently, Shafiee {\it et al.} ~ _cite_ introduced an evolutionary synthesis framework to progressively learn more efficient deep neural networks along successive generations . The proposed evolutionary deep intelligence approach mimics biological evolution mechanisms such as random mutation, natural selection, and heredity within a probabilistic graphical modeling paradigm to successively synthesize more efficient offspring network architectures. A crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and affects the architectural traits that are passed to the offspring networks in a significant way. Therefore, a more effective genetic encoding scheme can enable better transfer of genetic information from the ancestor network to its offspring networks to build a more efficient yet powerful future generation. The introduced genetic encoding scheme in~ _cite_ merely considers the individual synaptic properties in the sense that the probability of synthesizing each synapse within the network is independent of the rest of the synapses and thus it ignores the dependence between different synapses. However, there are neurobiological evidences that support the increasing probability of co-activation for synapses which encode similar information and locate close to each other on the same dendrite---synaptic clustering~ _cite_ . Inspired by this observation, incorporating synaptic clustering in the genetic encoding scheme of evolutionary deep models is potentially a fruitful direction to investigate. Moreover, synthesizing the offspring networks based on synaptic clusters (instead of basing it purely on individual synapses) can increase the efficiency of the offspring deep neural networks running on parallel computing devices such as GPUs and deep neural network accelerator chips. In this study, we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in the offspring networks. This process results in highly sparse offspring networks which are particularly tailored for parallel computing devices such as GPUs and deep neural network accelerator chips. We introduce a multi-factor genetic encoding scheme in which the synaptic probability considers both the probability of synthesis for the cluster of synapses that includes a particular synapse and the probability of synthesis for that particular synapse within the cluster. This genetic encoding scheme effectively promotes the formation of synaptic clusters over successive generations during the evolution process while supporting the formation of highly efficient deep neural networks.