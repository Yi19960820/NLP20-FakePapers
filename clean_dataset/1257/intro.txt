Convolutional neural networks (CNN) became very popular to solve classification problems in the last five years. Several authors have proposed to use CNNs to solve steganalysis problems _cite_, _cite_, _cite_, _cite_ . These methods yield encouraging results but remained comparable to the state-of-the-art algorithms performances. Authors have explored many approaches to improve it such as using a phase split _cite_, an ensemble of CNN _cite_, the transfer learning _cite_ or the augmentation of the database _cite_, _cite_ . Let us put aside the quest of the best deep learning network architecture for the steganalysis task. In this paper, our objective is to look at a "real-world" problem _cite_, which is to learn with a small size database. This problem is also known as low regime learning. It is well-known that supervised approaches based on the use of CNNs need a lot of samples when used for steganalysis purposes. The seminal propositions of Qian {et al.} _cite_ and Pibre {et al.} _cite_ used from N N to N N spatial images resized to N _inline_eq_ N (BOSSBase _cite_ or ImageNet _cite_) . In N the authors mainly use around N N pairs of images _cite_, _cite_, _cite_, _cite_, which is probably insufficient. The number of images for the learning has even reached five millions of samples in _cite_ . In an operational and realistic protocol, the number of available images for the learning task could be much smaller than what is used in "laboratory". Because all the CNN-based steganalysis are sensitive to the cover-source mismatch phenomenon _cite_, each time the source distribution is modified, the learning process has to be restarted. The aim of this paper is thus to look at the impact of {\it artificial data-augmentation}, which is probably more realist than having access to a huge database of a given source distribution. In all cases, using data-augmentation is an automatic process which requires less human time consumption than searching for images of similar distributions. Today, the classical scenario used to test an embedding algorithm efficiency is to use the BOSSBase _cite_ for training and testing, assigning N of the N images to the learning database, while the rest used as testing database. A classical way to artificially increase the learning database without changing the labels is to flip and rotate the learning database without interpolation _cite_ . Recently, Ye {\it et al.} _cite_ proposed to increase the size of the training database, by adding to the initial N \% of BOSSBase, the whole BOWSN _cite_ database (this gives a total of N pairs of images for the training set), while the test set is unchanged and is made of the remaining N \% of BOSSBase. This process effectively improves the results in terms of error probability of detection. However, it could be considered as {\it a very lucky measure} because the improvement is essentially due to the fact that BOSSBase and BOWSN share some identical camera models, and a similar "development" process . The question is thus still open: how should we process in order to enrich a learning database? Can we enrich even more the BOSS learning base in order to obtain a huge learning base, and thus improve the steganalysis results? In this paper we intend to experimentally explore efficient ways to {\bf increase} the learning database of a CNN based steganalyzer. In Section _ref_, we recall the topology of the CNN used for the various experiments _cite_ . In Section _ref_, we describe the experimental protocol and briefly present all the setups. In Section _ref_, we experimentally explore the different augmentation methods and we draw conclusions on the practical question of the learning database augmentation.