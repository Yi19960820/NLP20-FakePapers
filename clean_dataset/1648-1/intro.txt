Naming or describing real life objects is only meaningful with respect to a relevant scale _cite_ . For example, a view can be described as a leaf, a branch, or a tree depending on the distance of the observer. Natural and casual scenes are generally composed of many different entities/objects at different scales. During image acquisition, true physical scale is usually ignored. However, relative scale of the objects is somehow implicitly captured and stored in the image grid and pixels. An automated method to identify or describe objects in images can be analyzed in two parts: representation + classification. Basic classification algorithms without add-ons can not successfully handle variation and complexity of raw pixel level representation of objects. Usually dedicated representation functions map pixels into different constructs, which are named as features. Traditionally, computer vision researchers relied on manually designed features for representation. Recently, we are witnessing success of the algorithms which self-learn the appropriate features for the classification. In either case, size of an operator or a probe determines the scale of the entities that can be represented. However, the size of the probe or operator is often manually selected. On the other hand, last two decades has seen many automated object detection/recognition algorithms that were superior to their counterparts because they have comprised multi-scale processing of images _cite_, _cite_ . Multi-scale feature extractors gather and present the inherit scale information of image pixels to a subsequent classifier. In SIFT _cite_ and wavelets _cite_, this is done by creating a multi-scale pyramid from the input image, and then applying a fixed size probe-kernel to each scale. Serre et al. _cite_ aims to recognize objects similarly done in the visual cortex via collection of fixed Gabor filters. Although as Chan et al. _cite_ showed that for hand crafted features, adaptation to low level representations is difficult. Convolution neural networks (CNN) rely on stacked and hierarchical convolutions of the image to extract multi-scale information. Convention of CNNs in filter size selection is to use small fixed size weight kernels. Thanks to the stacked operation of convolutional layers, sandwiched by pooling layers which down-sample the intermediate outputs, the deeper levels of a network are able to learn representations of larger scale. Though, the optimality of fixed size kernels has not proven, the convention is to use filters small as NxN in the first layer, which can be larger NxN or NxN in the later stages _cite_ . During backpropagation training, filters are evolved to imitate lower level receptive fields in biological vision which are sensitive to certain shapes and orientations. Another justification is that larger filters require additional computation and causes overfitting. Though, the number of filters and their sizes in convolutional layers are usually selected intuitively, researchers are seeking alternatives to improve representation capacity of the network in deeper architectures. For example, Szegedy et al. _cite_ hand crafted their ``inception'' architecture to include mixing of parallel and wide convolutional layers which use different sized filter kernels. In a deep architecture this approach allows multi-scale, parallel and sparse representations. In summary, existing CNN based methods use fixed size convolution kernels and then rely on the fact that shape and orientation of the filters can be inferred from the training data. Additionally, CNNs employ stacked convolution layers to successfully create multi-scale representations. On the other hand, Hubel and Wiesel _cite_ discovered three type of cells in visual cortex: simple, complex, hypercomplex (i.e. end-stopped cells) . Simple cells are sensitive to orientation of the excitatory input whereas hypercomplex cells are activated with certain orientation, motion, and length of the stimuli. Therefore it is biologically plausible to assume that filters of different scales next to different orientations and direction may also work better in CNNs. In this study, we create a new and adaptive model of convolution layers where the filter size (actually scale) and orientation are learned during training. Therefore, a single convolution layer can have distinct filter scales and orientations. Broadly speaking, this corresponds to extracting multi-scale information using a single convolution layer. Our aim is not to fully replace the stacked architectures and deep networks for multi-scale information, instead the our approach improves information content that can be extracted from an input image, in a single layer. Additionally, our approach allows removing of the filter size from the list of hyper-parameters of deep learning networks. Thus, researchers can focus more on the architectural design of the network. Our experiments use MNIST, MNIST-cluttered and CIFAR-N datasets to show that the proposed model can learn and produce different scaled filters in the same layer whilst improving classification performance compared to a conventional layers. In experimental side, our work is concentrated on developing and proving an effective methodology for learnable adaptive filter scales and orientations, rather than improving highly optimized state-of-the-art results in these datasets.