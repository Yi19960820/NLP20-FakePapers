Deep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification~, face recognition, semantic segmentation and object detection . Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions. Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width . However, this in turn lays heavy burdens on the memory and other computational resources. For instance, ResNet-N, a specific instance of the latest residual network architecture wining ImageNet classification challenge in N, has a model size of about N MB and needs to perform about N billion FLOPs to classify a _inline_eq_ image crop. Therefore, it is very challenging to deploy deep CNNs on the devices with limited computation and power budgets. Substantial efforts have been made to the speed-up and compression on CNNs during training, feed-forward test or both of them. Among existing methods, the category of network quantization methods attracts great attention from researches and developers. Some network quantization works try to compress pre-trained full-precision CNN models directly. address the storage problem of AlexNet with vector quantization techniques. By replacing the weights in each of the three fully connected layers with respective floating-point centroid values obtained from the clustering, they can get over N _inline_eq_ model compression at about N \% loss in top-N recognition rate. HashedNet uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, present deep compression method which combines the pruning, vector quantization and Huffman coding, and reduce the model storage by _inline_eq_ on AlexNet and _inline_eq_ on VGG-N . use an SSE N-bit fixed-point implementation to improve the computation of neural networks on the modern Intel xN CPUs in feed-forward test, yielding N _inline_eq_ speed-up over an optimized floating-point baseline. Training CNNs by substituting the N-bit floating-point representation with the N-bit fixed-point representation has also been explored in . Other seminal works attempt to restrict CNNs into low-precision versions during training phase. propose expectation backpropagation (EBP) to estimate the posterior distribution of deterministic network weights. With EBP, the network weights can be constrained to + N and-N during feed-forward test in a probabilistic way. BinaryConnect further extends the idea behind EBP to binarize network weights during training phase directly. It has two versions of network weights: floating-point and binary. The floating-point version is used as the reference for weight binarization. BinaryConnect achieves state-of-the-art accuracy using shallow CNNs for small datasets such as MNIST and CIFAR-N. Later on, a series of efforts have been invested to train CNNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to BinaryNet, XNOR-Net, ternary weight network (TWN), DoReFa-Net and quantized neural network (QNN) . Despite these tremendous advances, CNN quantization still remains an open problem due to two critical issues which have not been well resolved yet, especially under scenarios of using low-precision weights for quantization. The first issue is the non-negligible accuracy loss for CNN quantization methods, and the other issue is the increased number of training iterations for ensuring convergence. In this paper, we attempt to address these two issues by presenting a novel incremental network quantization (INQ) method. \def In our INQ, there is no assumption on the CNN architecture, and its basic goal is to efficiently convert any pre-trained full-precision (i.e., N-bit floating-point) CNN model into a low-precision version whose weights are constrained to be either powers of two or zero. The advantage of such kind of low-precision models is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA. We noticed that most existing network quantization methods adopt a global strategy in which all the weights are simultaneously converted to low-precision ones (that are usually in the floating-point types) . That is, they have not considered the different importance of network weights, leaving the room to retain network accuracy limited. In sharp contrast to existing methods, our INQ makes a very careful handling for the model accuracy drop from network quantization. To be more specific, it incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition uses a pruning-inspired measure to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are quantized to be either powers of two or zero by a variable-length encoding method, forming a low-precision base for the original model. The weights in the other group are re-trained while keeping the quantized weights fixed, compensating for the accuracy loss resulted from the quantization. Furthermore, these three operations are repeated on the latest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure (as illustrated in Figure~ _ref_) . The main insight of our INQ is that a compact combination of the proposed weight partition, group-wise quantization and re-training operations has the potential to get a lossless low-precision CNN model from any full-precision reference. We conduct extensive experiments on the ImageNet large scale classification task using almost all known deep CNN architectures to validate the effectiveness of our method. We show that: (N) For AlexNet, VGG-N, GoogleNet and ResNets with N-bit quantization, INQ achieves improved accuracy in comparison with their respective full-precision baselines. The absolute top-N accuracy gain ranges from N to N, and the absolute top-N accuracy gain is in the range of N to N . (N) INQ has the property of easy convergence in training. In general, re-training with less than N epochs could consistently generate a lossless model with N-bit weights in the experiments. (N) Taking ResNet-N as an example, our quantized models with N-bit, N-bit and N-bit ternary weights also have improved or very similar accuracy compared with its N-bit floating-point baseline. (N) Taking AlexNet as an example, the combination of our network pruning and INQ outperforms deep compression method with significant margins.