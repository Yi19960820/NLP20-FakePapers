One of the reasons for the success of deep networks is their ability to learn higher level feature representations at successive nonlinear layers. In recent years, advances in both hardware and learning techniques have emerged to train even deeper networks, which have improved classification performance further~ _cite_ . The ImageNet challenge exemplifies the trend to deeper networks, as the state of the art methods have advanced from N layers (AlexNet), to N layers (VGGNet), and to N layers (ResNet) in the span of four years~ _cite_ . However, the progression towards deeper networks has dramatically increased the latency and energy required for feedforward inference. For example, experiments that compare VGGNet to AlexNet on a Titan X GPU have shown a factor of Nx increase in runtime and power consumption for a reduction in error rate of around N \% (from N \% to N \%) ~ _cite_ . The trade off between resource usage efficiency and prediction accuracy is even more noticeable for ResNet, the current state of the art method for the ImageNet Challenge, which has an order of magnitude more layers than VGGNet. This rapid increase in runtime and power for gains in accuracy may make deeper networks less tractable in many real world scenarios, such as real-time control of radio resources for next-generation mobile networking, where latency and energy are important factors. To lessen these increasing costs, we present BranchyNet, a neural network architecture where side branches are added to the main branch, the original baseline neural network, to allow certain test samples to exit early. This novel architecture exploits the observation that it is often the case that features learned at earlier stages of a deep network can correctly infer a large subset of the data population. By exiting these samples with prediction at earlier stages and thus avoiding layer-by-layer processing for all layers, BranchyNet significantly reduces the runtime and energy use of inference for the majority of samples. Figure~ _ref_ shows how BranchyNet modifies a standard AlexNet by adding two branches with their respective exit points. BranchyNet is trained by solving a joint optimization problem on the weighted sum of the loss functions associated with the exit points. Once the network is trained, BranchyNet utilizes the exit points to allow the samples to exit early, thus reducing the cost of inference. At each exit point, BranchyNet uses the entropy of a classification result (e.g., by softmax) as a measure of confidence in the prediction. If the entropy of a test sample is below a learned threshold value, meaning that the classifier is confident in the prediction, the sample exits the network with the prediction result at this exit point, and is not processed by the higher network layers. If the entropy value is above the threshold, then the classifier at this exit point is deemed not confident, and the sample continues to the next exit point in the network. If the sample reaches the last exit point, which is the last layer of the baseline neural network, it always performs classification. Three main contributions of this paper are: