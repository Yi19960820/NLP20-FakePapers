Deep learning models have achieved great success in visual recognition tasks _cite_ . However, these supervised learning models need large amounts of labelled data and many iterations to train their large number of parameters. This severely limits their scalability to new classes due to annotation cost, but more fundamentally limits their applicability to newly emerging (eg. new consumer devices) or rare (eg. rare animals) categories where numerous annotated images may simply never exist. In contrast, humans are very good at recognising objects with very little direct supervision, or none at all _inline_eq_, few-shot _cite_ or zero-shot _cite_ learning. For example, children have no problem generalising the concept of ``zebra" from a single picture in a book, or hearing its description as looking like a stripy horse. Motivated by the failure of conventional deep learning methods to work well on one or few examples per class, and inspired by the few-and zero-shot learning ability of humans, there has been a recent resurgence of interest in machine one/few-shot _cite_ and zero-shot _cite_ learning. Few-shot learning aims to recognise novel visual categories from very few labelled examples. The availability of only one or very few examples challenges the standard `fine-tuning' practice in deep learning _cite_ . Data augmentation and regularisation techniques can alleviate overfitting in such a limited-data regime, but they do not solve it. Therefore contemporary approaches to few-shot learning often decompose training into an auxiliary meta learning phase where transferrable knowledge is learned in the form of good initial conditions _cite_, embeddings _cite_ or optimisation strategies _cite_ . The target few-shot learning problem is then learned by fine-tuning _cite_ with the learned optimisation strategy _cite_ or computed in a feed-forward pass _cite_ without updating network weights. Zero-shot learning also suffers from a related challenge. Recognisers are trained by a single example in the form of a class description (c.f., single exemplar image in one-shot), making data insufficiency for gradient-based learning a challenge. While promising, most existing few-shot learning approaches either require complex inference mechanisms _cite_, complex recurrent neural network (RNN) architectures _cite_, or fine-tuning the target problem _cite_ . Our approach is most related to others that aim to train an effective metric for one-shot learning _cite_ . Where they focus on the learning of the transferrable embedding and {pre-define a fixed metric} (e.g., as Euclidean _cite_), we further aim to a transferrable deep metric for comparing the relation between images (few-shot learning), or between images and class descriptions (zero-shot learning) . By expressing the inductive bias of a solution (multiple non-linear learned stages at both embedding and relation modules), we make it easier to learn a generalisable solution to the problem. Specifically, we propose a two-branch Relation Network (RN) that performs few-shot recognition by learning to compare images against few-shot labeled {\em sample} images. First an {\em embedding module} generates representations of the query and training images. Then these embeddings are compared by a {\em relation module} that determines if they are from matching categories or not. Defining an episode-based strategy inspired by _cite_, the embedding and relation modules are meta-learned end-to-end to support few-shot learning. This can be seen as extending the strategy of _cite_ to include a comparator, instead of a fixed linear comparator. Our approach outperforms prior approaches, while being simpler (no RNNs _cite_) and faster (no fine-tuning _cite_) . Our proposed strategy also directly generalises to zero-shot learning. In this case the sample branch embeds a single-shot category description rather than a single exemplar training image, and the relation module learns to compare query image and category description embeddings. Overall our contribution is to provide a clean framework that elegantly encompasses both few and zero-shot learning. Our evaluation on four benchmarks show that it provides compelling performance across the board while being simpler and faster than the alternatives.