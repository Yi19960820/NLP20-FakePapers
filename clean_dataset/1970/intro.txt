Multiple viewpoint video of open spaces (\eg for sports or surveillance) is often captured using a sparse set of wide-baseline static cameras, in which human subjects are relatively small (tens of pixels in height) due to their physical distance. Nevertheless, it is useful to infer human behavioural data from this limited knowledge for performance analytics or security. In this paper, we explore the possibility of using a deeply learned prior inferring high fidelity three-dimensional (ND) body shape and skeletal pose data from a coarse (low-resolution) volumetric estimate of body shape estimated across a sparse set of camera views (Fig.~ _ref_) . The technical contribution of this paper is to explore the possibility of learning a deep representation for volumetric (ND) human body shape driven by a latent encoding for skeletal pose that can, in turn, be inferred from coarse volumetric shape data. Specifically, we investigate whether convolutional autoencoder architectures, commonly applied to ND visual content for de-noising and up-scaling (super-resolution), may be adapted to up-scale volumetric ND human shape whilst simultaneously providing high-level information on the ND human pose from the bottle-neck (latent) representation of the autoencoder. We propose a symmetric autoencoder with ND convolutional stages capable of refining a probabilistic visual hull (PVH) _cite_ \ie voxel occupancy data derived at very coarse scale (grid resolution _inline_eq_ encompassing the subject) . We demonstrate that our autoencoder is able to estimate an up-scaled body shape volume at up to _inline_eq_ resolution, whilst able to estimate the skeleton joint positions of the subject to equal or better accuracy than the current state of the art methods due to deep learning.