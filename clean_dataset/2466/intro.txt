Semantic inpainting refers to reconstructions of damaged portions of an image using available neighborhood information. In this paper, we are interested in investigating the role of automated dense semantic conditioning to generative adversarial networks (GAN) _cite_ for the specific task of semantic inpainting. We have focused on the special case of semantic inpainting of faces because faces are tough to inpaint due to presence of finer semantic details. Also, due to contemporary proliferation of multimedia services, video calling is deemed to become a frequent mode of communication and in such video streams, human faces occupy major part of a frame. Thus computer vision guided facial sequence inpainting is the call of the hour. Specifically, we wish to study and improve upon two aspects, viz., a) consistency and b) correctness. Consistency is applicable in case of sequence inpainting, in which we measure the coherence among a group of reconstructed frames. If not accounted for, generative models render abrupt structural changes and unpleasing flickering effects over stationary portions of frames. This is an intrinsic nature of generative model because the forward process of mapping a corrupted section to a valid image manifold is multimodal. An illustration is shown in Figure _ref_ (Refer to Figure _ref_ for actual comparison of outputs), wherein a generative model has multiple independent and equiprobable options to semantically fill in the corrupted portion of the image. However, if the model is applied on a stream of video frames, then such independent reconstructions renders the sequence unrealistic, because, for example, a smiling face has very low probability of transitioning into a neutral face in next frame. Our intuition to tackle this problem is to constrain the possible models of generation by an auxiliary conditional information. Such conditioning can be in the form of shape priors as used by Fi {} er et al. _cite_ for synthesis of stylized facial animations or consistency in optical flow field _cite_ for video style transfer. IIzuka et al. only concentrated on consistency of reconstruction at a local and global scale within a single frame _cite_, but did not address the issue of multimodal image completion in sequence inpainting. We illustrate, both numerically and visually, the inconsistencies in GAN based inpainting methods and offer a simple yet computationally frugal solution to enforce consistency. \par Regarding correctness: Correctness refers to a similarity metric quantifying the fidelity of reconstructed output to original version. As we are building upon the recent "DCGAN" _cite_ based inpainting method by Yeh et al. _cite_ (we abbreviate this as `DIP' in rest of the paper), the quality of reconstruction depends on the success of training the generator to approximate the underlying data distribution. Recent work by _cite_ shows that conditioning the GAN framework on positional constraints fosters in better sample generation. Our idea of improving upon _cite_ is to condition the GAN framework with automatically extracted facial semantics and thereby enabling (can be treated as constraining) the generator to generate specific facial components adhering to this conditioning input. In \textsection _ref_, we show that this simple yet elegant solution significantly improves quality of generated samples and also plays a pivotal role in achieving consistent reconstruction. Specifically, our key contributions in the paper are: