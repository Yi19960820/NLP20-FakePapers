Most physical phenomena in our visual environments are spatial-temporal processes. In this paper, we study a generative model for spatial-temporal processes such as dynamic textures and action sequences in video data. The model is a non-linear generalization of the linear state space model proposed by _cite_ for dynamic textures. The model of _cite_ is a hidden Markov model, which consists of a transition model that governs the transition probability distribution in the state space, and an emission model that generates the observed signal by a mapping from the state space to the signal space. In the model of _cite_, the transition model is an auto-regressive model in the _inline_eq_-dimensional state space, and the emission model is a linear mapping from the _inline_eq_-dimensional state vector to the _inline_eq_-dimensional image. In _cite_, the emission model is learned by treating all the frames of the input video sequence as independent observations, and the linear mapping is learned by principal component analysis via singular value decomposition. This reduces the _inline_eq_-dimensional image to a _inline_eq_-dimensional state vector. The transition model is then learned on the sequence of _inline_eq_-dimensional state vectors by a first order linear auto-regressive model. Given the high approximation capacity of the modern deep neural networks, it is natural to replace the linear structures in the transition and emission models of _cite_ by the neural networks. This leads to the following dynamic generator model that has the following two components. (N) The emission model, which is a generator network that maps the _inline_eq_-dimensional state vector to the _inline_eq_-dimensional image via a top-down deconvolution network. (N) The transition model, where the state vector of the next frame is obtained by a non-linear transformation of the state vector of the current frame as well as an independent Gaussian white noise vector that provides randomness in the transition. The non-linear transformation can be parametrized by a feedforward neural network or multi-layer perceptron. In this model, the latent random vectors that generate the observed data are the independent Gaussian noise vectors, also called innovation vectors in _cite_ . The state vectors and the images can be deterministically computed from these noise vectors. Such dynamic models have been studied in the computer vision literature recently, notably _cite_ . However, the models are usually trained by the generative adversarial networks (GAN) _cite_ with an extra discriminator network that seeks to distinguish between the observed data and the synthesized data generated by the dynamic model. Such a model may also be learned by variational auto-encoder (VAE) _cite_ together with an inference model that infers the sequence of noise vectors from the sequence of observed frames. Such an inference model may require a sophisticated design. In this paper, we show that it is possible to learn the model on its own using an alternating back-propagation through time (ABPTT) algorithm, without recruiting a separate discriminator model or an inference model. The ABPTT algorithm iterates the following two steps. (N) Inferential back-propagation through time, which samples the sequence of noise vectors given the observed video sequence using the Langevin dynamics, where the gradient of the log posterior distribution of the noise vectors can be calculated by back-propagation through time. (N) Learning back-propagation through time, which updates the parameters of the transition model and the emission model by gradient ascent, where the gradient of the log-likelihood with respect to the model parameters can again be calculated by back-propagation through time. The alternating back-propagation (ABP) algorithm was originally proposed for the static generator network _cite_ . In this paper, we show that it can be generalized to the dynamic generator model. In our experiments, we show that we can learn the dynamic generator models using the ABPTT algorithm for dynamic textures and action sequences. Two advantages of the ABPTT algorithm for the dynamic generator models are convenience and efficiency. The algorithm can be easily implemented without designing an extra network. Because it only involves back-propagations through time with respect to a single model, the computation is very efficient. The proposed learning method is related to the following themes of research. Dynamic textures. The original dynamic texture model _cite_ is linear in both the transition model and the emission model. Our work is concerned with a dynamic model with non-linear transition and emission models. See also _cite_ and references therein for some recent work on dynamic textures. Chaos modeling. The non-linear dynamic generator model has been used to approximate chaos in a recent paper _cite_ . In the chaos model, the innovation vectors are given as inputs, and the model is deterministic. In contrast, in the model studied in this paper, the innovation vectors are independent Gaussian noise vectors, and the model is stochastic. GAN and VAE. The dynamic generator model can also be learned by GAN or VAE. See _cite_ _cite_ and _cite_ for recent video generative models based on GAN. However, GAN does not infer the latent noise vectors. In VAE _cite_, one needs to design an inference model for the sequence of noise vectors, which is a non-trivial task due to the complex dependency structure. Our method does not require an extra model such as a discriminator in GAN or an inference model in VAE. Models based on spatial-temporal filters or kernels. The patterns in the video data can also be modeled by spatial-temporal filters by treating the data as ND (N spatial dimensions and N temporal dimension), such as a ND energy-based model _cite_ where the energy function is parametrized by a ND bottom-up ConvNet, or a ND generator model _cite_ where a top-down ND ConvNet maps a latent random vector to the observed video data. Such models do not have a dynamic structure defined by a transition model, and they are not convenient for predicting future frames. The main contribution of this paper lies in the combination of the dynamic generator model and the alternating back-propagation through time algorithm. Both the model and algorithm are simple and natural, and their combination can be very useful for modeling and analyzing spatial-temporal processes. The model is one-piece in the sense that (N) the transition model and emission model are integrated into a single latent variable model. (N) The learning of the dynamic model is end-to-end, which is different from _cite_ 's treatment. (N) The learning of our model does not need to recruit a discriminative network (like GAN) or an inference network (like VAE), which makes our method simple and efficient in terms of computational cost and model parameter size.