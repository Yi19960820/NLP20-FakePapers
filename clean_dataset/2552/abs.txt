It is one typical and general topic of learning a good embedding model to efficiently learn the representation coefficients between two spaces/subspaces. To solve this task, _inline_eq_ regularization is widely used for the pursuit of feature selection and avoiding overfitting, and yet the sparse estimation of features in _inline_eq_ regularization may cause the underfitting of training data. _inline_eq_ regularization is also frequently used, but it is a biased estimator. In this paper, we propose the idea that the features consist of three orthogonal parts, sparse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomposition, LBI is for the first time proposed to realize feature selection and dense estimation simultaneously. We provide theoretical and simulational verification that our method exceeds _inline_eq_ and _inline_eq_ regularization, and extensive experimental results show that our method achieves state-of-the-art performance in the few-shot and zero-shot learning.