Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition~ _cite_, object recognition~ _cite_, and natural language processing~ _cite_ . Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization~ _cite_, activation functions~ _cite_, and deeper architectures~ _cite_ . However, the neural connectivity formation of deep neural networks has remained largely the same over the past decade and thus further exploration and investigation on alternative approaches to neural connectivity formation can hold considerable promise. To explore alternate deep neural network connectivity formation, we take inspiration from nature by looking at the way brain develops synaptic connectivity between neurons. Recently, in a pivotal paper by Hill \el~ _cite_, data of living brain tissue from Wistar rats was collected and used to construct a partial map of a rat brain. Based on this map, Hill \el came to a very surprising conclusion. The synaptic formation, of specific functional connectivity in neocortical neural microcircuits, can be modelled and predicted as a random formation. In comparison, for the construction of deep neural networks, the neural connectivity formation is largely deterministic and pre-defined. Motivated by Hill \el's finding of random neural connectivity formation, we aim to investigate the feasibility and efficacy of devising stochastic neural connectivity formation to construct deep neural networks. To achieve this goal, we introduce the concept of StochasticNet, where the key idea is to leverage random graph theory~ _cite_ to form deep neural networks via stochastic connectivity between neurons. As such, we treat the formed deep neural networks as particular realizations of a random graph. Such stochastic synaptic formations in a deep neural network architecture can potentially allow for efficient utilization of neurons for performing specific tasks. Furthermore, since the focus is on neural connectivity, the StochasticNet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation, stochastic pooling, and Dropout~ _cite_, and DropConnect~ _cite_ . While a number of stochastic strategies for improving deep neural network performance have been previously introduced~ _cite_, it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets' main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity~ _cite_, or removal/introduction of neural connectivity for regularization during training. More specifically, StochasticNets is a realization of a random graph formed prior to training and as such the connectivity in the network are inherently sparse, and are permanent and do not change during training. This is very different from Dropout~ _cite_ and DropConnect~ _cite_ where the activations and connections are temporarily removed during training and put back during test for regularization purposes only, and as such the resulting neural connectivity of the network remains dense. There is no notion of 'dropping' in StochasticNets as only a subset of possible neural connections are formed in the first place prior to training, and the resulting network connectivity of the network is sparse. StochasticNets are also very different from HashNets~ _cite_, where connection weights are randomly grouped into hash buckets, with each bucket sharing the same weights, to explicitly sparsifying into the network, since there is no notion of grouping/merging in StochasticNets; the formed StochasticNets are naturally sparse due to the formation process. In fact, stochastic strategies such as HashNets, Dropout, and DropConnect can be used in conjunction with StochasticNets. The paper is organized as follows. First, a review of random graph theory is presented in Section N. The theory and design considerations behind forming StochasticNet as a random graph realizations are discussed in Section N. Experimental results using four image datasets (CIFAR-N~ _cite_, MNIST~ _cite_, SVHN~ _cite_, and STL-N~ _cite_) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section N. Finally, conclusions are drawn in Section N.