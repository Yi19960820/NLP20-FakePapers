Learning to count the number of objects in an image is a deceptively difficult problem with many interesting applications, such as surveillance _cite_, traffic monitoring _cite_ and medical image analysis _cite_ . In many of these application areas, the objects to be counted vary widely in appearance, size and shape, and labeled training data is typically sparse. These factors pose a significant computer vision and machine learning challenge. Lempitsky et al.~ _cite_ showed that it is possible to learn to count without learning to explicitly detect and localize individual objects. Instead, they propose learning to predict a density map whose integral over the image equals the number of objects in the image. This approach has been adopted by many later works (Cf.~ _cite_) . However, in many counting problems, such as those counting cells in a microscope image, pedestrians in a crowd, or vehicles in a traffic jam, regressors trained on a single image scale are not reliable _cite_ . This is due to a variety of challenges including overlap of objects and perspective effects which cause significant variance in object shape, size and appearance. The most successful recent approaches address this issue by explicitly incorporating multi-scale information in the network _cite_ . These approaches either combine multiple networks which take input patches of different sizes _cite_ or combine multiple filtering paths (``columns'') which have different size filters _cite_ . Following on the intuition that multiscale integration is key to achieving good counting performance, we propose to incorporate dilated filters _cite_ into a multicolumn convolutional neural network design _cite_ . Dilated filters exponentially increase the network's receptive field without an exponential increase in parameters, allowing for efficient use of multiscale information. Convolutional neural networks with dilated filters have proven to provide competitive performance in image segmentation where multiscale analysis is also critical _cite_ . By incorporating dilated filters into the multicolumn network design, we greatly increase the ability of the network to selectively aggregate multiscale information, without the need for explicit perspective maps during training and testing. We propose the ``aggregated multicolumn dilated convolution network'' or AMDCN which uses dilations to aggregate multiscale information. Our extensive experimental evaluation shows that this proposed network outperforms previous methods on many benchmark datasets.