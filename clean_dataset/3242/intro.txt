How do we ? Seeing an image of a plate of fruits, a fleet of battle ships, a pile of books, a flock of birds, pointing and counting seems effortless, even for children. It is also not a far fetched speculation that such ability could have played an important role in the development of human intelligence. Yet, behind this deceptively simple question lies a profound mystery: what are the necessary building blocks that enable us to pull this off? Given that deep neural networks share the same fundamental computation unit and the overall architecture as our brain, and that they are proven for their immense classification capability, we set out to extend them and explore designs that can solve the P \&C (Point and Count) problem. In the context of this paper, we restrict ourself to counting and localizing a group of objects of the class distributed in a ND image. Even with this restriction, this is a difficult task for today’s deep learning architecture, because it involves solving the ``what'' (classification) and ``where'' (localization) together _cite_ _cite_ . The anatomy of brain suggests that these two functions are embedded in different pathways, but their precise interaction is unknown. Obviously, this deadlock needs to be broken one way or the other. This paper offers a preliminary study that explores two directions, pivoting around whether we solve ``where'' or ``what'' first. In where-to-what approach, we first learn number of saliency objects from a deep hidden layer (count), and then apply clustering to activation heatmap to localize these objects (point) . In what-to-where approach, we record top-ranked features of a given object, and use them as the object's signature to prune heatmap. We then apply clustering to the feature-selected heatmap to simultaneously output both number of objects and their locations. In the context of this paper, we call these two approaches (CNP) and (PNC), respectively. These two approaches are qualitatively different. The PNC proposal is simple, it involves no learning at all, requires only a dictionary built from a pre-trained network, an off-the-shelf clustering algorithm, and can ideally deal with unbounded number of objects. The PNC approach is slightly involved, as it requires additional learning with bounded predictions to come up with counts first. Our experience points out that they are synergistic and can be combined. If the classification performance has exceeded the human capacity, the seemingly trivial P \&C problem is nowhere close to satisfactory. We perform analysis on the source of failures and identify a number of issues. For instance, we show that the pre-trained network has strong data bias and prefers classes it has seen more often, and that the CNN architecture has trouble dealing with large scale variance. We believe these insights are useful to identify future research directions. Finally, recognizing that the key challenge for problem like P \&C is the lack of well labelled training data, we propose the MNIST-LEGO dataset. Essentially, we take individual digits of MNIST and arrange them according to simple construction rules to build new classes of “objects”. The advantage of such methodology is that we can control the complexity, and can have virtually infinite number of training data to explore the design space quickly. We show how we can replicate the same problems identified in our study of natural images with this dataset. The rest of the paper is organized as follows. We discuss related work in Section _ref_, and then describe the two models in Section _ref_ . Experiment results are presented in Section _ref_, MNIST-LEGO is introduced in Section _ref_, and we conclude in Section _ref_ .