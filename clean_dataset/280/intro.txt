Neural networks with logistic sigmoid activations may look very similar to Bayesian networks of the same structure with logistic conditional distributions, aka {\em sigmoid belief networks} ~ _cite_ . However, hidden units in NNs are deterministic and take on real values while hidden units in Bayesian networks are binary random variables with an associated distribution. Given enough capacity and training data, both models can estimate the posterior distribution of their output arbitrary well. Besides somewhat different modelling properties, there is a principled difference that with stochastic models it is possible to pose, at least theoretically, a number of inference and learning problems with missing data by marginalizing over latent and unobserved variables. Unfortunately, even forward inference in Bayesian networks requires methods such as sampling or optimization of a variational approximation. Likewise, in more tightly coupled graphical models such as {\em deep Boltzmann machine} (DBM) ~ _cite_ or deep belief networks (DBN) ~ _cite_ practically all computations needed e.g.~for computing marginal and posterior probabilities are not tractable in the sense that approximations typically involve sampling or optimization. In this paper we propose stacked (deep) conditional independent model (DCIM) . There are two views how the model can be defined. One, as a Bayesian network with logistic conditional distributions. The other, just assuming that conditional probabilities of a general Bayesian network factor over the parent nodes up to a normalising factor. With binary units this necessary implies that conditional probabilities are logistic. It is noteworthy that we find the same form of conditional probabilities in most of the neural probabilistic models: restricted Boltzmann machine, DBM~ _cite_, DBN~ _cite_, etc. When the units are arranged in layers, as in a typical DNN, the layered Bayesian network can be viewed as a Markov chain with a state space of all variables in a layer. In this view, all necessary assumptions can be summarized in one property, termed {\em strong conditional independence} that the forward conditional transition probabilities between the layers factorize over the dimensions of both the input and output state spaces of the transition up to a normalising factor. Making only this assumption, we show that a simple approximate Bayesian inference in DCIM recovers main constructive elements of DNNs: sigmoid and softmax activations with its real-valued variables corresponding to expectations of binary random variables. With this interpretation we can view DNN as a way of performing approximate inference very efficiently. To our knowledge, this relationship has not been established before. Under this approximation conditional likelihood learning of a DCIM is equivalent to that of DNN and can be performed by back-propagation. Our second objective is to propose an alternative, generative learning approach for DNNs. In a number of recent works~ _cite_ and in the prior work~ _cite_ a pair of deterministic recognition (encoder) and a stochastic generator (decoder) networks is trained. Let us denote _inline_eq_ the input of the recognition network, to be specific, an image, and _inline_eq_ (random variables at layer _inline_eq_) the output of the recognition network, the latent state. Although the two networks often are taken to have a symmetric structure~ _cite_, their parameters are decoupled. The stochastic generator network can typically only generate samples, but cannot directly evaluate its posterior distribution or the gradient thereof, requiring variational or sampling-based approximations. The methods proposed in~ _cite_ assume that samples generated from the same latent state _inline_eq_ must fall close together in the image space. This prohibits using categorical latent spaces such as digit class in MNIST because digits of the same class naturally may look differently. Instead, a model's own continuous latent space is used such that fixing a point in it defines both the class and the shape of the digit. Works~ _cite_ are thus restricted to unsupervised learning. Given full training data pairs of _inline_eq_, the recognition network could learn a distribution _inline_eq_ and the generator network could in principle learn a distribution _inline_eq_ . With our link between DNN and DCIM, we ask the question when the two DCIMs, modelling the two conditional distributions _inline_eq_ and _inline_eq_, are consistent, i.e., correspond to some implicitly modelled joint distribution _inline_eq_ of the data and all hidden layers. It is the case when _inline_eq_ for some functions _inline_eq_ . While this cannot be strictly satisfied with our strong conditional independence assumptions, we observe that most of the terms in the ratio cancel when we set the weights of recognition network _inline_eq_ to be transposed of those of network _inline_eq_ . Both models therefore can be efficiently represented by one and the same DNN, share its parameters and can be learned simultaneously by using an estimator similar to pseudo-likelihood. We further use the link between DNN and DCIM to approximately compute the posterior distribution in the generator network _inline_eq_ given a sample of an inner layer _inline_eq_ . The approximation is reasonable when this posterior is expected to have a single mode, such as when reconstructing an image from lower level features. We thus can fully or partially avoid sampling in the generator model. We demonstrate in a tentative experiment that such a coupled pair can be learned simultaneously, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation. In~ we consider strongly conditional independent model with two layers and derive the NN from it. This model will serve as a building block of DCIM formally introduced in~ . In~ we consider coupled pairs of DCIMs and propose a novel approach for generative learning. In~ we discuss more connections with related work. In~ we propose our proof-of-concept experiments of generative learning using only DNN inference or sampling only a single layer.