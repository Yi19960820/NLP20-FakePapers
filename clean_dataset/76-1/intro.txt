Recognizing products displayed on store shelves based on computer vision is gathering ever-increasing attention thanks to the potential for improving the customer's shopping experience (\eg, via augmented reality apps, checkout-free stores, support to the visually impaired \dots) and realizing automatic store management (\eg, automated inventory, on-line shelf monitoring \dots) . The seminal work on product recognition dates back to _cite_, where Merler \etal highlight the peculiar issues to be addressed in order to achieve a viable approach. First of all, the number of different items to be recognized is huge, in the order of several thousands for small to medium shops, well beyond the usual target for current state-of-the-art image classifiers. Moreover, product recognition can be better described as a hard instance recognition problem, rather than a classification one, as it deals with lots of objects looking remarkably similar but for small details (\eg, different flavors of the same brand of cereals) . Then, any practical methodology should rely only on the information available within existing commercial product databases, \ie at most just one high-quality image for each side of the package, either acquired in studio settings or rendered (see _ref_-(b)) . Query images for product recognition are, instead, taken in the store with cheap equipment (\eg, a smart-phone) and featuring many different items displayed on a shelf (see _ref_-(a)) . Unfortunately, this scenario is far from optimal for state-of-the-art multi-class object detectors based on deep learning _cite_, which require a large corpus of annotated images as similar as possible to the deployment scenario in order to provide good performance. Even acquiring and manually annotating with product labels a huge dataset of in-store images is not a viable solution due to the products on sale in stores, as well as their appearance, changing frequently over time, which would mandate continuous gathering of annotated in-store images and retraining of the system. Conversely, a practical approach should be trained once and then be able to handle seamlessly new stores, new products and/or new packages of existing products (\eg, seasonal packages) . To tackle the above issues, we address product recognition by a pipeline consisting of three stages. Given a shelf image, we perform first a class-agnostic object detection to extract region proposals enclosing the individual product items. This stage relies on a deep learning object detector trained to localize product items within images taken in the store; we will refer to this network as to the . In the second stage, we perform product recognition separately on each of the region proposal provided by the . Purposely, we carry out K-NN (K-Nearest Neighbours) similarity search between a global descriptor computed on the extracted region proposal and a database of similar descriptors computed on the reference images available in the product database. Rather than deploying a general-purpose global descriptor (\eg, Fisher Vectors _cite_), we train a CNN using the reference images to learn an image embedding function that maps RGB inputs to n-dimensional global descriptors amenable for product recognition; this second network will be referred to as to the . Eventually, to help prune out false detections and improve disambiguation between similarly looking products, in the third stage of our pipeline we refine the recognition output by re-ranking the first K proposals delivered by the similarity search. An exemplary output provided by the system is depicted in _ref_-(a)) . It is worth pointing out how our approach needs samples of annotated in-store images only to train the product-agnostic, which, however, does not require product-specific labels but just bounding boxes drawn around items. In _ref_ we will show how the product-agnostic can be trained once and for all so to achieve remarkable performance across different stores despite changes in shelves disposition and product appearance. Therefore, new items/packages are handled seamlessly by our system simply by adding their global descriptors (computed through the Embedder) in the reference database. Besides, our system scales easily to the recognition of thousands of different items, as we use just one (or few) reference images per product, each encoded into a global descriptor in the order of one thousand float numbers. Finally, while computationally expensive at training time, our system turns out light (\ie memory efficient) and fast at deployment time, thereby enabling near real-time operation. Speed and memory efficiency do not come at a price in performance, as our system compares favorably with respect to previous work on the standard benchmark dataset for product recognition.