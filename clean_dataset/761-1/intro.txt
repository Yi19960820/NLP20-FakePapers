Image registration _cite_ is the process of aligning different images into the same coordinate frame, enabling comparison or integration of images taken at different times, from different viewpoints, or by different sensors. It is used in many medical image analysis tasks. Several medical image registration studies have been conducted _cite_ and toolkits such as SimpleITK _cite_, ANTs _cite_ and ND Slicer _cite_ have been developed. Typically, registration processes implemented in those tools are performed by iteratively updating transformation parameters until a predefined metric, which measures the similarity of two images to be registered, is optimised. These conventional methods have been achieving decent performance, but their applications are limited by the slow registration speed. This is mainly because the iterative algorithm is optimising the cost function from scratch for every new registration task instead of utilising the information obtained from previous experiences. To overcome this issue, many recent works done on medical image registration have proposed methods based on deep learning approaches, motivated by the successful applications of convolutional neural network (CNN) in the computer vision field. Wu et al. _cite_ use a convolutional stacked auto-encoder to learn the highly discriminative features of the images to be registered. However, the extracted features might not be optimal for registration purpose as the extraction is done separately for each image pairs. Also, the method proposed is not an end-to-end deep learning strategy as it still relies on other feature-based registration methods to find the mapping between the two images. Subsequently, several works on end-to-end unsupervised registration using CNN have been presented. Similar to the conventional image registration methods, the strategy proposed by de Vos et al. _cite_ and Shan et al. _cite_ does not require any ground truth label on the image pairs to be registered. A predefined metric is used as the loss function and back-propagated to the CNNs to learn the optimal parameters of the network that minimises the error. This strategy is implementable by using the spatial transformer network introduced by Jaderberg et al. _cite_, which enables neural networks to spatially transform feature maps and is fully differentiable. To ensure the satisfactory performance of these frameworks, a good optimising metric must be defined. This could be a potential drawback as different metrics have their pros and cons and the suitability of a metric varies from task to task. On the other hand, Miao et al. _cite_ use CNN regression to directly estimate the transformation parameters and there is no selection of the right optimising metric involved. The models were then trained on synthetic X-ray images as they provide ground truth labels without the need of manual annotation. Although higher registration success rates than conventional methods have been achieved, in their framework, six regressors were trained and applied in a hierarchical manner instead of estimating all the parameters simultaneously. In this paper, we build a system (Fig.~ _ref_) for medical image registration, which also utilises CNN to calculate the transformation parameters between two ND images. Unlike the unsupervised learning methods that maps the images to a scalar-valued metric function, our affine image registration network (AIRNet) requires two ND images to be registered (input values) and the transformation parameters (target values) . Here, we use a twelve-dimensional vector capturing ND affine transformation as the label of each input data. But since it is impractical to manually identify the transformation parameters between any two images, we explore a self-supervised learning method and leverage the abundance of cheap unlabelled data to generate a synthetic dataset for the training of the model. Through direct estimation of transformation matrix, our method performs one-shot registration instead of hierarchical manner as presented by Miao et al _cite_ . Furthermore, the structure of the AIRNet enables us to learn the discriminative features of the images which are useful for registration purpose. Additionally, the features allow us to obtain generalised representations of different image modalities. For instance, different set of a brain scan can be generated based on different settings of magnetic resonance (MR) brain imaging protocol, rendering the intensity of the brain tissue to differ across modalities. But for brain volumes which are aligned, the representation of these images would be similar as the registration focuses on geometric information. In short, we develop a deep learning framework for ND image registration which achieves better overall performance at Nx faster speed in execution as compared to some conventional methods. Our framework is applicable for two main areas in medical imaging: N) registration of images from different patients to identify different anatomical regions of the body, and N) registration of images from different modalities to integrate useful information across different type of data. The rest of the paper is organised as follows: Sect.~ _ref_ defines our problem; Sect.~ _ref_ introduces our proposed method; Sect.~ _ref_ presents the experimental results.