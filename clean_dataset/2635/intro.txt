With the increasing popularity of wearable or action cameras in recording our life experience, egocentric vision _cite_, which aims at automatic analysis of videos captured from a first-person perspective _cite_ _cite_ _cite_, has become an emerging field in computer vision. In particular, as the camera wearer's point-of-gaze in egocentric video contains important information about interacted objects and the camera wearer's intent _cite_, gaze prediction can be used to infer important regions in images and videos to reduce the amount of computation needed in learning and inference of various analysis tasks _cite_ _cite_ _cite_ _cite_ . This paper aims to develop a computational model for predicting the camera wearer's point-of-gaze from an egocentric video. Most previous methods have formulated gaze prediction as the problem of saliency detection, and computational models of visual saliency have been studied to the find image regions that are likely to attract human attention. The saliency-based paradigm is reasonable because it is known that highly salient regions are strongly correlated with actual gaze locations _cite_ . However, the saliency model-based gaze prediction becomes much more difficult in natural dynamic scenes, e.g. cooking in a kitchen, where high-level knowledge of the task has a strong influence on human attention. In a natural dynamic scene, a person perceives the surrounding environment with a series of gaze fixations which point to the objects/regions related to the person's interactions with the environment. It has been observed that the attention transition is deeply related to the task carried out by the person. Especially in object manipulation tasks, the high-level knowledge of an undergoing task determines a stream of objects or places to be attended successively and thus influences the transition of human attention. For example, to pour water from a bottle to a cup, a person always first looks at the bottle before grasping it and then change the fixation onto the cup during the action of pouring. Therefore, we argue that it is necessary to explore the task-dependent patterns in attention transition in order to achieve accurate gaze prediction. In this paper, we propose a hybrid gaze prediction model that combines bottom-up visual saliency with task-dependent attention transition learned from successively attended image regions in training data. The proposed model is mainly composed of three modules. The first module generates saliency maps directly from video frames. It is based on a two-stream Convolutional Neural Network (CNN) which is similar to traditional bottom-up saliency prediction models. The second module is based on a recurrent neural network and a fixation state predictor which generates an attention map for each frame based on previously fixated regions and head motion. It is built based on two assumptions. Firstly, a person's gaze tends to be located on the same object during each fixation, and a large gaze shift almost always occurs along with large head motion _cite_ . Secondly, patterns in the temporal shift between regions of attention are dependent on the performed task and can be learned from data. The last module is based on a fully convolutional network which fuses the saliency map and the attention map from the first two modules and generates a final gaze map, from which the final prediction of ND gaze position is made. Main contributions of this work are summarized as follows: