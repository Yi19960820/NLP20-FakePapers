{\let \thefootnote \relax} Most state-of-the-art solutions to visual recognition tasks use models that are specifically trained for these tasks _cite_ . For the tasks involving categories (such as object classification, segmentation), the complexity of the task (i.e. the number of target classes) limits the ability of these trained models. For example, a trained model aimed for object recognition can only classify object categories on which it has been trained. However, if the number of target classes increases, the model must be updated in such a way that it performs well on the original classes on which it has been trained, also known as base classes, while it incrementally learns new classes as well. If we retrain the model only on new, previously unseen classes, it would completely forget the base classes, which is known as catastrophic forgetting _cite_, a phenomenon which is not typically observed in humane learning. Therefore, most existing solutions _cite_ explore incremental learning (IL) by allowing the model to retain a fraction of the training data of base classes, while incrementally learning new classes. Yu et al. _cite_ proposed retaining trained models encoding base class information, to transfer their knowledge to the model learning new classes. However, this process is not scalable. This is because storing base class data or models encoding base class information is a memory expensive task, and hence is cumbersome when used in a lifelong learning setting. Also, in an industrial setting, when a trained object classification model is delivered to the end user, the training data is kept private for proprietary reasons. Therefore, the end user will be unable to update the trained model to incorporate new target classes in the absence of base class data. Moreover, storing base class data for incrementally learning new classes is not biologically inspired. For example, when a toddler learns to recognize new shapes/objects, it is observed that it does not completely forget the shapes or objects it already knows. It also does not always need to revisit the old information when learning new entities. Inspired by this, we aim to explore incremental learning in object classification by adding a stream of new classes without storing data belonging to classes that the classifier has already seen. While IL solutions that do not require base class data, such as _cite_ have been proposed, these methods mostly aim at incrementally learning new tasks, which means that at test time the model cannot confuse the incrementally learned tasks with tasks it has already learned, making the problem setup much easier. We explore the problem of incrementally learning object classes, without storing any data or model associated with the base classes (Figure _ref_) in the previous steps, while allowing the model to confuse new classes with old ones. In our problem setup, an ideal incremental learner should have the following properties: An existing work targeting the same problem is LwF-MC, which is one of the baselines in _cite_ . In the following sections, we use the following terminology (introduced in _cite_) at incremental step _inline_eq_: Teacher model, _inline_eq_, i.e. the model trained with only base classes. Student model, _inline_eq_, i.e. the model which incrementally learns new classes, while emulating the teacher model for maintaining performance on base classes. Information Preserving Penalty (IPP), i.e. the loss to penalize the divergence between _inline_eq_ and _inline_eq_ . Ideally, this helps _inline_eq_ to be as proficient in classifying base classes as _inline_eq_ . Initialized using _inline_eq_, _inline_eq_ is then trained to learn new classes using a classification loss, _inline_eq_ . However, an IPP is also applied to _inline_eq_ so as to minimize the divergence between the representations of _inline_eq_ and _inline_eq_ . While _inline_eq_ helps _inline_eq_ to learn new classes, IPP prevents _inline_eq_ from diverging too much from _inline_eq_ . Since _inline_eq_ is already initialized as _inline_eq_, the initial value of IPP is expected to be close to zero. However, as _inline_eq_ keeps learning new classes with _inline_eq_, it starts diverging from _inline_eq_, which leads the IPP to increase. The purpose of the IPP is to prevent the divergence of _inline_eq_ from _inline_eq_ . Once _inline_eq_ is trained for a fixed number of epochs, it is used as a teacher in the next incremental step, using which a new student model is initialized. In LwF-MC _cite_, the IPP is the knowledge distillation loss. The knowledge distillation loss _inline_eq_, in this context, was first introduced in _cite_ . It captures the divergence between the prediction vectors of _inline_eq_ and _inline_eq_ . In an incremental setup, when an image belonging to a new class (_inline_eq_) is fed to _inline_eq_, the base classes which have some resemblance in _inline_eq_ are captured. _inline_eq_ enforces _inline_eq_ to capture the same base classes. Thus, _inline_eq_ essentially makes _inline_eq_ learn `what' are the possible base classes in _inline_eq_, as shown in Figure _ref_ . Pixels that have significant influence on the models' prediction constitute the attention region of the network. However, _inline_eq_ does not explicitly take into account the degree of each pixel influencing the modelsâ€™ predictions. For example, in Figure _ref_, in the first row, it is seen that at step _inline_eq_, even though the network focuses on an incorrect region while predicting `dial \_telephone', the numerical value of _inline_eq_ (N) is same as that when the network focuses on the correct region in step _inline_eq_, in the bottom row. We hypothesize that attention regions encode the models' representation more precisely. Hence, constraining the attention regions of _inline_eq_ and _inline_eq_ using an Attention Distillation Loss (_inline_eq_, explained in Sec. _ref_), to minimize the divergence of the representations of _inline_eq_ from that of _inline_eq_ is more meaningful. This is because, instead of finding which base classes are resembled in the new data, attention maps explain `why' hints of a base class are present (as shown in Figure _ref_) . Using these hints, _inline_eq_, in an attempt to make the attention maps of _inline_eq_ and _inline_eq_ equivalent, helps to encode some visual knowledge of base class in _inline_eq_ . We show the utility of _inline_eq_ in Figure _ref_, where although the model correctly predicts the image as 'dial \_telephone', the value of _inline_eq_ in step _inline_eq_ increases if the attention regions diverge too much from the region in Step N. We propose an approach where an Attention Distillation Loss (_inline_eq_) is applied to _inline_eq_ to prevent its divergence from _inline_eq_, at incremental step _inline_eq_ . Precisely, we propose to constrain the _inline_eq_ distance between the attention maps generated by _inline_eq_ and _inline_eq_ in order to preserve the knowledge of base classes. The reasoning behind this strategy is described in Sec _ref_ . This is applied in addition to the distillation loss _inline_eq_ and a classification loss for the student model to incrementally learn new classes. The main contribution of this work is to provide an attention-based approach, termed as `Learning without Memorizing (LwM) ', that helps a model to incrementally learn new classes by restricting the divergence between student and teacher model. LwM does not require any data of the base classes when learning new classes. Different from contemporary approaches that explore the same problem, LwM takes into account the gradient flow information of teacher and student models by generating attention maps using these models. It then constrains this information to be equivalent for teacher and student models, thus preventing the student model to diverge too much from the teacher model. Finally, we show that LwM consistently outperforms the state-of-the-art performance in the iILSVRC-small _cite_ and iCIFAR-N _cite_ datasets.