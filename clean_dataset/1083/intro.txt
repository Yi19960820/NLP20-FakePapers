Video captioning, i.e., to automatically describe videos by full sentences or phrases, not only serves as a challenging testbed in computer vision and machine learning but also benefits many real-world applications. The automatically generated video captions may enable fast video retrieval, assist the visually impaired, and engage users in a versatile chatbot, to name a few. Most recent works~ _cite_ that tackle this problem fall under an encoder-decoder framework which has been shown effective in speech recognition~ _cite_, natural language translation~ _cite_, and image captioning~ _cite_ . The encoder extracts compact representations of the visual content. In the context of video captioning, the convolutional neural networks (CNNs) are usually used to encode the video frames followed by a temporal model~ _cite_ or simply temporal pooling~ _cite_ and the decoder maps the codes to a sequence of words often by the recurrent neural networks (RNNs) ~ _cite_ (e.g., the long short-term memory (LSTM) ~ _cite_ units are a popular choice) . In order to train such networks, most existing works employ a cross-entropy loss at each decoding step. We refer the readers to the seminal work that spurs the resurging interests in video captioning, sequence to sequence-video to text (SNVT) ~ _cite_, for a quick understanding about the backbone techniques. Despite the impact of the encoder-decoder framework on video captioning, it inherently impedes the use of end-to-end (ENE) training which has led to very impressive results on a large variety of tasks. Indeed, both CNNs and RNNs are memory consuming, leaving little GPU space to the training data which are yet key to the training procedure. Besides, the input videos and output sentences are both sequences, making the encoder-decoder framework very lengthy and data-hungry. On the one hand, it is tempting to explore the ENE training strategy on the video captioning task. On the other hand, this seemingly straightforward idea is confined by the hardware and the relatively small size of existing video captioning datasets. Our experiments show that the conventional cross-entropy loss coupled with stochastic gradient descent cannot effectively exploit the ENE training. In this paper, we propose a multitask reinforcement learning approach to training a video captioning model in an ENE manner. Our main idea is to mine and construct as many effective tasks as possible from the human captioned videos such that they can jointly regulate the search space of the encoder-decoder network, from which an ENE video captioning model can be found and generalized to the testing phase. The auxiliary tasks consist of two broad types: to predict the attributes extracted from the captions of the training videos and to maximize the rewards defined from the reinforcement learning perspective. When the training set is relatively small for the big encoder-decoder network, it is important to mine as much supervision as possible from the limited data so that it helps reduce the search space for the main task of interest. Although many existing video captioning models~ _cite_ can literally be trained in the ENE fashion, none of them were probably due to the hardware constraint and concerns on overfitting. Indeed, our study reveals that, without the proposed multitask reinforcement learning strategy, ENE learning is easy to overfit the training set. This work is the first to end-to-end train a model for video captioning, to the best of our knowledge. It is nontrivial because the model becomes very large in order to take as input a raw video sequence and output a sequence of words, causing challenges to the computational resources and raising the need for large-scale well-labeled data. Our multitask reinforcement learning method is able to alleviate those challenges and gives rise to state-of-the-art results on MSVD~ _cite_ and MSR-VTT~ _cite_, two popular benchmark datasets for the video captioning task. Nonetheless, we believe that, supplied with larger-scale labeled data, the ENE training can further advance the video captioning results. We summarize our contribution as the following. (N) We propose a multitask reinforcement training strategy which can effectively learn a video captioning model in an ENE fashion under the current constraints of hardware and data size. (N) We extract attributes from the captions of the training videos and define rewards upon the captions without using any external data. (N) Experiments show that our approach with a single model gives rise to state-of-the-art results on both the MSVD and MSR-VTT datasets.