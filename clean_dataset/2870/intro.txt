Recently, deep convolutional neural networks have been shown to perform very well on various challenging pattern-recognition benchmarks. Such networks trained in a supervised way via backpropagation achieve state of the art performance on Caltech-N _cite_, Caltech-N _cite_, PASCAL VOC dataset _cite_, MNIST _cite_, ImageNet _cite_ . However, the drawback of this approach is the requirement of vast amounts of labeled data that is not always available. This paper regards unsupervised training of deep neural networks and investigates whether a voting scheme (by a committee of networks) can improve the classification result. We test our method on the STL-N dataset _cite_, because it has only a small number of labeled training data. For filter training, we use k-means as in _cite_ . After the convolutional step, we found that the local normalization presented by _cite_ improves the classification result. For the connection between layers, we adopt the random grouping of _cite_ . Recent state of the art result of _cite_ on STL-N dataset proves that methods from supervised training can be adapted for unsupervised training of networks. Their method creates virtual classes by largely augmenting single images, then training networks on each of these virtual classes using backpropagation. In our paper, we show that better results can be obtained without using backpropatagion. Another important result on STL-N is presented in _cite_ where filters are trained layer-wise: in the first layer, filters are learned via k-means, while in the second layer, filters are being supervised trained via Fisher weight maps for maximizing the between-class distance of descriptors obtained after the second layer. In our work, we only perform layer-wise unsupervised learning of filters. Finally, we show that a committee of networks improves the classification result.