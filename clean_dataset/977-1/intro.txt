Motion is often a necessary cue for recognizing actions in a video clip. For example, it may be difficult to tell two actions apart from a single video frame, like ``open a door'' and ``close a door'', because the interpretation of the action depends on the direction of motion. To handle motion, much recent work on action recognition treats recognition from motion as a task separate from recognition from appearance. Typically, these two tasks are performed by separate networks, the ``temporal stream'' and ``spatial stream'', which are then ensembled, a technique first introduced by Two-Stream Networks~ _cite_ . In Two-Stream Networks, the spatial stream only observes a single RGB video frame at a time, while the temporal stream observes a brief sequence of optical flow frames, meaning the temporal stream is solely responsible for capturing features from motion. However, in more recent work, the spatial stream consists of a ND Convolutional Neural Network, which observes an entire video clip~ _cite_ . Conceptually, the spatiotemporal filters in a ND CNN have the ability to respond to movement, which should allow them to learn motion features, a claim echoed in the literature~ _cite_ . However, we still see strong gains in accuracy by ensembling these ND CNNs with ``temporal'' ND CNNs which take explicit motion representations as input. For example, we see a N \% increase in accuracy on HMDB-N by ensembling a ND CNN that takes RGB frames with a ND CNN that takes optical flow frames~ _cite_ . It is unclear why both streams are necessary. Is the temporal stream capturing motion features which the spatial stream is missing? If so, why is the ND CNN missing this information? In this work, we examine the spatial streams in ND CNNs to see what motion representations they learn, and we introduce a method, depicted in Figure~ _ref_, that combines the spatial and temporal streams into a single RGB-only model that achieves comparable performance. Because ND CNNs include temporal filters, we hypothesize that they should be able to capture motion representations if optimized to do so. Recent work has shown that it is possible for ND CNNs to learn motion representations such as optical flow, but the network structure was designed specifically for this purpose~ _cite_ . Instead of designing a ND network specifically for learning motion representations, we examine state-of-the-art ND CNNs designed for action recognition, with minimal modifications to their structure, to see what motion representations they are capable of learning. To do this, we train ND CNNs on an optical flow prediction task, described in Section~ _ref_, and we demonstrate experimentally that ND CNNs are capable of learning motion representations in this way. However, even if ND CNNs are capable of learning motion representations when optimized for optical flow prediction, it is not necessarily true that these motion representations will arise naturally when ND CNNs are trained to perform other tasks, such as action recognition. To answer whether this is the case, we evaluate the same state-of-the-art ND CNNs on the optical flow prediction task, but we use models with fixed spatiotemporal filters that are pretrained on an action recognition task. We find that these models underperform models that are fully fine-tuned for optical flow prediction, suggesting that ND CNNs have much room for improvement to learn higher-quality motion representations. In order to improve these motion representations, we propose to distill knowledge from the temporal stream into the spatial stream, effectively compressing the two-stream architecture into a single model. In Section~ _ref_, we train this Distilled ND Network (DND) by optimizing the spatial stream to match the temporal stream's output, a technique often used for model compression~ _cite_ . During inference we only use the distilled spatial stream, and we find that this spatial stream has improved performance on the optical flow prediction task. This suggests that distillation improves motion representations in ND CNNs. We apply DND to several benchmark datasets, and we find in Section~ _ref_ that DND strongly outperforms single-stream baselines, achieving accuracy on par with the two-stream model with only a single stream. We train and evaluate DND on Kinetics~ _cite_, and show that the weights learned by distillation also transfer to other tasks, including HMDB-N~ _cite_, and UCF-N~ _cite_ . DND does not require any optical flow computation during inference, making it less computationally expensive than two-stream approaches. DND can also benefit from ensembling for better performance, still without the need for optical flow. We compare DND to a number of other strong RGB-only baselines, and find that DND outperforms these approaches. In summary, we make the following contributions: