We consider the problem of function approximation with artificial neural networks (ANNs) . This generic task has many applications in science and engineering, such as signal processing, pattern recognition, and control. The goal is to model an unknown nonlinear function based on observed input-output pairs. ANNs are universal function approximators _cite_, and usually they deliver good performance in applications. Error-free data are rarely provided in applications. First, the data are usually contaminated by noise, which reflects inaccuracies in observations and stochastic nature of the underlying process. ANNs and other function approximators deal with such noise quite efficiently, by minimising the sum of squared differences between the observed and predicted values, or a more sophisticated fitting criterion, such as Huber-type functions, which are the basis of M-estimators in statistics~ _cite_ . The second type of data contamination has to do with either gross observation errors (e.g., equipment malfunction, or notorious replacing missing values with zeroes, wrong decimal points and other blunders), or the data reflecting a mixture of different phenomena. These data, which usually take aberrant values, are called . It has been noted that typically the occurrence of outliers in routine data ranges from N \% to N \%. When fitting a model to the data, outliers need to be identified and eliminated, or, alternatively, examined closely, as they may be of the main interest themselves. Notable examples are intrusion and cyberattack detection, detection of harmful chemicals and cancerous cells. The methods of function approximation based on the least squares (or, more generally, maximum likelihood principle) are not robust against outliers. In fact just one aberrant value can make the model's bias infinite (it is said that the method breaks down) . This phenomenon is well known in linear regression _cite_, where a number of robust high-breakdown methods have been developed. The popular methods of least median of squares (LMS) and least trimmed squares (LTS) _cite_ discard half of the data as potential outliers and fit a model to the remaining half. These methods determine numerically which half of the data should be discarded in order to obtain the smallest value of the respective objectives. That way, up to half of the data can be outliers but they do not break down the method. It is said that their (asymptotic) breakdown point is _inline_eq_ . The outliers themselves can be identified by their large residuals, something that cannot be achieved when using the least squares estimators, or maximum likelihood estimators (called M-estimators), because of the masking effect (i.e., the outliers affect the fitted model so much that their residuals do not stand out) . Much less work has been devoted to non-linear high-breakdown regression. There are very few papers dealing with the LTS method applied to ANNs, see for example, _cite_ . More recently, Liano~ _cite_ used M-estimators to study the mechanism by which outliers affect the resulting ANNs. Chen and et. al~ _cite_ also used M-estimators as a robust estimator in the presence of outliers. The Least Trimmed Squares estimator was discussed in~ _cite_ . A robust LTS backpropagation algorithm based on simulated annealing was considered in~ _cite_ . It is known that fitting the LTS or LMS criterion is an NP-hard problem even in linear regression. The objective has a large number of local minima and is non-smooth. The problem becomes even more complicated for ANNs, because Training ANNs with LTS or LMS criterion is very challenging because of a much higher number of local minima as well as non-applicability of the traditional fast backpropagation algorithm because of non-smoothness of the objective. In this article we advance the methods for robust fitting of ANNs using LTS and related fitting criteria. Our first contribution is to design a hybrid algorithm, which combines a derivative free optimisation method for initial training of ANN, removal of the detected outliers, and then fine tuning of ANN weights using clean data and backpropagation. The second contribution is the design of an improved fitting criterion, called Penalised CLTS (PCLTS), which prevents unnecessary removal of valid data. The LTS and LMS criteria have this undesirable effect, illustrated in our experiments. The PCLTS criterion prevents unnecessary removals by imposing a penalty on removal of every datum. This article is structured as follows. In Section~ _ref_, we introduce the problem of robust regression, recall the definitions and the main features of several existing high-breakdown estimators, and discuss the associated optimization problem. In Section _ref_ we introduce the PCLTS criterion for ANN fitting. In Section~ _ref_, we outline the existing approaches to solution of the related optimization problem and present three new methods we use in this study. Section~ _ref_ is devoted to a comparative numerical study of the optimization methods using several data sets. Section~ _ref_ concludes the article.