Keypoint representations have been a central component of matching, retrieval, pose estimation, and registration pipelines. With the advent of approaches based on deep neural networks, global representations became pervasive in solving these type of problems as they can be trained in a straightforward way in an end-to-end fashion. Their shortcomings are caused by occlusions, partial views or scenes that contain large amount of clutter. In case of local feature representations, deep learning has been also applied to the different stages of the matching pipeline, considering detection, description, or metric learning objectives. Most of the frameworks considered the above objectives separately, used image data, and required a large number of training examples. In order to mitigate these issues, we propose to use deep convolutional networks for learning keypoint representations and a keypoint detector for ND matching jointly without the need for separate annotations. The costly annotation stage can be avoided due to the availability of large repositories of ND models and the capability of obtaining depth images from different viewpoints. For the problem of jointly learning keypoint detectors and descriptors, we define a Siamese network architecture that receives as input a pair of depth images and their pose annotations. Each branch of the architecture is a proposal generation network used to generate patches in the two depth images. The branches share weights and lead to a sampling layer which selects pairs of patches. The pairs are labeled as positive or negative depending on the proximity of their ND re-projection calculated from the pose labels. In other words, the sampling layer is used to create ground truth data on-the-fly by taking advantage of the initial pose annotations. For training the network, we use the contrastive loss which attempts to minimize the distance in the feature space between positive pairs, and maximize the distance between negative pairs. Therefore, for patches that are very close in the ND space, but sampled from different images, we are learning a representation that has minimal distance in the feature space. In order to learn where to select patches from, we define a {\em score loss} to gauge the performance of the target task. For example, for pose estimation the {\em score loss} should consider the number of positive matches between two images from different viewpoints. To summarize, the key contributions of this paper include the following: We evaluate the matching accuracy of the proposed approach on multiple benchmark datasets and demonstrate improvements over state-of-the-art methods.