What are the people in Figure~ _ref_ doing? This question can be answered at numerous levels of detail--in this paper we focus on the group activity, a high-level answer such as ``team spiking acivity''. We develop a novel hierarchical deep model for group activity recognition. A key cue for group activity recognition is the spatio-temporal relations among the people in the scene. Determining where individual people are in a scene, analyzing their image appearance, and aggregating these features and their relations can discern which group activity is present. A volume of research has explored models for this type of reasoning~ _cite_ . However, these approaches have focused on probabilistic or discriminative models built upon hand-crafted features. Since they rely on shallow hand crafted feature representations, they are limited by their representational abilities to model a complex learning task. Deep representations have overcome this limitation and yielded state of the art results in several computer vision benchmarks~ _cite_ . A naive approach to group activity recognition with a deep model would be to simply treat an image as an holistic input. One could train a model to classify this image according to the group activity taking place. However, it isn't clear if this will work given the redundancy in the training data: with a dataset of volleyball videos, frames will be dominated by features of volleyball courts. The differences between the different classes of group activities are about spatio-temporal relations between people, beyond just global appearance. Forcing a deep model to learn invariance to translation, to focus on the relations between people, presents a significant challenge to the learning algorithm. Similar challenges exist in the object recognition literature, and research often focuses on designing pooling operators for deep networks (e.g.~ _cite_) that enable the network to learn effective classifiers. Group activity recognition presents a similar challenge--appropriate networks need to be designed that allow the learning algorithm to focus on differentiating higher-level classes of activities. Hence, we develop a novel hierarchical deep temporal model that reasons over individual people. Given a set of detected and tracked people, we run temporal deep networks (LSTMs) to analyze each individual person. These LSTMs are aggregated over the people in a scene into a higher level deep temporal model. This allows the deep model to learn the relations between the people (and their appearances) that contribute to recognizing a particular group activity. The main contribution of this paper is the proposal of a novel deep architecture that models group activities in a principled structured temporal framework. Our N-stage approach models individual person activities in its first stage, and then combines person level information to represent group activities. The model's temporal representation is based on the long short-term memory (LSTM): recurrent neural networks such as these have recently demonstrated successful results in sequential tasks such as image captioning _cite_ and speech recognition _cite_ . Through the model structure, we aim at constructing a representation that leverages the discriminative information in the hierarchical structure between individual person actions and group activities. The model can be used in general group activity applications such as video surveillance, sport analytics, and video search and retrieval. To cater the needs of our problem, we also propose a new volleyball dataset that offers person detections, and both the person action label, as well as the group activity label. The camera view of the selected sports videos allows us to track the players in the scene. Experimentally, the model is effective in recognizing the overall team activity based on recognizing and integrating player actions. This paper is organized as follows. In Section~ _ref_, we provide a brief overview of the literature related to activity recognition. In Section~ _ref_, we elaborate details of the proposed group activity recognition model. In Section~ _ref_, we tabulate the performance of approach, and end in Section~ _ref_ with a conclusion of this work.