In the past couple of years, deep learning has swept though computer vision like wildfire. One needs only to buy a GPU, arm oneself with enough training data, and turn the crank to see head-spinning improvements on most computer vision benchmarks. So it is all the more curious to consider tasks for which deep learning has {\em not} made much inroad, typically due to the lack of easily obtainable training data. One such task is {\em dense visual correspondence}--the problem of estimating a pixel-wise correspondence field between images depicting visually similar objects or scenes. Not only is this a key ingredient for optical flow and stereo matching, but many other computer vision tasks, including recognition, segmentation, depth estimation, etc. could be posed as finding correspondences in a large visual database followed by label transfer. In cases where the images depict the same physical object/scene across varying viewpoints, such as in stereo matching, there is exciting new work that aims to use the commonality of the scene structure as supervision to learn deep features for correspondence~ _cite_ . But for computing correspondence {\em across different object/scene instances}, no learning method to date has managed to seriously challenge SIFT flow~ _cite_, the dominant approach for this task. How can we get supervision for dense correspondence between images depicting different object instances, such as images _inline_eq_ and _inline_eq_ in Figure~ _ref_ ? Our strategy in this paper is to learn the things we don't know by linking them up to the things we do know. In particular, at training time, we use a large dataset of ND CAD models~ _cite_ to find one that could link the two images, as shown in Figure~ _ref_ . Here the dense correspondence between the two views of the same ND model _inline_eq_ and _inline_eq_ can serve as our ground truth supervision (as we know precisely where each shape point goes when rendered in a different viewpoint), but the challenge is to use this information to train a network that can produce correspondence between two real images at test time. A naive strategy is to train a network to estimate correspondence between the rendered views of the same ND model, and then hope that the network could generalize to real images as well. Unfortunately, this does not work in practice (see Table~ _ref_), likely due to N) the large visual difference between synthetic and real images and N) the lack of cross-instance ground truth correspondence for training. Instead, in this paper we utilize the concept of {\em cycle consistency} of correspondence flows~ _cite_--the notion that the composition of flow fields for any circular path through the image set should have a zero combined flow. Here, cycle consistency serves as a way to link the correspondence between real images and the rendered views into a single N-cycle chain. We can then train our correspondence network using cycle consistency as the supervisory signal. The idea is to take advantage of the known synthetic-to-synthetic correspondence as ground-truth anchors that allow cycle consistency to propagate the correct correspondence information from synthetic to real images, without diverging or falling into a trivial solution. Here we could interpret the cycle consistency as a kind of ``meta-supervision'' that operates not on the data directly, but rather on how the data should behave. As we show later, such ND-guided consistency supervision allows the network to learn cross-instance correspondence that potentially overcomes some of the major difficulties (e.g. significant viewpoint and appearance variations) of previous pairwise matching methods like SIFT flow~ _cite_ . Our approach could also be thought of as an extension and a reformulation of FlowWeb~ _cite_ as a learning problem, where the image collection is stored implicitly in the network representation. The main contributions of this paper are: N) We propose a general learning framework for tasks without direct labels through cycle consistency as an example of ``meta-supervision''; N) We present the first end-to-end trained deep network for dense cross-instance correspondence; N) We demonstrate that the widely available ND CAD models can be used for learning correspondence between ND images of different object instances.