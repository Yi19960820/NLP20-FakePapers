Human action recognition _cite_ aims to recognize the ongoing action from a video clip. As one of the most important tasks in computer vision, action recognition plays a significant role in many useful applications like video surveillance _cite_, human-computer interaction _cite_ and content retrieval _cite_, with great potentials in artificial intelligence. As a result, massive attention has been dedicated to this area which made large progress over the past decades. Most state-of-the-art methods have contributed to the tasks in visible imaging videos, and shows saturated performances among the widely-used benchmark datasets including KTH _cite_ and UCFN _cite_ . Generally speaking, the task of action recognition is quite well-addressed and has already been applied to real-world problems. However, there are still many occasions where visible imaging is limited. First, the RGB cameras rely heavily on the light conditions, and perform poorly when light is insufficient or over-abundant. Action recognition from night-view RGB data remains a rather difficult task. Moreover, as an act to protect the fundamental human dignity--Privacy, RGB cameras are strictly restricted from most private areas including the personal residential, public washroom where abnormal human activities are likely to threat personal security. Infrared cameras, that captures the heat radiation of objects, are excellent alternatives in these occasions _cite_ . The application of thermal imaging in military affairs and police surveillance has continued for years, and has more potentials beyond the government use. With many advantages over the RGB camera, it is predicted that infrared cameras will become more common in public spaces like hospitals, nursing centers for elderly and home security systems _cite_ . While infrared cameras can fill the limited spots of RGB cameras, many visible features are nevertheless lost in the infrared spectrum due to their similarity in temperature _cite_ . Visible features like color, texture are effective clues in activity representations. Since the two are complementary to each other, it is desired to utilize both visible and infrared features to benefit the task of action recognition. Furthermore, it will be more desired to utilize both feature domains when ONLY infrared data is available. In the previous cases when the demand of abnormal action recognition and the demand of privacy conflicted, it will be great if we can obtain both infrared and visible features, while use only the infrared data. The question is, how can one obtain visible features when the visible data is missing? The situation is not unique to the task of action recognition. In fact, data with different modalities of complementary benefits widely exist in multimedia such as systems with multiple sensors, product details with combined information of text description and images _cite_ . Here we are inspired by the intra-modal feature representations to make up for the missing data using adversarial learning with the available part of the data channel. Recently, much attention has been given to cross-modal feature representations _cite_ dealing with unpaired data, which maps multiple feature spaces onto a common one, or to generate a different representation via adversarial training. The basic model of generative adversarial networks (GANs) _cite_ consists of a generative model _inline_eq_ and a discriminative model _inline_eq_ . Many interesting image-to-image translations such as genre translation, face and pose transformation indicate the broader potentials of GANs to explore the hidden correlations in cross-modal representations _cite_ . Inspired by this, we therefore seek an algorithm that can translate from the infrared representation to the visible domain, which allows us to further exploit the benefits of both feature spaces with only part of the data modalities. More generally speaking, we aim at an architecture that learns a full representation for data of different modalities, using partial modalities. Different from the existing works of cross-modal which seeks a common representation from different data spaces, our goal is to exploit the transferable ability among different modalities, which is further utilized to construct a full-modal representation when only partial data modalities are available. With a completely different target, in this paper we propose a novel partial-modal Generative Adversarial Networks (PM-GANs), which aims to learn the transferable representation among data of heterogeneous modalities using cross-modal adversarial mechanism and build discriminative full-modal representation architecture using data of one/partial modalities. The main contributions are summarized as follows. The rest of the paper is organized as follows. In Section N, we review the background and related works. In Section N, we elaborate the details of our proposed method. Section N presents the newly-introduced dataset, its evaluations, and the experimental results of on it. Finally, Section N draws the conclusion.