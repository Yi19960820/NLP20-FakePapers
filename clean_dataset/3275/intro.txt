In the last decade, Convolutional Neural Networks (CNNs) have shown state of the art accuracy on a variety of visual recognition tasks such as image classification _cite_, object detection _cite_ and action recognition _cite_ . The success of CNNs is based on supervised learning which heavily relies on large manually annotated datasets. These are costly to obtain, however unannotated data in form of images and videos have become easily available on a very large scale. Recently this encouraged the investigation of self-supervised learning approaches which do not require semantic annotations for the data. Here the general procedure involves pretraining a network on a surrogate task which requires semantic understanding in order to be solved. Although the gap is closing, self-supervised approaches usually cannot compete with feature representations obtained by supervised pretraining on large scale datasets such as ImageNet _cite_ or Kinetics _cite_ . In this paper, we use cross-modal information as an alternative source of supervision and propose a new method to effectively exploit mutual information in order to train powerful feature representations for both modalities. The main motivation of our approach is derived from the following observation: Information shared across modalities has a much higher semantic meaning compared to information which is modality specific. We showcase this point in Fig. _ref_ where we present a tuple of pairs obtained from an action recognition video dataset. We can see that cross-modal information such as the barbell or the baseball bat provide good clues to identify the action. On the other hand modality specific information such as the background or camera motion do not help to identify the action. Feature representations which are sensitive to cross-modal information and invariant to modality specific content are therefore desirable. The latter condition is fulfilled if the feature representations of a pair are similar to each other. The former condition is fulfilled if the feature representations are also dissimilar across different pairs. To achieve that we utilize a trainable two stream architecture with one network per modality similar to _cite_ and propose two different loss contributions which guide the networks to learn the desired feature representations. For our method we require paired data from different modalities on a large scale. We therefore apply our method to video data which provide very easily accessible modalities RGB and optical flow in practically unlimited quantity. Our choice for the modalities is also motivated by past work where it has been shown that RGB and optical flow complement each other in the context of action recognition _cite_ . In order to demonstrate the effectiveness of our cross-modal pretraining, we conduct extensive experimental validation and a complete ablation study on our design choices. Our method significantly outperforms state-of-the-art unsupervised approaches on the two highly contested action recognition datasets UCF-N _cite_ and HMDB-N _cite_ while pretraining for only Nh GPU time on a single NVIDIA Titan X (Pascal) . We also show the transferability of our feature representation to object classification and detection by achieving competitive results on the PASCAL VOC N benchmark.