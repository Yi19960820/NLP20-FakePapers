Video Frame interpolation, also known as motion-compensated frame interpolation (MCFI)-motion interpolation for short-has long been an area of research in the field of Computer Vision. In simple terms, it is the process of generating an intermediate image between two frames of a video by processing them with an interpolation technique. The primary motivation for this is usually to make the video seem more smooth or fluid, as well as reducing the effects of motion blur. The main goal then is to generate high quality frames in order to increase the frame-rate of the video, without creating visible distortions. There are other areas of application, such as matching the frame-rate of a video to that of display hardware. Such techniques are sometimes run offline (i.e. the frames are pre-processed), but also occasionally on-the-fly, as is the case in some modern displays, creating a demand for computationally efficient solutions. Traditionally, frame interpolation is solved by first modelling motion between images explicitly, and then synthesizing the intermediate image from the motion representation. Most commonly, optical flow is used for the former. Optical flow describes, for every pixel, the direction (ND) and magnitude of movement between the two images. In the context of neural networks, and especially deep learning, the problem is often solved with the help of convolutional neural networks. The subject network described in this paper has the goal of generating intermediate frames based on the immediate successor and predecessor frames. It is important to note that this work is in essence a replication of the work of . From the start, we had the plan of attempting to use the same structure on a downscaled dataset and investigating the effect of different loss functions on the training. The network we create is intended to be an end-to-end solution which can take any given video as an input and generate a sequence of frames doubling the original frame-rate.