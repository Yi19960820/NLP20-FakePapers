Deep learning on point clouds has received tremendous interest in recent years. Since depth cameras capture point clouds directly, efficient and robust point processing methods like classification, segmentation and reconstruction have become key components in real-world applications. Robots, autonomous cars, ND face recognition and many other fields rely on learning and analysis of point clouds. Existing works like PointNet~ _cite_ and PointNet + + ~ _cite_ have achieved remarkable results in point cloud learning and shape analysis. But they focus on objects with canonical orientations. In real applications, these methods fail to be applied to rotated shape analysis since the model orientation is often unknown as a priori, as shown in Figure~ _ref_ . In addition, existing frameworks require massive data augmentation to handle rotations, which induces unacceptable computational cost. Spherical CNN~ _cite_ and a similar method~ _cite_ try to solve this problem and propose a global feature extracted from continuous meshes, while they are not suitable for point clouds since they project ND meshes onto their enclosing spheres using a ray casting scheme. Difficulty lies in how to apply spherical convolution in continuous domain to sparse point clouds. Besides, by projecting onto unit sphere, their method is limited to processing convex shapes, ignoring any concave structures. Therefore, we propose a pointwise rotation-invariant network (PRIN) to handle these problems. Firstly, we observe the discrepancy between unit spherical space and Euclidean space, and propose Density Aware Adaptive Sampling (DAAS) to avoid biases. Secondly, we come up with Spherical Voxel Convolution (SVC) without loss of rotation-invariance, which is able to capture any concave information. Furthermore, we propose Point Re-sampling module that helps to extract rotation-invariant features for each point. PRIN is a network that directly takes point clouds with random rotations as the input, and predicts both categories and pointwise segmentation labels without data augmentation. It absorbs the advantages of both Spherical CNN and PointNet-like network by keeping rotation-invariant features, while maintaining a one-to-one point correspondence between the input and output. PRIN learns rotation-invariant features at spherical voxel grids. Afterwards, these features could be aggregated into a global descriptor or per-point descriptor to achieve model classification or part segmentation, respectively. We experimentally compare PRIN with various state-of-the-art approaches on the benchmark dataset: ShapeNet part dataset~ _cite_ and ModelNetN~ _cite_ . Additionally, PRIN can be applied to ND point matching and label alignment. PRIN exhibits remarkable performance on all these tasks. The key contributions of this paper are as follows: