Neural networks have been widely adopted in many scenarios, achieving state-of-the-art results in numerous tasks~ _cite_ ~ _cite_ . One of the keys to improved performance is their increased depth and width and thus the increased number of parameters. In computer vision, we have witnessed orders of magnitude increase in the number of parameters in CNNs from LeNet with less than NM parameters in handwritten digit classification~ _cite_ to Deepface with more than NM parameters in human face classification~ _cite_ . Although CNNs with elegant network architectures are easy to deploy in real-world tasks, designing one can be hard and labor-intensive, which involves significant amount of effort in empirical experiments. In terms of designing the network architecture, one crucial part is to determine the number of neurons in each layer. There is no way to directly arrive at an optimal number of neurons for each layer and thus even the most successful network architectures use empirical numbers like N, N, N. Experienced scientists often arrive at the numbers once they deem the network have enough representation power for the specific task. However, the extremely sparse matrices produced by top layers of neural networks have caught our attention, indicating that empirically designed networks are heavily oversized. After some simple statistics, we find that many neurons in a CNN have very low activiations no matter what data is presented. Such weak neurons are highly likely to be redundant and can be excluded without damaging the overall performance. Their existence can only increase the chance of overfitting and optimization difficulty, both of which are harmful to the network. With the motivation of achieving more efficient network architectures by finding the optimal number of neurons in each layer, we come up with an iterative optimization method that gradually eliminates weak neurons in a network via a pruning-retraining loop. Starting from an empirically designed network, our algorithm first identifies redundant weak neurons by analyzing their activiations on a large validation dataset. Then those weak neurons are pruned while others are kept to initialize a new model. Finally, the new model is retrained or fine-tuned depending on the performance drop. The retrained new model can maintain the same or achieve higher performance with smaller number of neurons. This process can be carried out iteratively until a satisfying model is produced.