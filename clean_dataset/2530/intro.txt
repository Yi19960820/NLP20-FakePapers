[N] {} With the advent of big data, graphical representation of information has gained popularity. Being able to classify graphs has applications in many domains. We ask (Figure _ref_), ``Given a piece of a parent network, is it possible to identify the parent network?'' We address this problem using structured image representations of graphs. Adjacency matrices are notoriously bad for machine learning. It is easy to see why, from the unstructured image of a small fragment of a road network, in Figure (a) below. Though the road network is structured, the random image would convey little or no information to machine learning algorithms (in the image, a black pixel at position _inline_eq_ corresponds to an edge between nodes _inline_eq_ and _inline_eq_) . Reordering the vertices (Figure (b) above) gives a much more structured image . Now, the potential to learn distinguishing properties of the subgraph is evident. We propose to exploit this very observation to solve a basic graph problem (see Figure _ref_) . The datasets mentioned in Figure _ref_ are discussed in Section _ref_ . We stress that both images are representations of the adjacency matrix. We use the structured image to classify subgraphs in two modes: The essential difference between our work and previous approaches is that we transform graph classification into image classification. We propose an image representation of the adjacency matrix as input to machine learning algorithms for graph classification, yielding top performance. We further show that this representation is powerful enough to serve as input to a pure transfer learner that has been trained in a . The Adjacency Matrix Image Representation. Given a sample subgraph from a parent network, the first step is to construct the image representation. We illustrate the workflow below. We use the novel method proposed in~ _cite_ which produces an adjacency matrix that is invariant to permutations of the vertices in the adjacency matrix. The image is simply a ``picture'' of this permutation-invariant adjacency matrix. Deep Learning Using Adjacency Matrix Images. We train deep image classifiers (see Section _ref_) on our structured image representation as shown below. We compared several methods, including graph kernel classifiers and classifiers based on standard topological features. Our image representation performs best. Transfer Learning Using Adjacency Matrix Images. When data is scarce or there are many missing labels, a popular option is transfer learning to leverage knowledge from some other domain. Typically the other domain is closely related to the target application. It is unusual for learning in a completely unrelated domain to be transferable to a new target domain. We show that our image representation is powerful enough that one can from the real world image domain to the network domain (two completely unrelated domains) . That is, our image representation provides a link between these two domains enabling classification in the graph domain to leverage the wealth of techniques available to the image domain. The image domain has mature pre-trained models based on massive data. For example, the open-source Caffe deep learning framework is a convolutional neural network trained on the ImageNet data which can recognize everyday objects like chairs, cats, dogs etc. (_cite_) . We use Caffe . Caffe is a black box that provides a distribution over image classes which we refer to as the Caffe-classification vector. The Caffe results are then mapped back into the source domain using a distance-based heuristic e.g. Jaccard distance and _inline_eq_-nearest neighbors as shown below. Images,, are passed through the Caffe deep neural network, and as we shall show, one can get good performance from as little as N \% of the training data used in the machine learning approach. It is quite stunning that such little training data together with un-tweaked transfer learning from a completely unrelated domain can perform so well. The reason is that our image representation provides very structured images (human-recognizable) for real world networks. Though these images are not traditional images like those used in training Caffe, Caffe still maps the different structured images to different distributions over its known classes, hence we are able to this knowledge from Caffe to graph classification. _cite_ introduced the problem we study: Can one identify the parent network from a small subgraph? How much does local information reveal about the parent graph globally? Our approach is from the supervised setting and the unsupervised transfer learning setting. There is previous work on similar problems using graph kernels, _cite_, which use kernels to compute similarities between graphs and then algorithms like SVM for classification. Choosing kernels is not straightforward and is certainly not ``one-size-fits-all''. Further, these kernel methods do not scale well for very large graphs. We compare with one such method proposed by _cite_ . Some approaches construct features from topological attributes of the subgraph (_cite_) . Topological properties of social networks have been extensively studied by _cite_ . The challenges are that it is difficult to come up with a ``master set'' of features that can be used to represent graphs from different domains. For example, assortativity could be an important feature in social networks while being of little significance in road networks. It is hard to identify beforehand what features need to be computed for a given problem, thus leading to a trial and error scenario. Nevertheless, we still compare with classical features trained with logistic regression. Transfer learning is useful when a classification task in one domain can leverage knowledge learned in a related domain (see _cite_ for an extensive survey) . _cite_ introduced a method called which takes advantage of irrelevant unlabeled data to boost performance. _cite_ discuss heterogeneous transfer learning where they use information from text data to improve image classification performance. _cite_ create a new representation from kernel distances to large unlabeled data points before performing image classification using a small subset of reference prototypes. In Section _ref_ we give more details of our approaches to subgraph classification. Section _ref_ present the results comparing the performance of our approach with other approaches. We conclude in Section _ref_ with some possible future directions.