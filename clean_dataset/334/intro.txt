Most, if not all, of classic and contemporary vision-oriented algorithms, such as object detection _cite_ and tracking _cite_, can work reasonably well when facing images of high visibility, but dramatically degenerate or even fail if they are fed with low-quality inputs. In real-world scenarios, especially for outdoor scenes, rain effect has always been such an annoying and inevitable nuisance, which would significantly alter or degrade the content and color of images _cite_ . These situations frequently occur if one records an event happening at a square using a smart phone, a surveillance camera monitors a street, or an autonomous vehicle drives on a road, in rainy days. The rain in atmosphere generally has two existence styles, say steady rain and dynamic rain. The steady rain is caused by distant microscopic rain drops globally accumulated throughout the scene, while the dynamic one comes from large particles (rain streaks) that look like random and local corruptions. The left column of Fig. _ref_ gives two such examples. For eliminating or reducing negative effects brought by rain, the development of effective approaches is demanded. Formally, the rainy image can be seen as a superimposition of two layers _inline_eq_), where _inline_eq_ designates the observed data and, _inline_eq_ and _inline_eq_ represent the rain layer and the desired clean background, respectively. In addition, _inline_eq_ is a blending function. To decompose the two layers from a single image is mathematically ill-posed since the number of unknowns to recover is twice as many as the given measurements. Over the past decades, a lot of attentions to resolving the rain removal problem have been drawn from the community. From the perspective of required input amount, existing rain removal methods can be divided into two classes, i.e. multi-image based and single image based methods. Early attempts on deraining basically belong to the former category. A representative solution was proposed in _cite_, based on the recognition that the visibility of rain in images depends much on the exposure time and depth of field of the camera. One can achieve the goal by testing on several images and adjusting the operational parameters of the camera. {But this method is too professional to use for typical consumers.} The work in _cite_ employs two constraints to automatically find and exclude possible rain streaks, and then fills up the holes by averaging the values of their temporal neighbors, which releases the professional requirement. Several follow-ups along this technique line including _cite_ and _cite_ try to improve the accuracy of rain streak detection or/and the quality of background inpainting. A more elaborated review on the multi-image based rain streak removal approaches can be found in _cite_ . For the sake of flexibility and applicability, single image based approaches are more desirable but challenging. Kang et al. _cite_ proposed a two-step method. The first step is separating the input rain image into a low-frequency component containing its structure and a high-frequency one with both rain streaks and background textures. Then the image textures are distinguished from the rain streaks in the detail layer according to constructed dictionaries, and added back to the structure layer. However, the separation in the detail layer is challenging, always tending to either over-smooth the background or leave noticeable rain steaks. Its follow-ups include _cite_ . Chen and Hsu _cite_ proposed a unified objective function for rain removal by exploring the repetitive property of the rain streak appearance and using a low rank model to regularize the rain streak layer. This is problematic as other repetitive structures like building windows also fit the low-rank assumption. Kim et al. _cite_ tried to detect rain streaks by a kernel regression method, and remove the suspects via a nonlocal mean filtering. It frequently suffers from inaccurate detection of rain streaks. Luo et al. _cite_ created a new blending model and attempted to reconstruct the background and rain layers of image patches over a self-trained dictionary by discriminative sparse coding. Although the method has an elegant formulation, the blending model used still needs physical validation and the effectiveness in removing the rain is somehow weak as one can always see remaining thin structure at the rain streak locations in the output. Li et al. _cite_ used patch-based priors for both the two layers, namely a Gaussian mixture model (GMM) learned from external clean natural images for the background and another GMM trained on rain regions selected from the input image itself for the rain layer. With the emergence of deep learning, a number of low-level vision tasks have benefited from deep models supported by large-scale training data, such as _cite_ for denoising, _cite_ for super-resolution, _cite_ for compression artifact removal and _cite_ for dehazing, as the deep architectures can better capture explicit and implicit features. As for deraining, Fu et al. proposed a deep detail network (DDN) _cite_, inspired by _cite_ . It first decomposes a rain image into a detail layer and a structure layer. Then the network focuses on the high-frequency layer to learn the residual map of rain streaks. The restored result is formed by adding the extracted details back to the structure layer. Yang et al. _cite_ proposed a convolutional neural network (CNN) based method to jointly detect and remove rain streaks from a single image (JORDER) . They used a multi-stream network to capture the rain streak component with different scales and shapes. The rain information is then fed into the network to further learn rain streak intensity. By recurrently doing so, the rain effect can be detected and removed from input images. The work in _cite_ proposes a single image de-raining method called image deraining conditional general adversarial network (ID-CGAN), which considers quantitative, visual and also discriminative performance into the objective function. In order to address the aforementioned challenges, we propose a novel deep decomposition-composition network (DDC-Net) to effectively and efficiently remove the rain effect from a single image under various conditions. Concretely, the contributions can be summarized as follows. _url_ .