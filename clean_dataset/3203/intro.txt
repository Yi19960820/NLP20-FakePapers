Scene labeling, or scene parsing, which aims to assign one of predefined labels to each pixel in an image, is usually formulated as a pixel-level multi-classification problem. Borrowing from the successes of convolutional neural networks (CNNs) ~ _cite_ in image classification~ _cite_, there are attempts to apply CNNs on scene labeling~ _cite_ . Owing to the powerful feature representation of CNNs, these approaches demonstrate promising performance on scene parsing. However, a potential problem with these methods is that CNNs only explore limited contextual cues from a small local field for classification, which is prone to cause misclassifications for visually similar pixels of different categories. For example, the `sand' pixels can be visually indistinguishable from `road' pixels even for human with limited context. To alleviate this issue, a natural solution is to leverage richer context information to discriminate locally ambiguous pixels~ _cite_ . In these approaches, nevertheless, the long-range contextual dependencies among image regions are still not effectively explored, which are crucial in scene parsing. Motivated by the capacity of capturing long-range dependency among sequential data, recurrent neural networks (RNNs) ~ _cite_ have recently been employed to model semantic dependencies in images for scene labeling~ _cite_, allowing us to perform long-range inferences to discriminate ambiguous pixels. To model the dependencies among image units, a common way~ _cite_ is to represent an image with an undirected cyclic graph (UCG) in which the image units are vertices and their interactions are encoded by undirected edges (see Fig.~ _ref_ (a)) . Due to the loopy structure of UCGs, however, it is difficult to directly apply RNNs to model dependencies in images. To deal with this problem, an UCG is approximated with several directed acyclic graphs (DAGs) (see Fig.~ _ref_ (b)) . Then several DAG structured RNNs are adopted to model the dependencies in these DAGs. Though these DAG structured RNNs can capture dependencies in images to some extent, quiet a bit of information are discarded. For example in Fig.~ _ref_ (a), to correctly distinguish the `sand' unit in red region from the `road' unit, DAG structured RNNs can use the dependency information of `water' units in the pink region from its adjacent neighbors. However, the `water' information may be decaying because it needs to pass through conductors (\ie, the adjacent neighbors of this `sand' unit) . Instead, a better way is to directly leverage the dependency information from `water' units to discriminate `sand' unit from `road' unit. Recently, DenseNet~ _cite_ has demonstrated superior performance in image recognition by introducing dense connections to improve information flow in CNNs. Analogous to CNNs, the DAG structured RNNs can be unfolded to a feed-forward network, the dependency information in an image flows from the start vertex at top-left corner to end vertex at bottom-right corner. To incorporate richer dependency information for each image unit, it is natural to add more connections to the RNN feed-forward network as well, as proposed in this paper. Our {\bf first contribution} is to propose dense RNNs, which capture richer dependencies from various abundant connections in images for each image unit. Unlike existing approaches representing an image as an UCG, we formulate each image as a dense UCG (D-UCG), which is a complete graph. In D-UCG, each pair of vertexes are connected with an undirected edge (see Fig.~ _ref_ (c)) . By decomposing the D-UCG into several dense DAGs (D-DAGs), we propose the DAG structured dense RNNs (DD-RNNs) to model dependencies in images (see Fig.~ _ref_ (d)) . Compared to plain DAG structured RNNs, our DD-RNNs are able to gain richer dependencies from various levels. For instance in Fig.~ _ref_ (c), to correctly recognize the `sand' unit in red region, in addition to the dependencies from its neighbors, our DD-RNNs enable the firsthand use of dependencies from `water' units in the pink region to improve the discriminative power. The DD-RNNs are able to capture vast dependencies for each image unit through dense connections. For a specific unit, however, certain dependencies are irrelevant to help improve discriminative power. For example in Fig.~ _ref_ (d), the `sky' units in blue region are actually not useful to distinguish a `sand' unit in the red region from a `road' unit. Instead, the dependencies from `water' units in the pink region are the most crucial cues to infer its label. Thus, more importance should be assigned to dependencies from `water' units. To this end, we make the {\bf second contribution} by introducing an attention model into DD-RNNs. The attention model is able to automatically select relevant and meanwhile restrain irrelevant dependency information for each image unit, which further enhances their discriminative power. Last but not least, our {\bf third contribution} is to implement an end-to-end scene labeling system by integrating DD-RNNs with CNNs. For validation, we test the proposed method on three popular benchmarks: PASCAL Context~ _cite_, MIT ADENK~ _cite_ and SiftFlow~ _cite_ . In these experiments the proposed method significantly improves the baseline and outperforms other state-of-the-art algorithms. The code will be released upon the publication. The rest of this paper is organized as follows. Section _ref_ briefly reviews the related works of this paper. Section _ref_ describes the proposed approach in details. Experimental results are demonstrated in Section _ref_, followed by conclusion in Section _ref_ .