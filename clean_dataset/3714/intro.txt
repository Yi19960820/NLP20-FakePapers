Designing efficient neural architectures is extremely laborious. A typical design iteration starts with a heuristic design hypothesis from domain experts, followed by the design validation with hours of GPU training. The entire design process requires many of such iterations before finding a satisfying architecture. Neural Architecture Search has emerged as a promising tool to alleviate human effort in this trial and error design process, but the tremendous computing resources required by current NAS methods motivate us to investigate both the search efficiency and the network evaluation cost. AlphaGo/AlphaGoZero~ _cite_ have recently shown super-human performance in playing the game of Go, by using a specific search algorithm called Monte-Carlo Tree Search (MCTS) ~ _cite_ . Given the current game state, MCTS gradually builds an online model for its subsequent game states to evaluate the winning chance at that state, based on search experiences in the current and prior games, and makes a decision. The search experience is from the previous search trajectories (called) that have been tried, and their consequences (whether the player wins or not) . Different from the traditional MCTS approach that evaluates the consequence of a trajectory by random self-play to the end of a game, AlphaGo uses a (or value network) to predict the consequence, which enjoys much lower variance. Furthermore, due to its built-in exploration mechanism using Upper Confidence bound applied to Trees (UCT) ~ _cite_, based on its online model, MCTS dynamically adapts itself to the most promising search regions, where good consequences are likely to happen. Inspired by this idea, we present AlphaX, a NAS agent that uses MCTS for efficient architecture search with Meta-DNN as a predictive model to estimate the accuracy of a sampled architecture. Compared with Random Search, AlphaX builds an online model which guides the future search; compared to greedy methods, e.g. Q-learning, Regularized Evolution or Top-K methods, AlphaX dynamically trades off exploration and exploitation and can escape from locally optimal solutions with fewer search trials. Fig.~ _ref_ summarizes the trade-offs. Furthermore, while prior works applied MCTS to Architecture Search~ _cite_, they lack an effective model to accurately predict rewards, and the expensive network evaluations in MCTS rollouts still remain unaddressed. Toward a practical MCTS-based NAS agent, AlphaX has two novel features: first, a highly accurate multi-stage meta-DNN to improve the sample efficiency; and second, the use of transfer learning, together with a scalable distributed design, to amortize the network evaluation costs. As a result, AlphaX is the first MCTS-based NAS agent that reports SOTA accuracies on both CIFAR-N and ImageNet in on par with the SOTA end-to-end search time.