Visual object recognition in humans is mediated by complex multi-stage processing of visual information emerging rapidly in a distributed network of cortical regions~ _cite_ . Understanding visual object recognition in cortex thus requires a predictive and quantitative model that captures the complexity of the underlying spatio-temporal dynamics~ _cite_ . A major impediment in creating such a model is the highly nonlinear and sparse nature of neural tuning properties in mid-and high-level visual areas~ _cite_ that is difficult to capture experimentally, and thus unknown. Previous approaches to modeling object recognition in cortex relied on extrapolation of principles from well understood lower visual areas such as VN~ _cite_ and strong manual intervention, achieving only modest task performance compared to humans. Here we take an alternative route, constructing and comparing against brain signals a visual computational model based on deep neural networks (DNNs) ~ _cite_, i.e., computer vision models in which model neuron tuning properties are set by supervised learning without manual intervention~ _cite_ . DNNs are the best performing models on computer vision object recognition benchmarks and yield human performance levels on object categorization~ _cite_ . We used a tripartite strategy to reveal the spatio-temporal processing cascade underlying human visual object recognition by DNN model comparisons. First, as object recognition is a process rapidly unfolding over time~ _cite_, we compared DNN visual representations to millisecond resolved magnetoencephalography (MEG) brain data. Our results delineate, to our knowledge for the first time, an ordered relationship between the stages of processing in computer vision model and the time course with which object representations emerge in the human brain. Second, as object recognition recruits a multitude of distributed brain regions, a full account of object recognition needs to go beyond the analysis of a few pre-defined brain regions~ _cite_, determining the relationship between DNNs and the whole brain. Using a spatially unbiased approach, we revealed a hierarchical relationship between DNNs and the processing cascade of both the ventral and dorsal visual pathway. Third, interpretation of a DNN-brain comparison depends on the factors shaping the DNN fundamentally: the pre-specified model architecture, the training procedure, and the learned task (e.g. object categorization) . By comparing different DNN models to brain data, we demonstrated the influence of each of these factors on the emergence of similarity relations between DNNs and brains in both space and time. Together, our results provide an algorithmically informed perspective of the spatio-temporal dynamics underlying visual object recognition in the human brain.