Telling attributes from face images in the wild is known to be very useful in large scale face search, image understanding and face recognition. However, the problem indeed is very challenging since the faces captured in the real world are often affected by many unfavourable influences, such as illumination, pose and expression. To build usable attribute prediction, it is important to preserve the essential face traits in the representations and at the same time, make them insensitive to interference. The representations describing faces in prior literature generally form two groups: the hand-crafted representations and learned representations. Exemplified as in _cite_, local low-level features were constructed from detected face regions. The use of local features were mostly based on the consideration that they are less likely to be influenced than the holistic features by pose changes and facial expressions. In recent work _cite_, multi-scale Gabor features were used as a holistic face representation, which is then converted by a learned hashing process for efficient face retrieval and attribute prediction. Driven by the great improvements brought by the CNN in image classification _cite_ and face recognition _cite_, features extracted from deep architectures became a natural and reasonable choice to represent faces for attribute prediction. In _cite_, local semantic image patches were first detected and fed into deep networks to construct concatenated, pose normalized representation. In _cite_, through intensive training, two concatenating CNNs were built to locate and predict attributes from arbitrary size of faces in the wild. The first CNN stage was pre-trained by image categories and fine-tuned by face attribute tags to locate face from complex background; the second stage was pre-trained by identity labels and fine-tuned by face attribute data to achieve an effective fusion of the discrimination of inter-person representations and the variability of non-identity related factors in the face representations. As a common character, high-level hidden (Fully Connected, FC) layers were especially trained for representing attributes in these work. The best representation for retrieval tasks has been shown to come from the FC layer _cite_ . Naturally, features from FC layers are commonly used for attribute prediction. But considering that different levels of ConvNet encode different levels of abstraction, one can expect that such representations may not be optimal to describe the physical facial characteristics, especially the local attributes, such as ``open mouth'' and ``wearing glasses''. It is therefore rational to consider CNN features from earlier layers, which better preserve both discriminating power and spatial information, for predicting face attributes. As indicated in _cite_, the earlier-layer CNN features are likely to better describe the facial appearance than the high-level features. In this work, our focus is on identifying the best face representations for recognizing face attributes. We employ publicly available data, architecture and a deep learning framework to train a face classification CNN. Hierarchical representations are then constructed and evaluated in a face attribute prediction context. Through intensive investigations, we empirically show the effectiveness of the hierarchical deep representations and demonstrate the advantages of the mid-level representations for tackling the face attribute prediction problem. The major contributions of this work are: