Deep convolutional neural networks (DCNN), with its recent progress, has considerably changed the landscape of computer vision _cite_ and many other fields. To achieve close to state-of-the-art performance, a DCNN usually has a lot of parameters and high computational complexity, which may easily overwhelm resource capability of embedded devices. Substantial research efforts have been invested in speeding up DCNNs on both general-purpose _cite_ and specialized computer hardware _cite_ . Recent progress in using low bit-width networks has considerably reduced parameter storage size and computation burden by using N-bit weight and low bit-width activations. In particular, in BNN _cite_ and XNOR-net _cite_, during the forward pass the most computationally expensive convolutions can be done by combining xnor and popcount operations, thanks to the following equivalence when _inline_eq_ and _inline_eq_ are bit vectors: Specifically, an FPGA implementation of neural network can take more benefit from low bit-width computation, because the complexity of a multiplier is proportional to the square of bit-widths. However, most of previous researches on low bit-width networks have been focused on classification networks. In this paper, we are concerned with fully convolutional networks (FCN), which can be thought of as performing pixelwise classification of the input images and have applications in tasks like semantic segmentation _cite_ . Techniques developed in this paper can also be applied to other variants like RPN _cite_, FCLN _cite_ and Densebox _cite_ . Compared to a typical classification network, the following properties of FCN make it a better candidate to apply low bit-width quantizations. Considering the method of training a low bit-wdith network is still under exploration, it remains a challenge to find a way to train a BFCN efficiently as well. Our paper makes the following contributions: