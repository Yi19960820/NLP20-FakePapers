Neural network learning is typically slow, where back propagation usually dominates the computational cost during the learning process. Back propagation entails a high computational cost because it needs to compute full gradients and update all model parameters in each learning step. It is not uncommon for a neural network to have a massive number of model parameters. In this study, we propose a back propagation method, which we call, for neural network learning. The idea is that we compute only a very small but critical portion of the gradient information, and update only the corresponding minimal portion of the parameters in each learning step. This leads to sparsified gradients, such that only highly relevant parameters are updated and other parameters stay untouched. The sparsified back propagation leads to a linear reduction in the computational cost. To realize our approach, we need to answer two questions. The first question is how to find the highly relevant subset of the parameters from the current sample in stochastic learning. We propose a top-_inline_eq_ search method to find the most important parameters. Interestingly, experimental results demonstrate that we can update only N--N \% of the weights at each back propagation pass. This does not result in a larger number of training iterations. The proposed method is general-purpose and it is independent of specific models and specific optimizers (e.g., Adam and AdaGrad) . The second question is whether or not this minimal effort back propagation strategy will hurt the accuracy of the trained models. We show that our strategy does not degrade the accuracy of the trained model, even when a very small portion of the parameters is updated. More interestingly, our experimental results reveal that our strategy actually improves the model accuracy in most cases. Based on our experiments, we find that it is probably because the minimal effort update does not modify weakly relevant parameters in each update, which makes overfitting less likely, similar to the effect. The contributions of this work are as follows: