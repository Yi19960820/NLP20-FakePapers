Understanding text in images shared on platforms such as Facebook and Instagram along with the context in which it appears makes it possible to proactively identify inappropriate or harmful content and keep our community safe. While over the years we had gotten good at handling policy-violating text composed in posts and captions, we were exposed to hate speech, clickbait, policy-violating ads, and other low quality content that manifested as part of an image. Motivated by this problem, Rosetta _cite_ was built to extract overlaid and scene-text from images and video frames and identify policy violating content. Rosetta extracts text from more than a billion public Facebook and Instagram images and video frames (in a wide variety of languages), daily and in real time, and feeds the output to upstream classifiers to understand the context of the text and the image together. Rosetta employs a two-step approach. The text detection model, based on Faster-RCNN _cite_ with the ResNet convolutional body replaced with a ShuffleNet-based _cite_ architecture, is responsible of detecting regions of the image that contain words. Each detected region is cropped and fed to a fully-convolutional character-based text recognition model to extract the word. Widely used object detection models such as Faster-RCNN are designed for generic cases and thus output rectangular bounding boxes. For scene-text extraction, this is insufficient as text may come in arbitrary orientations while the second-stage text recognition model typically expects an image patch with horizontal text. According to an error analysis on Rosetta soon after it was deployed, oriented text was found to be the most common source of mistakes with orientations as minimal as N \degree failing to be recognized correctly. Therefore, it's important to enable the system to correctly handle rotated text amongst all the adversarial cases we might face. In traditional Faster-RCNN detection, while most words are detected with a bounding box, we cannot correctly infer the actual words due to lack of textual orientation information (Figure _ref_) . One solution is to apply a trained Spatial Transformer Network on the rectangular patch, which benefits from being standalone module. Another approach is to predict an oriented bounding box in detection stage, which benefits from being able to train end-to-end. The two approaches can be combined together if needed. We follow the idea of _cite_ to replace the Region Proposal Network (RPN) in our Faster-RCNN-based detection pipeline with RRPN. There are a few differences in our implementation compared to original RRPN: \newline RRoI transformation: We use Rotated Region of Interest Align (RRoI-Align) that applies bi-linear interpolation, instead of Rotated Region of Interest Pooling (RRoI-Pooling) as RoI transformation to avoid misaligned result on the boundaries due to rounding. \newline Boundary breaking anchors: The original RRPN will filter out boundary breaking R-anchors during training and they had to use a border padding of N times each side to reserve more positive proposals. Our approach automatically performs on-demand padding in the RRoI-Align operator as well as bounding box transformation between the detection and recognition pipeline. \newline Orientation coverage: Due to trade-offs between orientation coverage and computational efficiency, the original RRPN crops the angle range to be within _inline_eq_ . However, it's not a symmetrical range as the text rotated by _inline_eq_ degrees would be treated as text rotated by _inline_eq_ degrees. Therefore, we use a more natural orientation coverage of _inline_eq_ degrees so that any text that is not rotated by more than N degrees can be correctly identified. The RPN anchors are chosen accordingly.