In the past few years, the most popular representation learning frameworks are dictionary learning and deep learning. Dictionary learning aims at learning a set of atoms such that a given feature can be well approximated by a sparse linear combination of these atoms, while deep learning methods focus on extracting semantic features via a deep network. So far most studies in dictionary learning employ a shallow (single layer) architecture,, currently popular dictionary learning techniques are K-SVD _cite_, Discriminative K-SVD (D-KSVD) _cite_ and Label Consistent K-SVD (LC-KSVD) _cite_ which decompose the training data into a dense basis and sparse coefficients. In addition, both Local Coordinate Coding (LCC) _cite_ and its fast implementation algorithm _cite_ are traditional dictionary learning methods. LCC and Locality Constrained Coding (LLC) _cite_ are based on the empirical observation that the sparse representations tend to be ``local''. In other words, nonzero coefficients are often assigned to the atoms nearby to the encoded signal _inline_eq_ . However, LLC has a major disadvantage: to achieve higher approximation, one has to use a large number of so-called ``anchor points'' to make a better linear approximation of the signal. Since LLC is a local linear approximation of a complex signal _inline_eq_, for a nonlinear function on _inline_eq_ the local linear approximation may not necessarily be optimal. It means that the anchor points need to provide higher approximation power, allowing some of them to not necessary be ``real'' local anchors on the manifold where _inline_eq_ resides. In this context, our goal is to equip anchors with more descriptive power for better approximating _inline_eq_ in order to finally make more accurate inferences from it. An illustrative example is shown in Figure~ _ref_ . Recent work _cite_ has shown that deeper architectures can be built from dictionary learning. Chun et al.~ _cite_ present a Block Proximal Gradient method using a Majorizer for convolutional dictionary learning. Hu et al.~ _cite_ propose a nonlinear dictionary learning method and apply it to image classification task. Xiao et al.~ _cite_ propose a two-layer local coordinate coding framework for object recognition task. Zhang et al.~ _cite_ introduce an analysis discriminative dictionary learning framework for image classification task. Nguyen et al.~ _cite_ propose a domain adaptation framework using a sparse and hierarchical network, which shares the ideas with our work. However, our DDLCN is different from~ _cite_ in two ways: (i) Our dictionary is learned from features and then the learned dictionary acts as a candidate pool for the next layer dictionary. Our dictionaries from different layers have connections while in _cite_ which used a fixed dictionary in different layers, i.e., there is no message passing between the dictionaries of different layers; (ii) To represent an atom in the previous layer, we pick out a few atoms in the next layer and linearly combine them. These atoms have a linear contribution in constructing the atom in the previous layer. This is vital for the diversity, and in this way could incorporate more information into the next layerâ€™s codes and alleviate the influence of incorrect atoms. However, there is no such mechanism in~ _cite_ . Inspired by both dictionary and deep learning, the goal of this paper is to improve the deep representation ability of dictionary learning. To address this problem, we present a novel network, named Deep Micro-Dictionary Learning and Coding Network (DDLCN), which is composed of several layers: input, feature extraction, dictionary learning, feature coding, pooling, fully connected and output layer as shown in Figure _ref_ . The idea of the DDLCN comes from the standard architecture of Convolutional Neural Networks (CNNs), the biggest difference being that the convolutional layers in CNNs are replaced by our compound dictionary learning and coding layers. In this way, edges, lines and corners can be learned from the shallow layers which correspond to the shallow dictionaries. The more complicated ``hierarchical'' patterns/features can be obtained from deeper dictionaries. DDLCN takes advantage of the manifold geometric structure of the underlying data to locally embed points from the underlying data manifold into a lower dimensional deep structural space. The benefit of DDLCN is that the learned feature representation after the feature learning and coding layers has a better approximation capability of the original data, in other words, the deep dictionary learning structure can fully exploit the space where the data reside. Meanwhile, the deep dictionary structure is very flexible, making it possible to use a micro dictionary,, we can learn only one dictionary item per category. Our contributions are summarized as follows: