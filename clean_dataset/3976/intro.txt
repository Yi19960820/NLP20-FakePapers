Image reconstruction is one of the most widely studied problems in computational imaging. Since the problem is often ill-posed, the process is traditionally regularized by constraining the solutions to be consistent with our prior knowledge about the image. Some traditional imaging priors include nonnegativity, transform-domain sparsity, and self-similarity~ _cite_ . Recently, however, the attention in the field has been shifting towards new imaging formulations based on deep learning~ _cite_ . The most common deep-learning approach is based on an end-to-end training of a convolutional neural network (CNN) for reproducing the desired image from its noisy measurements~ _cite_ . A popular alternative considers training a CNN as an image denoiser and using it within an iterative reconstruction algorithms~ _cite_ . However, recently, it was also shown that a CNN can by itself regularize image reconstruction data-driven training~ _cite_ . This framework naturally regularizes reconstruction by optimizing the weights of a CNN for it to synthesize the measurements from a given random input vector. The intuition behind DIP is that natural images can be well represented by CNNs, which is not the case for the random noise and certain other image degradations. DIP was shown to achieve remarkable performance on a number of image reconstruction tasks~ _cite_ . In this paper, we propose to further improve DIP by combining an CNN regularization with an TV penalty. The idea of our DIP-TV approach is simple: by including an additional TV term into the objective function, we restrict the solutions synthesized by CNN to those that are piecewise smooth. We experimentally show that our DIP-TV method outperforms the traditional formulations of DIP and TV, and performs on a par with other state-of-the-art image restoration methods such as BMND~ _cite_ and IRCNN~ _cite_ .