Human face recognition is a challenging problem in computer vision with several biometrics applications. This problem essentially faces difficulties due to variations in facial appearance caused by factors such as illumination, expression, and partial occlusion from accessories including glasses, scarves, hats, and the like. In recent years, deep learning based approaches have been increasingly applied for face recognition with promising results _cite_ . These methods take raw data as their network input and convolve filters in multiple levels to automatically discover low-level and high-level representations from labeled or unlabeled data for detecting, distinguishing, and/or classifying their underlying patterns _cite_ . However, optimizing millions of parameters to learn the multi-stage weights from scratch in deep learning architectures requires millions of training samples and an access to powerful computational resources such as Graphical Processing Units (GPUs) . Consequently, the method of transfer learning _cite_ is efficiently utilized to apply previously learned knowledge of a relevant visual recognition problem to the new, desired task domain. Transfer learning can be applied in two different ways with respect to the size and similarity between the pre-training dataset and the new database. The first approach is fine-tuning the pre-trained network weights using the new dataset via backpropagation. This method is only suggested for large enough datasets since fine-tuning the pre-trained networks with few training samples can lead to overfitting _cite_ . The second approach is the direct utilization of learned weights in the desired problem to extract and later classify features. This scheme is especially efficient when the new dataset is small and/or a few number of classes exists. Depending on the task similarity between the two datasets, one can decide whether to use lower layers' weights--as generic low-level feature extractors--or higher layers' weights--as task specific motif extractors _cite_ . In this paper, the higher layer portion of learned weights from two deep convolutional neural networks (CNNs) of VGG-Face _cite_ and Lightened CNN _cite_, pre-trained on very large face recognition collections, have been employed to extract face representation. These two models are selected since they have been found to be successful for face recognition in the wild while being publicly available. The former network includes a very deep architecture and the latter is a computationally efficient CNN. Robustness of these deep face representations against variations of different factors including illumination, occlusion, pose, and misalignment has been thoroughly assessed using five popular face datasets, namely the AR _cite_, CMU PIE _cite_, Extended Yale dataset _cite_, Color FERET _cite_, and FRGC _cite_ . The main contributions and outcomes of this work can be summarized as follows: (i) A comprehensive evaluation of deep learning based representation under various conditions including pose, illumination, occlusion, and misalignment has been conducted. In fact, all the proposed deep learning based face recognition methods such as DeepFace _cite_, DeepID _cite_, FaceNet _cite_, and VGG-Face _cite_ have been trained and evaluated on very large wild face recognition datasets, i.e. Labeled Faces in the Wild (LFW) _cite_, YouTube Faces (YTF) _cite_, and MegaFace _cite_ . However, their representation capabilities to handle individual appearance variations have not been assessed yet. (ii) We have shown that although deep learning provides a powerful representation for face recognition, it is not able to achieve state-of-the-art results against pose, illumination, and occlusion. To enable deep learning models achieve better results, either these variations should be taken into account during training or preprocessing methods for pose and illumination normalization should be employed along with pre-trained models. (iii) We have found that deep learning based face representation is robust to misalignment and able to tolerate facial feature localization errors up to N \% of the interocular distance. (iv) The VGG-Face model _cite_ is shown to be more transferable compared to the Lightened CNN model _cite_ . Overall, we believe that deep learning based face recognition requires further research to address the problem of face recognition under mismatched conditions, especially when there is a limited amount of data available for the task at hand. The rest of the paper is organized as follows. covers a review of existing deep learning methods for face recognition. describes the details of two deep CNN models for face recognition and presents the extraction and assessment approach for face representation based on these models. explains the utilized datasets and presents the designed experiments and their results. Finally, concludes the paper with the summary and discussion of the conducted experiments and implications of the obtained results.