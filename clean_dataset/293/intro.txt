Consider the following problem: given two images taken by cameras at different horizontal positions, we wish to compute the disparity _inline_eq_ for each pixel in the left image. Disparity refers to the difference in horizontal location of an object in the left and right image---an object at position _inline_eq_ in the left image appears at position _inline_eq_ in the right image. If we know the disparity of an object we can compute its depth _inline_eq_ using the following relation: where _inline_eq_ is the focal length of the camera and _inline_eq_ is the distance between the camera centers. Figure~ _ref_ depicts the input to and the output from our method. The described problem of stereo matching is important in many fields such as autonomous driving, robotics, intermediate view generation, and ND scene reconstruction. According to the taxonomy of, a typical stereo algorithm consists of four steps: matching cost computation, cost aggregation, optimization, and disparity refinement. Following we refer to the first two steps as computing the matching cost and the last two steps as the stereo method. The focus of this work is on computing a good matching cost. We propose training a convolutional neural network on pairs of small image patches where the true disparity is known (for example, obtained by LIDAR or structured light) . The output of the network is used to initialize the matching cost. We proceed with a number of post-processing steps that are not novel, but are necessary to achieve good results. Matching costs are combined between neighboring pixels with similar image intensities using cross-based cost aggregation. Smoothness constraints are enforced by semiglobal matching and a left-right consistency check is used to detect and eliminate errors in occluded regions. We perform subpixel enhancement and apply a median filter and a bilateral filter to obtain the final disparity map. The contributions of this paper are This paper extends our previous work by including a description of a new architecture, results on two new data sets, lower error rates, and more thorough experiments.