Artificial neural networks enjoy increasing popularity for image classification tasks. They have shown excellent performance in large scale competitions _cite_ . One reason is the ability to train neural networks with millions of training samples by parallelizing them on GPU hardware. This allows to use numbers of training samples which match the large number of parameters in deep neural networks. However, understanding what region of the image is important for a classification decision, is still an open question for neural networks, as well as for many other non-linear models. The work of _cite_ proposed Layer-wise Relevance Propagation (LRP) as a solution for explaining what pixels of an image are relevant for reaching a classification decision. This was done for neural networks, bag of word models _cite_, and in a subsequent work _cite_, for Fisher vectors. This paper proposes an approach to extend LRP to neural networks with nonlinearities beyond the commonly used neural network formulation. One example of such nonlinearities are local renormalization layers which can not be handled by standard LRP _cite_ . The presented approach is based on first (or higher) order Taylor expansion. We consider a classification setup with real-valued outputs. A classifier _inline_eq_ is a mapping of an input space _inline_eq_ such that _inline_eq_ denotes the presence of the class.