Human pose tracking is an essential component of many video understanding tasks such as visual surveillance, action recognition, and human-computer interaction. Compared to human pose estimation in single images, pose tracking in videos has shown to be a more challenging problem due in part to issues such as motion blur and uncommon poses with few or no examples in training sets. To help alleviate these problems, temporal information and spatial structure have been utilized in locating body joints. A variety of approaches have been presented for exploiting temporal information in tracking human pose. Among them are techniques that use dense optical flow to propagate joint estimates from previous frames~ _cite_, local model fitting with the predicted pose of the preceding frame as initialization~ _cite_, and temporal smoothness priors as a pose estimation constraint~ _cite_ . These methods take advantage of pose similarity from frame to frame, but do not learn explicitly about the effect of human motions on the appearance and displacement of joints. Rather, the motions are predicted through generic machinery like optical flow, local optimization of an initialized model, and smoothness constraints. Other methods take advantage of the spatial structure of joints to facilitate joint localization in tracking. These include techniques that integrate graphical models into deep networks~ _cite_ and that employ structural priors~ _cite_ . Such approaches are mainly designed to avoid invalid pose configurations rather than directly localize joints. Alternatively, several methods refine part locations based on the predicted positions of other parts~ _cite_, which provides only vague constraints on joint positions. In this paper, we propose an approach that learns about spatiotemporal joint relationships and utilizes them in conjunction with conventional joint heatmaps to elevate performance on human pose tracking. For learning temporal relationships, our system design reconfigures the optical flow framework to introduce elements from human pose estimation. Instead of densely predicting the motion vectors of general scene points as done for optical flow, our network is trained specifically to infer the displacements only of human joints, while also accounting for possible appearance changes due to motion. Inspired by heat maps in pose estimation, we leverage the other pixels to support the inference of each joint displacement. With training on labeled pose tracking datasets, our neural network learns to make use of information from both joint and non-joint pixels, as well as appearance change caused by joint motion, to predict joint displacements from one frame to the next. Our system additionally learns spatial joint relationships, specifically the offset between a joint and a particular neighboring joint. These spatial relationships are learned in a manner analogous to the temporal relationships, except in the spatial domain. The spatial and temporal relationships, as well as single-frame joint heatmaps, all provide orthogonal cues based on different source information for predicting joint locations. We show that optimizing with respect to these cues together leads to appreciable improvements in pose tracking performance. The presented technique comes with the following practical benefits: The code and model will be released online upon publication of this work.