Advances in machine learning have led to rapid developments towards automation of various tasks, such as detection of pathology in medical scans. The currently most successful models are supervised neural networks. A network is trained using a manually annotated database for a specific task. However the amount of potential tasks in healthcare is immense. Thus annotating a large database for each task, to train specialized models from scratch, is not a scalable solution. Instead we envision that a model with pre-acquired knowledge about common tasks could be supplied to clinicians and, according to their needs, they could further train it to perform a new task. First, by utilizing existing knowledge from previous tasks, the model should quickly grasp the new task with only limited supervision. Furthermore, it should be able to incorporate the new knowledge with existing one, improving its knowledge both for the original and any future tasks. Finally, learning a new task should be possible without access to training data for the earlier tasks, which may no longer be available. Such sequential knowledge acquisition is known as and _cite_ . It is related to _cite_, which assumes training data for all tasks of interest are available and all tasks are learnt concurrently. Instead, here, we assume that when learning a new task, the training data for previous tasks are no longer available. Sequential learning poses a great challenge for neural networks, known as _cite_, where knowledge about an old task is lost when changing a network's parameters during training to meet the objective for a new task. Countering catastrophic forgetting in neural networks has recently attracted increased research attention. The first category of works derive regularization costs such that knowledge of the new task can be incorporated in existing capacity while preserving model behaviour on the old task _cite_ . Other approaches extend a network with extra capacity or components for each new task _cite_ . This may alleviate forgetting but does not effectively fuse old and new knowledge. Such approach was applied for supervised domain adaptation of a model to different MRI scanners _cite_, but not for different tasks, where the label spaces and labelling functions differ. This work explores catastrophic forgetting when learning sequentially two different tasks in medical imaging: segmentation of normal structures and segmentation of white matter lesions in brain MRI. We investigate the potential of a recently proposed method, (EWC) _cite_, originally evaluated for reinforcement learning of Atari games. The method tries to preserve network connections important for previous tasks, by regularizing connections with high Fisher information. We show experimentally that EWC reduces catastrophic forgetting in our settings. This study, the first of its kind in medical imaging, indicates that there is potential in this approach, while showing that there is significant space for further research and improvements towards continual learning.