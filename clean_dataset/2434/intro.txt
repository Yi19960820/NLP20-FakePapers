Object Recognition is arguably one of the most fundamental tasks in computer vision field. Most of the conventional frameworks, \eg Deep Neural Networks (DNN) _cite_, rely on a large number of training samples to build statistical models. However, such a premise is unattainable in many real-world situations. The main reasons can be summarised as follows: N) Obtaining well-annotated training samples is expensive. Although abundant digital images and videos can be retrieved from the Internet, existing search engines crucially depend on user-defined keywords that are often vague and not suitable for learning tasks. N) The number of newly defined classes is ever-growing. Meanwhile, fine-grained tasks make existing categories go deeper, \eg to recognise a newly released handbag in a novel pattern. Training a particular model for each of them is infeasible. N) Collecting instances for rare classes is difficult. For example, one might wish to detect an ancient or rare species automatically. It could be difficult to provide even a single example for them since available knowledge could be only textual descriptions or some distinctive attributes. As a feasible solution, Zero-shot Learning (ZSL) aims to leverage a closed-set of semantic models that can generalise to unseen classes _cite_ . The common paradigm of ZSL methods first train a prediction model that can map visual data to a semantic representation. Hereafter, new objects can be recognised by only knowing their semantic descriptions. However, existing methods cannot expand the training data for new unseen classes. As illustrated in Fig. _ref_, such frameworks impede existing methods from scaling up since the fixed seen data is eventually limited to represent the ever-growing semantic concepts. In this paper, we investigate to synthesise high-quality visual features from semantic attributes so that the ZSL problem can be converted into conventional supervised classification. Our idea is inspired by the ability of human imagination, as shown in Fig. _ref_ . Given a semantic description, we human can associate familiar visual elements and then imagine an approximate scene. Accordingly, we synthesise discriminative low-level features from semantic attributes to substitute feature extraction from real images. Our contributions can be summarised as follows: N) We provide a feasible framework to synthesise unseen visual features from given semantic attributes without acquiring real images. The synthesised data obtained at the training stage can be straightforwardly fed to conventional classifiers so that ZSL recognition is skilfully converted into the conventional supervised problem and leads to state-of-the-art recognition performance on four benchmark datasets. N) We introduce the variance decay problem during semantic-visual embedding and propose a novel Diffusion Regularisation that can explicitly make information diffuse to each dimension of the synthesised data. We achieve information diffusion by optimising an orthogonal rotation problem. We provide an efficient optimisation strategy to solve this problem together with the structural difference and training bias problem.