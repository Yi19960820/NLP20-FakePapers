key to graph-based learning algorithms is the sparse eigenvalue problem, i.e., constructing a block-diagonal affinity matrix whose nonzero entries correspond to the data points belonging to the same subspace (i.e., intra-subspace data points) . Based on the affinity matrix, a series of algorithms~ _cite_ can be derived for various tasks such as subspace clustering and subspace learning. Currently, there are two popular ways to build a similarity graph, one is based on pairwise distances (e.g., Euclidean distance) and the other is based on reconstruction coefficients (e.g., sparse representation) . The second family of methods has recently attracted a lot of interest from the community, where one assumes that each data point can be represented as a linear combination of other points. When the data is clean and the subspaces are mutually independent or disjoint, the approaches such as~ _cite_ are able to well handle the subspace clustering and subspace learning problems. In real applications, however, the data sets are likely to contain various types of noise and data could often lie near the intersection of multiple dependent subspaces. As a result, inter-subspace data points (i.e., the data points with different labels) may connect to each other with very high edge weights, which degrades the performance of graph-based methods. To achieve more robust results, some algorithms have been proposed~ _cite_ . In~ _cite_, Vidal conducted a comprehensive survey regarding subspace clustering. Recently, ~ _cite_ provided a new way to construct the graph using the sparsest or lowest-rank representation. Moreover, ~ _cite_ remove errors from the inputs by modeling the errors in their objective functions. Both theoretical analysis and experimental results show that the methods can handle certain specific types of errors and have achieved good performance. Inspired by their success, the error-removing method is widely adopted in a number of approaches~ _cite_ . One major limitation of these approaches is that the structure of errors should be known as the prior knowledge so that the errors can be formulated into the objective function. In practice, this prior knowledge is difficult to get and the algorithms may work well only if the adopted assumption is consistent with the true structure of the errors. Moreover, these methods must solve a convex problem whose computational complexity is at least proportional to the cubic of the data size. Different from these approaches, we propose a novel error-removing method which aims to eliminate the effect of errors from the _inline_eq_-norm-and nuclear-norm-based projection space (i.e., encoding and then removing errors), where _inline_eq_ . The method is based on a mathematically trackable property of the projection space, i.e., Intra-subspace Projection Dominance (IPD) . Based on our theoretical result, we further propose LN-Graph for subspace clustering and subspace learning by considering the case of _inline_eq_-norm. The proposed method can handle various errors even though the structure of errors is unknown and the data are grossly corrupted. The contributions of this paper is summarized as follows: N) We prove the property of IPD shared by _inline_eq_-, _inline_eq_-, _inline_eq_-, and nuclear-norm-based projection space, i.e., the coefficients with small values (trivial coefficients) always correspond to the projections over the errors. N) We propose a graph-building method based on _inline_eq_-norm, named LN-Graph. The method has a closed-form solution and is more efficient than most existing methods such as~ _cite_ . N) We incorporate LN-Graph into the graph embedding framework~ _cite_ and develop two new algorithms for robust subspace clustering and subspace learning. The paper is an extension of the work in~ _cite_ . Compared with~ _cite_, we further improve our work from the following several aspects: N) Besides _inline_eq_-, _inline_eq_-, and _inline_eq_-norm based projection space, we prove that nuclear-norm-based projection space also possesses the property of IPD; N) Motivated by the success of sparse representation in subspace learning~ _cite_, we propose a new subspace learning method derived upon the LN-Graph. Extensive experimental results show that our method outperform state-of-the-art feature extraction method in accuracy and robustness; N) We explore the potential of LN-Graph in estimating the latent structures of data space. N) Besides image clustering, we extend LN-Graph in the applications of motion segmentation and unsupervised feature extraction; N) We investigate the performance of our method more thoroughly (N new data sets) ; N) We conduct compressive analysis for our method, including the effect of different parameters, different errors (additive and non-additive noises and partial disguises), and different experimental settings. The rest of the article is organized as follows: Section _ref_ presents some related works on graph construction methods. Section~ _ref_ prove that it is feasible to eliminate the effects of errors from the representation. Section~ _ref_ proposes the LN-Graph algorithm and two methods for subspace learning and subspace clustering derived upon LN-Graph. Section~ _ref_ reports the performance of the proposed methods in the context of feature extraction, image clustering, and motion segmentation. Finally, Section~ _ref_ summarizes this work. Notations: Unless specified otherwise, lower-case bold letters represent column vectors and upper-case bold ones represent matrices. _inline_eq_ and _inline_eq_ denote the transpose and pseudo-inverse of the matrix _inline_eq_, respectively. _inline_eq_ denotes the identity matrix. \tablename~ _ref_ summarizes some notations and abbreviations used throughout the paper.