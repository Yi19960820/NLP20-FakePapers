Convolutional Neural Network (CNN) has become one of the most successful computational models in machine learning and artificial intelligence. Remarkable progress has been achieved in the design of successful CNN {\it network structures}, such as the VGG-Net _cite_, ResNet _cite_, and DenseNet _cite_ . Less attention has been paid to the design of {\it filter structures} in CNNs. Filters, namely the weights in the convolutional layers, are one of the most important ingredients of a CNN model, as filters contain the actual model parameters learned from enormous amounts of data. Filters in CNNs are typically randomly initialized, and then updated using variants and extensions of gradient descent (``back-propagation") . As a result, trained CNN filters have no specific structures, which often leads to significant redundancy in the learned model _cite_ . Filters with improved properties will have a direct impact on the accuracy and efficiency of CNN, and the theoretical analysis of filters is also of central importance to the mathematical understanding of deep networks. This paper suggests to decompose convolutional filters in CNN into a truncated expansion with pre-fixed bases in the spatial domain, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. By representing the filters in terms of functional bases, which can come from prior data or task knowledge, rather than as pixel values, the number of trainable parameters is reduced to the expansion coefficients; and furthermore, regularity conditions can be imposed on the filters via the truncated expansion. For image classification tasks, we empirically observe that DCFNet is able to maintain the accuracy with a significant reduction in the number of parameters. Such observation holds even when random bases are used. In particular, we adopt in DCFNet the leading Fourier-Bessel (FB) bases _cite_, which correspond to the low-frequency components in the input. We experimentally observe the superior performance of DCFNet with FB bases (DCF-FB) in both image classification and denoising tasks. DCF-FB network reduces the response to the high-frequency components in the input, which are least stable under image variations such as deformation and often do not affect recognition after being suppressed. Such an intuition is further supported by a mathematical analysis of the CNN representation, where we firstly develop a general result for the CNN representation stability when the input image undergoes a deformation, under proper boundedness conditions of the convolutional filters (Propositions _ref_, _ref_, _ref_) . After imposing the DCF structure, we show that as long as the trainable expansion coefficients at each layer of a DCF-FB network satisfy a boundedness condition, the _inline_eq_-th-layer output is stable with respect to input deformation and the difference is bounded by the magnitude of the distortion (Theorems _ref_, _ref_) . Apart from FB bases, the DCFNet structure studied in this paper is compatible with general choices of bases, such as standard Fourier bases, wavelet bases, random bases and PCA bases. We numerically test several options in Section _ref_ . The stability analysis for DCF-FB networks can be extended to other bases choices as well, based upon the general theory developed for CNN representation and using similar techniques. Our work is related to recent results on the topics of the usage of bases in deep networks, the model reduction of CNN, as well as the stability analysis of the deep representation. We review these connections in Section _ref_ . Finally, though the current paper focuses on supervised networks for classification and recognition applications in image data, the introduced DCF layers are a generic concept and can potentially be used in reconstruction and generative models as well. We discuss possible extensions in the last section. . The usage of bases in deep networks has been previously studied, including wavelet bases, PCA bases, learned dictionary atoms, etc. Wavelets are a powerful tool in signal processing _cite_ and have been shown to be the optimal basis for data representation under generic settings _cite_ . As a pioneering mathematical model of CNN, the {\it scattering transform} _cite_ used pre-fixed weights in the network which are wavelet filters, and showed that the representation produced by a scattering network is stable with respect to certain variations in the input. The extension of the scattering transform has been studied in _cite_ which includes a larger class of bases used in the network. Apart from wavelet, deep network with PCA bases has been studied in _cite_ . Making a connection to dictionary learning~ _cite_, _cite_ studied deep networks in form of a cascade of convolutional sparse coding layers with theoretical analysis. Deep networks with random weights have been studied in _cite_, with proved representation stability. The DCFNet studied in this paper incorporates structured pre-fixed bases combined by {\it adapted} expansion coefficients learned from data in a supervised way, and demonstrates comparable and even improved classification accuracy on image datasets. While the combination of fixed bases and learned coefficients has been studied in classical signal processing _cite_, dictionary learning _cite_ and computer vision _cite_, they were not designed with deep architectures in mind. Meanwhile, the representation stability of DCFNet is inherited thanks to the filter regularity imposed by the truncated bases decomposition. {\bf Network redundancy} . Various approaches have been studied to suppress redundancy in the weights of trained CNNs, including model compression and sparse connections. In model compression, network pruning has been studied in _cite_ and combined with quantization and Huffman encoding in _cite_ . _cite_ used hash functions to reduce model size without sacrificing generalization performance. Low-rank compression of filters in CNN has been studied in _cite_ . _cite_ explored model compression with specific CNN architectures, e.g., replacing regular filters with _inline_eq_ filters. Sparse connections in CNNs have been recently studied in _cite_ . On the theoretical side, _cite_ showed that a sparsely-connected network can achieve certain asymptotic statistical optimality. The proposed DCFNet relates model redundancy compression to the regularity conditions imposed on the filters. In DCF-FB network, redundancy reduction is achieved by suppressing network response to the high-frequency components in the inputs.