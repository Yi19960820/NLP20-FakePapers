Visual odometry (VO) enables a robot to localize itself in various environments by only using low-cost cameras. In the past few decades, model-based VO or geometric VO has been widely studied and its two paradigms, feature-based method _cite_ and direct method _cite_, have both achieved great success. However, model-based methods tend to be sensitive to camera parameters and fragile in challenging settings, e.g., featureless places, motion blurs and lighting changes. In recent years, data-driven VO or deep learning based VO has drawn significant attention due to its potentials in learning capability and the robustness to camera parameters and challenging environments. Starting from the relocalization problem with the use of supervised learning, Kendall et al. _cite_ first proposed to use a Convolutional Neural Network (CNN) for N-DoF pose regression with raw RGB images as its inputs. Li et al. _cite_ then extended this into a new architecture for raw RGB-D images with the advantage of facing the challenging indoor environments. Video clips were employed in _cite_ to capture the temporal dynamics for relocalization. Given pre-processed optical flow, a CNN based frame-to-frame VO system was reported in _cite_ . Wang et al. _cite_ then presented a Recurrent Convolutional Neural Network (RCNN) based VO method resulting in a competitive performance against model-based VO methods. Ummenhofer _cite_ proposed ``DeMoN'' which can simultaneously estimate the camera's ego-motion, image depth, surface normal and optical flow. Visual inertial odometry with deep learning was also developed in _cite_ and _cite_ . However, all the above mentioned methods require the ground truth of camera poses or depth images for conducting the supervised training. Currently obtaining ground truth datasets in practice is typically difficult and expensive, and the amount of existing labeled datasets for supervised training is still limited. These limitations suggest us to look for various unsupervised learning VO schemes, and consequently we can train them with easily collected unlabeled datasets and apply them to localization scenarios. VO related unsupervised deep learning research mainly focuses on depth estimation, inspired by the image wrap technique ``spatial transformer'' _cite_ . Built upon it, Garg et al. _cite_ proposed a novel unsupervised depth estimation method by using the left-right photometric constraint of stereo image pairs. This method was further improved in _cite_ by wrapping the left and right images across each other. In this way, the accuracy of depth prediction was improved by penalizing both left and right photometric losses. Instead of using stereo image pairs, Zhou et al. _cite_ proposed to use consecutive monocular images to train and estimate both ego-motion and depth, but the system cannot recover the scale from learning monocular images. Nevertheless, these unsupervised learning schemes have brought deep learning technologies and VO methods closer and showed great potential in many applications. In this paper, we propose UnDeepVO, a novel monocular VO system based on unsupervised deep learning scheme (see Fig. _ref_) . Our main contributions are as follows: Since UnDeepVO only requires stereo imagery for training without the need of labeled datasets, it is possible to train it with an extremely large number of unlabeled datasets to continuously improve its performance. The rest of this paper is organized as follows. Section \uppercase introduces the architecture of our proposed system. Section \uppercase describes different types of losses used to facilitate the unsupervised training of our system. Section \uppercase presents experimental results. Finally, conclusion is drawn in Section \uppercase .