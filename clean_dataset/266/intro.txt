Deep learning algorithms automatically learn a discriminating set of features and have depicted commendable performance in a variety of computer vision applications. Unfortunately, training a deep model necessitates a large volume of labeled data, which can be time consuming and expensive to acquire. However, labeled data from a different, but related domain is often available, which has motivated the development of algorithms which can leverage labeled data in a source domain to develop a machine learning model for the target domain. Learning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation _cite_ . Unsupervised domain adaptation is a challenging setting, where labeled data is available only in the source domain; no labeled data is available in the target domain. Conventional shallow transfer learning methods develop their models in two stages, feature extraction followed by domain adaptation. The features are fixed and then a model is trained to align the source and target domains _cite_ . On the other hand, deep transfer learning procedures exploit the feature learning capabilities of deep networks to learn transferable feature representations for domain adaptation and have demonstrated impressive empirical performance _cite_ . The explosive growth of digital data in the modern era has posed fundamental challenges regarding their storage, retrieval and computational requirements. Against this backdrop, hashing has emerged as one of the most popular and effective techniques due to its fast query speed and low memory cost _cite_ . Hashing techniques transform high dimensional data into compact binary codes and generate similar binary codes for similar data items. Motivated by this fact, we propose to train a deep neural network to output binary hash codes (instead of probability values), which can be used for classification. We see two advantages to estimating a hash value instead of a standard probability vector in the final layer of the network: (i) the hash values are used to develop a unique loss function for target data in the absence of labels and (ii) during prediction, the hash value of a test sample can be compared against the hash values of the training samples to arrive at a more robust category prediction. In this paper, we first introduce a new dataset, Office-Home, which we use to evaluate our algorithm. The Office-Home dataset is an object recognition dataset which contains images from _inline_eq_ domains. It has around _inline_eq_ images organized into _inline_eq_ categories. We further propose a novel deep learning framework called Domain Adaptive Hashing (DAH) to learn informative hash codes to address the problem of unsupervised domain adaptation. We propose a unique loss function to train the deep network with the following components: (i) supervised hash loss for labeled source data, which ensures that source samples belonging to the same class have similar hash codes; (ii) unsupervised entropy loss for unlabeled target data, which imposes each target sample to align closely with exactly one of the source categories and be distinct from the other categories and (iii) a loss based on multi-kernel Maximum Mean Discrepancy (MK-MMD), which seeks to learn transferable features within the layers of the network to minimize the distribution difference between the source and target domains. Figure _ref_ illustrates the different layers of the DAH and the components of the loss function.