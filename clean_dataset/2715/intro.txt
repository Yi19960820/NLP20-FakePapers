Recently, Recurrent Neural Networks (RNNs) _cite_, especially Long Short-Term Memory (LSTM) model _cite_, have gained significant attention in solving many challenging problems involving sequential time-series data, such as action recognition _cite_, multilingual machine translation _cite_, multimodal translation between videos and sentences _cite_, and robot control _cite_ . In these applications, learning an appropriate representation of sequences is an important step in achieving artificial intelligence. Compared with many existing spatio-temporal features _cite_ from the time-series data, RNN use either a hidden layer _cite_ or a memory cell _cite_ to learn the time-evolving states which models the underlying dynamics of the input sequence. For example, _cite_ have used LSTM to model the video sequences to learn their long short-term dynamics. In contrast to the conventional RNN, the major component of LSTM is the memory cell which is modulated by three gates-input, output and forget gates. These gates determine the amount of dynamic information entering/leaving the memory cell. The memory cell has a set of internal states, which store the information obtained over time. In this context, these internal states constitute a representation of an input sequence learned over time. In many recent works, the LSTMs have shown tremendous potential in action recognition tasks _cite_ _cite_ _cite_ . The existing LSTM model represents a video by integrating over time all the available information from each frame. However, we observed that for an action recognition task, not all frames contain salient spatio-temporal information which are discriminative to different classes of actions. Many frames contain non-salient motions which are irrelevant to the performed action. This inspired us to develop a new family of LSTM model that automatically learns the dynamic saliency of the actions performed. The conventional LSTM fails to learn the salient dynamic patterns comprehensively, since the gate units do not explicitly consider whether a frame contains salient motion information when they modulate the memory cells. Thus the model is insensitive to the dynamic evolution of the hidden states given the input video sequences. To address this problem, we propose the differential RNN (dRNN) model that learns these salient spatio-temporal representations of actions. Specifically, dRNN models the dynamics of actions by computing different-orders of Derivative of States (DoS) that are sensitive to the spatio-temporal structure of actions. Depending on the DoS, the gate units can learn the appropriate information that should be required to model the dynamic evolution of actions. To train the dRNN model, we use truncated Back Propagation algorithm to prevent the exploding or diminishing errors through time _cite_ . In particular, we follow the rule that the errors propagated through the connections to those DoS nodes would be truncated once they leave the current memory cell. Finally, we demonstrate that the dRNNs can achieve the state-of-the-art performance on both ND and ND action recognition datasets. Specifically, dRNNs outperform the existing LSTM model on these action recognition tasks, consistently achieving the better performance with the same input sequences. On the other hand, when compared with the other algorithms tailored to model special assumptions on spatio-temporal structure of actions, the proposed general-purpose dRNN model can still reach competitive performance. The remainder of this paper is organized as follows. In the next section _ref_, we review several related work to the action recognition problem. The background and details of RNNs and LSTMs are reviewed in section _ref_ . Section _ref_ presents the proposed differential RNNs (dRNNs) . The experimental results are presented in section _ref_ . Finally, we conclude and discuss the future work related to dRNNs in section _ref_ .