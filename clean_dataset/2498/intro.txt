Scene parsing is a fundamental topic in computer vision and is critical for various challenging tasks such as autonomous driving and virtual reality. The goal is to predict the label of each pixel, i.e., the category label of the object that the pixel belongs to. Various techniques based on deep convolutional neural networks have been developed for scene parsing since the pioneering fully convolutional network approach~ _cite_ . There are two main paths to tackle the segmentation problem. The first path is to raise the resolution of response maps for improving the spatial precision, e.g., through dilated convolutions~ _cite_ . The second path is to exploit the context~ _cite_ for improving the labeling robustness, which our work belongs to. Existing representative works mainly exploit the context formed from spatially nearby or sampled pixels. For instance, the pyramid pooling module in PSPNet~ _cite_ partitions the feature maps into multiple regions, and the pixels lying within each region are regarded as the context of the pixel belonging to the region. The atrous spatial pyramid pooling module (ASPP) in DeepLabvN~ _cite_ regards spatially regularly sampled pixels at different atrous rates as the context of the center pixel. Such spatial context is a mixture of pixels that might belong to different object categories, thus the resulting representations obtained from context aggregation are limitedly reliable for label prediction. Motivated by that the label of a pixel in an image is the category of the object that the pixel belongs to, we present a so-called object context for each pixel, which is the set of pixels that belong to the same object category with such a pixel. We propose a novel object context pooling (OCP) to aggregate the information according to the object context. We compute a similarity map for each pixel _inline_eq_, where each similarity score indicates the degree that the corresponding pixel and the pixel _inline_eq_ belongs to the same category. We call such similarity map as object context map, which serves as a surrogate of the true object context. Figure~ _ref_ shows several examples of object context map. We exploit the object context to update the representation for each pixel. The implementation of object context pooling, inspired by the self-attention approach~ _cite_, computes the weighted summation of the representations of all the pixels contained in the object context, with the weights from the object context map. We further present two extensions: (i) pyramid object context, which performs object context pooling in each region in the spatial pyramid and follows the pyramid design introduced in PSPNet~ _cite_ . (ii) atrous spatial pyramid object context, which combines ASPP~ _cite_ and object context pooling. We demonstrate our proposed approaches by state-of-the-art performance on two challenging scene parsing datasets, Cityscapes and ADENK, and the challenging human parsing dataset LIP.