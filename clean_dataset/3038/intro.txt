Recently, deep convolutional neural networks (CNN) show promising performances in various computer vision tasks such as object classification~ _cite_, localization (or detection) ~ _cite_, segmentation~ _cite_, video classification~ _cite_, and pose estimation~ _cite_ . CNN hierarchically builds up high-level semantic concepts from low-level visual features in a layer-by-layer manner based on convolution kernels which convolve pixels on local receptive fields. Among those tasks, object localization (or detection) is one of the fundamental problems in this research field. In object localization tasks, region-of-interests (ROIs), the most discriminative region in terms of semantic concepts, should be properly defined for each given image. A lot of training images with annotated bounding boxes or segmentation maps of ROIs are required in order to achieve good performance in object localization since those information gives strong prior in terms of exploring exact ROIs on test images~ _cite_ . However, a dataset with such location information is hard to obtain because it requires heavy annotation efforts. Weakly supervised learning for localization only uses a weak-labeled (i.e. image-level label) dataset which does not have any location information to localize objects in an image. In terms of finding common semantic features within a set of images having the same class label, this can be interpreted as a varient of multiple instance learning (MIL) ~ _cite_ . Several previous works for CNN-based weakly supervised object localization have been presented with reasonable methods and good performances with the help of transfer learning from pre-trained networks~ _cite_ . Those approaches, however, require base networks pre-trained on relatively well-localized datasets (e.g., ILSVRC classification dataset) which is able to extract discriminative regions appropriately from semantically similar datasets (e.g., VOC) while providing good initial seed for localization. In other words, they fine-tune good initial feature maps extracted from pre-trained networks with respect to the objectives of localization tasks. Weakly supervised localization methods which rely on those base networks cannot be used in the applications which do not have enough well-localized images. Medical image analysis is representative because it is impossible to obtain such pre-trained networks. Furthermore, it is not feasible to use base networks pre-trained on general images such as ILSVRC or VOC datasets since ROI characteristics of medical images are thoroughly different from general images. In this work, we propose a self-transfer learning (STL) framework for fully weakly supervised localization. STL co-optimizes both classification and localization networks simultaneously in order to guide the localization network with the most discriminative features in terms of the classification task (see Figure~ _ref_) . The term fully means that the proposed method does not require not only the location information but also any types of pre-trained networks in a training stage, and the term self stands for weight sharing between classification and localization networks. Our contributions can be summarized as follows: The remainder of this paper is organized as follows. Section N presents previous works for the weakly supervised learning for object localization. In Section N, the proposed STL framework is described in detail including its architecture and training scenarios. Section N shows experimental setup and results, and finally Section N concludes this paper.