Deep learning has led to remarkable breakthroughs in automatically learning hierarchical representations from images. Models such as Convolutional Neural Networks (CNNs) _cite_, Restricted Boltzmann Machine-based generative models _cite_, and Auto-encoders _cite_ have been successfully applied to produce multiple layers of increasingly abstract visual representations. However, there is relatively little work on characterizing the optimal representation of the data. While Cohen {\em et al.} _cite_ have considered this problem by proposing a theoretical framework to learn irreducible representations having both invariances and equivariances, coming up with the best representation for any given task is an open question. Various work _cite_ has been done on the theory and practice of representation learning, and from this work a consistent set of desiderata for representations has emerged: invariance, meaningfulness of representations, abstraction, and disentanglement. In particular, Bengio {\em et al.} _cite_ propose that a disentangled representation is one for which changes in the encoded data are sparse over real-world transformations; that is, changes in only a few latents at a time should be able to represent sequences which are likely to happen in the real world. The ``vision as inverse graphics'' model suggests a representation for images which provides these features. Computer graphics consists of a function to go from compact descriptions of scenes (the graphics code) to images, and this graphics code is typically disentangled to allow for rendering scenes with fine-grained control over transformations such as object location, pose, lighting, texture, and shape. This encoding is designed to easily and interpretably represent sequences of real data so that common transformations may be compactly represented in software code; this criterion is almost identical to that of Bengio {\em et al.}, and graphics codes conveniently align with the properties of an ideal representation. Recent work in inverse graphics _cite_ follows a general strategy of defining a probabilistic or deterministic model with latent parameters, then using an inference or optimization algorithm to find the most appropriate set of latent parameters given the observations. Recently, Tieleman {\em et al.} _cite_ moved beyond this two-stage pipeline by using a generic encoder network and a domain-specific decoder network to approximate a ND rendering function. However, none of these approaches have been shown to automatically produce a semantically-interpretable graphics code and to learn a ND rendering engine to reproduce images. In this paper, we present an approach for learning interpretable graphics codes for complex transformations such as out-of-plane rotations and lighting variations. Given a set of images, we use a hybrid encoder-decoder model to learn a representation that is disentangled with respect to various transformations such as object out-of-plane rotations and lighting variations. To achieve this, we employ a deep directed graphical model with many layers of convolution and de-convolution operators that is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm _cite_ . We propose a training procedure to encourage each group of neurons in the graphics code layer to distinctly represent a specific transformation. To learn a disentangled representation, we train using data where each mini-batch has a set of active and inactive transformations, but we do not provide target values as in supervised learning; the objective function remains reconstruction quality. For example, a nodding face would have the ND elevation transformation active but its shape, texture and other affine transformations would be inactive. We exploit this type of training data to force chosen neurons in the graphics code layer to specifically represent active transformations, thereby automatically creating a disentangled representation. Given a single face image, our model can re-generate the input image with a different pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a ND rendering engine.