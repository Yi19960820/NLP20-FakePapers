Joint surgical gesture segmentation and classification is fundamental for objective surgical skill assessment and for improving efficiency and quality of surgery training _cite_ . The goal is to segment robotic kinematic data or video sequence and to classify segmented pieces into surgical gestures, such as {\itshape reaching for the needle}, {\itshape orienting needle} and {\itshape pushing needle through the tissue}, etc. Variant temporal models have been exploited in prior works on surgical gesture segmentation and classification. One branch of works has been based on hidden Markov models (HMMs) _cite_, differing from each other in how the emission probability is modeled. HMM-based methods assume that gesture label at frame _inline_eq_ is only conditioned on previous frame _inline_eq_, leaving long-term dependency unconsidered. Another branch has been based on conditional random fields (CRFs) _cite_ and their extensions, which obtains the gesture sequence by minimizing an overall energy function. Although these methods capture temporal patterns by the pairwise potentials in their energy functions, they produce severe over-segmentation and therefore suboptimal segmental edit scores. In recent years, a third branch using deep learning has set new benchmarks for this task. Recurrent neural networks, in particular LSTMs, were applied in _cite_ . A memory cell is maintained in LSTM to remember and forget action changes over time. _cite_ proposed a spatiotemporal CNN, in which the spatial component described relationships of objects in the scene, and a long temporal convolutional filter captured how the relationships change temporally. Thereupon, _cite_ went further and built a hierarchical encoder-decoder network called Temporal Convolutional Network (TCN) composed of long temporal convolutional filters, upsampling/downsampling layers, and normalization layers. In spite of the promising performance improvement achieved, these methods are only driven by frame-wise accuracy due to their cross-entropy training loss. Unlike prior works, we propose an essentially different deep reinforcement learning approach for joint surgical gesture segmentation and classification, which is driven by both frame-wise accuracy and segment-level edit score. Reinforcement learning has gained remarkable success recently in domains like playing Go _cite_, Atari games _cite_, and anatomical landmark detection in medical images _cite_, etc. However, reinforcement learning has not been applied in surgery gesture segmentation in existing works. We formulate the task as a sequential decision-making process and train an agent to operate in a human-like manner. The agent looks through the surgical data sequence from the beginning, segment the sequence step by step and classify frames simultaneously. To highlight, our agent learns a strategical policy---skim fast in the middle of segments and examine attentively at segment boundaries, which resembles human intelligence. Additionally, current deep learning methods like RNN and TCN handle temporal consistency {\itshape implicitly} by memory cells or temporal convolutions. On the contrary, we enforce temporal consistency {\itshape explicitly} by the design of action and reward. The reward consists of two terms that guide the agent to high accuracy and high edit score respectively. To combine reinforcement learning with the hierarchical representation learned by deep neural networks, features extracted by TCN are utilized as powerful state representation for the agent. The proposed method is tested on the suturing task of the JIGSAWS dataset _cite_ . Experiments show that our method outperforms state-of-the-art methods in terms of edit score. In summary, our contributions are three-fold: