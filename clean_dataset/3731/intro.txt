Vision-based problems have benefited from recent advances in deep learning. The past decade has witnessed a tremendous growth in employment of convolution neural networks (CNN) as a solution to vision-related challenges _cite_ . CNNs have the ability to learn feature maps from the given training data, these learned features can embed complex representations of the input data. The efforts to enhance the performance of CNN have led to various architectures having the skip connections _cite_ and deeper layers _cite_ . Recently, encoder-decoder architectures have helped in providing end-to-end learning for many complex vision tasks such as semantic segmentation, optical flow, disparity and scene-flow estimations, etc. _cite_ . The encoder part extracts the feature maps according to the scope of the problem whereas the decoder combines and upscales the desired features to the target resolutions. This architecture, which is combined with the ability of the CNN to learn any kind of end-to-end mappings, has become a very flexible neural-net for solving many vision problems. A common choice for the upscaling and aggregation of features in the decoder part is to use a standard deconvolution operation _cite_ . These pipelines have enabled end-to-end learning which is desirable. Nevertheless, deconvolution often results in the so-called checkerboard artifacts. These artifacts appear when the deconvolution layers have "uneven overlap". More precisely speaking, this occurs when the kernel size is not divisible by the stride _cite_ . Since the up-scaling of spatial resolution has been intensively studied for super-resolution _cite_, our idea of using sub-pixel convolution was motivated to replace the deconvolutions in the decoder part. So, in this paper, we propose a modified encoder-decoder model by interpreting the transposed convolution (deconvolution) as upscaling operation. Deconvolution has been a standard choice for upscaling in the decoder part ever since it was formally introduced by Long et al. _cite_ . In principle, we replace the deconvolution operation with an efficient sub-pixel convolution module. This sub-pixel convolution layer has been inspired by the work of Shi et al. _cite_ . The benefit of this approach versus other super-resolution networks is that the Efficient Sub-Pixel Convolution Network (ESPCN) operates in the Low Resolution (LR) domain instead of the High Resolution (HR) domain, making it computationally less expensive. Shi et al. _cite_ gave insight that architectures with convolutions purely in the LR domain have more representation power than a model which first up-samples the input images and performs convolution for them afterward. Therefore, in this paper, we not only demonstrate a new encoder-decoder pipeline based on ESPCN but also give a general guideline to potentially convert any model that uses transposed convolution into an ESPCN-based architecture. In order to demonstrate the generality of our idea, we apply it to a wide variety of tasks e.g. optical flow, disparity, and structures from motion. FlowNet _cite_ was devised as an encoder-decoder architecture for optical flow estimation. Optical flow is the apparent motion of objects in a scene. In this paper, our encoder-decoder architecture with sub-pixel convolution for optical flow estimation is named as Flospnet which stands for an optical flow sub-pixel network. In the Flospnet, its encoder part is essentially the same as that of the FlowNet, whereas the decoder part of the Flospnet is different. In the FlowNet, every stage of the decoder part consists of the concatenated output of deconvolution, the skip connection from the encoder and an intermediate prediction layer. The Flospnet maintains the same entire architecture except for replacing the deconvolution layers with sup-pixel convolution modules. The disparity estimation baseline network, DispNet _cite_, can be changed in a similar way as done for the Flospnet. In this case, our encoder-decoder pipeline architecture with sub-pixel convolution for disparity estimation is called Despnet which stands for a disparity efficient sup-pixel network. The difference between the Flospnet and the Despnet is the numbers of output channels at each sup-pixel convolution module in the decoder parts. All ESPCN modules in the Flospnet yield two output channels since it estimates optical flows in horizontal and vertical directions while each sup-pixel convolution module in the Despnet has only one output channel because the disparity is estimated only along the horizontal direction. As mentioned before, our Despnet is devised for unsupervised monocular depth estimation by replacing the deconvolution of the original DispNet by Zhou et al. _cite_ with the sub-pixel convolution. We also adopt the sub-pixel convolution for the decoder part of the pose network in their work, thereby completely eliminating any deconvolution layer. In our experiments, we show that the Flospnet and Despnet significantly outperform their corresponding baseline architectures _cite_ . The superiorities of the Flospnet and Despnet come from the following two reasons: (i) the sub-pixel convolution performs more precisely the mapping from a lower dimension to higher dimensions than the deconvolution operation where it is often adopted in super-resolution problems; and (ii) the both models are fast trained since our sub-pixel convolution models use a fewer number of parameters. Consequently, any network pipeline containing deconvolution layers can be replaced by sub-pixel convolution, which can lead to improved performance and a less number of network parameters. We can summarize the contributions of our work as follows: N) {\it Flospnet}: It is designed based on the FlowNet by adopting sub-pixel convolution for optical flow estimation, which has brought a significant improvement on the estimation accuracy. N) {\it Despnet}: It is a novel disparity estimation network after replacing the standard deconvolution layers in DispNet with sub-pixel convolution. We also show a variation of Despnet with rectangular convolution which outperforms all trained models.