detection in self-driving cars is one of the most challenging and important impediments to full autonomy. Self-driving cars need to be able to detect cars and pedestrians to safely navigate their environment. In recent years, state-of-the-art deep learning approaches such as Convolutional Neural Networks (CNNs) have enabled great advances in using camera imagery to detect and classify objects. In part these advances have been driven by benchmark datasets that have large amounts of labeled training data (one such example is the KITTI~ _cite_ dataset) . Our understanding of how well we solve the object detection task has largely been measured by assessing how well novel detectors perform on this prerecorded human labeled data. Alternatives such as simulation have been proposed to address the lack of extensive labeled data~ _cite_ . However, such solutions do not directly address how to find errors in streams of novel data logged from fleets of deployed autonomous vehicles (AVs) . The current AV testing pipeline of repeatedly gathering and labeling large test datasets to benchmark object detector success is time-consuming and arduous. This solution does not scale well as object detectors fail less frequently and as the number of deployed AVs increase. A typical AV has multiple cameras, each camera capturing tens of images per second, and hundreds of such AVs could be deployed in a city; all of these images are being processed by the object detector algorithm which could potentially miss objects in any of these images. Without hand labeled ground truth, understanding when an object detector has failed to recognize an object is a relatively unstudied problem for self-driving cars. This paper introduces a novel automated method to identify mistakes made by object detectors on the raw unlabeled perception data streams from an AV. The proposed system allows AVs to continuously evaluate their object detection performance in the real world for different locations, changing weather conditions and even across large time scales when the locations themselves evolve. As testing groups of AVs becomes more commonplace this approach provides an unsupervised mechanism to understand algorithmic, spatial, temporal, and environmental failures of a system's perception stack at the fleet level. Detecting mistakes within unlabeled data is an inherently ill-posed problem. Without relying on additional data it is fair to assume an object detector will be maximizing its use of the information contained within a single image. We leverage the inherent spatial and temporal nature of the AV object detection domain. Typically either the vehicle or the objects in the scene are moving and often multiple views of the scene are taken (often with overlap as in the case of stereo cameras) . While it is important to note object trackers also utilize temporal information (and in fact we leverage trackers for our approach) we are not attempting to solve the tracking problem. Object detectors are still required on AVs to initialize a tracker and if an object detector fails to fire, the tracking system is of no use and accidents may ensue. For example if an object detector fails to identify a car as it pulls out of a driveway, finding it N frames later and tracking it from that point may be moot as a collision may have already occurred. We propose that inconsistencies in object detector output between a pair of similar images (either spatially or temporally), if properly filtered, can be used to identify errors as a vehicle traverses the world. The power of this should not be understated. It means that even miles driven by humans for testing purposes can be used to validate object detectors in an unsupervised manner and furthermore any archives of logged sensor data can be mined for the purposes of evaluating a vehicle's perception system. The key contributions of our paper are as follows: N) We present the first full system, to the best of our knowledge, that autonomously detects errors made by single frame object detectors on unlabeled data; N) We show that inconsistencies in object detector output between pairs of similar images-spatially or temporally-provides a strong cue for identifying missed detections; N) We pose the error detection problem as binary classification problem where for each inconsistent detection, we propose novel set of meta classification features that are used to predict the likelihood of the inconsistency being a real error; N) In conjunction with additional localization data available in AV systems we show that our system facilitates the analysis of correlations to geo-locations in errors; N) We release a tracking dataset with sequences of stereo images gathered at _inline_eq_ Hz with ground truth labels following the KITTI format with _inline_eq_ sequences totaling _inline_eq_ pairs of images along with ground truth disparity maps from a game engine making it the largest publicly available dataset of its kind. The remainder of the paper is structured as follows. In Section~ _ref_, we discuss related work. Next, we detail our technical approach in Section~ _ref_ . Section~ _ref_ contains extensive experimental results on a number of datasets for different state-of-the-art object detectors followed by a discussion in Section~ _ref_ . Finally, Section~ _ref_ concludes and addresses future work.