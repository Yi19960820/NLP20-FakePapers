Action understanding in videos is an important problem in computer vision and has received extensive research attention in this community rencently. Most of the research works focused on the problem of action classification _cite_, which aims at predicting an action label given a video. State-of-the-art classification methods _cite_ have achieved relatively good performance on several challenging datasets, such as HMDBN _cite_ and UCFN _cite_ . However, these classification methods are only able to answer ``is there an action of certain type present in the video'', but fail to provide the information about ``where is it if there is an action in the video''. To overcome this limitation, the problem of action detection has been studied by several recent works _cite_, but these methods still perform relatively poorly on the realistic datasets, such as UCF Sports _cite_ and JHMDB _cite_ . For action detection in videos, we need to estimate bounding boxes of the action of interest at each frame, which together form a spatio-temporal tube in the input video. Sliding window becomes computationally prohibitive due to the huge numbers of candidate windows in the video space. For example, give a video of size _inline_eq_, the number of possible boxes for each frame is around _inline_eq_ and the number of possible tubes for the video is as large as _inline_eq_ . Motivated by fast object detection using proposals _cite_, the idea of ``action proposal'' _cite_ has been introduced for efficient action detection _cite_ . Like object proposal algorithms, most of these methods depend on low-level visual cues, such as spatial edges and motion boundaries, and generate action candidates by hierarchically merging super-voxels _cite_ . Therefore, these methods usually require heuristic designs and sophisticated merging algorithms, which are difficult to be optimized for action detection and may be sensitive to input noise. Besides, a large amount of candidate regions (around N-NK) are usually generated by these methods for each frame, which still leads to large computational cost in the subsequent processing. In this paper we focus on a more general problem regarding action understanding and try to estimate the interestingness maps of generic action given the raw frames, called as _cite_ . Each value of the actionness maps describes the confidence of containing an action instance at this place, where higher value indicates larger probability. According to the recent work _cite_, from the perspective of computer vision, action is defined as intentional bodily movement of biological agents (such as people, animals) . Therefore, there are two important visual cues for actionness estimation: and . Appearance information is helpful to locate the biological agents, while motion information contributes to detect bodily movements. In addition, the visual cues of appearance and motion are complementary to each other and fusing them may lead to more accurate actionness estimation. To accomplish the goal of actionness estimation, we propose a two-stream fully convolutional architecture to transform the raw videos into the map of actionness, called as (H-FCN) . Our H-FCN is composed of two separate neural networks: (i) appearance fully-convolutional network (A-FCN), taking RGB image as input, which captures the spatial and static visual cues, (ii) motion fully-convolutional neural network (M-FCN), using optical flow fields as input, that extracts the temporal and motion information. The actionness maps from these two different FCNs are complementary to each other as shown in Figure _ref_ . Each FCN is essentially a discriminative network trained in an end-to-end and pixel-to-pixel manner. By using fully-convolutional architecture, our H-FCN allows for input with arbitrary size and produces the actionness map of corresponding size. Specifically, we adopt the contemporary classification networks (ClarifaiNet _cite_) into fully-convolutional architecture and transfer the pre-trained model parameters from the large dataset (e.g. ImageNet _cite_, UCFN _cite_) to the task of actionness estimation by fine tuning. We verify the performance of H-FCN for actionness estimation on both images and videos. For image data, there is no motion information available and we only use the A-FCN to produce the actionness map on the dataset of StanfordN _cite_ . For video data with human movement, we use the H-FCN to estimate the actionness on the datasets of UCF Sports _cite_ and JHMDB _cite_ . The experimental results on these two datasets demonstrate that our proposed actionness estimation method outperforms previous methods. Moreover, actionness map can be viewed as a new kind of feature and could be exploited to assist many video based tasks such as action classification, action detection, and actor tracking. In this paper we incorporate our estimated actionness maps into the successful RCNN-alike _cite_ detection framework to perform action detection in videos. We first design a NMS score sampling method to produce action proposals based on actionness maps for each frame. Then, we choose the two-stream convolutional networks _cite_ as classifiers to perform action detection. We extensively evaluate the effectiveness of our proposed method on two tasks: action proposal generation on the datasets of Stanford N _cite_ and JHMDB _cite_, and action detection on the dataset of JHMDB _cite_ . \iffalse Contributions. (i) It is the first time to study actionness estimation with deep networks. We propose hybrid fully convolutional networks (H-FCN) for actionness estimation in videos. It significantly outperforms previous method. (ii) Actionness can be viewed as generic features, based on which we propose an action proposal generation methods. Our action proposal method obtains better performance in images and videos than previous methods. (iii) We also verify the effectiveness of our action proposal method on task of action detection. It obtains better performance. \fi \iffalse The rest of this paper is organized as follows. In Section _ref_, we review the related works to our method on actionness estimation, action detection and fully convolutional networks. Then, we describe the technical details of our proposed method in Section _ref_ . After that, in Section _ref_, we present our experimental results about actionness estimation, action proposals, and action detection. Finally, we conclude this paper in Section _ref_ .