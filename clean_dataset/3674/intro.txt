Recent technological developments in digital imaging solutions have led to wide-spread adoption of whole slide imaging (WSI) in digital pathology which offers unique opportunities to quantify and improve cancer treatment procedures. Stained tissue slides are digitally scanned to produce digital slides~ _cite_ at different resolutions till _inline_eq_ as shown in Figure~ _ref_ . These digital slides result in an explosion of data which leads to new avenues of research for computer vision, machine learning and deep learning communities. Moreover, these multi-gigapixel histopathological WSIs can be excellently absorbed by data hungry deep learning methods to tackle digital pathology problems. Convolutional Neural Network (CNN) models have significantly improved the state-of-the-art in many natural image based problems such as visual object detection and recognition~ _cite_ and scene labelling~ _cite_ . However, classification of WSIs through a CNN raises serious challenges due to multi-gigapixel nature of images. Feeding the complete WSI or resizing WSI either leads to computationally unfeasible methods or loss of crucial cell level features essential for segmentation. This results in processing WSIs which are typically _inline_eq_ K _inline_eq_ K pixels in size in a patch-by-patch manner. Since patch based approaches face difficulties in handling images larger than a few thousand pixels, therefore using larger patches to capture maximum context is not a solution. A huge difference between patch size and WSI size results in loss of global context information which is extremely important for many tumour classification tasks~ _cite_ . We propose Representation-Aggregation Networks (RANs) to efficiently model spatial context in multi-gigapixel histology images. RANs employ a representation learning network as a CNN which encodes the appearance and structure of a patch as a high dimensional feature vector. This network can be any state-of-the-art network such as AlexNet~ _cite_, GoogLeNet~ _cite_, VGGNet~ _cite_ or ResNet~ _cite_ . A ND-grid of features is generated by packing feature vectors for neighbouring patches in the WSI as encoded by the representation learning network. The first variant of context-aggregation network (RAN-CNN) in RAN utilizes a CNN with only convolutional and dropout layers. RAN-CNN takes input as a ND-grid and outputs a tumour probability for each cell in the ND-grid. Recurrent Neural Networks (RNNs) along with their variants Long Short Term Memory (LSTM) ~ _cite_ and Gated Recurrent Units (GRUs) ~ _cite_ have excelled at modelling sequences in challenging tasks like machine translation and speech recognition. We build the second variant of RAN (RAN-LSTM) by combining CNNs with ND-LSTMs. RAN-LSTM captures the context information by treating WSIs as a two-dimensional sequence of patches. RAN-LSTM extends ND-LSTMs for tumour segmentation task in multi-gigapixel histology images by using learned representations of neighbouring patches from represenation learning network as a context for tumour classification of a single patch. RAN-LSTM is constituted by four ND-LSTMs running diagonally, one from each corner. Tumour predictions across all the dimensions are averaged together to get the final tumour classification. The complete workflow of the proposed architecture is shown in Figure~ _ref_ . We demonstrate the effectiveness of modelling context using RANs for tumour segmentation. RANs significantly outperform traditional methods on the dataset from Camelyon'N challenge~ _cite_ on all metrics. Our main contributions can be summarized as follows: