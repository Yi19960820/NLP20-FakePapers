Data-driven Artificial Intelligence (AI) is behind the new generation of success stories in the field, and is predicated not just on a few technological breakthroughs, but on a cultural shift amongst its practitioners: namely the belief that predictions are more important than explanations, and that correlations count more than causations {_cite_} . Powerful black-box algorithms have been developed to sift through data and detect any possible correlation between inputs and intended outputs, exploiting anything that can increase predictive performance. Computer vision (CV) is one of the fields that has benefited the most from this choice, and therefore can serve as a test bed for more general ideas in AI. {This paper targets the important problem of ensuring trust in AI systems.} Consider a case as simple as object classification. It is true that exploiting contextual clues can be beneficial in CV and generally in AI tasks. After all, if an algorithm thinks it is seeing an elephant (the object) in a telephone box (the context), or Mickey Mouse driving a Ferrari, it is probably wrong. This illustrates that even though your classifier might have an opinion about the objects in an image, the context around it can be used to improve your performance ({\em e.g.} telling you that it is unlikely to be an elephant inside a telephone box), as shown in many recent works _cite_ . However making predictions based on context can also lead to problems and creates various concerns, one of which is the use of classifiers in ``out of domain'' situations, a problem that leads to research questions in domain adaptation _cite_ . Other concerns are also created around issues of bias, {\em e.g.} classifiers incorporating biases that are present in the data and are not intended to be used _cite_, which run the risk of reinforcing or amplifying cultural (and other) biases _cite_ . Therefore, both predictive accuracy and fairness are heavily influenced by the choices made when developing black-box machine-learning models. Since the limiting factor in training models is often sourcing labelled data, a common choice is to resort to {reusing existing data for a new purpose, such as using web queries to generate training data, and employing various strategies to annotate labels, {\em i.e.}} using proxy signals that are expected to be somewhat correlated to the intended target concept _cite_ . These methods come with no guarantees of being unbiased, or even to reflect the deployment conditions necessarily, with any data collected ``in the wild'' _cite_ carrying with it the biases that come from the wild. To address these issues, a shift in thinking is needed, from the aforementioned belief that predictions are more important than explanations, to ideally developing models that make predictions that are right for the right reason {, and consider other metrics, such as fairness, transparency and trustworthiness, as equally important as predictive performance} . This means that we want to ensure that certain protected concepts are not used as part of making critical decisions ({\em e.g.} decisions about jobs should not be based on gender or race) for example, or that similarly, predictions about objects in an image should not be based on contextual information (gender of a subject in an image should not be based on the background) . In this direction, we demonstrate how the Domain-Adversarial Neural Network (DANN) developed in the context of domain adaptation _cite_ can be modified to generate `agnostic' feature representations that do not incorporate any implicit contextual (correlated) information that we do not want, and is therefore unbiased and fair. {We note that this is a far stronger requirement than simply removing protected features from the input that might otherwise implicitly remain in the model due to unforeseen correlations with other features.} We present a series of experiments, showing how the relevant pixels used to make a decision move from the contextual information to the relevant parts of the image. This addresses the problem of relying on contextual information, exemplified by the Husky/Wolf problem in _cite_, but more importantly shows a way to de-bias classifiers in the feature engineering step, allowing it to be applied generally for different models, whether that is word embeddings, support vector machines, or deep networks etc. Ultimately, this ties into the current debate about how to build trust in these tools, whether this is about their predictive performance, their being right for the right reason, their being fair, or their decisions being explainable.