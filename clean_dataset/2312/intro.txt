\noindent Recent rise of deep learning methods including convolutional neural networks (CNN) and recurrent neural networks (RNN) has escalated a large number of artificial intelligence tasks to an unprecedented stage, where the performance frequently rivals that of humans. Tasks such as object classification, scene classification, and object detection demonstrated the ability to correctly recognize and locate the images both holistically and regionally, whereas tasks such as caption generation or object retrieval demonstrated that deep learning methods can successfully bridge the gap between images and language. Visual question answering (VQA) task further promotes the boundary of deep learning applicability and complicates the problem by necessitating multiple prerequisites, potentially encompassing all of the above-mentioned capabilities; as it needs to understand the question, locate or classify the objects/scenes mentioned in the question, and generate appropriate answers. In this paper, we introduce DualNet, which attempts to fully exploit the discriminative information provided by the images and textual features, by separately performing addition and multiplication of input features to form a common embedding space. As we shall see in Experiment Section, it shows clear advantage over performing only one operation, and outperforms many recent state-of-the-art methods, without using any attention mechanism. Furthermore, it turns out that building an ensemble of DualNets with varying dimensions leads to even more superior performances, despite feeding identical set of input features to all DualNet units. Another advantage of our DualNet is that it is applicable to both real images and abstract scenes categories. So far, it has widely been considered that successful methods for real images cannot be directly ported to abstract scenes domain, as they have fundamentally different characteristics. In fact, applying the basic setting of fcN features for images and long short-term memory (LSTM) with one hidden layer for questions, which results in N for real images, yields only about N in abstract scenes domain. Indeed, most of the previous papers on VQA have tackled only one domain, presumably due to such reason. Our DualNet, however, results in superior performances in both domains, demonstrating that it is applicable to a wider domain, provided that features are plausible. It is also noteworthy that we do not employ any attention mechanism, which has become one of the most common approaches in VQA. While useful, building attention mechanism necessitates a separate stage of training to map language to specific regions of image, and complicates the procedure. Instead, our method demonstrates that basic set of features can provide rich amount of information without building attention mechanism, provided a network is designed in a way that fully exploits the features' discriminative capacity. This paper is hereafter structured as follows; In Related Works, we review the recent innovations and trends in VQA, and briefly discuss how our method diverts from them. In Method, we describe both the motivation behind and the implementation details of our DualNet architecture. In Experiment, we apply our proposed model to actual VQA dataset and discuss the results with examples and comparisons to other methods. Finally, we conclude the paper and discuss future work in Conclusion.