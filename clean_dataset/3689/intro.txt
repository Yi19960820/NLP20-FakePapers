Classifying human actions in video, commonly called action recognition in the literature, has received wide attention over the last decade. Advances in both features _cite_ and representations _cite_ coupled with more challenging datasets such as UCFN/N _cite_ and HMDBN _cite_ have led to an unforeseen action classification capability. Novel and socially enriching applications such as video search with semantic action indexing instead of strictly low-level feature indexing _cite_ are around the corner. However, many potential applications of action recognition in video require more than just action classification. For example, unconstrained human-robot interaction _cite_ requires localization of action; natural language video description requires full detection, localization and classification of action to generate rich text, unlike current methods that have been able to do with only classification _cite_ . Yet, relatively few works have emphasized these important aspects of action understanding---solutions to action localization, detection and classification. Most early works are based on rigid, manually chosen templates _cite_, or deforming models _cite_ that miss joint space-time deformation (see Sec. _ref_ for a longer review) . More recently, a space-time deformable parts model (SDPM) was proposed by Tian et al. _cite_ that can capture space-time articulation for full action understanding. But, this model is limited: first, as a direct extension from state of the art object detection method _cite_, the cuboid-nature of the parts and the two layer star model render them limited in modeling the rich structural, kinematic and dynamic variability of human motion _cite_ . Second, it depends on a weak underlying feature (HOGND) _cite_, which is shown to be less powerful than HOG/HOF _cite_ in representing the variation in human action. A second line of promising work for action understanding is based on point trajectories. Originally proposed by Messing et al. _cite_, point trajectories capture motion articulation in space-time and when coupled with rich descriptors like HOG/HOF _cite_ and are densely computed _cite_, achieves state of the art performance for action classification. A limitation of the dense trajectories are that they are short-lived and limited in modeling the full extent of an articulated; another limitation is that they may fall on moving background rather than human action. Furthermore, grouping trajectories seems promising in capturing relationships between various articulating action parts, Raptis et al. _cite_ recently made a step in this direction to overcome above limitations by clustering trajectories. But, in their model the location of the structures is fixed before learning, therefore limiting the generality of the approach. As discussed by Chen et al. _cite_, motion in a video can occur in various forms such as agent (human/animal) moving, camera panning or jittering, background object moving, among many others. We are particularly interested in human action understanding, where a video can be decomposed into human action and other motion, then human action can be decomposed into articulated body parts with motion and appearance, and further decomposed into articulated sub-parts and so on. We observe that actions of different classes, such as ``moving arm" in running and walking, share many common and recurring elements, and when those articulated elements merge together we further obtain highly discriminative, long-range action parts. In order to model the compositionality of human action from low-level representation to high-level semantic action parts, we propose a compositional model in two steps: (N) we learn a compositional hierarchy based on co-occurring statistics; (N) given the hierarchical representation, we learn a structured model with multiple layers of parts (see Fig. _ref_ (b) for overview of the two steps) . In the first step of our model, we adopt a bottom-up approach and propose a new mid-level representation called compositional trajectories . The basic idea is that we learn a hierarchical compositional model that starts with dense trajectories as the basic elements and then recursively groups frequently co-occurring pairs of elements. At higher levels, the composed trajectories focus on the salient action parts (filtering is a byproduct) and discriminative articulations among action parts hierarchies (see Fig. _ref_ for an example of three layers in the hierarchy) . The new representation itself has already outperforms a complex Markov random field over trajectory grouping _cite_ in action localization without bounding box annotation for training (see Sec. _ref_ for detail) . In the second step, as Fig. _ref_ (c) illustrates, we learn a structured model called locally articulated spatiotemporal deformable parts model or LASTDPM. Our model is based on the learned compositional hierarchies and a three-layer deformable parts hierarchy, which enables us to capture the global articulation of an action with parts that are more locally discriminative compared with _cite_, as demonstrated by our action recognition and detection results in Sec. _ref_ and Sec. _ref_ . \iffalse Our paper unites these two promising directions in two contributions that make it possible both to have highly discriminative, long-range action parts as mid-level features and represent their relative articulated deformation in space-time. The first contribution is a new mid-level action feature called compositional trajectories . The basic idea is that we learn a hierarchical compositional model that starts with dense trajectories as the basic elements and then recursively groups frequently co-occurring pairs of elements. At higher levels, the composed trajectories focus on the salient action parts (filtering is a byproduct) and discriminative articulation among action parts hierarchies (see Fig. _ref_ for an example of three layers in the hierarchy) . The second contribution is the locally articulated spatiotemporal deformable parts model or LASTDPM. Our model replaces the cuboid HOGND filters in the SDPM _cite_ with histograms of compositional trajectories. By doing so it is able to capture the global articulation of an action with parts that are more locally discriminative. It also allows for the domain of the part histograms to translate in space to account for variation in action, such as limb bending. Together, our contributions are able to outperform state of the art methods on the three action understanding problems.