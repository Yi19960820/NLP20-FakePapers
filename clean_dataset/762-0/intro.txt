A pastiche is an artistic work that imitates the style of another one. Computer vision and more recently machine learning have a history of trying to automate pastiche, that is, render an image in the style of another one. This task is called {\em style transfer}, and is closely related to the texture synthesis task. While the latter tries to capture the statistical relationship between the pixels of a source image which is assumed to have a stationary distribution at some scale, the former does so while also attempting to preserve some notion of content. On the computer vision side, _cite_ and _cite_ attempt to ``grow'' textures one pixel at a time using non-parametric sampling of pixels in an examplar image. _cite_ and _cite_ extend this idea to ``growing'' textures one patch at a time, and _cite_ uses the approach to implement ``texture transfer'', i.e. transfering the texture of an object onto another one. _cite_ approaches the texture synthesis problem from an energy minimization perspective, progressively refining the texture using an EM-like algorithm. _cite_ introduces the concept of ``image analogies'': given a pair of ``unfiltered'' and ``filtered'' versions of an examplar image, a target image is processed to create an analogous ``filtered'' result. More recently, _cite_ treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and _cite_ extends Kwatra's energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. _cite_ uses the VGG-N network to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. _cite_ extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-N classifier. While very flexible, this algorithm is expensive to run due to the optimization loop being carried. _cite_, _cite_ and _cite_ tackle this problem by introducing a {\em feedforward style transfer network}, which is trained to go from content to pastiche image in one pass. However, in doing so some of the flexibility of the original algorithm is lost: the style transfer network is tied to a single style, which means that separate networks have to be trained for every style being modeled. Subsequent work has brought some performance improvements to style transfer networks, e.g. with respect to color preservation or style transfer quality, but to our knowledge the problem of the single-purpose nature of style transfer networks remains untackled. We think this is an important problem that, if solved, would have both scientific and practical importance. First, style transfer has already found use in mobile applications, for which on-device processing is contingent upon the models having a reasonable memory footprint. More broadly, building a separate network for each style ignores the fact that individual paintings share many common visual elements and a true model that captures artistic style would be able to exploit and learn from such regularities. Furthermore, the degree to which an artistic styling model might generalize across painting styles would directly measure our ability to build systems that parsimoniously capture the higher level features and statistics of photographs and images~ . In this work, we show that a simple modification of the style transfer network, namely the introduction of {\em conditional instance normalization}, allows it to learn multiple styles (_ref_) .We demonstrate that this approach is flexible yet comparable to single-purpose style transfer networks, both qualitatively and in terms of convergence properties. This model reduces each style image into a point in an embedding space. Furthermore, this model provides a generic representation for artistic styles that seems flexible enough to capture new artistic styles much faster than a single-purpose network. Finally, we show that the embeddding space representation permits one to arbitrarily combine artistic styles in novel ways not previously observed (_ref_) .