are primarily modeled as members of a vector space. The dimensionality of this space is usually much higher than the number of underlying causes. This is mainly due to the inherent limitations of natural and artificial sensors, which often neglect the underlying causes of real world phenomena and therefore sample data at rates far exceeding the effective dimension of the signals. Learning these causes and thus representing a signal in a low-dimensional model is the goal of a recent trend of research known as dictionary learning _cite_ . The idea is to represent a signal by a linear combination of a few elements from a dictionary that is learned from the data. Each data point is thus represented through a sparse vector of coefficients, as a member of a low-dimensional subspace spanned by a few dictionary elements. When the dictionary is fixed this process is commonly known as sparse coding. Dictionary learning has achieved great success in signal reconstruction tasks such as compression _cite_ and denoising _cite_ . More recently, it has also been applied to discriminative tasks such as classification _cite_ and clustering _cite_ with state-of-the-art results. A hyperspectral image is a collection of pixels that represent a given scene or object, where pixels represent the reflected solar radiation from the Earth's surface in many narrow spectral bands _cite_ . At each pixel, the spectral features form a vector whose elements correspond to the narrow bands covering visible to infrared regions of the spectrum. The wealth of data provided by hyperspectral imagery (HSI) has promoted its application in many domains such as agriculture _cite_, defense _cite_, and environmental management _cite_ . The reflectance spectra of a pixel are influenced by a number of factors. Apart from measurement noise caused by variation in illumination and viewing angle, and environmental effects such as aerosols and moisture, the spectral features of a pixel are determined by the material present at the given pixel and its surrounding area. Due to the spatial resolution of the imaging device, scattering from the local scene, and material mixtures within a pixel, each pixel is often composed of a number of different materials plus noise _cite_ . Spectral unmixing is the process of identifying the pure materials present in the mixture, called and their respective . The linear mixture model (LMM) which is commonly used for unmixing assumes that each pixel, _inline_eq_ is composed of a linear combination of endmembers, _inline_eq_ plus an additive noise, _inline_eq_, i.e. where the fractional abundances, _inline_eq_ are commonly assumed to be nonnegative and sum to unity. This is essentially the idea encouraged by dictionary learning with one difference, that in dictionary learning the fractional abundances are mainly assumed to be sparse. The aim of dictionary learning is to reduce the error in representing each signal while inducing sparsity in the representation coefficients. This is commonly accomplished through a formulation such as: where _inline_eq_ is the number of signals available for training, _inline_eq_ is an sparsity inducing regularizer such as the well known _inline_eq_ norm, _inline_eq_ is a regularization parameter balancing representation error with representation complexity, and _inline_eq_ represents constraints on the sparse coefficients. Recently, inspired by the ability of dictionary learning to model high-dimensional data and its potential to learn high-level information from the training samples _cite_, sparse coding and dictionary learning have been used for spectral unmixing with encouraging results. The sparse unmixing approach proposed in _cite_ assumes that a set of pure spectral signatures are available which compose the dictionary. The fractional abundances are estimated using sparse coding with an _inline_eq_ regularizer, taking into account the fact that a few number of endmembers contribute to a given pixel. This approach was later extended in _cite_, where it is assumed that all pixels in an image have fractional abundances with a common sparsity pattern. This is achieved by using a joint sparse regularizer to also decrease the total number of endmembers activated for an image. Since spectral libraries are often composed of groups of spectral signatures, _cite_ proposes a group Lasso formulation to exploit this fact. Motivated by the observation that pixels in a hyperspectral image are usually surrounded by similar pixels, _cite_ include the total variation (TV) regularization in the sparse unmixing formulation to encourage smooth variation in the fractional abundance of each endmember among adjacent pixels. The aforementioned approaches assume a library of pure spectra is given a priori which make up the dictionary (i.e. _inline_eq_ is fixed in (_ref_)) . Selecting endmembers from the data has also been attempted both manually, based on the similarity between eigenvectors of the scene and the data _cite_ or automatically, based on a measure of modeling quality _cite_ . In contrast, _cite_ attempt to learn the set of spectral endmembers using dictionary learning. In _cite_ the dictionary is learned from unsupervised training data by considering a probabilistic LMM framework, wherein the additive noise is assumed to be Gaussian and the fractional abundances are i.i.d. Laplacian and constrained to be nonnegative. Experimental results show that the learned dictionary elements are similar to the material spectra available in the scene and may be used to infer samples with HSI-resolution from multispectral-level measurements. The work of Greer _cite_ differs from _cite_ in that the dictionary is assumed to have full rank and the fractional abundances must sum to unity. Hence, the _inline_eq_ sparsity regularizer used in previous work no longer applies. A recent survey of different approaches for hyperspectral unmixing is presented in _cite_ . The high spatial and spectral resolution of a hyperspectral image provides the potential for each pixel to be accurately and robustly labeled as one of a known set of classes. Hyperspectral image classification has been applied to both urban _cite_ and agricultural _cite_ scenery. Various methods have been developed for this application. Among them are supervised techniques such as maximum likelihood and Bayesian classifiers _cite_, decision trees _cite_, neural networks _cite_, and support vector machines (SVM) _cite_ . Semisupervised learning based on graph construction _cite_ and transductive SVMs _cite_ have also been proposed which take advantage of both labeled and unlabeled samples for classification. Inspired by the fact that pixels in a hyperspectral image are often surrounded by pixels of the same class, recent methods have focused on both spectral and contextual characteristics of HSI. The composite-kernel SVM _cite_ makes use of Mercer's theorem to construct kernels composed from a spectral kernel and a contextual kernel. In particular, the weighted sum of the spectral and contextual kernels has been successful in classifying images with limited training samples. The graph kernel SVM _cite_ incorporates both spectral and contextual characteristics simultaneously into a recursive graph kernel, and is also effective for small training sample sizes. Due to its recent success in discriminative tasks with small training data _cite_, dictionary learning and sparse coding have also been applied to hyperspectral image classification. A sparsity-based model is proposed in _cite_ where a test spectral sample joined with its surrounding pixels is represented by a few training samples from a fixed training dictionary. The test pixel is then labeled as the class whose training samples have the largest contribution in representing the pixel and its surrounding neighbors. Although the main focus of _cite_ is spectral unmixing, the authors demonstrate that using the fractional abundances instead of the raw spectral features improves the classification accuracy of the linear SVM for small training sets. In the approach proposed by Castrodad et al. _cite_, a dictionary is learned for each class of hyperspectral data in a supervised manner. For classification, the different dictionaries are concatenated to form a single dictionary. The sparse code (fractional abundance) corresponding to a new pixel is calculated using the sparse unmixing formulation accompanied by a spectral-spatial regularizer to enforce smooth variations in the sparse codes for neighboring pixels. The new pixel is then labeled as the class whose dictionary produces the lowest representation error and complexity. The work discussed above for hyperspectral unmixing and classification enjoy a number of common and individual advantages and pose a number of remaining challenges which motivate our work. Specifically, we focus on a simple yet efficient approach to learn a dictionary for hyperspectral data that can incorporate contextual information, with the aim of hyperspectral image classification. Of the approaches for spectral unmixing, some assume that a set of pure spectral signatures are available which may compose the dictionary _cite_ . Chen et al. _cite_ use the complete set of training data as the dictionary, but with the aim of classification. Similar to _cite_ we learn a dictionary using the training data, which is less complex (i.e. consists of fewer atoms) yet is more effective for classification (Section _ref_) . In terms of incorporating contextual information, as we discuss in Section _ref_, _cite_ employ a window centered at the pixel of interest to gather contextual information. This hinders the potential for parallel computation _cite_ and the methods tend to use only contextual information. In fact, as we shall see in the experimental results of Section _ref_, SVM classification using the weighted sum of the spectral and contextual kernels _cite_ achieves highest accuracy when the spectral kernel is given zero weight. The methods of _cite_ are also based on the contextual characteristics of HSI since the pixel and its surrounding neighbors are indistinguishable to the classifying process. Of the dictionary-based approaches, Iordache et al. _cite_ and Castrodad et al. _cite_ incorporate contextual information by augmenting (_ref_) with a regularization term that enforces smooth variation in the sparse representation for neighboring pixels. In _cite_ the dictionary is fixed and _cite_ learns the dictionary without the proposed regularization term. This is perhaps in regard of the complex optimization procedure, which is in turn due to the fact that the sparse representations of different pixels can not be computed independent of each other. In contrast, we attempt to learn the dictionary and incorporate contextual information simultaneously, yet our optimization is simple and amenable to parallel computations. In this paper, we propose a structured dictionary-based model for hyperspectral data that is impervious to the aforementioned issues and incorporates both spectral and contextual characteristics of a spectral sample into the sparse set of coefficients. The idea is to partition the pixels of a hyperspectral image into a number of spatial neighborhoods called contextual groups and to model each pixel with a linear combination of a few elements from a dictionary. Since pixels inside a contextual group are often made up of the same materials, their linear combinations are constrained to use common elements from the dictionary. Equivalently, they belong to the same subspace and their sparse coefficients have a common sparsity pattern. This is realized by using a joint sparsity inducing regularizer in the dictionary learning formulation of (_ref_) . We also show how this model may be viewed from a probabilistic perspective by building upon the basic probabilistic framework introduced in _cite_ and employed in _cite_ . Solving for the dictionary and sparse coefficients leads to a two-step optimization procedure that iterates between updating the dictionary and the sparse coefficients. Each step is a convex optimization which is well known in the literature and for which efficient solutions exist. Recent work in the field of computer vision using sparse coding and dictionary learning techniques _cite_ has shown that the extracted features therein are discriminative enough to be well classified using a simple classifier such as linear SVM. Motivated by these recent findings, we employ a linear SVM to classify the sparse representation corresponding to each pixel. Extensive experiments on real hyperspectral images are provided to assess the properties of the proposed model. To summarize, we make the following main contributions: (i) We show that the proposed model is capable of incorporating both spectral and contextual characteristics of a spectral sample into the sparse set of coefficients. Extensive experiments on three hyperspectral datasets show that the inferred sparse coefficients are discriminative enough to be classified with state-of-the-art accuracy using a linear SVM. (ii) Charles et al. _cite_ show that the sparse coding model accompanied by an HSI dictionary can be used to infer HSI resolution data from simulated multispectral imagery (MSI) . We classify the sparse representations retrieved from simulated MSI-resolution data and show that our model is capable of finding representations that may effectively be used for classification of MSI-level samples. (iii) Moreover, compared to dictionary-based hyperspectral image classification methods _cite_, we use a smaller number of dictionary elements for classification and show that our method is amenable to efficient parallel processing. The remainder of this paper is organized as follows. Section II provides the necessary background on dictionary learning and introduces the structured dictionary-based model for hyperspectral data. To gain further insight, the models are also analyzed from a probabilistic point of view. The details of both basic and structured models and their application to HSI classification is discussed in Section III. To demonstrate the effectiveness of the proposed model, extensive experimental results on several hyperspectral images are reported and analyzed in Section IV. Section V concludes this paper and discusses paths for future research.