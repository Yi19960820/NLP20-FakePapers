CNNs have achieved great successes in various pattern recognition tasks, especially in large scale image classification _cite_ . However, these deep learning models always contain dozens of layers and millions or billions of parameters. For example, AlexNet _cite_ network contains about N millions of parameters, while VGG network contains about N millions of parameters. The memory and computation cost of such models are so high that it is difficult to apply them to resource-limited devices such as mobile phones. On the other hand, it turns out that the deep learning models are always over-parameterized _cite_, which means that the huge amount of connections of deep learning models could be properly pruned and compressed to reduce the storage and the computation cost. This need has driven the development of the research of deep learning model compression, also known as Deep Compression. Researchers have proposed various of methods on deep compression. We roughly group them into three categories. (N) Approximation _cite_: weight matrices and tensors in deep model could be approximated using tensor decomposition techniques. The storage cost of deep model is thus saved. (N) Quantization _cite_: by searching or constructing a finite set for candidate parameters, one could map parameters from real number to several candidates. These candidates could be properly encoded later for further compression. The extreme case for quantization is the binary networks, in which the parameters only have two possible values. (N) Pruning _cite_: methods in this category aim to reduce redundant connections, neurons or entire layers of the model. Model pruning could be conducted with different granularities, resulting in reduction in model depth, width or number of connections. Approaches directly training sparse networks are not deemed to be pruning methods because they did not really prune any networks. Compared with directly training network from scratch, deep compression could make the most of existing pre-trained models, which were carefully trained by experts and are efficient in extracting features. Compared with approximation and quantization in deep compression, model pruning would directly change the structure of the model. The pruned model will have sparse connections, fewer layers or neurons according to different pruning granularities. In practical terms, model pruning at neuron level essentially selects a sub-network in original network, keeping the regularity unchanged. As a comparison, pruning at connection level always results in sparse-connected networks, which not only need extra representation efforts but also not fit well for parallel computation _cite_ . In this paper, we will mainly focus on neuron level pruning, whose aim is to reduce the width of layers in the model. To avoid confusion, we will use the term ``neuron'' to refer to a single neuron in fully-connected layers or a filter in convolution layers. A normal neuron level pruning scheme usually contains three steps: (N) Select neurons to be removed. (N) Drop redundant neurons. (N) Fine-tune the model to recover the performance. This process is usually done in layer-wise. The main disadvantage of the layer-wise scheme is that it is time-consuming and for a given performance target, it is hard to determine how many neurons should be dropped in each layer. We try to solve these problems by pruning the network globally. In each pruning step, all neurons in the network will be taken into consideration at the same time. The main contributions of this paper are (N) we propose a gradually global pruning scheme for neuron level model pruning, which could automatically find a near-optimal structure for given performance. (N) we propose a simple method to evaluate the contribution score in different layers when selecting redundant neurons. In our scheme, the redundant neurons are selected globally to avoid the difficulty of determining the number of redundant neurons in each layer. In each pruning step, only a small percent of neurons were selected to keep the model stay close to the original local optimal point as far as possible, so that we can recover the performance fleetly through just few epochs of fine-tuning. As a result, one could find a near-optimal structure for specific task and obtain a thinner network, which is particularly suitable for resource-limited devices. Note that the size of the pruned model could be further compressed through other deep compression methods. In Section N, we will introduce some neuron contribution evaluation methods and adjust them to be compatible with global neuron selection. The gradually global pruning scheme will be given in Section N Our experiment results with different contribution score evaluation methods and contrast experiments on layer-wise pruning are shown in Section N. Finally, in Section N we make a brief summary of the work and provide some insights for future research.