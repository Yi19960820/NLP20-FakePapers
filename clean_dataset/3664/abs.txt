Fully convolutional neural networks give accurate, per-pixel prediction for input images and have applications like semantic segmentation. However, a typical FCN usually requires lots of floating point computation and large run-time memory, which effectively limits its usability. We propose a method to train Bit Fully Convolution Network (BFCN), a fully convolutional neural network that has low bit-width weights and activations. Because most of its computation-intensive convolutions are accomplished between low bit-width numbers, a BFCN can be accelerated by an efficient bit-convolution implementation. On CPU, the dot product operation between two bit vectors can be reduced to bitwise operations and popcounts, which can offer much higher throughput than N-bit multiplications and additions. To validate the effectiveness of BFCN, we conduct experiments on the PASCAL VOC N semantic segmentation task and Cityscapes. Our BFCN with N-bit weights and N-bit activations, which runs N faster on CPU or requires less than N \% resources on FPGA, can achieve comparable performance as the N-bit counterpart.