Deep neural networks suffer from serious computational and storage overhead. For example, the VGGN~ _cite_ model, which has N million parameters, requires more than N billion floating-point operations (FLOPs) to recognize a single _inline_eq_ input image. It is impossible to deploy such cumbersome models on real-time tasks or resource constrained devices like mobile phones~ _cite_ . To address this problem, many model compression or acceleration algorithms have been proposed~ _cite_ . Among these methods, pruning is an important direction. By removing unimportant neurons, the model size and computational cost can be reduced. We can roughly divide pruning approaches into N categories:, and pruning. A simple method is to discard connections according to the magnitude of their weight values~ _cite_ . However, such an unconstrained pruning strategy will lead to an irregular network structure. It may slow down the actual inference speed even though the sparsity is high~ _cite_ . Hence, structured pruning, such as filter level pruning~ _cite_, is attracting more and more attentions in recent years. In filter level pruning, the whole filter will be removed if it is less important. Hence, the original network structure will not be damaged after pruning. As illustrated in the first row of Figure~ _ref_, most current filter pruning methods adopt a three-stage pipeline. Starting from a pre-trained model, they try to find a better evaluation criteria for measuring the importance of filters, discard several weak filters, then fine-tune the pruned model to recover its accuracy. However, it is hard to find a perfect criterion that can work well on all networks and tasks. More importantly, pruning and model training are two processing steps in this pipeline. Hence, here comes an interesting question: ? In other words, ? In order to answer this question, we propose a novel end-to-end trainable method, namely AutoPruner, to explore a new way for CNN pruning. By integrating filter selection into model training, the fine-tuned network can select unimportant filters automatically. Our AutoPruner can be regarded as a new CNN layer, which takes the activation of previous layer as an input, and generate a unique code. A N value in the binary code means its corresponding filter's activation will always be N, hence can be safely eliminated. And ``unique'' means our AutoPruner is a static method, all the zero filters will be removed forever. Experimental results on fine-grained CUBN-N dataset~ _cite_ and the large-scale image recognition task ILSVRC-N dataset~ _cite_ have demonstrated the effectiveness of the proposed AutoPruner. AutoPruner outperforms previous state-of-the-art approaches with a similar or even higher compression ratio. We also compared AutoPruner with a simple but powerful method: training from scratch. The result of this experiment reveals that our AutoPruner achieves better accuracy, which is really useful to obtain a more accurate small model. The key advantages of AutoPruner and our contributions are summarized as follows.