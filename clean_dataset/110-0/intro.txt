Many applications of machine learning, and most recently computer vision, have been disrupted by the use of convolutional neural networks (CNNs) . The combination of an end-to-end learning system with minimal need for human design decisions, and the ability to efficiently train large and complex models, have allowed them to achieve state-of-the-art performance in a number of benchmarks~ _cite_ . However, these high performing CNNs come with a large computational cost due to the use of chains of several convolutional layers, often requiring implementations on GPUs~ _cite_ or highly optimized distributed CPU architectures~ _cite_ to process large datasets. The increasing use of these networks for detection in sliding window approaches~ _cite_ and the desire to apply CNNs in real-world systems means the speed of inference becomes an important factor for applications. In this paper we introduce an easy-to-implement method for significantly speeding up pre-trained CNNs requiring minimal modifications to existing frameworks. There can be a small associated loss in performance, but this is tunable to a desired accuracy level. For example, we show that a N _inline_eq_ speedup can still give state-of-the-art performance in our example application of character recognition. While a few other CNN acceleration methods exist, our {\bf key insight is to exploit the redundancy that exists between different feature channels and filters} ~ _cite_ . We contribute two approximation schemes to do so (Sect.~ _ref_) and two optimization methods for each scheme (Sect.~ _ref_) . Both schemes are orthogonal to other architecture-specific optimizations and can be easily applied to existing CPU and GPU software. Performance is evaluated empirically in Sect.~ _ref_ and results are summarized in Sect~ _ref_ . There are only a few general speedup methods for CNNs. Denton~ \etal~ _cite_ use low rank approximations and clustering of filters achieving N _inline_eq_ speedup of single convolutional layers (not of the whole network) with a N \% drop in classification accuracy. Mamalet~ \etal~ _cite_ design the network to use rank-N filters from the outset and combine them with an average pooling layer; however, the technique cannot be applied to general network designs. Vanhoucke~ \etal~ _cite_ show that N-bit quantization of the layer weights can result in a speedup with minimal loss of accuracy. Not specific to CNNs, Rigamonti~ \etal~ _cite_ show that multiple image filters can be approximated by a shared set of separable (rank-N) filters, allowing large speedups with minimal loss in accuracy. Moving to hardware-specific optimizations, ~ _cite_ and ~ _cite_ show that highly optimized CPU and GPU code can give superior computational performance in CNNs. _cite_ performs convolutions in the Fourier domain through FFTs computed efficiently over batches of images on a GPU. Other methods from~ _cite_ show that specific CPU architectures can be taken advantage of, \eg by using SSSEN and SSSEN fixed-point instructions and appropriate alignment of data in memory. Farabet~ \etal~ _cite_ show that using bespoke FPGA implementations of CNNs can greatly increase processing speed. To speed up test-time in a sliding window context for a CNN, ~ _cite_ shows that multi-scale features can be computed efficiently by simply convolving the CNN across a flattened multi-scale pyramid. Furthermore search space reduction techniques such as selective search~ _cite_ drastically cut down the number of times a full forward pass of the CNN must be computed by cheaply identifying a small number of candidate object locations in the image. Note, the methods we proposed are not specific to any processing architecture and can be combined with many of the other speedup methods given above.