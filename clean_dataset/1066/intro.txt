Raindrops attached to a glass window, windscreen or lens can hamper the visibility of a background scene and degrade an image. Principally, the degradation occurs because raindrop regions contain different imageries from those without raindrops. Unlike non-raindrop regions, raindrop regions are formed by rays of reflected light from a wider environment, due to the shape of raindrops, which is similar to that of a fish-eye lens. Moreover, in most cases, the focus of the camera is on the background scene, making the appearance of raindrops blur. In this paper, we address this visibility degradation problem. Given an image impaired by raindrops, our goal is to remove the raindrops and produce a clean background as shown in Fig.~ _ref_ . Our method is fully automatic. We consider that it will benefit image processing and computer vision applications, particularly for those suffering from raindrops, dirt, or similar artifacts. A few methods have been proposed to tackle the raindrop detection and removal problems. Methods such as _cite_ are dedicated to detecting raindrops but not removing them. Other methods are introduced to detect and remove raindrops using stereo _cite_, video _cite_, or specifically designed optical shutter _cite_, and thus are not applicable for a single input image taken by a normal camera. A method by Eigen et al. _cite_ has a similar setup to ours. It attempts to remove raindrops or dirt using a single image via deep learning method. However, it can only handle small raindrops, and produce blurry outputs _cite_ . In our experimental results (Sec. _ref_), we will find that the method fails to handle relatively large and dense raindrops. In contrast to _cite_, we intend to deal with substantial presence of raindrops, like the ones shown in Fig.~ _ref_ . Generally, the raindrop-removal problem is intractable, since first the regions which are occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. The problem gets worse when the raindrops are relatively large and distributed densely across the input image. To resolve the problem, we use a generative adversarial network, where our generated outputs will be assessed by our discriminative network to ensure that our outputs look like real images. To deal with the complexity of the problem, our generative network first attempts to produce an attention map. This attention map is the most critical part of our network, since it will guide the next process in the generative network to focus on raindrop regions. This map is produced by a recurrent network consisting of deep residual networks (ResNets) _cite_ combined with a convolutional LSTM~ _cite_ and a few standard convolutional layers. We call this attentive-recurrent network. The second part of our generative network is an autoencoder, which takes both the input image and the attention map as the input. To obtain wider contextual information, in the decoder side of the autoencoder, we apply multi-scale losses. Each of these losses compares the difference between the output of the convolutional layers and the corresponding ground truth that has been downscaled accordingly. The input of the convolutional layers is the features from a decoder layer. Besides these losses, for the final output of the autoencoder, we apply a perceptual loss to obtain a more global similarity to the ground truth. This final output is also the output of our generative network. Having obtained the generative image output, our discriminative network will check if it is real enough. Like in a few inpainting methods (e.g. _cite_), our discriminative network validates the image both globally and locally. However, unlike the case of inpainting, in our problem and particularly in the testing stage, the target raindrop regions are not given. Thus, there is no information on the local regions that the discriminative network can focus on. To address this problem, we utilize our attention map to guide the discriminative network toward local target regions. Overall, besides introducing a novel method of raindrop removal, our other main contribution is the injection of the attention map into both generative and discriminative networks, which is novel and works effectively in removing raindrops, as shown in our experiments in Sec. _ref_ . We will release our code and dataset. The rest of the paper is organized as follows. Section _ref_ discusses the related work in the fields of raindrop detection and removal, and in the fields of the CNN-based image inpainting. Section _ref_ explains the raindrop model in an image, which is the basis of our method. Section _ref_ describes our method, which is based on the generative adversarial network. Section _ref_ discusses how we obtain our synthetic and real images used for training our network. Section _ref_ shows our evaluations quantitatively and qualitatively. Finally, Section _ref_ concludes our paper.