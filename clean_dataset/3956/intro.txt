Deep Convolutional Neural Networks (CNNs) have been used to achieve state-of-the-art performances in many supervised computer vision tasks such as image classification _cite_, retrieval _cite_, detection _cite_, and captioning _cite_ . Deep CNN-based generative models, a branch of unsupervised learning techniques in machine learning, have become a hot research topic in computer vision area in recent years. A generative model trained with a given dataset can be used to generate data having similar properties as the samples in the dataset, learn the internal essence of the dataset and "store" all the information in the limited parameters that are significantly smaller than the training dataset. Variational Autoencoder (VAE) _cite_ has become a popular generative model, allowing us to formalize this problem in the framework of probabilistic graphical models with latent variables. By default, pixel-by-pixel measurement like _inline_eq_ loss, or logistic regression loss is used to measure the difference between the reconstructed and the original images. Such measurements are easily implemented and efficient for deep neural network training. However, the generated images tend to be very blurry when compared to natural images. This is because the pixel-by-pixel loss does not capture the perceptual difference and spatial correlation between two images. For example, the same image offsetted by a few pixels will have little visual perceptual difference for humans, but it could have very high pixel-by-pixel loss. This is a well known problem in the image quality measurement community _cite_ . From image quality measurement literature, it is known that loss of spatial correlation is a major factor affecting the visual quality of an image _cite_ . Recent works on texture synthesis and style transfer _cite_ have shown that the hidden representations of a deep CNN can capture a variety of spatial correlation properties of the input image. We take advantage of this property of a CNN and try to improve VAE by replacing the pixel-by-pixel loss with feature perceptual loss, which is defined as the difference between two images' hidden representations extracted from a pretrained deep CNN such as AlexNet _cite_ and VGGNet _cite_ trained on ImageNet _cite_ . The main idea is trying to improve the quality of generated images of a VAE by ensuring the consistency of the hidden representations of the input and output images, which in turn imposes spatial correlation consistency of the two images. We also show that the latent vectors of the AVE trained with our method exhibits powerful conceptual representation capability and it can be used to achieve state-of-the-art performance in facial attribute prediction.