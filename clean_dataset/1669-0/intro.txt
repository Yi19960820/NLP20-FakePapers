Deep convolutional neural networks (CNNs) have taken an important role in artificial intelligence algorithm which has been widely used in computer vision, speech recognition, data analysis and etc _cite_ . Nowadays, deep CNNs become more and more complex consisting of more layers, larger model size and denser connections. However, from the hardware acceleration point of view, deep CNNs still suffer from the obstacle of hardware deployment due to their massive cost in both computation and storage. For instance, VGG-N _cite_ from ILSVRC N requires NMB of parameters and N GFLOP per image, which is difficult to deploy in resource limited mobile systems. Research has shown that deep CNN contains significant redundancy, and the state-of-the-art accuracy can also be achieved through model compression _cite_ . Many recent works have been proposed to address such high computational complexity and storage capacity issues of existing deep CNN structure using model compression techniques. As one of the most popular model compression technique, weight quantization techniques are widely explored in many related works _cite_ which can significantly shrink the model size and reduce the computation complexity. It is worthy to note that weight binarization (-N, + N) or ternarization (-N, N, + N) is more preferred compared to other methods since floating point multiplication is not needed and most complex convolution operations are converted to bit-wise xnor and bit-count operations, which could greatly save power, area and latency. Meanwhile, it does not modify the network topology or bring extra computation to the system, which may increase hardware deployment complexity, such as pruning or hash compression. However, for compact deep CNN models that are more favored by hardware deployment, the existing aggressive weight binarization or ternarization methods still encounter large accuracy degradation for large scale benchmarks. In this work, we propose statistical weight scaling and residual expansion methods to convert the entire network weight parameters to ternary format (i.e.-N, N, + N), with objectives to greatly reduce model size, computation cost and accuracy degradation caused by model compression. Our main contributions in this work can be summarized as: