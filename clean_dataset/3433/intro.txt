The impressive gains in performance obtained using deep neural networks (DNNs) for automatic speech recognition (ASR) _cite_ have motivated the application of DNNs to other speech technologies such as speaker recognition (SR) and language recognition (LR) _cite_ . Two general methods of applying DNN's to the SR and LR tasks have been shown to be effective. The first or ``direct'' method uses a DNN trained as a classifier for the intended recognition task. In the direct method the DNN is trained to discriminate between speakers for SR _cite_ or languages for LR _cite_ . The second or ``indirect'' method uses a DNN trained for a different purpose to extract data that is then used to train a secondary classifier for the intended recognition task. Applications of the indirect method have used a DNN trained for ASR to extract frame-level features _cite_, accumulate a multinomial vector _cite_ or accumulate multi-modal statistics _cite_ that were then used to train an i-vector system _cite_ . The unified DNN approach described in this work uses two of the indirect methods described above. The first indirect method (``bottleneck'') uses frame-level features extracted from a DNN with a special bottleneck layer _cite_ and the second indirect method (\textquotedblleft DNN-posterior \textquotedblright) uses posteriors extracted from a DNN to accumulate multi-modal statistics _cite_ . The features and the statistics from both indirect methods are then used to train four different i-vector systems: one for each task (SR and LR) and each method (bottleneck and DNN-posterior) . A key point in the unified approach is that a single DNN is used for all four of these i-vector systems. Additionally, we will examine the feasibility of using a single i-vector extractor for both SR and LR.