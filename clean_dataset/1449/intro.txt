Deep learning usually benefits from large training sets~ _cite_ . However, for many applications only relatively small training data exist. In Time Series Classification (TSC), this phenomenon can be observed by analyzing the UCR archive's datasets~ _cite_, where N datasets have N or fewer training instances. These numbers are relatively small compared to the billions of labeled images in computer vision, where deep learning has seen its most successful applications~ _cite_ . Although the recently proposed deep Convolutional Neural Networks (CNNs) reached state of the art performance in TSC on the UCR archive~ _cite_, they still show low generalization capabilities on some small datasets such as the DiatomSizeReduction dataset. This is surprising since the nearest neighbor approach (N-NN) coupled with the Dynamic Time Warping (DTW) performs exceptionally well on this dataset which shows the relative easiness of this classification task. Thus, inter-time series similarities in such small datasets cannot be captured by the CNNs due to the lack of labeled instances, which pushes the network's optimization algorithm to be stuck in local minimums~ _cite_ . \figurename~ _ref_ illustrates on an example that the lack of labeled data can sometimes be compensated by the addition of synthetic data. This phenomenon, also known as in the machine learning community, can be solved using different techniques such as regularization or simply collecting more data~ _cite_ (which in some domains are hard to obtain) . Another well-known technique is data augmentation, where synthetic data are generated using a specific method. For example, images containing street numbers on houses can be slightly rotated without changing what number they actually are~ _cite_ . For deep learning models, these methods are usually proposed for image data and do not generalize well to time series~ _cite_ . This is probably due to the fact that for images, a visual comparison can confirm if the transformation (such as rotation) did not alter the image's class, while for time series data, one cannot easily confirm the effect of such ad-hoc transformations on the nature of a time series. This is the main reason why data augmentation for TSC have been limited to mainly two relatively simple techniques: slicing and manual warping, which are further discussed in Section~ _ref_ . In this paper, we propose to leverage from a DTW based data augmentation technique specifically developed for time series, in order to boost the performance of a deep Residual Network (ResNet) for TSC. Our preliminary experiments reveal that data augmentation can drastically increase the accuracy for CNNs on some datasets while having a small negative impact on other datasets. We finally propose to combine the decision of the two trained models and show how it can reduce significantly the rare negative effect of data augmentation while maintaining its high gain in accuracy on other datasets.