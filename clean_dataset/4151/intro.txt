Smart mobile devices have become an important part of our lives and people use them to take and upload millions of photos every day. Among these photos we can find large numbers of clothing and food images. Naturally, we would like to answer questions like 	``What outfit matches this pair of shoes?'' or ``What desserts would go well along this entr e?'' A straightforward approach to answer this type of questions would be to use fine grained recognition of subcategories and attributes, e.g., ``slim dark formal pants, '' with a graph that informs which subcategories match together. However, these approaches require significant domain knowledge and do not generalize well to the introduction of new subcategories. Further, they require large datasets with fine grained category labels, which are difficult to collect. Getting domain knowledge and collecting large datasets becomes especially hard in domains like clothing, where fashion collections change every season. In this paper, we propose a novel learning framework to overcome these challenges and help answer the raised questions. Our framework allows learning a feature transformation from the images of the items to a latent space, which we call style space, so that images of items from different categories that match together are close in the style space and items that don't match are far apart. Our proposed framework is capable of retrieving bundles of compatible objects. A bundle refers to a set of items from different categories, like shirts, shoes and pants. The challenge of this problem is that the bundle of objects come from visually distinct categories. For example, clothing items with completely different visual cues may be similar in our style space, \eg white shirts and black pants. However, this high contrast does not generally imply a stylistic match; for example, white socks tend to not match to black pants. Figure~ _ref_ shows pairs of items that are very close in the style space (top rows) and also pairs that are very far apart (bottom rows) . The proposed framework consists of four parts. Figure~ _ref_ provides an illustration of the basic flow. First, the input data comprises item images, category labels and links between items, describing co-occurrences. Then, to learn style across categories, we strategically sample training examples from the input data such that pairs of items are co-occurring heterogeneous dyads, i.e., the two items belong to different categories and frequently co-occur. Subsequently, we use Siamese CNNs~ _cite_ to learn a feature transformation from the image space to the latent style space. Finally, we generate structured bundles of compatible items by querying the learned latent space and retrieving the nearest neighbors from each category to the query item. To evaluate our learning framework, we use a large-scale dataset from Amazon.com, which was collected by _cite_ . As a measure of compatibility between products, we use co-purchase data from Amazon customers. In our experiments, we observe that the learned style space indeed expresses extensive semantic information about visual clothing style. Further, we find that the feature transformation learned with our framework quantitatively outperforms the vanilla ImageNet features _cite_ as well as the common approach where Siamese CNNs are trained without the proposed strategic sampling of training examples _cite_ . Our main contributions are the following: