Photo Optical Character Recognition (photo OCR), which aims to read scene text in natural images, is an essential step for a wide variety of computer vision tasks, and has enjoyed significant success in several commercial applications. These include street-sign reading for automatic navigation systems, assistive technologies for the blind (such as product-label reading), real-time text recognition and translation on mobile phones, and search/indexing the vast corpus of image and video on the web. The field of photo OCR has been primarily focused on constrained scenarios with hand-engineered image features. (Here, constrained means that there is a fixed lexicon or dictionary and words have known length during inference.) . Specifically, examples of constrained text recognition methods include region-based binarization or grouping _cite_, pictorial structures with HOG features _cite_, integer programming with SIFT descriptor _cite_, Conditional Random Fields (CRFs) with HOG features _cite_, Markov models with binary and connected component features _cite_ . Some early attempts _cite_ try to learn local mid-level representation on top of the hand-crafted features, and some methods in _cite_ incorporate deep convolutional neural networks (CNNs) _cite_ for a better image feature extraction. These methods work very well when candidate ground-truth word strings are known at testing stage, but do not generalize to words that are not present in the list of a lexicon at all. A recent advance in the state-of-the-art that moves beyond this constrained setting was presented by Jaderberg \etal in _cite_ . The authors report results in the unconstrained setting by constructing two sets of CNNs--one for modeling character sequences and one for N-gram language statistics--followed by a CRF graphical model to combine their activations. This method achieved great success and set a new standard in photo OCR field. However, despite these successes, the system in _cite_ does have some drawbacks. For instance, the use of two different CNNs incurs a relatively large memory and computation cost. Furthermore, the manually defined N-gram CNN model has a large number of output nodes (_inline_eq_ output units for N = N), which increases the training complexity--requiring an incremental training procedure and heuristic gradient rescaling based on N-gram frequencies. Inspired by _cite_, we continue to focus our efforts on the unconstrained scene text recognition task, and we develop a recursive recurrent neural networks with attention modeling (R _inline_eq_ AM) system that directly performs image to sequence (word strings) learning, delivering improvements over their work. The three main contributions of the work presented in this paper are: \noindent (N) Recursive CNNs with weight-sharing, for more effective image feature extraction than a ``vanilla'' CNN under the same parametric capacity. \noindent (N) Recurrent neural networks (RNNs) atop of extracted image features from the aforementioned recursive CNNs, to perform implicit learning of character-level language model. RNNs can automatically learn the sequential dynamics of characters that are naturally present in word strings from the training data without the need of manually defining N-grams from a dictionary. \noindent (N) A sequential attention-based modeling mechanism that performs ``soft'' deterministic image feature selection as the character sequence is being read, and that can be trained end-to-end within the standard backpropagation. We pursue extensive experimental validation on challenging benchmark datasets: Street View Text, IIITNk, ICDAR and SynthNk. We also provide a detailed ablation study by examining the effectiveness of each of the proposed components. Our proposed network architecture achieves the new state-of-the-art results and significantly outperforms the previous best reported results for unconstrained text recognition _cite_ ; \ie we observe an absolute accuracy improvement of N \% on Street View Text and N \% on ICDAR N.