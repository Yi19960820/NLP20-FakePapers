on Hyperspectral Imagery (HSI) is becoming increasingly popular in remote sensing. Notable applications include military aerial surveillance _cite_, mineral identification and material defects detection _cite_ . However, numerous difficulties impede the improvement of HSI classification performance. For instance, the high dimensionality of HSI pixels introduce the problem of the `curse of dimensionality' _cite_, and the classifier is always confronted with the overfitting problem due to the small number of labelled samples. Additionally, most HSI pixels are indiscriminative since they are undesirably highly coherent _cite_ . In the past few decades, numerous classification techniques, such as SVM _cite_, k-nearest-neighbor classifier _cite_, multimodel logistic regression _cite_ and neural network _cite_, have been proposed to alleviate some of these problems to achieve an acceptable performance for HSI classification. More recently, researchers have focused attention on describing the high dimensional data as a sparse linear combination of dictionary atoms. Sparse representation classifier (SRC) was proposed in _cite_ and has been successfully applied to a wide variety of applications, such as face recognition _cite_, visual tracking _cite_, speech recognition _cite_ and aerial image detection _cite_ . SRC has also been applied to HSI classification by Chen _cite_, where a dictionary was constructed by stacking all the labelled samples. Success of SRC requires that the high dimensional data belonging to the same class to lie in a low dimensional subspace. The outstanding classification performance is due to the robustness of sparse recovery, which is largely provided by the high redundancy and low coherency of the dictionary atoms. A low reconstruction error and a high sparsity level can be achieved if the designed dictionary satisfies the above properties. Unfortunately, in practice, the HSI dictionary usually does not have the above properties due to the small number of blue {highly correlated} labelled training samples _cite_ . Due to these undesired properties of the HSI dictionary, the sparse recovery can become unstable and unpredictable such that even pixels belonging to the same class can have totally different sparse codes. The problem induced by the high-coherency of the dictionary atoms, which can be alleviated through decreasing the variation between the sparse codes of the hyperspectral pixels that belong to the same class. In HSI, pixels that are spatially close to each other usually have similar spectral features and belong to the same class. Previous research has shown that the sparse codes of neighboring pixels can become similar by enforcing a structured sparsity constraint (prior) . The simultaneous sparse recovery is analytically guaranteed to achieve a sparser solution and a lower reconstruction error with a smaller dictionary _cite_ . A variety of structured sparsity priors are proposed in the literature _cite_ that are capable of generating different desired sparsity patterns for the sparse codes of neighboring pixels. The joint sparsity prior _cite_ assumes that the features of all the neighboring pixels lie in the same low dimensional subspace and all the corresponding sparse codes share the same set of dictionary atoms. Therefore, the sparse codes have a row sparsity pattern, where only a few rows of the sparse codes are nonzero _cite_ . The collaborative group sparsity prior _cite_ enforces the coefficients to have a group-wise sparsity pattern, where the coefficients within each active group are dense. The collaborative hierarchical sparsity prior _cite_ enforces the sparse codes to be not only group-wise sparse, but also sparse within each active group. The low rank prior _cite_ assumes that the neighboring pixels are linearly dependent. It does not necessary lead the coefficients to be sparse, which is detrimental for a good classification. However, the low rank group prior proposed in _cite_ is able to enforce both a group sparsity prior and a low rank prior on the sparse codes by forcing the same group of dictionary atoms to be active if and only if the corresponding neighboring pixels are linearly dependent. The Laplacian sparsity prior _cite_ uses a Laplacian matrix to describe the degree of similarity between the neighboring pixels. The neighboring pixels that have less spectral features in common are less encouraged to have a similar sparse codes. It has been shown that all the structured sparsity priors are capable of obtaining a smoother classification map and improving the classification performance _cite_ . In the classical SRC, the dictionary is constructed by stacking all the training samples. The sparse recovery can be computationally burdensome when the training set is large. Besides, the dictionary constructed in this manner can neither be optimal for reconstruction purposes nor for classification of signals. Previous literature have shown that a dictionary can be trained to have a better representation of the dataset. Unsupervised dictionary learning methods, such as the method of optimal direction (MOD) _cite_, K-SVD _cite_ and online dictionary learning _cite_, are able to improve the signal restoration performance of numerous applications, such as compressive sensing, signal denoising and image inpainting. However, the unsupervised dictionary learning method is not suitable for solving classification problems since a lower reconstruction error does not necessarily lead to a better classification performance. In fact, it is observed that the dictionary can have an improved classification result by sacrificing some signal reconstruction performance _cite_ . Therefore, supervised dictionary learning methods _cite_ are proposed to improve the classification result. Unlike the unsupervised dictionary learning, which only trains the dictionary by pursuing a lower signal reconstruction error, the supervised learning is able to directly improve the classification performance by optimizing both the dictionary and classifier's parameter simultaneously. The discriminative dictionary learning in _cite_ minimizes the classification error of SRC by minimizing the reconstruction error contributed by the atoms from the correct class and maximizing the error from the remaining classes. The incoherent dictionary learning in _cite_ uses SRC as the classifier and tries to eliminate the atoms shared by pixels from different classes. It increases the discriminability of the sparse codes by decreasing the coherency of the atoms from different classes. The label consistent K-SVD (LC-KSVD) _cite_ optimizes the dictionary and classifier's parameter by minimizing the summation of reconstruction and classification errors. It combines the dictionary and classifier's parameter into a single parameter space, which makes it possible for the optimization procedure to be much simpler than those used in classical SRC. However, a desired and accurate solution is not guaranteed _cite_ because the cost function can be minimized by decreasing the reconstruction error while the classification error is increased. A bilevel optimization formulation would be more appropriate _cite_, where the update of the dictionary is driven by the minimization of the classification error. The task-driven dictionary learning (TDDL) _cite_ exploits this idea with theoretical proof and demonstrates a superior performance. The supervised translation-invariant sparse coding, which uses the same scheme as TDDL, is developed independently by _cite_ . It is a more general framework that can be applied not only to classification, but also nonlinear image mapping, digital art authentiÔ¨Åcation and compressive sensing. More recently, the group sparsity prior is enforced on a single measurement and the corresponding TDDL optimization algorithm is developed in _cite_ in order to improve the performance of region tagging. In this paper, we propose a novel method that enforces the joint or Laplacian sparsity prior on the sparse recovery stage of TDDL. The existing dictionary learning methods have only been developed for reconstructing or classifying a single measurement. Therefore, it is advantageous to incorporate structured sparsity priors into the supervised dictionary learning in order to achieve a better performance. This paper makes the following contributions: The remainder of the paper is organized as follows. In Section _ref_, a brief review of TDDL is given. In Section _ref_, we propose a modified TDDL algorithm with the joint sparsity prior. TDDL with the Laplacian prior and a new algorithm for recovering the Laplacian sparse problem are stated in Section _ref_ . In Section _ref_, we show that our method is superior to other HSI classification methods through experimental results on several HSI images. Finally, we provide our conclusion in Section _ref_ .