Inspired by the success of convolutional neural networks (on either grid-like or sequential data), graph neural networks (GNNs) including graph convolutional networks (GCNs) have been developed and have demonstrated improvements over a number of machine learning/computer vision tasks such as node classification _cite_, community clustering _cite_, link prediction _cite_, _inline_eq_ point cloud segmentation~ _cite_, etc. As a special case of GCNs, spatio-temporal graph convolutional networks (STGCN), have been proposed for skeleton-based activity recognition _cite_ . STGCN leverages the spatial connection between the joints of the human body and connects the same joints across time to form a spatio-temporal graph. STGCN has shown performance improvements on Kinetics-skeleton _cite_ and NTU RGB + D _cite_ datasets via exploiting primarily actor poses. In addition to actor poses, there frequently exist abundant contextual cues that would help in recognizing an action. Leveraging these contextual cues becomes critical for improving accuracy and robustness of action recognition/segmentation, especially for actions with subtle changes in the actor's movement/pose. A graph is an intuitive data structure to jointly represent various contextual cues (e.g., scene graph, situation recognition, etc.) . Therefore, in this paper, we plan to construct a comprehensive spatio-temporal graph (STG) to jointly represent an action along with its associated actors, objects, and other contextual cues. Specifically, graph nodes represent actions, actors, objects, scenes, etc., spatial edges represent spatial (e.g., next to, on top of, etc.) and functional relationships (e.g., attribution, role, etc.) between two nodes with importance weights, and temporal edges represent temporal and causal relationships. We exploit a variety of descriptors in order to capture these rich contextual cues. In the literature, there exist various techniques such as situation recognition _cite_, object detection, scene classification, semantic segmentation, etc. The output of these networks provides embeddings that can serve as the node features of the proposed STGs. We perform action segmentation on top of this spatio-temporal graph via stacked spatio-temporal graph convolution. Our STGCN stems from the networks originally proposed for skeleton-based action recognition~ _cite_ and introduces two major advancements as our innovations. First, as mentioned before, to accommodate various contextual cues, the nodes of our STG have a wide range of characteristics, leading to the need for using descriptors with varied length. Second, our STG allows arbitrary edge connections (even fully connected graph as an extreme case) to account for the large amount of graph deformation caused by missed detections, occlusions, emerging/disappearing objects, etc. These two advancements are achieved via enhanced designs with additional layers. Another innovation we introduce is the use of stacked hourglass STGCN. Stacked hourglass networks using CNNs have been shown to improve results for a number of tasks like human pose estimation~ _cite_, facial landmark localization~ _cite_, etc. They allow repeated upsampling and downsampling of features and combine these features at different scales, leading to better performance. We, therefore, propose to apply this encoder-decoder architecture to STGCN. However, different from CNN, STGCN (or more general GCN) employs adjacency matrices to represent irregular connections among nodes. To address this fundamental difference, we adapt the hourglass networks by adding extra steps to down-sample the adjacency matrices at each encoder level to match the compressed dimensions of that level. In summary, the proposed Stacked-STGCN offers the following unique innovations: N) joint inference over a rich set of contextual cues, N) flexible graph configuration to support a wide range of descriptors with varied feature length and to account for large amounts of graph deformation over long video sequences, and N) stacked hourglass architecture specifically designed for GCNs including STGCNs. These innovations promise improved recognition/localization accuracy, robustness, and generalization performance for action segmentation over long video sequences. We demonstrate such improvements via our experiments on the CADN and Charades datasets.