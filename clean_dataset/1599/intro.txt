Convolutional neural networks (CNNs) have achieved significant progress in computer vision tasks such as image classification _cite_, object detection _cite_ and semantic segmentation _cite_ . However, state-of-the-art CNNs require computation at billions of FLOPs, which prevents them from being utilized in mobile or embedded applications. For instance, ResNet-N _cite_, which is broadly used in detection tasks _cite_, has a complexity of N GFLOPs and fails to achieve real-time detection even with a powerful GPU. In view of the huge computational cost of modern CNNs, compact neural networks _cite_ have been proposed to deploy both accurate and efficient networks on mobile or embedded devices. Compact networks can achieve relatively high accuracy under a tight computational budget. For better computational efficiency, these networks are inclined to utilize ``sparsely-connected'' convolutions such as depthwise convolution and group convolution rather than standard ``fully-connected'' convolutions. For instance, ShuffleNet _cite_ utilizes a lightweight version of the bottleneck unit _cite_ termed ShuffleNet unit. In a ShuffleNet unit, the original _inline_eq_ convolution is replaced with a _inline_eq_ depthwise convolution, while the _inline_eq_ convolutions are substituted with pointwise group convolutions. This modification significantly reduces the computational cost, but blocks the information flow between channel groups and leads to severe performance degradation. For this reason, ShuffleNet introduces the channel shuffle operation to enable inter-group information exchange. As shown in Fig.~ _ref_, a channel shuffle operation permutes the channels so each group in the second convolutional layer contains channels from every group in the first convolutional layer. Benefiting from the channel shuffle operation, ShuffleNet achieves N \% top-N accuracy on ILSVRC N dataset _cite_ with N MFLOPs, and N \% top-N accuracy with N MFLOPs, which is state-of-the-art. However, the channel shuffle operation fails to eliminate the performance degradation and ShuffleNet still suffers from the loss of inter-group information. Fig.~ _ref_ illustrates a channel shuffle operation with N channels and N channel groups. Each group in the second convolutional layer receives only N channel from every group in the first convolutional layer, whereas there are N other channels in each group being ignored. As a result, a large portion of the inter-group information cannot be leveraged. This problem is aggravated given more channel groups. Although there are more channels in total given more groups, the number of channels in each group is smaller, which increases the loss of inter-group information. Consequently, when the computational budget is relatively larger, ShuffleNet architectures with more channel groups perform worse than the narrower ones which have less groups. This indicates that it is difficult for ShuffleNet to gain performance increase by increasing the number of channels directly. To address this issue, we propose two novel operations named and to directly fuse features across all channels in a group convolution and alleviate the loss of inter-group information. For a feature map generated from a group convolution, a merging operation aggregates the features at the same spatial position across all channels and encodes the inter-group information into a narrow feature map. An evolution operation is performed afterwards to extract spatial information from the feature map. Then, based on the proposed operations, we introduce the, a powerful and efficient architectural unit specifically for compact networks. For computational efficiency, ME modules exploit depthwise convolutions and group convolutions to reduce the computational cost. For better representation, ME modules utilize merging and evolution operations to leverage the inter-group information. Finally, we present a new family of compact neural networks called which is built with ME modules. Compared with ShuffleNet _cite_, MENet alleviates the loss of inter-group information and gains substantial improvements as the group number increases. We conduct extensive experiments to evaluate the effectiveness of MENet. Firstly, we compare MENet with other state-of-the-art network structures on the ILSVRC N classification dataset _cite_ . Then, we examine the generalization ability of MENet on the PASCAL VOC N detection dataset _cite_ . Experiments show that MENet consistently outperforms other state-of-the-art compact networks under different computational budgets. For instance, under a complexity of N MFLOPs, MENet achieves improvements of N \% over ShuffleNet and N \% over MobileNet on ILSVRC N top-N accuracy, while N \% and N \% on PASCAL VOC N mAP, respectively. Our models have been made publicly available at .