The domain of face verification, or what is commonly referred to as facial recognition nowadays, has witnessed significant advances in the past few decades. Facial recognition is a classic example of a modern real-world challenge and it is directly implemented in places that are required to be impregnable to any kind of human intrusion. The concept has been tested and implemented widely and has laid the foundation for several other feature-extraction based algorithms for general object detection. One of the most commonly used algorithm for face description and disambiguation is the eigenfaces architecture, as proposed by Turk et. al. [N], which transformed the approach to tackling face verification. Seven years down the line elastic bunch graph matching [N] and virtual eigensignatures [N] became quite popular as standard face description techniques. Later in the year N, Ahonen et. al. [N] came up with a novel technique called local binary patterns, which became one of the most extensively used face descriptors of all times. Even OpenCV's ([N] and [N]) default descriptor function for faces is also based on the same algorithm, and is called LBPFaceRecognizer. Interestingly, around the same time as [N] and [N], neural networks were gaining popularity in the field of machine learning. People have implemented decision-based [N] neural nets and convolutional neural nets [N] for face recognition tasks, and with the recent invention of the concept of light convolutional neural network by Wu et. al. [N], face recognition has now seemed to have reached the zenith of facial classification rates. On the other hand, due to the complexities present in disguised face identification, it has not witnessed much research focus in the past few decades. In the modern world, tons of cases have been reported where masked intruders have been accused of several wrongdoings. Nonetheless, identification of such people becomes a problem when the autonomous facial recognition systems fail to perform. There are security measures almost everywhere on this planet but very few have the capabilities of isolating and identifying disguised faces in the wild, and the need for such a development is on the rise. But, due to inherent complexities of the problem, previous research results on the same, indicate more of chance-based performance rather than a feature-based classification. An interesting approach to solving problems related to facial expression detection or face recognition itself, is by extracting facial key-points. For instance, Berretti et. al. [N] have used SIFT descriptors of auto-detected keypoints for examining facial expressions. They trained a SVM for every single facial expression and were able to achieve a classification rate of N \% on the BU-NDFE database. Sun et. al. [N] attempted to predict five facial key-points, the left eye center, right eye center, nose tip, left mouth corner and right mouth corner and they cascaded three levels of convolutional networks to perform a coarse-to-fine tuning of the predictions. Wang et. al. [N] have used histogram stretching and principal component analysis on the stretched images to obtain the eigenfaces. In these so obtained eigenfaces they performed their mean patch searching algorithm to predict the left and right eye centres, for any input face image. Very recently, Shi et. al. [N] used principal component analysis and local binary patterns descriptor to process the data and they reviewed the outputs of various algorithms to that processed data like linear regression, tree based model, CNNs etc. for facial key-point detection. The paper provided a lot of insights to the various architectures for the same. However, to our knowledge, using key-points for identifying disguised faces was a method which was not adopted and people continued to use the existing face descriptors for the same. Yoshine et. al. [N] had suggested a method of morphometrical matching for identifying disguised faces, where they took the ND right oblique images (with three disguises only) and superimposed them on N different subjects. They reported a difference of N-N for a match (which approximately comes to an offset of N pixels) . Singh et. al. [N] suggested an algorithm called NDLPGNN which resulted in the best verification performance. But for the challenging scenarios of multi-disguise variations, the accuracy drops to N \% This research however proved that existing algorithms are not effective enough to handle a substantial degree of disguise variation. Righi et. al. [N] conducted three different visual cognition experiments to observe the effects of disguises on observer's face recognition activity using natural images in which individuals had a mixture of eyeglasses and wigs as disguises. Their paper presented some fundamental cognitive techniques that humans use to disambiguate disguised faces. But the method suggested by Dhamecha et. al. [N] proved to be state-of-the-art classification technique for disguised faces, when they reported an accuracy of N \%. Facial Key points based disguised face verification was first used in N by Singh et. al. [N] where they proposed to use an existing architecture of spatial fusion convolutional network, which was suggested earlier by Pfister et. al. [N] for human pose estimation. They were however, able to significantly improve the previous state-of-the-art performance by a margin of N \%. Inspired by the above mentioned works, we present a novel solution for disguised face recognition, the DFR model. We propose to use a CNN for key-points prediction and a support vector machine (SVM) for classification. It is a common understanding that SVMs are meant for real-time applications, and that is the precise motivation behind using it. The primary contributions of this paper could be listed down as: