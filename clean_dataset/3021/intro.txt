The chest X-ray (radiograph) is a fast and painless screening test that is commonly performed to diagnose various thoracic abnormalities, such as pneumonias, pneumothoraces and lung nodules. It is one of the most cost-effective imaging examinations and imparts minimal radiation exposure to the patient while displaying a wide range of visual diagnostic information. Identifying and distinguishing the various chest abnormalities in chest X-rays is a challenging task even to the human observer. Therefore, its interpretation has been performed mostly by board-certified radiologists or other physicians. There are huge demands on developing computer-aided detection (CADe) methods to assist radiologists and other physicians in reading and comprehending chest X-ray images. Currently, deep learning methods, especially convolutional neural networks (CNN) ~ _cite_, have become ubiquitous. They have achieved compelling performance across a number of tasks in the medical imaging domain~ _cite_ . Most of these applications typically involve only one particular type of disease or lesion, such as automated classification of pulmonary tuberculosis~ _cite_, pneumonia detection~ _cite_, and lung nodule segmentation~ _cite_ . Wang~ \etal _cite_ recently introduced a hospital-scale chest X-ray (ChestX-rayN) dataset containing N, N frontal-view X-ray images, with N thoracic disease labels text-mined from associated radiology reports using natural language processing (NLP) techniques. Furthermore, a weakly-supervised CNN based multi-label thoracic disease classification and localization framework was proposed in~ _cite_ using only image-level labels. Li~ \etal~ _cite_ presented a unified network that simultaneously improves classification and localization with the help of extra bounding boxes indicating disease location. In addition to the disease labels that represent the presence or absence of certain disease, we also want to utilize the attributes of those diseases contained in the radiology reports. Disease severity level (DSL) is one of the most critical attributes, since different severity levels are correlated with highly different visual appearances in chest X-rays (see examples in Fig. N) . Radiologists tend to state such disease severity levels (\ie, [minimal, tiny, small, mild], [middle-size, moderate], [remarkable, large, severe], etc.) when describing the findings in chest X-rays. This type of disease attribute information can be exploited to enhance and enrich the accuracy of NLP-mined disease labels, which consequently may facilitate to build more accurate and robust disease classification and localization framework than~ _cite_ . More recently, Wang~ \etal~ _cite_ proposed the TieNet (Text-Image Embedding Network), which was an end-to-end CNN-RNN architecture for learning to embed visual images and text reports for image classification and report generation. However, the disease attributes were not explicitly modeled in the TieNet framework. In this paper, we propose an attention-guided curriculum learning (AGCL) framework for the task of joint thoracic disease classification and weakly supervised localization, where only image-level disease labels and severity level information of a subset are available. Note, we do not use bounding boxes for training. In AGCL, we use the disease severity level to group the data samples as a means to build the curriculum for curriculum learning ~ _cite_ . For each disease category, we begin by learning from severe samples, progressively adding moderate and mild samples as the CNN matures and converges gradually by seeing samples from ``easy'' to ``hard''. The intuition behind curriculum learning is to mimic the common human process of gradual learning, starting from the easiest or obvious samples to harder or more ambiguous ones, which is notably the case for medical students learning to read radiographs. Furthermore, we use the CNN generated disease heatmaps (visual attention) of ``confident'' seed images to guide the CNN in an iterative training process. The initial seeds are composed of: (N) images of severe and moderate disease level, and (N) images with high classification probability scores from the current CNN classifier. A two-path multi-task learning network architecture is designed to regress the heatmaps from selected seed samples in addition to the original classification task. In each iteration, the joint learning scheme can harvest more seeds of high quality as the network fine-tuning process iterates, resulting in increased guidance and more discriminative CNN for better classification and localization. We test our proposed method on the public ChestXrayN dataset to evaluate the multi-label disease classification and localization performance. Comprehensive experimental results demonstrate the effectiveness of our framework in acquiring high-quality seeds, and the visual attention generated from seed images are evidently beneficial in guiding the learning procedure to improve both the classification and localization accuracy.