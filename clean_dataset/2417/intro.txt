Visual Question Answering (VQA) is an emerging topic that has received an increasing amount of attention in recent years _cite_ . Its attractiveness lies in the fact that it combines two fields that are typically approached individually (Computer Vision and Natural Language Processing (NLP)) . This allows researchers to look at both problems from a new perspective. Given an image and a question, the objective of VQA is to answer the question based on the information provided by the image. Understanding both the question and image, as well as modelling their interactions requires us to combine Computer Vision and NLP techniques. The problem is generally framed in terms of classification, such that the network learns to produce answers from a finite set of classes which facilitates training and evaluation. Most VQA methods follow a two-stream strategy, learning separate image and question embeddings from deep Convolutional Neural Networks (CNNs) and well known word embedding strategies respectively. Techniques to combine the two streams range from element-wise product to bilinear pooling _cite_ as well as attention based approaches _cite_ . Recent computer vision works have been exploring higher level representation of images, notably using object detectors and graph-based structures for better semantic and spatial image understanding _cite_ . Representing images as graphs allows one to explicitly model interactions, so as to seamlessly transfer information between graph items (e.g. objects in the image) through advanced graph processing techniques such as the emerging paradigm of graph CNNs _cite_ . Such graph based techniques have been the focus of recent VQA works, for abstract image understanding _cite_ or object counting _cite_, reaching state of the art performance. Nonetheless, an important drawback of the proposed techniques is the fact that the input graph structures are heavily engineered, image specific rather than question specific, and not easily transferable from abstract scenes to real images. Furthermore, very few approaches provide means to interpret the model's behaviour, an essential aspect that is often lacking in deep learning models. Contributions. In this paper, we propose a novel, interpretable, graph-based approach for visual question answering. Most recent VQA approaches focus on creating new attention architectures of increasing complexity, but fail to model the semantic connections between objects in the scene. Here, we propose to address this issue by introducing a prior in the form of a scene structure, defined as a graph which is learned from observations according to the context of question. Bounding box object detections are defined as graph nodes, while graph edges conditioned on the question are learned via an attention based module. This not only identifies the most relevant objects in the image associated to the question, but the most important interactions (e.g. relative position, similarities) without any handcrafted description of the structure of the graph. Learning a graph structure allows to learn question specific object representations that are influenced by relevant neighbours using graph convolutions. Our intuition is that learning a graph structure not only provides strong predictive power for the VQA task, but also interpretability of the model's behaviour by inspecting the most important graph nodes and edges. Experiments on the VQA vN dataset confirm our hypothesis. Combined with a relatively simple baseline, our graph learner module achieves N \% accuracy on the test set and provides interpretable results via visualisation of the learned graph representations.