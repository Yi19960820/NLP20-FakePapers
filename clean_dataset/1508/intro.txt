This paper addresses activity recognition in videos, each showing a group activity or event (e.g., spiking in volleyball) arising as a whole from a number of individual actions (e.g., jumping) and human interactions (e.g., passing the ball) . Our goal is to recognize events, interactions, and individual actions, for settings where training examples of all these classes are annotated. When ground truth annotations of interactions are not provided in training data, we only pursue recognition of events and actions. Recent deep architectures _cite_, representing a multi-level cascade of Long Short-Term Memory (LSTM) networks _cite_, have shown great promise in recognizing video events. In these approaches, the LSTMs at the bottom layer are grounded onto individual human trajectories, initially obtained from tracking. These LSTMs are aimed at extracting deep visual representations and predicting individual actions of the respective human trajectories. Outputs of the bottom LSTMs are forwarded to a higher-level LSTM for predicting events. All predictions are made in a feed-forward way using the softmax layer at each LSTM. Such a hierarchy of LSTMs is trained end-to-end using backpropagation-through-time of the cross-entropy loss. Motivated by the success of these approaches, we start off with a similar two-level hierarchy of LSTMs for recognizing individual actions, interactions, and events. We extend this hierarchy for producing more reliable and accurate predictions in the face of the uncertainty of the visual input. Ideally, the aforementioned cascade should be learned to overcome uncertainty in a given domain (e.g., occlusion, dynamic background clutter) . However, our empirical evaluation suggests that existing benchmark datasets (e.g., the Collective Activity dataset _cite_ and the Volleyball dataset _cite_) are relatively too small for a robust training of all LSTMs in the cascade. Hence, in cases that have not been seen in the training data, we observe that the feed-forwarding of predictions is typically too brittle, as errors made at the bottom level are directly propagated to the higher level. One way to address this challenge is to augment the training set. But it may not be practical as collecting and annotating group activities is usually difficult. As shown in Fig.~ _ref_, we take another two-pronged strategy toward more robust activity recognition that includes: Hence the name of our approach--Confidence-Energy Recurrent Network (CERN) . Our first contribution is aimed at mitigating the brittleness of the direct cascading of predictions in previous work. We specify an energy function for capturing dependencies between all LSTM predictions within CERN, and in this way enable recognition by energy minimization. Specifically, we extend the aforementioned two-layer hierarchy of LSTMs with an additional energy layer (EL) for estimating the energy of our predictions. The EL replaces the common softmax layer at the output of LSTMs. Importantly, this extension allows for a robust, energy-based, and end-to-end training of the EL layer on top of all LSTMs in CERN. Our second contribution is aimed at improving the numerical stability of CERN's predictions under perturbations in the input, and resolving ambiguous cases with multiple similar-valued local minima. Instead of directly minimizing the energy, we consider more reliable solutions, as illustrated in Fig.~ _ref_ . The reliability or confidence of solutions is formalized using the classical tool of a statistical hypothesis test _cite_--namely, p-values of the corresponding LSTM's hypotheses (i.e., class predictions) . Thus, we seek more confident solutions by regularizing energy minimization with constraints on the p-values. This effectively amounts to a joint maximization of confidence and minimization of energy of CERN outputs. Therefore, we specify the EL to estimate the minimum energy with certain confidence constraints, rather than just the energy. We also use the energy regularized by p-values for robust deep learning. Specifically, we formulate an energy-based loss which not only accounts for the energy but also the p-values of CERN predictions on the training data. Our evaluation on the Collective Activity _cite_ and Volleyball _cite_ datasets demonstrates: (i) advantages of the above contributions compared with the common softmax and energy-based formulations and (ii) a superior performance relative to the state-of-the-art methods. In the following, Sec.~ _ref_ reviews prior work, Sec.~ _ref_ specifies CERN, Sec.~ _ref_ ~and~ _ref_ formulate the energy and confidence, Sec.~ _ref_ describes the energy layer, Sec.~ _ref_ specifies our learning, and finally Sec.~ _ref_ presents our results.