The methodology of constructing feature representations has been recently advanced from a well-known hand-crafted manner to a learning-based one. Conventional hand-crafted features are typically designed by leveraging the domain knowledge of human experts _cite_ . The learning based approaches often generate discriminative image representations using large-scale labeled datasets, such as ImageNet _cite_, Places _cite_, MS COCO _cite_, and CelebA _cite_, with deep and complex convolutional neural networks (ConvNets) . Thanks to the generic transferability, these learned deep representations from the ConvNets can also be utilized for other unseen tasks by means of transfer learning _cite_: one way is to directly use the pre-trained ConvNet to map images to the learned feature space with or without selecting more discriminative features _cite_ ; a more common practice, however, is to transfer a large off-the-shelf (OTS) model to a target task by mildly tuning the ConvNet parameters to make it more specific to a new target task _cite_ . Despite the successful applications of ConvNets along the scenario of transfer learning, a basic question with respect to the model structure is still unaddressed: a question about whether the off-the-shelf network is sufficiently complex or more than ample to model the target task. That is, most of the existing approaches to ConvNet transfer learning take a predefined network architecture as given and optimize the parameters therein, without seeking for a better suited network structure even though the given task can be much simpler and therefore requiring a less complex model. Indeed, it has been shown that a model built for a source task could be less activated on target tasks _cite_ . This suggests that ConvNets can be made more compact to derive discriminative feature representations more efficiently in transfer learning scenarios. Moreover, learning representations by reusing a model designed for a very large-scale problem (such as ImageNet classification _cite_) for smaller target tasks would risk models to get overfitted to the target domain. Tailoring a network suitable for a target task helps to enhance regularization when learning the target task representations. As a natural progression for the transferred model to be more effectively adapted to target task, which we call {\it target-aware transfer learning}, an intuitive next step is to automatically remove possible redundancy from the transferred off-the-shelf model. Besides an obvious benefit in improving computational efficiency, one could expect increased accuracy on a target task if it were possible to find a less complex model that is better structured to target data. Thus, in this paper, we propose an automatic network adaptation method for target-aware transfer learning. To this end, we exploit cumulative sum of activation statistics for each ConvNet layer to determine the priority of filters to prune across the network while fine-tuning the parameters in an iterative pipeline. Although there are approaches to simplifying ConvNets in general _cite_ or learning a sparse network structure _cite_, to the best of our knowledge this is the first work that addresses an automatic network adaptation for transfer learning. The work presented in this paper is close to _cite_ in that they deploy a framework consisting of iterative filter pruning and fine-tuning, and _cite_ also provides a comparative study on criteria for network pruning. As a development orthogonal to those criteria, our method has a functionality to modify and adapt the network structure according to the target task while also avoiding a greedy search to select parts of the network to be removed. The contributions of this paper are summarized as follows: In Section _ref_, we introduce recent related work. The Network Adaptation method is described in detail in Section _ref_ . Comparative experimental results are demonstrated in Section _ref_ and Section _ref_ with ablation studies of different pruning strategies in Section _ref_ . Section _ref_ concludes the paper.