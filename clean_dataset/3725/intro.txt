Person re-identification (RE-ID) is a challenging problem focusing on pedestrian matching and ranking across non-overlapping camera views. It remains an open problem although it has received considerable exploration recently, in consideration of its potential significance in security applications, especially in the case of video surveillance. It has not been solved yet principally because of the dramatic intra-class variation and the high inter-class similarity. Existing attempts mainly focus on learning to extract robust and discriminative representations _cite_, and learning matching functions or metrics _cite_ in a supervised manner. Recently, deep learning has been adopted to RE-ID community _cite_ and has gained promising results. However, supervised strategies are intrinsically limited due to the requirement of manually labeled cross-view training data, which is very expensive _cite_ . In the context of RE-ID, the limitation is even pronounced because manually labeling may not be reliable with a huge number of images to be checked across multiple camera views, and more importantly the astronomical cost of time and money is prohibitive to label the overwhelming amount of data across disjoint camera views. Therefore, in reality supervised methods would be restricted when applied to a new scenario with a huge number of unlabeled data. but they are still not very satisfactory. One of the main reasons is that without the help of labeled data, it is rather difficult to model the dramatic variances across camera views, such as the variances of illumination and occlusion conditions. Such variances lead to view-specific interference/bias which can be very disturbing in finding what is more distinguishable in matching people across views (see Figure _ref_) . In particular, existing unsupervised models treat the samples from different views in the same manner, and thus the effects of view-specific bias could be overlooked. In order to better address the problems in unsupervised RE-ID scenarios, we propose a novel unsupervised RE-ID model named . The ideas behind are on the two considerations. conditions can vary among camera views, we assume that there should be some shared space where the data representations are less affected by view-specific bias. By projecting original data into the shared space, the distance between any pair of samples _inline_eq_ and _inline_eq_ is computed as: where _inline_eq_ is the transformation matrix and _inline_eq_ . where _inline_eq_ and _inline_eq_ are indices of camera views. An asymmetric metric is more acceptable for unsupervised RE-ID scenarios as it explicitly models the variances among views by treating each view differently. By such an explicit means, we are able to better alleviate the disturbances of view-specific bias. The other consideration is that since we are not clear about how to separate similar persons in lack of labeled data, it is reasonable to pay more attention to better separating dissimilar ones. Such consideration us to structure our data by clustering. Therefore, we develop that clusters cross-view person images. By clustering together with asymmetric modelling, the data can be better characterized in the shared space, contributing to better matching performance (see Figure _ref_) . In summary, the proposed CAMEL aims to learn view-specific projection for each camera view by jointly learning the asymmetric metric and seeking cluster separations. In this way, the data from different views is projected into a shared space where view-specific bias is aligned to an extent, and thus better performance of cross-view matching can be achieved.