Convolutional neural networks (CNNs) _cite_ achieve state-of-the-art performance in many areas of computer vision including image classification, object detection, and segmentation _cite_ . However, the training time of large networks is a major bottleneck when evaluating different architectures and new types of layers. In this work, we study the QuickProp algorithm _cite_ for decreasing training time in the domain of semantic segmentation and potentially reaching a better optimum during training. QuickProp _cite_ is a second-order optimization algorithm that uses a simple approximation of the Hessian's diagonal to accelerate optimization and therefore belongs to the class of Quasi-Newton algorithms. So far, it has been evaluated and studied only for standard neural network training~ _cite_ . However, current neural network architectures, like CNNs, are characterized by a significantly larger number of parameters. Furthermore, the increased number of layers leads to a higher numerical error when computing the gradients with back-propagation~ _cite_ . For an evaluation of its performance, we test the algorithm in synthetic and real-world experiments and compare its behavior to the traditional and widely used optimization approach gradient descent (GD) . In this work, we focus on the task of semantic segmentation. Each pixel of an image is classified into known classes and thereby the image is segmented into meaningful regions. In contrast to unsupervised segmentation, this not only returns possible object boundaries as regions but also semantic labels for each of those regions. For this task, we make use of the convolutional network architectures as proposed in~ _cite_ and use them throughout all our experiments. The remainder of the paper is structured as follows. In Section~ _ref_, we give a brief review of related work and optimization techniques for the task of network training. In Section~ _ref_, we elaborate on the QuickProp algorithm which was originally introduced in~ _cite_ . An evaluation using synthetic and real-world experiments is given in Section~ _ref_ . A discussion and summary in Section~ _ref_ concludes the paper.