Suppose you are asked to compare and contrast two different shoes, shown in Figure _ref_ . You might say that the left shoe is more formal than the right shoe, then perhaps state that the left shoe is more shiny and less comfortable than the right shoe. As soon as you are given the images, these differences stick out and are most noticeable. However, consider that the two shoes have a huge number of differences. For instance, the left shoe is more rugged than the right shoe, and also darker . Although these other differences are certainly present and true, they are much less noticeable to us, and we would likely mention them later, or not at all. In general, when we perform any comparison task on a pair of images, certain differences stick out as being most noticeable out of the space of all discernible differences. These most noticeable differences, or prominent differences, stand out and would be described first, while most other differences are not as noticeable and would typically not be mentioned in a description. In this work, we introduce and learn prominent differences in images, expressing them through relative attributes. When people compare images, they can describe differences in their attributes, human-nameable visual properties of images~ _cite_ used to describe anything from materials (smooth, furry) and parts (has leg, has glasses) to styles (sporty, formal) and expressions (smiling, sad) . Relative attributes, or attributes that indicate an image's attribute strength relative to other images, provide an intuitive and meaningful representation for visual comparison, and have been widely used for vision tasks~ _cite_ . Relative attributes express comparisons of attribute strength (\eg, image X is smiling more than Y, but smiling less than Z), and are the natural vocabulary of our proposed prominent differences. Prominent differences have many practical applications in vision. Humans interact with vision systems as both users and supervisors, and naturally communicate prominence. For instance, in an interactive search task, where humans provide comparative feedback (\eg, I would like to see images like this shoe, but more formal ~ _cite_), the attributes that people elect to comment on are prominent differences. In zero-shot learning with relative attributes~ _cite_, where humans describe unseen visual categories to a machine by comparing with seen categories, prominence could enhance learning by better understanding these comparisons. Prominent differences are the properties humans provide first when making comparisons, and thus directly influence how humans interpret comparison descriptions. Prominence could also be used to highlight the differences that stick out to people between fashion styles, extending recent work that uses attributes for fashion description~ _cite_ . Modeling prominent differences is challenging due to several key reasons. First, there is a large variety of reasons why an attribute stands out as prominent for any image pair. For instance, large differences in attribute strength can play a role (Figure _ref_), as well as absence of other significant differences (Figure _ref_) and unusual occurrences (Figure _ref_) . In general, complex interactions between the attributes of two images cause certain differences to stand out. Second, humans use a large and diverse vocabulary when expressing prominence, which a model should support. Finally, prominent differences are observed between individual images. As we will show, simply predicting prominence based on the prior frequency of usage for the attribute words is not sufficient. Thus, prominent differences must be modeled at the image instance level. In this work, we propose to model prominent differences. We collect a novel dataset of prominent difference annotations, propose a model based on relative attribute features learned with deep spatial transformer networks or large margin rankers, and evaluate on two unique and challenging domains: the UT-ZapNK shoes dataset~ _cite_ and the LFWN faces dataset~ _cite_ . We show that our model significantly outperforms an array of baselines for predicting prominent differences, including an adaptation of the state-of-the-art binary attribute dominance approach~ _cite_ . Finally, we demonstrate how our prominence model can be used to enhance two vision applications: interactive image search and description generation.