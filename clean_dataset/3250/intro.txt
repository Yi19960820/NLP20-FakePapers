Image annotation (also known as caption retrieval) is the task of automatically associating an input image with a describing text. Image annotation methods are an emerging technology, enabling semantic image indexing and search applications. The complementary task of associating an input text with a fitting image (known as image retrieval or image search) is also of relevance for the same sort of applications. State-of-the-art image annotation methods are currently based on deep neural net representations, where an image embedding (\eg obtained from a convolutional neural network or CNN) and a text embedding (\eg obtained from a recurrent neural network or RNN) are combined into a unique multimodal embedding space. While several techniques for merging both spaces have been proposed _cite_, little effort has been made in finding the most appropriate image embeddings to be used in that process. In fact, most approaches simply use a one-layer CNN embedding _cite_ . In this paper we explore the impact of using a Full-Network embedding (FNE) _cite_ to generate the required image embedding, replacing the one-layer embedding. We do so by integrating the FNE into the multimodal embedding pipeline defined by, which is based in the use of a Gated Recurrent Units neural network (GRU) _cite_ for text encoding and CNN for image encoding. Unlike one-layer embeddings, the FNE represents features of varying specificity in the context of the visual dataset, while also discretizes the features to regularize the space and alleviate the curse of dimensionality. These particularities result in a richer visual embedding space, which may be more reliably mapped to a common visual-textual embedding space. The generic pipeline defined by has been recently outperformed in image annotation and image search tasks by methods specifically targeting one of those tasks _cite_ . We choose to test our contribution on this pipeline for its overall competitive performance, expecting that any conclusion may generalize when applied to other solutions and tasks (\eg caption generation) . This assumption would be dimmer if a more problem-specific methodology was chosen instead. Our main goal is to establish the competitiveness of the FNE as an image representation to be used in caption related tasks. We test the suitability of this approach by evaluating its performance on both image annotation and image retrieval using three publicly available datasets: FlickrNk _cite_, FlickrNk _cite_ and MSCOCO _cite_ . Results obtained by the pipeline including the FNE are compared with the original pipeline of using a one-layer embedding, and also with the methods currently obtaining state-of-the-art results on the three datasets.