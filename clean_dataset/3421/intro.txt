Anticipating a possible future based on experience is an important part of the human decision-making process. Simulating this process in machines by teaching them to anticipate future events based on internal representations of the environment could be of relevance for many tasks. Automatically predicting the future frames of a video is one approach to tackle this problem. Video predictions can be of use for planning in robotics, as well as in autonomous driving, especially in reinforcement learning settings. They can lead to better decisions, or at least to faster executions, when used as an additional input to the agent. As shown by, other tasks, such as object recognition, detection, and tracking, can benefit from the representations that are implicitly learned by such a model. There are several different approaches that address the pixel-level prediction of video frames. Early, often purely deterministic, approaches tended to insufficiently model the uncertainty of the output, which led to blurry predictions. Using is one way to appropriately model the uncertainty of the multi-modal output. We build on this idea of training a generative model in an adversarial setting. GANs learn to model the underlying data distribution implicitly by utilizing a critic, the network, during training time. While being trained, the critic constantly provides feedback to the, whether the generated samples appear real or not. This forces the generator to output samples of a similar data distribution as those of the real samples. Although GAN based video prediction methods usually manage to better preserve the sharpness in the predicted frames, there are two major drawbacks. First, GANs are hard to train because the training process is highly unstable. Secondly, GANs often suffer from mode collapse effects, where the generator learns to fool the discriminator by producing samples of a limited set of modes. This means, the resulting generative model will not be able to fully capture the underlying data distribution. In our model, we utilize the training strategy of that effectively managed to overcome these problems. The PGGAN of was originally designed for generating high-resolution images from a set of random latent variables. On this task, it achieved high-quality results. The basic principle is to gradually increase the image resolution as the training proceeds by progressively adding layers in both networks. For further stabilization of the training, the authors introduced normalization techniques to constrain the signal magnitudes and the competition in both the discriminator and the generator. We extend this architecture for the complex task of video prediction to benefit from the positive effects on the GAN training. The primary contribution of this paper is to provide a simple GAN-based model for video prediction that is directly applicable to different datasets using, in general, the same setting. Our FutureGAN predicts multiple future frames at once when conditioned on a set of past frames. Contrary to other approaches, both networks solely use the raw pixel value information as an input, without relying on additional conditions. To evaluate the FutureGAN framework, we conducted experiments on three datasets of increasing complexity, i.e. the dataset, the dataset, and the dataset . Figure _ref_ provides example predictions. We show that our model is able to generate plausible futures for all three datasets, while avoiding the problems that typically arise when training GANs. The predicted frames indicate that the model effectively learned representations of spatial and temporal transformations.