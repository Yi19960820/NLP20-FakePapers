Many types of compelling image stylization effects have been demonstrated over the years, including color transfer, texture transfer, and style transfer. Their appeal is especially strong in the context of social media, where photo sharing and entertainment are important elements. A number of popular apps such as Prisma and Facetune have successfully capitalized on this appeal. The applications of color transfer, texture transfer, and style transfer share a common theme, which we characterize as {\em visual attribute transfer} . In other words, visual attribute transfer refers to copying visual attributes of one image such as color, texture, and style, to another image. In this paper, we describe a new technique for visual attribute transfer for a pair of images that may be very different in appearance but semantically similar. In the context of images, by "semantic", we refer to high-level visual content involving identifiable objects. We deem two different images to be semantically similar if both are of the same type of scene containing objects with the same names or classes, e.g., nature scenes featuring pandas and foliage, headshots, or indoor scenes featuring dining rooms with tables, chairs, walls, and ceiling. Figure~ _ref_ shows two semantically similar pairs. The images in each pair of inputs (_inline_eq_ and _inline_eq_) look dramatically different, but have objects with similar identities. Our technique establishes semantically-meaningful dense correspondences between the input images, which allow effective visual attribute transfer. Low-level matching methods, such as optical flow~ _cite_ and PatchMatch~ _cite_, are designed to match local intensities. Hence, they fail to match under large visual variations. While other methods such as SIFT flow~ _cite_ or deep match~ _cite_ are more reliable in matching sparse features, they are also not able to handle extreme visual variations, such as matching across an artistic painting and a real photograph. This is because these methods are still fundamentally based on low-level features. Methods have been proposed to match different particular domains, such as drawings/paintings to photographs~ _cite_, sketches to photos~ _cite_, and photos under different illuminants~ _cite_ . However, these methods typically are very domain-specific and do not easily generalize. Schechtman and Irani~ propose a more general solution using local self-similarity descriptors that are invariant across visual domains, and Shrivastava et. al.~ consider relative weighting between the descriptors for cross-domain image matching. Unfortunately, these methods do not produce dense correspondences between images of different domains. We handle the dense correspondence problem using ideas related to ~ _cite_, which involve dense mappings between images from different domains. An image analogy is codified as _inline_eq_, where _inline_eq_ relates to _inline_eq_ in the same way as _inline_eq_ relates to _inline_eq_, and additionally, _inline_eq_ and _inline_eq_ (also _inline_eq_ and _inline_eq_) are in pixel-wise correspondences. In forming an image analogy, typically _inline_eq_, _inline_eq_, and either _inline_eq_ or _inline_eq_ are given, and the goal is to solve for the sole remaining image. In contrast, for our scenario only a source image _inline_eq_ and an example image _inline_eq_ are given, and both _inline_eq_ and _inline_eq_ represent latent images to be estimated, imposing a bidirectional constraint to better match _inline_eq_ to _inline_eq_ . Solving the visual attribute transfer problem is equivalent to finding both unknown images _inline_eq_ and _inline_eq_ . Instead of applying image analogy to image pixels directly, we use a Deep Convolutional Neutral Network (CNN) ~ _cite_ to construct a feature space in which to form image analogies. It has been shown that such deep features are better representations for semantic structures~ _cite_ . We call our new technique . Our approach leverages pre-trained CNNs for object recognition (e.g., VGG-N~ _cite_) to construct such a feature space. A nice property of CNN representations is that they gradually encode image information from low-level details to high-level semantic content. This provides a good decomposition of semantic structure and visual attributes for transfer. Besides, the spatial correspondence between intermediate feature layers in CNN architectures is approximately maintained. Both properties facilitate a coarse-to-fine strategy for computing the nearest-neighbor field (NNF), a core component used in reconstructing images. To speed up the required nearest-neighbor search, we adapt PatchMatch~ _cite_ to the CNN feature domain. Our method uses the bidirectional constraint in the patch similarity metric, which have previously been shown to work well on re-targeting problems~ _cite_ . In the present context, the use of the bidirectional constraint introduces a useful symmetry and helps to mitigate the risk of mismatches. Our major technical contributions are: We show how our deep image analogy technique can be effectively applied to a variety of visual attribute transfer cases, namely style/texture transfer, color/style swap, sketch/painting to photo, and time lapse. Our technique also has the effect of generating {\em two} results instead of the typical one generated from a one-way transfer. Such results can be seen in~ . Our deep image analogy is designed to work on images with similar content composition. It is not effective for images which are semantically unrelated (e.g., a headshot and a countryside photo), and is not designed to handle large geometric variations (including scales and rotations) .