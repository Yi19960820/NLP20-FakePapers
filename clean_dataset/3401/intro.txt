ConvNet is a type of deep learning model which have proved to be an effective tool for learning generalized visual representations of various entities appearing in the image _cite_ . The basic approach towards learning features for a specific task is to start of by a base image classification model _cite_ where the features are learned with a fully supervised approach using large scale datasets like ImageNet _cite_, and then perform transfer learning on this base model for the new desired task with a new dataset. This approach of ``building up on base features" is considered to be a standard approach for learning good features, specific to a particular task. However, there are a few drawbacks in this approach: N) It requires huge amount of labelled training data in order to learn the base classification model, which eventually takes a lot of human efforts. N) This kind of feature learning strategy lacks some sort resemblance with how the human/animal learns object features about various objects it sees around the world. Human visual system makes use of the massive amount of unlabelled data that it observes in the world in order to learn about the objects it observes. Thus, it becomes extremely beneficial if we can learn good features by leveraging the massive amount of unlabelled data present on the internet. People have tried to solve the problem of unsupervised feature learning by targeting the problem from several angles, _cite_ and _cite_ have come up with an approach where they try to learn a feature embedding where the distance between two object patches would be less, whereas the distance between the object and non object will be more. However, both the approaches have their own drawbacks, N) _cite_ use tracking in order to sample dynamic object in a scene and consider the patch sampled from the first frame and the last frame as the same object and use that pair of samples during the training of the Siamese network. However, such an approach is biased towards the dynamic object in the scene and neglects the static objects, because of which it creates some noisy samples during training (as there is a high probability that the non-object patches is sampled from the regions containing the static objects) because of which quality of training sample patches are affected. Moreover, applying tracking to process massive amount of unlabelled video is computationally expensive, and error in tracking may influence the quality of the patches used for feature learning. N) _cite_ made use of the temporal coherence of the same object in a video as a free supervision in order to learn features. The patches extracted from adjacent frames in the video represents same object in the embedded space. The overall approach was same as _cite_ but the only difference was that, instead of extracting patches with tracking, they made use of object proposals with high IoU in adjacent frames as the proposals representing the same object. However, considering the objects only in the adjacent frames, the algorithm fails to consider the pose variations that the object may undergo throughout the video because of which the training process will not get variety of training samples. _cite_ came up with a segmentation based approach where they perform an unsupervised motion segmentation on all the videos and then use that segments for learning features. However, such an approach is biased by the accuracy of the motion segmentation algorithm and thus directly affect the feature learning process. By considering these limitations, we propose a new approach towards feature learning from unlabelled videos. We target the feature learning process through a foreground/background classification approach. Training data for the classification task comes from a foreground/background patch extraction technique proposed by us. Our approach for unsupervised feature learning tries to exploit the fundamental feature difference between the foreground and the background region in an image through a machine learning model. Our approach for foreground and background patch extraction is based on the fact that in a video the foreground region is generally the salient and dynamic object in the scene, whereas the background region is the least salient and the region which is nearly static.