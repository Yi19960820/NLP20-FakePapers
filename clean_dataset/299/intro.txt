Recognizing and segmenting arbitrary objects, posed as a primary research direction in computer vision, has achieved great success driven by the advance of convolutional neural networks (CNN) . However, current segmentation models using generic deeper and wider network layers~ _cite_ still show unsatisfactory results of recognizing objects in a large concept vocabulary with limited segmentation annotations. The reason is that they ignore the intrinsic taxonomy and semantic hierarchy of all concepts. For example,, and categories share one super-class that depicts their common visual characteristics, which makes them be easily distinguished from / . In addition, due to diverse level of expertise and application purposes, the target concept set of semantic segmentation can be inherently open-ended and highly structured for each specific task/dataset. However, some few techniques also explored the semantic hierarchy for visual recognition by resorting to complex graphical inference~ _cite_, hierarchical loss~ _cite_ or word embedding priors~ _cite_ on final prediction scores. Their loss constraints can only indirectly guide visual features to be hierarchy-aware, which is hard to be guaranteed and often leads to inferior results compared to generic CNN models. Furthermore, this lack of modeling semantic hierarchy also prohibits the research towards a universal segmentation model that can address the segmentation of all concepts at once. Existing works~ _cite_ often strive to train a task-specific model due to the label discrepancy across dataset with limited annotations. That way largely limits the model generation capability and deviates from human perception that can recognize and associate all concepts by considering the concept hierarchy. If one wants to improve one task by fully utilizing other annotations with different label set, prior works must remove the classification layer and only share intermediate representations. Our target of learning a universal segmentation model also has some connections to very recent researches in combining different visual tasks~ _cite_ or multi-modal tasks~ _cite_ in one model, which often use several fixed network branches with specialized losses to integrate all tasks. In this work, we aim at explicitly integrating a semantic concept hierarchy into the dynamic network optimization, called as Dynamic-Structured Semantic Propagation Network (DSSPN) . In the spirit of curriculum learning~ _cite_ that gradually increases the target difficulty levels and exploits previously learned knowledge for learning new fine-grained concepts, DSSPN first progressively builds a semantic neuron graph following the semantic concept hierarchy in which each neuron is responsible for segmenting out regions of one concept in the word hierarchy. The learned features of each neuron are further propagated into its child neurons for evolving features in order to recognize more fine-grained concepts. For each image or dataset, DSSPN performs the dynamic-structured semantic propagation over an activated semantic neuron sub-graph where only the present concepts and their ancestors are selected. Benefiting from the merits of semantically ordered network modules and the dynamic optimization strategy, our DSSPN would enable the learned visual representation to naturally embed rich semantic correlations between diverse concepts. Such explicit neuron definition mechanism makes the proposed DSSPN be a semantically explainable dynamic network architecture with good memory and computation efficiency. Rather than only taking into account features of the parent neuron for each neuron, we introduce a new dense semantic-enhanced neural block which densely integrates the features of all ancestor neurons to evolve feature representation of each neuron, inspired by DenseNets~ _cite_ . By broadcasting the learned knowledge of all ancestor neurons into each neuron, our DSSPN can fully exploit the semantic correlation and inheritance into the feature learning in a more efficient way. As explained in very recent information bottleneck theory~ _cite_, the deep networks often tend to squeeze the information through a bottleneck and retain only the features most relevant to targets. Such dense semantic connection thus alleviates the information loss along deeper layers by explicitly enforcing ancestor neurons to preserve discriminate features for recognizing more fine-grained concepts. Note that our DSSPN activates dynamic computation graphs for each sample during training. For scalability, a dynamic batching optimization scheme is proposed to enable optimize multiple computation graphs within one batch by configuring a dynamic number of samples for learning distinct neural modules at each step. A memory efficient implementation of our DSSPN is also described. Extensive experiments on four popular semantic segmentation datasets (i.e. Coco-Stuff~ _cite_, ADENk~ _cite_, Cityscape~ _cite_ and Mapillary~ _cite_) demonstrate the effectiveness of incorporating our DSSPN into the state-of-the-art basic segmentation networks. We thus demonstrate that our dynamic-structure propagation mechanism is an effective way to implement a semantic explaining way that is needed for segmenting massive intrinsically structured concepts. Moreover, we show that learning a unified DSSPN model over diverse models can also bring the performance over the commonly used fine-tuned scheme for utilizing annotations in multiple domains.