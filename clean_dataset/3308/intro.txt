The capability to perceive functional aspects of an environment is highly desired because it forms the essence of devices intended for collaborative use. These aspects can be categorized into abstract descriptive properties called attributes ~ _cite_ or physically grounded regions called affordances . Affordances are important as they form the key representation to describe potential interactions. For instance, autonomous navigation depends heavily on understanding outdoor semantics to decide if the lane is changable or if the way ahead is drivable ~ _cite_ . Similarly, assistive robots must have the capability of anticipating indoor semantics like which regions of the kitchen are openable or placeable ~ _cite_ . Further, because forms of interaction are fixed for virtually any object class, it is desirable to have recognition systems that are capable of localizing functionally meaningful regions or affordances alongside contemporary object recognition systems. In most previous works, affordance labeling has been addressed as a stand-alone task. For instance, the methods~ _cite_ learn pixel-wise affordance labels using supervised learning techniques. Creating pixelwise annotated datasets, however, is heavily labor intensive. Therefore, in order to simplify the annotation process, current affordance datasets have been captured in highly controlled environments like a turntable setting~ _cite_ . This, however, does not allow to study contextual information, specially those of humans, which affordances are intrinsically related to. One of the contributions of this work is to propose a pixel annotated affordance dataset within the purview of human interactions, thus creating possibilities to tap rich contextual information thereby fostering work towards reduced supervision levels. In addition, we show that state-of-the-art end-to-end learning techniques in semantic segmentation significantly outperform state-of-the-art supervised learning methods for affordances. As a second contribution, we propose a weakly supervised learning approach for affordance segmentation. Our approach is based on the expectation-maximization (EM) framework as proposed in~ _cite_ . The method introduces a constant bias term to learn a deep convolutional neural network (DCNN) for semantic segmentation only from image level labels. In this work, we consider keypoints or click-points as weak annotations, which are easy to obtain and have been used in~ _cite_ for annotating a large material database. In order to learn from keypoints, we extend the framework to handle spatial dependencies. The approach can also be used to learn from mixed sets of training images where one set is annotated by keypoints and the other is annotated by image labels. An overview of the proposed EM approach is illustrated in Figure~ _ref_ . As our third contribution, we show that automatically extracted human pose information can be effectively utilized as context for affordances. We use it to transfer keypoint annotations to images without keypoint annotations, which are then used to initialize the proposed EM approach.