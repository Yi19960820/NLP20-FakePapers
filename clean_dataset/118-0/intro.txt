Convolutional neural networks heavily rely on equivariance of the convolution operator under translation. Weights are shared between different spatial positions, which reduces the number of parameters and pairs well with the often occurring underlying translational transformations in image data. It naturally follows that a large amount of research is done to exploit other underlying transformations and symmetries and provide deep neural network models with equivariance or invariance under those transformations (\cf~Figure~ _ref_) . Further, equivariance and invariance are useful properties when aiming to produce data representations that disentangle factors of variation: when transforming a given input example by varying one factor, we usually aim for equivariance in one representation entry and invariance in the others. One recent line of methods that aim to provide a relaxed version of such a setting are . Our work focuses on obtaining a formalized version of capsule networks that guarantees those properties as well as bringing them together with by, which also provide provable equivariance properties under transformations within a group. In the following, we will shortly introduce capsule networks, as proposed by and, before we outline our contribution in detail. Capsule networks~ and the recently proposed routing by agreement algorithm~ represent a different paradigm for deep neural networks for vision tasks. They aim to hard-wire the ability to disentangle the pose of an object from the evidence of its existence, also called in the context of vision tasks. This is done by encoding the output of one layer as a tuple of a pose vector and an activation. Further, they are inspired by human vision and detect linear, hierarchical relationships occurring in the data. Recent advances describe the dynamic routing by agreement method that iteratively computes how to route data from one layer to the next. One capsule layer receives _inline_eq_ pose matrices _inline_eq_, which are then transformed by a trainable linear transformation _inline_eq_ to cast _inline_eq_ votes for the pose of the _inline_eq_ th output capsule: The votes are used to compute a proposal for an output pose by a variant of weighted averaging. The weights are then iteratively refined using distances between votes and the proposal. Last, an agreement value is computed as output activation, which encodes how strong the votes agree on the output pose. The capsule layer outputs a set of tuples _inline_eq_, each containing the pose matrix and the agreement (as activation) of one output capsule. General capsule networks do not come with guaranteed equivariances or invariances which are essential to guarantee disentangled representations and viewpoint invariance. We identified two issues that prevent exact equivariance in current capsule architectures: First, the averaging of votes takes place in a vector space, while the underlying space of poses is a manifold. The vote averaging of vector space representations does not produce equivariant mean estimates on the manifold. Second, capsule layers use trainable transformation kernels defined over a local receptive field in the spatial vector field domain, where the receptive field coordinates are agnostic to the pose. They lead to non-equivariant votes and consequently, non-equivariant output poses. In this work, we propose possible solutions for these issues. Our contribution can be divided into the following parts. First, we present group equivariant capsule layers, a specialized kind of capsule layer whose pose vectors are elements of a group (_inline_eq_ (\cf~Section~ _ref_) . Given this restriction, we provide a general scheme for dynamic routing by agreement algorithms and show that, under certain conditions, equivariance and invariance properties under transformations from _inline_eq_ are mathematically guaranteed. Second, we tackle the issue of aggregating over local receptive fields in group capsule networks (\cf~Section~ _ref_) . Third, we bring together capsule networks with group convolutions and show how the group capsule layers can be leveraged to build convolutional neural networks that inherit the guaranteed equi-and invariances, as well as producing disentangled representations (\cf~Section~ _ref_) . Last, we apply this combined architecture as proof of concept application of our framework to MNIST datasets and verify the properties experimentally.