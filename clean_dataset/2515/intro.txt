Deep convolutional neural networks (DCNNs), such as AlexNet ~ _cite_, VGGNet ~ _cite_, GoogLeNet ~ _cite_, ResNet ~ _cite_ and DenseNet ~ _cite_, have significantly improved the baselines of most computer version tasks. Despite their distinct advantages over traditional approaches, DCNNs are still specialist systems that leverage a myriad amount of human expertise and data. They provide a uniform feature extraction-classification framework to free human from troublesome handcrafted feature extraction at the expense of handcrafted network design. Designing the architecture of DCNN automatically can not only bypass this issue but also take a fundamental step towards the long-standing ambition of artificial intelligence \ie. creating autonomous learning systems that require lest human intervention ~ _cite_ . Automated design of DCNN architectures has drawn more and more research attentions in recent years, resulting in a number of algorithms in the literature, which can be roughly divided into four categories: (N) DCNN architecture selection from a group of candidates, (N) DCNN architecture optimization using deep learning, (N) reinforcement learning-based DCNN architecture optimization, and (N) evolutionary optimization of DCNN architectures. Among them, evolutionary optimization approaches have a long history and seem to be very promising due to their multi-point global search ability, which enable them to quickly locate the areas of high quality solutions even in case of a very complex search space ~ _cite_ . Despite their success, most evolutionary approaches pose restrictions either on the obtained DCNN architectures, such as the fixed depth, fixed filter size, fixed activation, fixed pooling operation ~ _cite_ and skipping out the pooling operation ~ _cite_, or on the employed genetic operations, such as abandoning the crossover ~ _cite_ . These restrictions may reduce the computational complexity, but also lead to lower performance. Alternatively, other evolutionary approaches may require hundreds even thousands of computers to perform parallel optimization ~ _cite_ . In this paper, we propose a genetic DCNN designer to automatically generate the architecture of a DCNN for each given image classification problem. To reduce the complexity of representing a DCNN architecture, we develop a simple but effective encoding scheme, in which almost all the operations in a DCNN, such as the convolution, pooling, batch normalization, activation, fully connection, drop out and optimizer, are encoded as an integer vector. We start with a population of randomly initialized DCNN architectures and iteratively evolve this population on a generation-by-generation basis to create better architectures using the redefined genetic operations, including selection, crossover and mutation. We have evaluated our approach on six image classification tasks using the MNIST ~ _cite_, EMNIST-Digits ~ _cite_, EMNIST-Letters ~ _cite_, Fashion-MNIST ~ _cite_, CIFARN ~ _cite_ and CIFARN ~ _cite_ datasets. Our results indicate that the proposed genetic DCNN designer is able to generate automatically a DCNN architecture for each given image classification task, whose performance is comparable to the state of the art.