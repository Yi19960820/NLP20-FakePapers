\noindent Ever since the designation of pain as the “fifth vital sign” in medical diagnosis, pain assessment has been an issue of utmost importance in clinical practice _cite_ . The current standard for clinical pain assessment in conscious adults is via self-report. A number of scales have been developed to assist with the measurement of pain using this standard e.g. the Numerical Rating Scale (NRS) _cite_ and the Visual Analogue Scale (VAS) _cite_ . Although self-report tools have been extensively researched, validated and used in clinical settings they are still limited. These tools are not applicable to patients who do not have the ability to articulate or describe their pain; e.g. young infants, the mentally impaired and individuals whose verbal ability is inhibited by a critical medical condition or device _cite_ . Other challenges associated with self reports include differences in patient and clinician’s definition or quantification of pain, patients attempting to mask pain or report more pain than is actually experienced, or disparities in measurement properties across scales. Studies by Williams et al. on NRS have also shown that patients tend to avoid choosing values in the upper range when the scale uses large numbers based on the impression that higher values imply unmanageable pain _cite_ . They also discovered that some patients re-label the scale point to something they can better relate with before assigning a score. For example, 'worst imaginable pain' becomes 'worst pain I have ever felt'. In cases where self-report is not applicable, pain assessment is done by proxy i.e. pain intensity is estimated by an observer based on the behavioural and physiological changes in the patient _cite_ . While useful, the approach has its limitations some of which include bias due to subjectivity, contextual factors, desensitization of proxy due to prolonged exposure to pain, training/experience etc _cite_ _cite_ . This has led to the development of automatic pain assessment based on audio-visual recordings of expressive behaviour using state-of-the art machine learning and digital signal processing methods. This automatic analysis of expressive behaviour altered by medical condition was recently coined as "Behaviomedics" _cite_ . A plethora of research has been documented on automatic pain recognition based on a variety of data sources e.g facial expression, audio and body postures. Research efforts started at distinguishing between pain and no pain _cite_ _cite_ in data samples and gradually extended to continuous pain estimation _cite_ _cite_ _cite_ ; a more valuable outcome for clinical diagnosis. Despite current achievements, there are still open challenges with automatic pain recognition. Like most recognition systems, the performance of pain recognition models is largely dependent on the quantity and quality of data used in training. Human expression data is limited in supply to begin with but pain data is particularly difficult to obtain. Where available, there is the additional complication of a sparse representation for higher pain levels. This imbalance reduces the ability of recognition models to predict high pain intensity levels. For example, this imbalance is very evident in the popularly used UNBC McMaster pain database which contains N \% of 'no pain' frames and only N \% of 'pain' frames. There is a clear need for novel methods that can achieving good pain estimation results from the limited data available. Deep learning has successfully been applied to various computer vision problems. However, deep learned nets require massive amounts of data for good performance. This has hindered it application to automatic pain recognition. Even though deep learned features can not currently work by itself for pain recognition due to the above reasons, we hypothesise that their ability to learn features has value even when small sample sizes are available for training. We will show that such learned features contain valuable information that is complementary to handcrafted features. In this work, we explore deep learning for continuous pain intensity estimation in the face of limited data. We show that combining deep learned features with handcrafted features yields significant improvement in automatic pain recognition compared to using only the former. To the best of our knowledge our work is the first to use deep learned features for continuous pain estimation. Secondly, good facial expression analysis uses a combination of shape based and appearance features. In this work, We encode shape and appearance information in both the hand crafted and deep learned features. We achieve this in our deep learned features by learning features not only from the original images but also from binary image masks defined by a set of facial point locations. Thus our deep learned features are learned from a combination of original image pixels (appearance) and the binary masks (face shape) . Furthermore, learning facial expression from static features is not sufficient. Encoding dynamic information of facial actions significantly improves the recognition performance. In this study, we encode dynamic information in the deep learned features by ensuring that features are learned from a sequence of input images defined as a specified time window centered on the current frame being analysed. Lastly, we adopt person-specific adaptive post processing techniques and show how they can be applied to boost the base performance of pain estimation models. We evaluate our method on the UNBC McMaster database and compare with previous studies based on handcrafted features. Our results show a significant improvement over the state of the art, and show that for a small sample size, combinations of hand-crafted and learned features obtain highest performance. The remainder of the paper is structured as follows: section N presents a review of previous work in automatic pain and AU detection in general. Section N describes our proposed methodology of fusing deep learned features with hand crafted features continuous pain estimation. In section N, we describe the pain database used in this work and show the experimental results in comparison to current studies. In section N, we discuss the limitations of the PSPI metric in the light of clinical pain indicators and possible ways of incorporating other indicators to achieve a more representative pain score. We also discuss the limitations of current performance measures used in pain recognition.