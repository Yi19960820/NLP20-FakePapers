Modern computer vision has seen extensive use of two separate machine learning methodologies: deep architectures and learning on distributions. However, research has only just begun to consider the joint use of these two methods. In this paper we present deep mean maps (DMMs), a novel framework to jointly use and learn distributions of high-level features from deep architectures. We show that DMMs are easy to implement into existing deep learning infrastructure, and are able to improve the results of existing architectures on various datasets. A substantial amount of work has been devoted to constructing distribution-based features for vision tasks. These distribution-based approaches are often patch-level histogram features such as bag of words (BoW) representations _cite_, where the ``words'' are local features such as histogram of gradients (HoG) _cite_ or dense SIFT features _cite_ . More recent developments have extended distribution-based methods beyond histograms to nonparametric continuous domains _cite_ . Such methods are adept at providing robust representations that and give a holistic, aggregate view of an image helpful in complex classification tasks. Deep architectures, which compose many layers of computational neural units, have recently achieved a tremendous amount of success in computer vision tasks by replacing hand-designed features with learned convolutional filters. In fact, deep architectures, often convolutional neural networks (CNNs), have almost become the de-facto standard in some datasets with their recent success _cite_ . It is believed that the depth and hierarchical nature of deep CNNs lead to top-level convolutional features that exhibit semantically meaningful representations _cite_ . Typically, the last levels of these features are then concatenated and fed through a few fully-connected layers to obtain a final classification result. It would be informative to study the distributions of these high-level features in an image for supervised tasks (e.g. scene classification), rather than treating the extracted features simply as a vector. In this paper we address the lack of the application of distributions in deep architectures through the development of a deep mean map layer, which will provide a fast and scalable featurization of top-level convolutional features that will be representative of their distributions in a nonparametric fashion. This combination of distribution-based and deep architecture approaches has seen some initial success with an ad-hoc method of extracting features from distributions of fixed, pre-trained high-level features _cite_ . However, there has yet to been work that jointly learns high-level features and uses their distributions for supervised learning tasks. To this aim we present deep mean maps, which do so in a scalable, nonparametric fashion. Deep mean maps employ mean map embeddings _cite_, which with the use of random features _cite_, provide a finite-dimensional nonparametric representation of distributions of top-level features for discriminant learning. We will show that deep mean maps can be implemented with typical CNN machinery, making both forward and backward propagation tractable for learning effective features and discriminant models based on their distributions. The rest of the paper is structured as follows. First, we detail our DMM framework in Section~ _ref_ . We then study its behavior on a synthetic problem and illustrate its efficacy on several real-world datasets in Section~ _ref_ . Section~ _ref_ then discusses the relationship to other method and gives concluding remarks.