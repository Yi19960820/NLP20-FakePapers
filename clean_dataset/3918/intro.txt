Recently, deep learning became a dominant field of machine learning for various vision tasks, such as recognition and classification. In particular, Convolutional Neural Networks (CNNs) have achieved an unprecedented success through AlexNet _cite_, which has incurred a new line of research concentrating on constructing better performing CNNs _cite_ . {Increasingly deeper architectures are being created and trained based on the observation that, the deeper the network is, the higher-level features it is able to extract} . AlexNets have N convolutional layers _cite_, VGG Nets _cite_ have N or N, GoogLeNets _cite_ have N, and ResNets _cite_ feature over N layers employing residual connections. As the networks became very deep, two common issues have emerged: gradients explosion and vanishing. To deal with these problems, several creative architectures, such as Highway networks _cite_, Deeply-Supervised Nets _cite_ and ResNets _cite_, have been designed. The key ideas are passing information flow from one layer to another via shortcuts or adding ``companion'' objective functions at each hidden layer respectively. Stochastic depth _cite_ trains an ensemble of ResNets with different depth values by randomly dropping a set of layers during the training phase. FractalNets _cite_ repeatedly utilize a simple expansion rule to generate an ultra-deep network containing interacting subpaths of different lengths. Based on the above work, DenseNet _cite_ was introduced, which connects each layer to every subsequent layer. As a result, a given layer in DenseNet takes all feature maps extracted by preceding layers as input. This new connection pattern allows DenseNets to obtain significant improvements over the state-of-the-art on several object recognition benchmark tasks. On another front, inception series _cite_ have been shown to achieve remarkable performance at very low memory costs. This module is composed by convolutions with different kernel size (N _inline_eq_ N, N _inline_eq_ N, N _inline_eq_ N) and a N _inline_eq_ N max pooling, and then concatenates results from the convolutions and pooling. This design strengthens the regularization and scale invariance of extracted features. Recently, feature pyramid networks (FPN) _cite_ and deep layer aggregation _cite_ have been proposed, which aim at exploiting the inherent multi-scale, pyramidal hierarchy of CNNs. Features at different scale levels are merged together to achieve higher accuracy with fewer parameters. Inspired by the benefits of multi-scale convolutions _cite_ and features fusion for training deep networks, we design a novel module, referred as Multi-scale Convolution Aggregation (MCA) to work with DenseNets. As shown in Fig.~ _ref_, the MCA module consists of layers for multi-scale convolutions, cross-scale aggregation, maxout, and concatenation. We observe that DenseNets utilizing MCA module can substantially reduce parameters number and classification error than using other multi-scale designs. The reduction in parameters results from the new design of fusing pyramidal convolutions instead of {simply concatenating them. The increase in accuracy is attributed to the following} factors: N) strengthening scale-invariance because of the multi-scale convolutions with {four kernels with different receptive field sizes; N) given a specific task, the network automatically chooses the most suitable scales} via four trainable gating units to adaptively make use of multi-scale information; N) the use of two maxout activations {stimulates} the competition among neural units of different receptive fields and enhances the learning ability of the network; N) higher non-linearity; and N) compared with traditional concatenation in GoogleNets, our module dramatically reduces the number of parameters while preserving sufficient multi-scale information by aggregation and maxout functions. In addition to various methods of architecture design, difficulties in training deep networks motivated research on optimization and initialization techniques. These include dropout _cite_, maxout activation _cite_, batch normalization _cite_, group normalization _cite_, Xavier initialization _cite_, He initialization _cite_, etc., which have been applied in a wide range of networks as essential components. To reduce the possibility of overfitting in DenseNets and to further boost the generalization of networks, we also develop a regularization method named Stochastic Feature Reuse (SFR) . Similar to stochastic depth _cite_, SFR contains gates for dropping selected feature maps delivered from preceding layers; see Fig.~ _ref_ . During training step, each layer randomly reuses different preceding feature maps for different mini-batch, resulting each mini-batch is trained under a sub-network with a unique connection scheme. This approach effectively addresses overfitting problem of DenseNet by substantially reducing the number of parameters while improving the performance of DenseNets. We evaluate the impacts of both MCA module and SFR on three widely used benchmark datasets: CIFAR-N _cite_, CIFAR-N _cite_ and Street View House Number (SVHN) _cite_ . The comparisons show that our model can achieve comparable test accuracy with relatively lower computation costs and outperform the state-of-the-art performance of DenseNets.