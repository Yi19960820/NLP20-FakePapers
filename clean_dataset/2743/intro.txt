The problem we address is neither VQA nor optimization of a single architecture. Our motivation is to accelerate a large class of dynamic architectures such that they become computationally comparable to their static counterparts. This cause is not motivated only by the recent successes of dynamic architectures, but by their numerous desirable properties that make them likely to retain and increase in importance in the future, particularly their ability to explicitly modularize knowledge. We specifically explore Johnson et al.'s recent work _cite_ in greatest detail because it serves as a useful testbed for multiple approaches to dynamic batching. Their execution engine's modules (see or original work) not only yield dramatic accuracy gains over all strong baselines, but are also a prime example of explicit modularization of knowledge. We view this as a key advantage of dynamic architectures directly comparable to facets of human intelligence. Our work successfully enables efficient parallelization over minibatches in a large class of architectures despite the fact that a new network is assembled for each example. Previous notable dynamic graph results include neural module networks _cite_, which form the basis of the execution engine of Johnson et al. in their CLEVR _cite_ IEP result. The difference is that latter's architecture is built on generic, minimally-engineered neural network blocks that are more likely to generalize to a wider class of problems than the original neural module networks approach, which uses a heavily-engineered question parser and custom per-module architectures. Whereas improvement upon neural module networks constitutes improvement upon a single architecture, improvement on the CLEVR architecture is generalizable to a wide class of models under a minimal set of assumptions (see) . Additional dynamic graph results include neural Turing machines _cite_ _cite_ and memory networks _cite_ _cite_, which both provide auxiliary queryable memory for read/write use during inference. While such architectures are applicable in problems requiring long-term memory, visual question answering places more focus on short term memory. Like the IEP result, these works tend towards higher level reasoning. However, they are perhaps less directly comparable than approaches that explicitly attempt to build generalizable program structures, such as neural program interpreters _cite_ . The main difference is that the IEP result assembles programs that are defined in their entirety before being executed, thus additional dynamic batching optimizations are possible. Note that a subset of our results are applicable in both cases. Much of our work is built atop the recently published CLEVR dataset and subsequent IEP result. We briefly outline these for convenience. CLEVR is a VQA dataset comprising NK images and NK questions/answers/programs triplets. Images are synthetic but high quality ND renders of geometric objects with varying shapes, sizes, colors, and textures. The standard VQA task is given by (question, image) _inline_eq_ (answer) . The difference lies in the inclusion of programs in CLEVR, which are functional representations of the questions. CLEVR therefore allows VQA to be split between two intermediate tasks, as in the IEP result: (question) _inline_eq_ (program) and (program, image) _inline_eq_ (answer) . One might argue that intermediate programs are unrealistic, as one is unlikely to have program annotations in large, realistic tasks. From the CLEVR result, it seems likely that one could collect a small number of annotations on realistic datasets and use these to initialize the program generator. This is similar to the transfer learning experiment in the IEP result. However, performance did degrade compared to the original task; additional work is required to close the gap. The IEP result consists of a program generator and execution engine. The program generator is a N-layer word-level question encoder LSTM _cite_ and N-layer word (function)-level program decoder LSTM. We focus on the execution engine, as it is the dynamic portion of the architecture and the source of the majority of computation time. The program generator predicts a sequence of functions over the function vocabulary with a standard argmax. As the arity (number of arguments) of each function is predetermined, there exists a unique mapping from the predicted vector of functions to a program tree. This is assembled via a depth-first search. Each function is itself a neural network, with the exception of a special SCENE token, which instead outputs ResNet-N features _cite_ taken from an intermediate layer. This program tree is then directly executed, and the outputs are passed through a small classifier network (one convolutional and two fully connected layers) to yield a softmax confidence distribution over answers, which is then optimized as normal via backpropagation over the cross-entropy loss.