he ever increasing number of video cameras are generating unprecedenting amounts of big data that needs efficient algorithms for analysis. In particular, a large proportion of these videos depict events about people such as their activities and behaviors. To effectively interpret this data requires computer vision algorithms that have the ability to understand and recognize human actions. Its core task is to find out the compelling human action frames in videos which are able to well represent the distinct actions for their corresponding videos. Based on this insight, several papers have presented key frame based action recognition and video summarization approaches~ _cite_ . Therefore, detecting compelling key frames in human action videos has been crucial for video interpretation. Early researchers have proposed various key frame detection methods by several strategies: segment-based~ _cite_ cluttering~ _cite_ based. Recently, inspired by the remarkable successes of deep convolutional neural networks (CNNs) for image classification, object classification, and complex events detection~ _cite_, several key frame based works ~ _cite_ have explored deep CNNs for key frame detection in human action videos. However, most of these key frame based video understanding methods have not provided the ground truth key frames in human action videos. To the best of our knowledge, the only works that have mentioned the ground truth of key frames _cite_, report the key frames that are manually selected by two or three people with video processing background. Since this method is biased by human subjective factors, the ground truth of key frame selected by this method may not be consistent and may not be in line with how computer algorithms consider as key frames. Moreover, little attention has been devoted to the task of automatically generating ground truth frame for key frame detection in videos. This raises an interesting yet challenging question, how we can automate the process for key frame labeling in human action videos. In this work, we introduce a automatic label generation algorithm that learns to discriminate each frame for key frame detection. Based on the frame-level video label, we devise a deep key frame detection in human action videos network that reasons directly on the entire video frames. Our key intuition is that the human action video can be well interpreted by only several compelling frames in a video. Based on this intuition, we draw support from the powerful feature representation ability of CNNs for images and the class separation ability and dimensionality reduction of linear discriminant analysis (LDA), we devise a novel automatically generating labels for training the deep key frame detection model. Our key frame detection model takes a long human action video as input, and output the key frames (compelling frames) of a action instance, as shown in Figure~ _ref_ . The core idea of this paper is to introduce a model that automatically output key frames from human action videos. Specifically, the contributions are threefold. (N) We introduce a novel deep two-stream ConvNets based key frame detection model, which combines the advantages of CNN model and LDA. (N) We also introduce a novel automatically generating label method to train the deep key frame detection model. (N) When applied to UCFN~ _cite_, the most popular action recognition dataset, our deep two-stream ConvNets model achieves excellent performance.