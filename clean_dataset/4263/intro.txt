Research in robotics promises to revolutionize surgery towards safer, more consistent and minimally invasive intervention _cite_ . New developments continues on the way to robot-assisted systems and moving toward a future with fully autonomous robotic surgeons. Thus far, the most widespread surgical system is the da Vinci robot, which has already proved its favor via remote controlled laparoscopic surgery in gynecology, urology, and general surgery _cite_ . Information in a surgical console of a robot-assisted surgical system includes valuable details for intra-operative guidance that can help the decision making process. This information is usually represented as ND images or videos that contain surgical instruments and patient tissues. Understanding these data is a complex problem that involves the tracking and pose estimation for surgical instruments in the vicinity of surgical scenes. A critical component of this process is semantic segmentation of the instruments in the surgical console. Semantic segmentation of robotic instruments is a difficult task by the virtue of light changes such as shadows and specular reflections, visual occlusions such as blood and camera lens fogging, and due to the complex and dynamic nature of background tissues. Segmentation masks can be used to provide a reliable input to instrument tracking systems. Therefore, there is a compelling need for the development of accurate and robust computer vision methods for semantic segmentation of surgical instruments from operational images and video. There is a number of vision-based methods developed for the robotic instrument detection and tracking _cite_ . Instrument-background segmentation can be treated as a binary or instance segmentation problem for which classical machine learning algorithms have been applied using color and/or texture features _cite_ . Later applications addressed this problem as semantic segmentation, aiming to distinguish between different instruments or their parts _cite_ . Recently, deep learning-based approaches demonstrated performance improvements over conventional machine learning methods for many problems in biomedicine _cite_ . In the domain of medical imaging, convolutional neural networks (CNN) have been successfully used, for example, for breast cancer histology image analysis _cite_, bone disease prediction _cite_ and age assessment _cite_, and other problems _cite_ . Previous deep learning-based applications to robotic instrument segmentation have demonstrated competitive performance in binary segmentation _cite_ and promising results in multi-class segmentation _cite_ . In this paper, we present a deep learning-based solution for robotic instrument semantic segmentation that achieves state-of-the-art results in both binary and multi-class setting. We used this method to produce a submission to the MICCAI N Endoscopic Vision SubChallenge: Robotic Instrument Segmentation _cite_ to achieve one of the top results. Here we describe the details of the solution based on a modification of the U-Net model _cite_ . Moreover, we provide further improvements over this solution utilizing recent deep architectures: TernausNet _cite_ and a modified LinkNet _cite_ .