video is ubiquitous today in almost every aspect of life, and applications such as high definition television, video chat, or internet video streaming are used for information programs, entertainment and private communication. In most applications, digital images are intended to be viewed by humans. Thus, assessing its perceived quality is essential for most problems in image communication and processing. In a communication system, for example, when an image arrives at the ultimate receiver, typically a human viewer, it has passed a pipeline of processing stages, such as digitization, compression or transmission. These different stages introduce distortions into the original image, possibly visible to human viewers, that may exhibit a certain level of annoyance in the viewing experience. Hence, predicting perceived visual quality computationally is crucial for the optimization and evaluation of such a system and its modules. Generally, are classified depending on the amount of information available from an original reference image---if existing at all. While approaches have access to the full reference image, no information about it is available to approaches. Since for only a set of features extracted from the reference image is available to the algorithm, it lies somewhere in the middle of this spectrum. As no information about the original is exploited, is a more difficult problem than and potentially the most challenging problem in . has a wide range of applications as in practice often no reference is available. However, for many applications, such as the optimization of video coding systems, unconstrained is not a feasible approach---imagine a video codec that, as one example, reconstructs a noise and blur free version of the movie Blair Witch Project or, as another example, independently of the input, always reconstructs the same image (but this of perfect quality) . Perceptually accurate relies on computational models of the and/or natural image statistics and is not an easy task _cite_ . The underlying model employed in an allows for conceptual classification, traditionally in bottom-up and top-down approaches. While former approaches are based on a computational system simulating the by modeling its relevant components, latter ones treat the as a black box and track distortion specific deviations in image statistics. In the case this is typically done based on a model of general statistical regularities of natural images _cite_ . Typically, these statistical image models are used to extract features related to perceived quality that are input to trainable regression models. With the rise of machine learning, recently a third category of emerged, comprising approaches that are purely data-driven, do not rely on any explicit model and allow for end-to-end optimization of feature extraction and regression. Our approach presented in this paper belongs to this new class of data-driven approaches and employs a deep neural network for and . In order to improve prediction accuracy, can be combined with saliency models. This extension aims at estimating the likelihood of regions to be attended to by a human viewer and to consider this likelihood in the quality prediction. Until recently, problems in computer vision such as image classification and object detection were tackled in two steps: (N) designing appropriate features and (N) designing learning algorithms for regression or classification. Although the extracted features were input to the learning algorithm, both of these steps were mostly independent from each other. Over the last years, have shown to outperform these traditional approaches. One reason is that they allow for joint end-to-end learning of features and regression based on the raw input data without any hand-engineering. It was further shown that in classification tasks deep with more layers outperform shallow network architectures _cite_ . This paper studies the use of a deep CNN with an architecture _cite_ largely inspired by the organization of the primates' visual cortex, comprising N convolutional layers and N pooling layers for feature extraction, and N fully connected layers for regression, in a general setting and shows that network depth has a significant impact on performance. We start with addressing the problem of in an end-to-end optimization framework. For that, we adapt the concept of Siamese networks known from classification tasks _cite_ by introducing a feature fusion stage in order to allow for a joint regression of the features extracted from the reference and the distorted image. Different feature fusion strategies are discussed and evaluated. As the number of parameters to be trained in deep networks is usually very large, the training set has to contain enough data samples in order to avoid overfitting. Since publicly available quality-annotated image databases are rather small, this renders end-to-end training of a deep network a challenging task. We address this problem by artificially augmenting the datasets, i.e., we train the network on randomly sampled patches of the quality annotated images. For that, image patches are assigned quality labels from the corresponding annotated images. Different to most data-driven approaches, patches input to the network are not normalized, which enables the proposed method to also cope with distortions introduced by luminance and contrast changes. To this end, global image quality is derived by pooling local patch qualities simply by averaging and, for convenience, this method is referred to as . However, neither local quality nor relative importance of local qualities is uniformly distributed over an image. This leads to a high amount of label noise in the augmented datasets. Thus, as a further contribution of this paper, we assign a patchwise relative weight to account for its influence on the global quality estimate. This is realized by a simple change to the network architecture and adds two fully connected layers running in parallel to the quality regression layers, combined with a modification of the training strategy. We refer to this method as . This approach allows for a joint optimization of local quality assessment and pooling from local to global quality, formally within the classical framework of saliency weighted distortion pooling. After establishing our approaches within a context, we abolish one of the feature extraction paths in the Siamese network. This adaptation allows to apply the network within a context as well. Depending on the spatial pooling strategy used, we refer to the models as and . Interestingly, starting with a model, our approach facilitates systematic reduction of the amount of information from the reference image needed for accurate quality prediction. Thus, it helps closing the gap between and . We show that this allows for exploring the space of from a given model without retraining. In order to facilitate reproducible research, our implementation is made publicly available at . The performance of the models trained with the proposed methods are evaluated and compared to state-of-the-art on the popular LIVE, TIDN and CISQ image quality databases. The models obtained for are additionally evaluated on the more recent LIVE In the Wild Image Quality Challenge Database (that, for convenience, we will refer to as CLIVE) . Since the performance of data-driven approaches largely relies on on the data used for training we analyze the generalization ability of the proposed methods in cross-database experiments. The paper is structured as follows: In we give an overview over state-of-the-art related to the work presented in this paper. develops and details the proposed methods for deep neural network-based and with different patch aggregation methods. In the presented approaches are evaluated and compared to related methods. Further, weighted average patch aggregation, network depth, and reference reduction are analyzed. We conclude the paper with a discussion and an outlook to future work in .