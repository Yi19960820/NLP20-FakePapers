Convolutional neural networks (CNNs) _cite_ have demonstrated recognition accuracy better than or comparable to humans in several visual recognition tasks, including recognizing traffic signs _cite_, faces _cite_, and hand-written digits _cite_ . In this work, we present a result that surpasses human-level performance on a more generic and challenging recognition task-the classification task in the N-class ImageNet dataset _cite_ . In the last few years, we have witnessed tremendous improvements in recognition performance, mainly due to advances in two technical directions: building more powerful models, and designing effective strategies against overfitting. On one hand, neural networks are becoming more capable of fitting training data, because of increased complexity (\eg, increased depth _cite_, enlarged width _cite_, and the use of smaller strides _cite_), new nonlinear activations _cite_, and sophisticated layer designs _cite_ . On the other hand, better generalization is achieved by effective regularization techniques _cite_, aggressive data augmentation _cite_, and large-scale data _cite_ . Among these advances, the rectifier neuron _cite_, \eg, Rectified Linear Unit (ReLU), is one of several keys to the recent success of deep networks _cite_ . It expedites convergence of the training procedure _cite_ and leads to better solutions _cite_ than conventional sigmoid-like units. Despite the prevalence of rectifier networks, recent improvements of models _cite_ and theoretical guidelines for training them _cite_ have rarely focused on the properties of the rectifiers. In this paper, we investigate neural networks from two aspects particularly driven by the rectifiers. First, we propose a new generalization of ReLU, which we call (PReLU) . This activation function adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost. Second, we study the difficulty of training rectified models that are very deep. By explicitly modeling the nonlinearity of rectifiers (ReLU/PReLU), we derive a theoretically sound initialization method, which helps with convergence of very deep models (\eg, with N weight layers) trained directly from scratch. This gives us more flexibility to explore more powerful network architectures. On the N-class ImageNet N dataset, our PReLU network (PReLU-net) leads to a single-model result of N \% top-N error, which surpasses all existing multi-model results. Further, our multi-model result achieves N top-N error on the test set, which is a N \% relative improvement over the ILSVRC N winner (GoogLeNet, N \% _cite_) . To the best of our knowledge, our result surpasses for the first time the reported human-level performance (N \% in _cite_) on this visual recognition challenge.