Designing effective object recognition systems requires features that can describe salient aspects in an image, while being robust to variations within a class. Furthermore, adapting to variations in the visual appearance of images beyond the training set is crucial. As a result, recognition systems often employ feature extraction methods that provide high discrimination between the classes. However, it is observed that no feature descriptor can provide good discrimination for all classes of images. Hence, it is common to adaptively combine multiple feature descriptors that describe diverse aspects of the images such as color, shape, semantics, texture etc. The advantage of using multiple features in object recognition has been demonstrated in a number of research efforts _cite_ . Another inherent challenge in visual recognition is the need to understand the intrinsic structure of high-dimensional features for improved generalization. Different assumptions on the data distribution will enable us to adapt suitable linear/non-linear models to analyze the features and thereby build more effective classifiers. Multiple kernel learning (MKL) is a well-known framework in computer vision that allows the use of multiple descriptors in a unified kernel space _cite_ . The individual kernels used with each descriptor can be combined either using a non-negative linear combination or a Hadamard product or in any other manner, as long as the resulting kernel is positive semi-definite and thereby a valid kernel according to the Mercer theorem _cite_ . An extensive review of the various approaches used to combine individual kernels can be found in _cite_ . Graph-embedding principles _cite_ can be integrated with multiple kernel learning to perform discriminative embedding in the unified space and the applications of this approach in supervised and unsupervised learning have been explored _cite_ . Since any symmetric positive definite kernel defines a unique reproducing kernel Hilbert space (RKHS) _cite_, any data vector in the space can be represented as a weighted combination of the training samples used to construct the kernel matrix. Since operations in the RKHS can be performed using just the kernel similarities without the knowledge of the actual form of the kernel, several pattern analysis and machine learning methods can be tractably solved using kernel methods. Since the kernel similarities can be measured using any suitable non-linear function, linear models learned in its RKHS can provide the power of non-linear models. In the recent years, a variety of linear and non-linear modeling frameworks have been developed in the machine learning literature, aimed at exploiting the intrinsic structure of high-dimensional data. Sparse methods form an important class of models, where the data samples are approximated in a union of subspaces. Sparsity has been exploited in a variety of data processing and computer vision tasks such as compression _cite_, denoising _cite_, compressed sensing _cite_, face classification _cite_, blind source separation _cite_ and object recognition _cite_ . The generative model for representing a data sample _inline_eq_ using the sparse code _inline_eq_ can be written as where _inline_eq_ is the dictionary matrix of size _inline_eq_ and _inline_eq_ is the noise component not represented using the sparse code. Given the dictionary _inline_eq_, a variety of methods can be found in the literature to obtain sparse representations efficiently _cite_ . When a sufficient amount of training data is available, the dictionary _inline_eq_ can be adapted to the data itself. The joint optimization problem of dictionary learning and sparse coding can be expressed as where the training data matrix _inline_eq_, the coefficient matrix _inline_eq_, and _inline_eq_ is the _inline_eq_ norm (_inline_eq_) which measures the sparsity of the vector. Since (_ref_) is not jointly convex, it is solved as an alternating optimization, where the dictionary and the sparse codes are optimized iteratively _cite_ . A wide range of dictionary learning algorithms have been proposed in the literature _cite_, some of which are tailored for specific tasks. The primary utility of sparse models with learned dictionaries in data processing stems from the fact that the dictionary atoms serve as predictive features, capable of providing a good representation for some aspects of novel test data. From the viewpoint of statistical learning theory _cite_, a good predictive model is one that is generalizable, and dictionaries that satisfy this requirement have been proposed _cite_ . Sparse models have been used extensively in recognition and clustering frameworks. One of the first sparse coding based object recognition frameworks used codes obtained from raw image patches _cite_ . However, since then methods that use sparse codes of local descriptors aggregated to preserve partial spatial ordering have been proposed _cite_ and they have achieved much better performance. In order to improve the performance further, algorithms that incorporate class-specific discriminatory information when learning the dictionary have been deveoped, and successfully applied for digit recognition and image classification _cite_ . Improved discrimination can also be achieved by performing simultaneous sparse coding to enforce similar non-zero coefficient support _cite_, and ensuring that the sparse codes obey the constraints induced by the neighborhood graphs of the training data _cite_ . By incorporating constraints that describe the underlying data geometry into dictionary learning, sparse models have been effectively used in non-linear manifold learning _cite_ and activity recognition _cite_ . Sparsity has also been shown to be useful in unsupervised clustering applications. In _cite_, the authors show that graph-regularized sparse codes cluster better when compared to using the data directly. Sparse coding graphs can be obtained by selecting the best representing subspace for each training sample, from the remaining samples _cite_ or from a dictionary _cite_ and subsequently used with spectral clustering. Similar to other machine learning methods, adapting sparse representations to the RKHS has resulted in improved classifiers for computer vision tasks. Sparse models learned in the unified feature space often lead to discriminatory codes. Since the similarity function between the features in the RKHS is linear, samples that belong to the same class are typically grouped together in subspaces. Several approaches to perform kernel sparse coding have been proposed in _cite_ . In _cite_, the authors propose to learn dictionaries in the RKHS using a fixed point method. The well-known K-SVD _cite_ and MOD learning algorithms have also been adapted to the RKHS, and an efficient object recognition system that combines multiple classifiers based on the kernel sparse codes is presented in _cite_ . Sparse codes learned in the RKHS obtained by fusing intensity and location kernels have been successfully used for automated tumor segmentation _cite_ . In this paper, we propose to perform dictionary learning and sparse coding in the multiple kernel space optimized for discriminating between various classes of images. Since the optimization of kernel weights is performed using graph-embedding principles, we also extend this approach to perform unsupervised clustering. Figure _ref_ illustrates our proposed approach for obtaining multiple kernel sparse representations (MKSR) . As described earlier, generalization is a desired characteristic in learned dictionaries. Since the multilevel dictionary (MLD) learning _cite_ has been shown to be generalizable to novel test data and stable with respect to perturbations of the training set, we choose to adapt this to the RKHS for creating kernel multilevel dictionaries. We learn the dictionaries by performing multiple levels of _inline_eq_ D subspace clustering in the RKHS obtained using the combination of multiple features. For novel test data, multiple kernel sparse codes can be obtained with a levelwise pursuit scheme that computes _inline_eq_ sparse representations in each level. In our setup, we construct the ensemble kernel as a non-negative linear combination of the individual kernels, and optimize the kernel weights using graph-embedding principles such that maximum discrimination between the classes is achieved. Since the graph affinity matrix for computing the embedding is based on the kernel sparse codes, this encourages the sparse codes to be discriminative across classes. The proposed algorithm iterates through the computation of embedding directions, updating of the weights, adaptation of the dictionary and calculation of new graph affinity matrices. Note that, the ensemble RKHS is modified in each iteration of the algorithm and a new dictionary is inferred for that kernel space. We empirically evaluate the use of the proposed framework in object recognition by using the proposed MKSR features with benchmark datasets (Oxford Flowers _cite_, Caltech-N _cite_, and Caltech-N _cite_) . Results show that our proposed method outperforms existing sparse-coding based approaches, and compares very well with other state-of-the-art methods. Furthermore, we extend the proposed algorithm to unsupervised learning and demonstrate performance improvements in image clustering. In Section _ref_, we provide a brief overview on sparse coding in the RKHS. Section _ref_ presents a review of dictionary design principles, and summarizes MLD learning and kernel K-hyperline clustering. The kernel MLD algorithm and the pursuit scheme for obtain kernel sparse codes are also proposed in this Section. Section _ref_ describes the proposed discriminative multiple kernel dictionary learning approach, which uses the KMLD and graph embedding to obtain discriminative dictionaries in the multiple kernel space. The effectiveness of the proposed algorithm is evaluated in object recognition (Section _ref_) and unsupervised clustering (Section _ref_) applications. A note on the notation used in this paper. We use uppercase bold letters for matrices, lowercase bold for vectors and non-bold letters for scalars and elements of matrices or vectors. The argument of the kernel _inline_eq_, specifies whether it is a matrix or a vector.