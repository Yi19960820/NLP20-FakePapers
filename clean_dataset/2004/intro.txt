Hand pose estimation is ubiquitously required in many critical applications to support human computer interaction, such as autonomous driving, virtual/mixed reality, and robotics _cite_ . Given a single image of a hand, we would like to estimate the ND hand pose, i.e. the location of each hand joint in ND space. While many early works _cite_ took color images as input, methods built upon depth images usually exhibit superior performance _cite_, because depth provides essential ND information that is extreme helpful to localize joints. In this paper, we focus on estimating ND hand pose from a single depth image. This task is known to be extremely challenging because of the severe occlusions caused by articulate hand pose, and the noisy input from affordable depth sensors. Most of the previous methods _cite_ estimates the location of each hand joint on ND depth image domain, followed by a post-processing in which hand joints are projected to ND space and optimized by a predefined hand model to reduce error that is significant in ND space but not apparent in ND image domain. Such post-process has been thoroughly studied but far from optimal, and _cite_ shows that even a directly searching of nearest neighbors pose in ND space could also achieve performance close to the state of the art, which shows the benefits of directly solving pose estimation in ND space. Inspired by _cite_, we propose a ND convolutional neural network architecture for ND hand pose estimation (Refer to Fig.~ _ref_) . The input depth images are firstly aligned with respect to the center of mass (COM), so that we do not require perfect hand alignment as initial. The aligned input depth image is then converted to a ND volumetric representation, and a ND convolutional neural network (CNN) is trained directly on it to estimate the location of each hand joint in the COM coordinate. Our network naturally learns to use both the ND feature and the ND global context for hand pose estimation. Hence, the output of our network satisfies the context prior and does not require any further post-processing to integrate context in predefined hand model. The unsatisfactory quality and quantity of the data is yet another problem that makes models learned for hand pose estimation unreliable. The depth image from affordable/portable sensor usually suffers from noise and missing depth. On the other hand, the relatively small training set collected from a few subjects could hardly cover either the pose space or different hand configurations, e.g. bone length. To solve these issues, we perform data augmentation to improve both the quality and quantity of the data during both training and testing. We train a fully convolutional network (FCN) to complete the hand shape from a single depth view. Empirically we find this network significantly improves the quality of the input depth, e.g. filling up holes, synthesizing occluded parts. To have a large training data with better coverage of the hand configuration, we transfer hand pose from existing dataset _cite_ to handle with variable bone lengths, and render the depth accordingly. Both these two approaches of data augmentation brings significant improvement to pose accuracy. We evaluate our method on two popular datasets, and demonstrate state-of-the-art performance on hand pose estimation. The contributions of our method are mainly two aspects. N) We propose a ND network to directly estimate the ND hand pose. Our method does not rely on any predefined model and require no post-processing for ND/ND projection which may potentially increase error. N) We perform data augmentation to increase both the quality and quantity of the training data, which are essential to achieve good performance. We use a ND FCN to refine the TSDF which completes missing depth. We also learn poses from real datasets and transfer them to synthetic CAD models.