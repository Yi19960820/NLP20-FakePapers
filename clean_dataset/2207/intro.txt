Image denoising has been a long-time open and challenging research topic in computer vision, aiming to restore the latent clean image from a noisy observation. Generally, a noisy image can be modeled as _inline_eq_ = _inline_eq_ + _inline_eq_, where _inline_eq_ is the latent clean image and _inline_eq_ is the additive Gaussian white noise. To restore the clean mapping _inline_eq_ from a noisy observation _inline_eq_, there are two main categories of methods, namely image prior modeling based and discriminative learning based. Traditional methods, such as BMND _cite_, LSSC _cite_, EPLL _cite_, and WNNM _cite_, lie in the first category. And the second category, pioneered by Jain et al. _cite_, includes MLP _cite_, CSF _cite_, DGCRF _cite_, NLNet _cite_, and TNRD _cite_ . Until recently, Zhang et al.~ _cite_ discovered a deep residual denoising method to learn the noisy mapping with excellent results. However, there is still leeway to boost the denoising performance by reconsidering the activation and the loss function in convolutional neural network (CNN) . In this paper, we propose a deep CNN with exponential linear unit (ELU) _cite_ as the activation function and total variation (TV) as the regularizer of LN loss function for image denoising, which achieves noticeable improvement compared to the state-of-the art work _cite_ in which the rectified linear unit (ReLU) _cite_ was used as the activation function. By analyzing the traits of ELU and its connection with trainable nonlinear reaction diffusion (TNRD) _cite_ and residual denoising _cite_, we show that ELU is more suitable for image denoising applications. Specifically, our method is based on residual learning, and the noisy mapping learned with ELU has a higher probability to obtain a desired \textquoteleft energy \textquoteright ~value than that learned with ReLU. It indicates that more noise can be removed from the original noisy observation, hence the denoising performance can be improved. On the other hand, batch normalization (BN) _cite_ is also applied in the model for the purpose of training convergence. However, Clevert et al. _cite_ pointed out that the direct combination of BN and ELU would degrade the network performance. Instead, we construct a new combination of layers by incorporating _inline_eq_ N convolutional layers, which can better integrate the BN and ELU layers. In our model, we set \textquoteleft Conv-ELU-Conv-BN \textquoteright ~as the fundamental block, where the second \textquoteleft Conv \textquoteright ~denotes the _inline_eq_ N convolutional layer. Furthermore, we utilize TV, which is a powerful regularizer in traditional denoising methods _cite_, to regularize LN loss to further improve the network training performance. Without considering the dual formulation, the TV regularizer can still be solved by stochastic gradient decent (SGD) algorithm during the network training. Finally, we conduct extensive experiments to validate the effectiveness of our proposed approach. The main contributions of this work can be generalized in three-folds. First, we have analyzed the suitability of ELU to denoising task. Second, we have proposed a novel combination of layers to better accommodate ELU and BN. Third, we have applied total variation to regularize LN loss function. The rest of paper is organized as follows. The proposed network with ELU and TV is presented in section N with the analysis of rationale. Extensive experiments and evaluation results can be found in section N. Section N concludes our work with future plan.