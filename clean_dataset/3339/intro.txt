Over the past few years, convolutional neural networks (CNNs) have significantly improved from the standpoint of the network architectures needed to facilitate recognition accuracy and to reduce processing costs~ _cite_ . Currently, CNNs are primarily used to help users understand objects and scenes in an image. In our study, we applied a CNN to an ImageNet dataset containing over N million images and N, N object categories~ _cite_ . Use of such a large-scale dataset allows us to model a wide variety of object recognition image features. By using the pre-trained ImageNet dataset model, we found that CNN is capable of presenting significantly more effective feature variations. For feature extraction, Donahue et al. employed CNN features as a feature vector by combining those features with a support vector machine (SVM) classifier~ _cite_, while other researchers have evaluated and visualized CNN features with an eight-layer AlexNet architecture~ _cite_ . More recent architectures utilize deep structures, such as the very deep convolutional network (VGGNet) ~ _cite_ and GoogLeNet~ _cite_, which were developed by Oxford University's Visual Geometry Group and Google Inc., respectively. According to He et al. ~ _cite_, the most important CNN feature is deep architecture. Along this line, the VGGNet contains N to N layers and GoogLeNet utilizes N layers. VGGNet is frequently used in the computer vision field, not only in full scratch neural net models, but also as a feature generator. CNN's utility as a feature generator is also important because it can function well even if only a few learning samples are available. Thus, large-scale databases such as ImageNet can provide recognition rates that outperform human-level classification (e.g., ~ _cite_) . However, this performance will fluctuate depending on the amount and variance of the data. Therefore, when CNN is used for feature generation, it provides better performance for some recognition problems than others. Donahue et al. argued that usage should be limited to the last two layers before output, which are extracted from first and second fully connected layers in CNN features with AlexNet. However, we believe that more detailed evaluations should be undertaken since several different architectures have recently been proposed, and because middle layers have not been examined as feature descriptors. Accordingly, in this study, we performed more detailed experiments to evaluate two famous CNN architectures--AlexNet and VGGNet. In addition, we carried out simple tuning for feature concatenation (e.g., layer N + layer N + layer N) and transformations (e.g., principal component analysis: PCA) . The rest of this paper is organized as follows. In Section N, related works are listed. The feature settings are evaluated in Section N. The results are shown in Section N. Finally, we conclude the paper in Section N.