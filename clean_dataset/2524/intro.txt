Despite the immense popularity and availability of online video content via outlets such as Youtube and Facebook, most work on object detection focuses on static images. Given the breakthroughs of deep convolutional neural networks for detecting objects in static images, the application of these methods to video might seem straightforward. However, motion blur and compression artifacts cause substantial frame-to-frame variability, even in videos that appear smooth to the eye. These attributes complicate prediction tasks like classification and localization. Object-detection models trained on images tend not to perform competitively on videos owing to domain shift factors _cite_ . Moreover, object-level annotations in popular video data-sets can be extremely sparse, impeding the development of better video-based object detection models. Girshik \etal _cite_ demonstrate that even given scarce labeled training data, high-capacity convolutional neural networks can achieve state of the art detection performance if first pre-trained on a related task with abundant training data, such as N-way ImageNet classification. Followed the pretraining, the networks can be fine-tuned to a related but distinct domain. Also relevant to our work, the recently introduced models Faster R-CNN _cite_ and You Look Only Once (YOLO) _cite_ unify the tasks of classification and localization. These methods, which are accurate and efficient, propose to solve both tasks through a single model, bypassing the separate object proposal methods used by R-CNN _cite_ . In this paper, we introduce a method to extend unified object recognition and localization to the video domain. Our approach applies transfer learning from the image domain to video frames. Additionally, we present a novel recurrent neural network (RNN) method that refines predictions by exploiting contextual information in neighboring frames. In summary, we contribute the following: