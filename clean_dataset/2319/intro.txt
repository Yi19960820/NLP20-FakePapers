For most people, watching a brief video and describing what happened (in words) is an easy task. For machines, extracting the meaning from video pixels and generating natural-sounding description is a very challenging problem. However, due to its wide range of applications such as intelligent video surveillance and assistance to visually-impaired people, video captioning has drawn increasing attention from the computer vision community recently. Different from {\it image} captioning which aims at describing a static scene, {\it video} captioning is more challenging in the sense that a series of coherent scenes need to be understood in order to jointly generate multiple description segments (e.g., see Figure~ _ref_) . Current video captioning tasks can mainly be divided into two families, single-sentence generation~ _cite_ and paragraph generation~ _cite_ . Single-sentence generation tends to abstract a whole video to a simple and high-level descriptive sentence, while paragraph generation tends to grasp more detailed actions, and generates multiple sentences of descriptions. However, even for paragraph generation, the paragraph is often split into multiple, single-sentence generation scenarios associated with ground truth temporal video intervals. In many practical cases, human activities are too complex to be described with short, simple sentences, and the temporal intervals are hard to be predicted ahead of time without a good understanding of the linguistic context. For instance, in the bottom example of Figure~ _ref_, there are five human actions in total: sit on a bed, put a laptop into a bag are happening simultaneously, and then followed by stand up, put the bag on one shoulder and walk out of the room in order. Such fine-grained caption requires a subtle and expressive mechanism to capture the temporal dynamics of the video content and associate that with semantic representations in natural language. In order to tackle this issue, we propose a ``divide and conquer" solution, which first divides a long caption into many small text segments (e.g. different segments are in different colors as shown in Figure~ _ref_), and then employs a sequence model to conquer each segment. Instead of forcing the sequence model to generate the whole sequence in one shot, we propose to guide the model to generate sentences segment by segment. With a higher-level sequence model designing the context of each segment, the low-level sequence model follows the guidance to generate the segment word by word. In this paper, we propose a novel hierarchical reinforcement learning (HRL) framework to realize this two-level mechanism. The textual and video context can be viewed as the reinforcement learning environment . Our framework is a fully-differentiable deep neural network (see Figure~ _ref_) and consists of (N) the higher-level sequence model manager that sets goals at a lower temporal resolution, (N) the lower-level sequence model worker that selects primitive actions at every time step by following the goals from the Manager, and (N) an internal critic that determines whether a goal is accomplished or not. More specifically, by exploiting the context from both the environment and finished goals, the manager emits a new goal for a new segment, and the worker receives the goal as guidance to generate the segment by producing words sequentially. Moreover, the internal critic is employed to evaluate whether the current textual segment is accomplished. Furthermore, we equip both the manager and worker with an attention module over the video features (Sec~ _ref_) to introduce hierarchical attention internally so that the manager will focus on a wider range of temporal dynamics while the worker's attention is narrowed down to local dynamics conditioned on the goals. To the best of our knowledge, this is the first work that strives to develop a hierarchical reinforcement learning approach to reinforce video captioning at different levels. Our main contributions are four-fold: