\vspace* {-Nmm} Recently there has been a renewed interest in the use of first-person point-of-view cameras to better understand human activity. In order to accurately recognize first-person activities, recent work in first-person activity understanding has highlighted the importance of taking into consideration both appearance and motion information. Since the majority of actions are centered around hand-object interactions in the first-person sensing scenario, it is important to capture appearance corresponding to such features as hand regions, grasp shape, object type or object attributes. Capturing motion information such as local hand movements and global head motion, is another important visual cue as the temporal motion signature can be used to differentiate between complementary actions such as take and put or periodic actions such as the cut with knife action. It is also critical to reason about appearance and motion jointly . It has been shown in both third-person _cite_ and first-person activity analysis _cite_ that these two streams of activity information, appearance and motion, should be analyzed jointly to obtain best performance. Based on these insights, we propose a deep learning architecture designed specifically for egocentric video, that integrates both action appearance and motion within a single model _ref_ . More specifically, our proposed network has a two stream architecture composed of an appearance-based CNN that works on localized object of interest image frames and a motion-based CNN that uses stacked optical flow fields as input. Using the terminology of _cite_, we use late fusion with a fully-connected top layer to formulate a multi-task prediction network over actions, objects and activities . The term action describes motions such as put, scoop or spread . The term object refers to item such as bread, spoon or cup. The term activity is used to represent an action-object pair such as take milk container . The appearance-based stream is customized for ego-centric video analysis by explicitly training a hand segmentation network to enable an attention-based mechanism to focus on certain regions of the image near the hand. The appearance-based stream is also trained with object images cropped based on hand location to identify objects of manipulation. In this way, the appearance-based stream is enabled to encode such features such as hand-object configurations and object attributes. The motion-based stream is a generalized CNN that takes as input a stack of optical-flow motion fields. This stream is trained to differentiate between action labels such as put, take, close, scoop and spread . Instead of compensating for camera ego-motion as a pre-processing step, we allow the network to automatically discover which motion patterns (camera, object or hand motion) are most useful for discriminating between action types. Results show that the network automatically learns to differentiate between different motion types. We train the appearance stream and motion stream jointly as a multi-task learning problem. Our experiments show that by learning the parameters of our proposed network jointly, we are able to outperform state-of-the-art techniques by over _inline_eq_ on the task of egocentric activity recognition without the use of gaze information, and in addition improve the accuracy of each sub-task (N \% for action recognition and N \% object recognition) . Perhaps more importantly, the trained network also helps to better understand and to reconfirm the value of key features needed to discriminate between various egocentric activities. We include visualizations of neuron activations and show that the network has learned intuitive features such as hand-object configurations, object attributes and hand motion signatures isolated from global motion. Contributions: (N) we formulate a deep learning architecture customized for ego-centric vision; (N) we obtain state-of-the-art performance propelling the field towards higher performance; (N) we provide ablative analysis of design choices to help understand how each component contributes to performance; and (N) visualization and analysis of the resulting network to understand what is being learned at the intermediate layers of the network. The related work is summarized as follows. \noindent Human Activity Recognition: Traditionally, in video-based human activity understanding research _cite_, many approaches make use of local visual features like HOG _cite_, HOF _cite_ and MBH _cite_ to encode appearance information. These features are typically extracted from spatio-temporal keypoints _cite_ but can also be extracted over dense trajectories _cite_, which can improve recognition performance. Most recently, it has been shown that the visual feature representation can be learned automatically using a deep convolutional neural network for image understanding tasks _cite_ . In the realm of action recognition, Simonyan and Zisserman _cite_ proposed a two-stream network to capture spatial appearance on still images and temporal motion between frames. Ji \etal _cite_ used ND convolutions to extract both spatial and temporal features using a one stream network. Wang \etal _cite_ further develops trajectory-pooled deep-convolutional descriptor (TDD) to incorporate both specially designed features and deep-learned features to achieve state-of-the-art results. \noindent First-Person Video Analysis: In a similar fashion to third-person activity analysis, the first-person vision community has also explored various types of visual features for representing human activity. Kitani \etal _cite_ used optical flow-based global motion descriptors to discover ego-action in sports videos. Spriggs \etal _cite_ performed activity segmentation GIST descriptors. Pirsiavash \etal _cite_ developed a composition of HOG features to model object and hand appearance during an activity. Bambach \etal _cite_ used hand regions to understand activity. Fathi \etal proposed mid-level motion features and gaze for recognizing ego-centric activities in _cite_ . To encode first-person videos using those features, the most prevalent representations are BoW and improved Fisher Vector _cite_ . In _cite_, Li \etal performed a systemic evaluation of features and provided a list of best practices of combining different cues to achieve state-of-the-art results for activity recognition. Similar to third-person vision activity recognition research, there has also been a number of attempts to use CNN for understanding activities in first-person videos. Ryoo \etal _cite_ develops a new pooled feature representation and shows superior performance using CNN as a appearance feature extractor. Poleg \etal _cite_ proposes to use temporal convolutions over optical flow motion fields to index first-person videos. However, a framework to integrate the success of ego-centric features and the power of CNNs is still missing due to challenges of feature diversity and limited training data. In this paper, we aim to design such a framework to address the problem of ego-centric activity recognition. \vspace* {-Nmm}