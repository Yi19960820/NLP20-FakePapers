Deep neural networks are responsible for a large number of recent breakthroughs in computer vision applications-particularly in object recognition, where in some cases they even exceed human-level performance. Traditionally, training neural networks relies on computing a loss function in the form of the cross-entropy between a predictive softmax distribution as an interpretation of the last layer of features and a Dirac delta distribution (encoded as a one-hot vector) which encodes the ground-truth class representation of the labels ~ _cite_ . Training with respect to the softmax output and ground truth delta distribution is a provably correct assumption when training a maximum likelihood classifier given that the classes observed in the training data make up the entire possible set of classes ~ _cite_ . While it is inadvisable to completely ignore traditional delta distribution loss in the traditional classification scheme, it is reasonable to ask if it would be possible to borrow ideas from a more general case, in which classes are considered sub-spaces of a larger image space. In this more general world, it is natural to think that two classes may be closer together in the image space. This leads to the intuition that it is inherently more likely for a classifier to ``confuse" images from similar classes and some classes may have high inter-class mutual information. For a network to properly minimize the cross entropy loss for a given class with a softmax predictive distribution requires that the output logit function for the class be infinitely large. Under the assumption of high inter-class mutual information (such as in the near-class scenario), this requirement is no longer a reasonable assumption. In addition, trying to approximate infinite logits leaves something to be desired. As computers are finite-representational, using back-propagation to push the class logits beyond the inherent representation capacity of the computer is computationally futile. Even further, the traditional cross-entropy loss formulation leads to large variations in weight changes during back-propagation for examples from classes that are difficult to distinguish from other classes (``near-class" examples) as well as for examples which have been ambiguously annotated. These variations lead to distortion along the classification boundary which can detrimentally affect the training procedure and the overall generalization capacity of a neural network. We might resolve these issues in one of two ways: N) we could abandon the odds-likelihood approach to classification, which seems unnecessary (and perhaps incorrect), or N) we can explore loss functions that do not try to approximate a ground-truth Dirac distribution. Thus we ask: ``is training the network to exactly approximate the ground truth necessarily the best option?'' In order to investigate this question we explore a new class of loss functions that are sensitive to the challenges presented above and learned in a data-driven manner. Our major contributions in this paper are: Though our exploration of the data-dependent prior belief is only a basic exposition and exploration of the MCEL framework presented in this paper, we provide a framework from which further research into data-driven loss functions for neural networks can proceed.