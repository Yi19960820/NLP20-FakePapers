The task of scene labeling is to densely predict every pixel of an image into one of the pre-defined classes. One of the most popular ways is to take the image patch around the pixel of interest as input for a learning system. However, large intra-class variation, i.e., the variation of samples within the same class, is one of the key challenges in scene labeling. It is caused by various factors such as the change of illumination, posture, scale, viewpoint and background, and these factors are coupled together in images in nonlinear ways. It is difficult to design features that can be used to well classify samples with significantly different appearances into the same class. Recent works have shown that the performance of scene labeling can be improved by effectively exploiting rich contextual information, and image context is the most commonly used one, i.e., training classifiers that take large image patches as input and predicting class labels of pixels. It is shown that deep models which have large receptive fields contain more image contextual information and generally lead to higher segmentation accuracy . Semantic contextual information, on the other hand, is usually utilized to regularize or smooth the predicted label maps as a post-processing step or as final neural network layers in an end-to-end learning system . Limited by the learning capacity, conventional learning models such as SVM cannot effectively utilize useful information from large image context. In recent years, significant research progress on scene labeling has been achieved by using deep Convolutional Neural Networks (CNNs) . It has been discovered that CNN pre-trained on a large-scale dataset such as the ImageNet has good generalization ability . It provides a good initial point and can be further fine-tuned on the scene labeling datasets, being adapted to the new task. The success of CNN relies on its capability of learning highly discriminative deep feature representations from training image patches. Once the feature representations are effectively learned, even a simple linear classifier can well predict class labels based on that. Its neurons at different levels serve as detectors of various visual patterns at different scales. However, current deep learning approaches for scene labeling still face the challenge of large intra-class variation, especially at the stage of feature learning. As shown in Fig. _ref_ (a), all the image patches with large diversity in appearance are assigned to the class ``building'' according to the labels of their centered pixels. It raises a lot of ambiguity, which confuses CNN when the weights of neurons are adjusted to detect meaningful visual patterns. It would make the feature learning process much easier if the training patches could be grouped into subclasses with better visual consistency. However, more supervised information is needed in order to obtain such subclasses. One could argue that such subclasses can be obtained by clustering image context. However, it requires discriminative feature representations learned by deep models in order to achieve satisfactory clustering results, which leads to a chicken-and-egg problem. State-of-the-art deep learning methods focus on using semantic context for smoothing or regularizing the predicted label maps, while ignoring other rich semantic context available in the datasets. In this paper, we exploit using two types of semantic context: scene names of images and label map statistics of image patches, as supervision signals for learning deep feature representations. In some scene labeling datasets, each image has a name which indicates the scene category. As shown in Fig. _ref_ (b), image patches that have the same class label and are from the same scene category are likely to have similar appearance. On the other hand, the label map statistics of image patches also provides crucial prior information on the appearance of patches, since they specify the spatial layout of the semantics in the surrounding region. As shown by the examples in Fig. _ref_ (c), image patches with similar label map statistics show consistent appearance. Although all these patches belong to the class ``building'', the patches in the top row of Fig. _ref_ (c) capture buildings by the road with sky on the top, while those patches in the bottom row contain buildings in the mountains. Such distinction can be well reflected by label map statistics. In this paper, we create a two-level label hierarchy for each of the original classes by exploiting semantic context, and CNN is fine-tuned with the proposed label hierarchies. The deep feature representations learned in this way are more discriminative and can better predict the original classes. Extensive experiments have shown the effectiveness of the proposed label hierarchy and training schemes on four datasets: SIFTFlow, Stanford background, Barcelona and LM + Sun datasets. Our proposed approaches outperform or achieve the state-of-the-art accuracies on all four datasets. Our contributions can be summarized as three-fold: