Traditionally, robot arm control has been solved by hand-crafting solutions in a modular fashion, for example: ND reconstruction, scene segmentation, object recognition, object pose estimation, robot pose estimation, and finally trajectory planning. However, this approach can result in loss of information between each of the modules resulting in accumulation of error, and it is not flexible for learning a range of tasks, due to the need for prior knowledge. A recent trend has seen robot arm control being learned directly from image pixels, in an end-to-end manner using deep reinforcement learning. This exploits the power of deep learning with its ability to learn complex functions from raw data, avoiding the need for hand-engineering cumbersome and complex modules. However, applying deep reinforcement learning to robot control is challenging due to the need for large amounts of training data to deal with the high dimensionality of policies and state space, which makes efficient policy search difficult. Deep reinforcement learning for robot manipulation often uses real-world robotic platforms, which usually require human interaction and long training times. We believe that a promising alternative solution is using simulation to train a controller, which can then be directly applied to real-world hardware, scaling up the available training data without the burden of requiring human interaction during the training process. Building upon the recent success of deep Q-networks (DQNs) _cite_, we present an approach that uses deep Q-learning and ND simulations to train a N-DOF robotic arm in a robot control task in an end-to-end manner, without any prior knowledge. Images are rendered of the virtual environment and passed through the network to produce motor actions. To ensure that the agent explores interesting states of this ND environment, we use intermediate rewards that guide the policy to higher-reward states. As a test case for evaluation, our experiments focus on a task that involves locating a cube, grasping it, and then finally lifting it off a table. Whilst this particular task is simpler than some others trained with real-world data, the novelty of end-to-end deep reinforcement learning requires evaluation of these initial experiments in order to set the scene for further, more complex applications in future work.