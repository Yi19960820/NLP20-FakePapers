Relations between image entities are an important facet of higher-level visual understanding. Building relationships, such as the spatial distance between people in a scene, their relative motions, or concurrent actions can be used to drive recognition of higher-level activities. Models for interpreting such scenes require the need to accurately interpret image cues, determine relevant relations between entities, and infer the properties of these relations. In this paper we present a general-purpose method for this task, illustrated in Fig.~ _ref_ . The method builds upon deep networks for image analysis, endowing these networks with the ability to reason over structures and relationships. This is accomplished by building higher-level recurrent networks that equip the model with the ability to perform inference over lower-level network outputs, including learning structures that are effective for higher-level tasks. We ground the work by developing specific models for group activity analysis. Group activity analysis involves reasoning over individual people in a scene and considering their relations. Multiple people in a scene could either be performing the same action at the same time, or have varied actions and interactions that compose a collective activity. Effective models need to jointly consider the rich relations between components of visual appearance. Standard approaches to this problem utilize graphical models to encode spatial relations and interactions. Recent work in this vein includes Choi et al.~ _cite_, who discover sub-groups of interacting people. Lan et al.~ _cite_ proposed a hierarchical graphical model that considers the interactions on the social role level. Hajimirsadeghi and Mori _cite_ proposed a gradient boosting training method. Amer et al.~ _cite_ adopted a HiRF model to perform recognition and detection simultaneously. On another track, deep learning has proven successful in many computer vision applications, such as image classification, object recognition, and action recognition. On the image side, seminal work by Krizhevsky et al.~ _cite_ demonstrated the effectiveness of deep networks for object recognition; recent state of the art methods include GoogLeNet~ _cite_ . On the video side, Simonyan and Zisserman~ _cite_ proposed a two-stream convnet pipeline to apply deep learning to video analysis. Karpathy et al.~ _cite_ adopted various fusion techniques in convolutional neural networks to consider temporal information in video sequences. These methods have demonstrated the power of deep networks for classification tasks. However, these models are trained to produce a flat classification output, categorizing an image/video according to the existence of a set of object/action labels. For highly compositional tasks such as group activity recognition, models reasoning over structures can bring benefits, allowing the classification of higher-level concepts built from recognition over lower-level entities. The main contribution of this paper centers on developing a model that bridges from low-level classifications to higher-level compositions. We contribute an end-to-end trainable deep network that (N) classifies low-level image inputs according to their content, (N) refines these classifications by passing messages between outputs, (N) performs structure learning via gating functions that determine which outputs to connect, and (N) results in effective classification of high-level concepts.