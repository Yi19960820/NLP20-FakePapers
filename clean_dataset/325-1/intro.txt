Deep learning really took off in N when Krizhevsky et al. showed record breaking results on the Imagenet dataset _cite_ . They demonstrated that deep convolutional neural networks trained end to end on large labelled datasets can beat most other techniques for image recognition. Deep learning quickly became the default algorithm for image classification and now even achieves super-human level performance _cite_ . Deep learning has also revolutionized other fields like speech recognition and natural language processing _cite_ . \newline The two key ingredients needed to successfully apply deep neural networks are large amounts of labelled training data and powerful computing systems such as GPUs. Mobile devices including smartphones, Internet-of-Things (IoT) devices or smart home assistants have very limited processing power because of their intrinsic limitations on size and energy consumption. One possible solution is to offload all computations to the cloud but this introduces a latency and potentially even a privacy risk when sensitive data is processed remotely. \newline There is a considerable amount of active research on techniques to reduce the computational cost of deep learning models. One approach is to prune the network by removing redundant weights. The idea of pruning already goes back to the eighties when LeCun et al. used second order derivatives to calculate the impact of each weight on the loss of the network _cite_ . Weights with a small impact are then removed from the network. More recently pruning was used on modern deep neural networks. Song et al. proposed a pruning pipeline where first weights with a small magnitude where removed after which the network was fine-tuned to recover the lost accuracy _cite_ . Network pruning is especially effective when a network is used for transfer learning. In transfer learning the model is first trained on a large dataset such as Imagenet and is then fine-tuned on a small domain specific dataset. Because the network was pretrained on a general dataset it will contain convolutional kernels that are not useful for the domain specific dataset. Molchanov et al. introduced a criterion based on a first-order Taylor expansion to decide which feature maps to remove and demonstrated impressive results when used together with transfer learning _cite_ . \newline Instead of compressing a trained model it is also possible to train efficient models from scratch. The recently introduced MobileNets _cite_ use depthwise separable convolutions to reduce the computational cost. Depthwise separable convolutions factorize a standard convolution into a depthwise convolution and a NxN pointwise convolution. The depthwise convolution applies a single convolution to each input channel while the pointwise convolution combines the information in the different channels. Factorizing a traditional convolution into these two convolutions dramatically reduces the computational cost and size of the network with only a minimal reduction in accuracy. \newline Most implementations of deep neural networks use N bit floating point numbers for weights and activations. Various works have shown that this is not necessary and that it is possible to use N bit _cite_ or N bit _cite_ numbers. In the extreme case it is even possible to use binary weights and activations. Courbariaux et al. successfully trained convolutional neural networks for image recognition with binary weights and activations _cite_ . This works surprisingly well for small scale datasets such as CIFARN but there is still a large drop in accuracy for larger datasets such as Imagenet. Neural networks with binary weights and activations are attractive because they replace the costly floating point multiplications and additions with bitwise XNORs and left and right bit shifts. These operations are very efficient to implement in hardware. \newline Another problem with deep learning is the need for large labelled datasets. Training a new model from scratch requires a large amount of training data. A well known technique is to use transfer learning where a model is first trained on a large dataset like Imagenet and afterwards the last layer is removed and retrained using a small amount of new domain specific training data. Transfer learning works because the first layers in the network learn to detect features such as color transitions and basic shapes that are present in images from different domains _cite_ .