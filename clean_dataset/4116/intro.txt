Nowadays people are able to capture and store more and more personal experiences and memories in videos with the decreasing cost of cameras and storages. To organize these captured videos, they are usually edited and processed to be a concise format at a later time. Since the manual post-processing is time-consuming and labor-intensive, automatic algorithms are developed to process these unorganized videos, e.g., generation of a ``short story'' from a collection of videos _cite_ . In this work, we adopt this problem setting and aim to composite a smooth and meaningful video-story from video clips. Specifically, we describe our task as: given a set of clips taken by a person during an activity or experience, we find out an order of the clips which composes a story containing smooth transitions in terms of semantics, motions, and activity dynamics that match the storyline structures _cite_ (see Figure~ _ref_) . Note that, different from the video summarization task that aims to select keyframes out of a long video _cite_, the ``story composition'' problem described in this paper considers transitions between selected subshots and produces the consistent story in the temporal domain. Recently, numerous methods address the temporal consistency problem by identifying temporal alignments _cite_, storyline graph _cite_ or learning temporal relations _cite_ from images, in order to make the story more meaningful. However, most temporal alignment based methods suffer from two difficulties in practice. First, the results may look incoherent when the story is extracted from multiple video clips taken at different times. Second, the ambiguous scene transition of shots also affects the overall quality of the composition. In order to solve the above challenge, hand-crafted features can be used to represent the relations between video clips. State-of-the-art method _cite_ uses the color based bidirectional similarity to describe the relations between video clips and the dense optical flow to generate dynamics scores for each clip. The video sequence order is then formulated and generated via a branch-and-bound algorithm _cite_ . However, the coherence between clips is built directly through feature matching, which is likely to fail in the cases when there are ambiguous appearances or interrupted motions. In contrast, we address this problem by modeling the coherence of adjacent clips through a learning-based recurrent network. Our network learns how to select the next connected clip from the remaining set of video clips based on previous selections in the temporal domain. Specifically, we train the two-stream RNN, including a semantic RNN that uses the spatial-temporal features, and a motion RNN that exploits the motion dynamics in each video clip. To train this network, a generated initial clip is fed into the streams, and two output probabilities are jointly fused as the coherence scores between video clips to predict the next clip. We further rearrange the probabilities from the two-stream RNN by a submodular ranking process to align with the storyline structure, which consists of the exposition, rising, action, climax, and resolution. Generally, the storyline structure ensures that video-story contains rising dynamics and has an ending with more activity than its beginning to attract the viewers _cite_ . Finally, we compose the video-story by solving this submodular ranking optimization. We demonstrate the effectiveness of the proposed learning based video-story composition algorithm on the benchmark dataset _cite_ . We conduct a user study via Amazon Mechanical Turk to evaluate the overall video-story quality and quantitatively verify the composition quality based on pairwise annotations. Overall, our experimental results show that the proposed learning based algorithm performs favorably against the state-of-the-art methods in terms of visually quality and accuracy. The main contributions of this work are summarized as follows. First, we propose a novel learning-based framework via the two-stream RNN for video-story composition. Second, we show that the proposed model explicitly learns better representations to model the coherence between video clips. Third, we develop a submodular ranking algorithm to improve the video-story composition results that better match the storyline structure.