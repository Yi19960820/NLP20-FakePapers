Neural networks (NNs) in machine learning systems are critical drivers of new technologies such as image processing and speech recognition. Modern NNs are gigantic in size with millions of parameters, such as the ones described in Alexnet _cite_, Overfeat _cite_ and ResNet _cite_ . They therefore require an enormous amount of memory and silicon processing during usage. Optimizing a network to improve performance typically involves making it deeper and adding more parameters _cite_, which further exacerbates the problem of large storage complexity. While the convolutional (conv) layers in these networks do feature extraction, there are usually fully connected layers at the end performing classification. We shall henceforth refer to these layers as, of which fully connected layers are a special case. Owing to their high density of connections, the majority of network parameters are concentrated in FCLs. For example, the FCLs in Alexnet account for N \% of the network parameters _cite_ . We shall refer to the spaces between CLs as CL (or simply junctions), which are occupied by connections, or weights. Given the trend in modern NNs, we raise the question--``How necessary is it to have FCLs?" or, in other words, ``What if most of the junction connections never existed? Would the resulting, when trained and tested, still give competitive performance?" As an example, consider a network with N CLs of N neurons each and the junction between them has N weights instead of the expected N, N. Then this is a sparse network with connection density of N \%. Given such a sparse architecture, a natural question to ask is ``How can the existing N weights be best distributed so that network performance is maximized?'' In this regard, the present work makes the following contributions. In Section _ref_, we formalize the concept of sparsity, or its opposite measure, and explore its effects on different network types. We show that CL parameters are largely redundant and a network pre-defined to be sparse before starting training does not result in any performance degradation. For certain network architectures, this leads to CL parameter reduction by a factor of more than N, or an overall parameter reduction by a factor of more than N. In Section _ref_, we discuss techniques to distribute connections across junctions when given an overall network density. Finally, in Section _ref_, we formalize pre-defined sparse connectivity patterns using adjacency matrices and introduce the metric. Our results show that scatter is a quick and useful indicator of how good a sparse network is.