The state-of-art convolutional neural networks _cite_ have been a great success in a bunch of computer vision tasks. Many researchs _cite_ have been conducted with respect to the structure design of convolutional neural networks for a better feature extraction performance. However, most of the works related to object recognition and objection does not consider the visual attention (i.e. glimpses) which plays an important role in the natural human eyes working process. In this paper, we propose a recurrent soft attention model which is specialized for common object recognition. We use one classic LSTM cell, propsed by Hochreiter et al. _cite_, for context information memorization and attention mask generation, and the other LSTM cell with the same size for generalizing glimpse information and generating class probabilities. The soft attention is provided by a shallow two-layer convolution network with a downsampling NxN convolution layer, and is transfered to the context LSTM cell. The glimpse images generated by the soft attention masks on the original image are shown in Figure _ref_ . We provided a recurrent soft attention model which apply the visual attention to the common object recognition area. Since the model is quite flexible to be combined with the modern deep convolution neural network (the combination only need replacing the glimpse network), it may provide some inspirations for future computer vision piplines with better performance. The source code can be referenced through the link: . Our work is built on some basic researches _cite_ with respect to recurrent attention model and reinforcement learning. Mnih et al. _cite_ uses the reinforcement learning algorithm for training locator, while Ba et al. _cite_ uses the Monte Carlo algorithm. However, these works all focus on the recognition of digit numbers which have clear features for attention genralization and classification. Thus, their abilities to recognize common daily-life object are doubted. On the other hand, inspired by the soft attention concept proposed by Xu et al. _cite_, we built our soft attention model in a more natural way for attention genralization, which is proved to be effective.