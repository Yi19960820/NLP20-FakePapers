Video-based action recognition has drawn a significant amount of attention from the academic community~ _cite_, owing to its applications in many areas like security and behavior analysis. In action recognition, there are two crucial and complementary aspects: appearances and dynamics. The performance of a recognition system depends, to a large extent, on whether it is able to extract and utilize relevant information therefrom. However, extracting such information is non-trivial due to a number of complexities, such as scale variations, view point changes, and camera motions. Thus it becomes crucial to design effective representations that can deal with these challenges while preserve categorical information of action classes. Recently, Convolutional Networks (ConvNets) ~ _cite_ have witnessed great success in classifying images of objects, scenes, and complex events~ _cite_ . ConvNets have also been introduced to solve the problem of video-based action recognition~ _cite_ . Deep ConvNets come with great modeling capacity and are capable of learning discriminative representation from raw visual data with the help of large-scale supervised datasets. However, unlike image classification, end-to-end deep ConvNets remain unable to achieve significant advantage over traditional hand-crafted features for video-based action recognition. In our view, the application of ConvNets in video-based action recognition is impeded by two major obstacles. {\em First}, long-range temporal structure plays an important role in understanding the dynamics in action videos _cite_ . However, mainstream ConvNet frameworks~ _cite_ usually focus on appearances and short-term motions, thus lacking the capacity to incorporate long-range temporal structure. Recently there are a few attempts~ _cite_ to deal with this problem. These methods mostly rely on dense temporal sampling with a pre-defined sampling interval. This approach would incur excessive computational cost when applied to long video sequences, which limits its application in real-world practice and poses a risk of missing important information for videos longer than the maximal sequence length. {\em Second}, in practice, training deep ConvNets requires a large volume of training samples to achieve optimal performance. However, due to the difficulty in data collection and annotation, publicly available action recognition datasets (e.g. UCFN~ _cite_, HMDBN~ _cite_) remain limited, in both size and diversity. Consequently, very deep ConvNets~ _cite_, which have attained remarkable success in image classification, are confronted with high risk of over-fitting. These challenges motivate us to study two problems: . In particular, we build our method on top of the successful two-stream architecture~ _cite_ while tackling the problems mentioned above. In terms of temporal structure modeling, a key observation is that consecutive frames are highly redundant. Therefore, dense temporal sampling, which usually results in highly similar sampled frames, is unnecessary. Instead a sparse temporal sampling strategy will be more favorable in this case. Motivated by this observation, we develop a video-level framework, called ~ (TSN) . This framework extracts short snippets over a long video sequence with a sparse sampling scheme, where the samples distribute uniformly along the temporal dimension. Thereon, a segmental structure is employed to aggregate information from the sampled snippets. In this sense, \SEGNET s are capable of modeling long-range temporal structure over the whole video. Moreover, this sparse sampling strategy preserves relevant information with dramatically lower cost, thus enabling end-to-end learning over long video sequences under a reasonable budget in both time and computing resources. To unleash the full potential of \SEGNET~framework, we adopt very deep ConvNet architectures~ _cite_ introduced recently, and explored a number of good practices to overcome the aforementioned difficulties caused by the limited number of training samples, including N) cross-modality pre-training; N) regularization; N) enhanced data augmentation. Meanwhile, to fully utilize visual content from videos, we empirically study four types of input modalities to two-stream ConvNets, namely a single RGB image, stacked RGB difference, stacked optical flow field, and stacked warped optical flow field. We perform experiments on two challenging action recognition datasets, namely UCFN~ _cite_ and HMDBN~ _cite_, to verify the effectiveness of our method. In experiments, models learned using the temporal segment network significantly outperform the state of the art on these two challenging action recognition datasets. We also visualize the our learned two-stream models trying to provide some insights for future action recognition research.