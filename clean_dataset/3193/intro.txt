Researchers have been focused a lot on neural networks. In recent years, a series of neural networks have been introduced. Many of the neural networks can reach a satisfying performance in simple cognitive tasks [N] . Examples would be the convolutional neural network in the field of image object recognition, recurrent neural network in speech recognition, and LSTM in machine translation. However, currently, it is still difficult for a neural network to solve complex cognitive tasks [N, N] . A complex cognitive task describes a more difficult task which requires cognitive process compared to a simple cognitive task. In this paper, it includes multi-cognitive task processing and learning with reflection. Firstly, multi-cognitive task processing is nearly impossible. Using a single deep neural network to process image, audio and video together might lead a miserable performance and require a large amount of training data. In the current research, the Multitask learning mainly focused on an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks [N], like transfer learning. Also, in a learning process, based on the current architecture, we miss the part of reflection. Human will perform a reflection when approaching its limit in performance [N] . The current neural network architecture cannot perform reflection. For solving the complex cognition tasks, a key is missing here. In the architecture of the brain, biological neural networks are not the only structure that is important for processing of intellectual activities. In the cerebral cortex, the sensory area will recognize different inputs based on sensations and then different neural networks located in association cortex will be active to process different tasks (in Figure N) . In fact, the architecture of cortex is necessary for the brain to handle complex cognitive tasks [N, N] . Since it is necessary for the human brain to let its biological neural network to perform complex cognition task with the architecture of cortex, for the Artificial Neural Network, as a simulated logical architecture of the biological neural network, should also require the architecture of cortex to perform complex cognition tasks [N] . In this paper, we would like to introduce the Cortex Neural Network (CrtxNN) . CrtxNN is an architecture that can empower artificial neural network to process complex cognitive tasks. It is a system that learns with neural network groups based on the architecture cerebral cortex. We view this as the missing key for Artificial Neural Networks to solve complex cognitive tasks. In our implementation, the CrtxNN is able to solve the complex cognitive tasks: multi-cognition tasks learning and reflection, in following methods: N. Multi-cognition tasks learning and processing . Multi-cognition task learning and processing is important for a powerful learning system. In our architecture, CrtxNN is possible to learn multiple different cognitive tasks with mixed datasets. The CrtxNN will separate task by different sensations, such as image, audio and video, and train a series of neural networks for different tasks. After the learning phase, it is able to recognize different tasks and to process them with a different solution using the corresponding neural network. N. Reflection . Reflection is a complex cognitive task and an important learning process [N] . In the CrtxNN, a single neural network will be initially trained for a task, which will be viewed as a general situation processor. Then, a series of neural networks will be trained on exceptional situations which are the parts where the general situation processor has a bad performance. Normally, after training and reflecting, the CrtxNN will be able to perform a strategy decision: to decide which network to be used based on the incoming data. This is able to get a higher performance for a single neural network when a single neural network is approaching its performance limit. We provided a series of experiments. The CrtxNN was built on typical neural networks. We compared the CrtxNN with the typical neural network. We tested the result on two tasks: approximation of two-dimensional functions, and mixed image datasets. Our experiment reached a satisfying result. For the approximation of two-dimensional function, we reduced the loss at most by N \%. For the mixed image datasets, out model can process MNIST and CIFARN at the same time and lower the loss by N \% in average. Our contribution of this paper can be summarized as the following points: