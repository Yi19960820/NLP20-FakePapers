For human-machine interface (HMI), emotion recognition is becoming more and more important. Emotion recognition could be done through texts, pictures and physiological signals. Bravo-Marquez {} learned an expanded opinion (positive, neutral and negative) lexicon from emoticon annotated tweets~ _cite_ . Wang and Pal used constraint optimization framework to discover user' emotions from social media content~ _cite_ . \par Recently, many researchers studied emotion recognition from EEG. Liu {} used fractal dimension based algorithm to recognize and visualize emotions in real time~ _cite_ . Murugappan {} employed discrete wavelet transform to extract frequency features from EEG signals and two classifiers are used to classify the features~ _cite_ . Duan {} found that differential entropy features are more suited for emotion recognition tasks~ _cite_ . Zheng and Lu employed deep neural network to classify EEG signals and examined critical bands and channels of EEG for emotion recognition~ _cite_ . \par Besides EEG signals, eye movement data can be used to find out what is attracting users' attention and observe users' unconscious behaviors. It is widely believed that when people are in different emotions, the paradigm of eye movements and pupil diameters will be different. Nelson {} studied the relationship between attentional bias to threat and anxiety by recording eye movement signals in different situations~ _cite_ . Bradley and Lang recorded eye movement signals to study the relationship between memory, emotion and pupil diameters~ _cite_ . \par To deal with information from different modalities, Yang {} proposed an auxiliary information regularized machineï¼Œ which treats different modalities with different strategies~ _cite_ . Zhang {} proposed a multimodal ranking aggregation framework for fusion of multiple visual tracking algorithms~ _cite_ . In ~ _cite_, the authors built a single modal deep autoencoder and a bimodal deep autoencoder to generate shared representations of images and audios. Srivastava and Salakhutdinov extended the methods developed by ~ _cite_ to bimodal deep Boltzmann machines to handle multimodal deep learning problems~ _cite_ . \par As for multimodal emotion recognition, Verma and Tiwary carried out emotion classification experiments with EEG singals and peripheral physiological signals~ _cite_ . Lu {} used two different fusion strategies for combining EEG and eye movement data: feature level fusion and decision level fusion~ _cite_ . Their experimental results indicated that the best recognition accuracy was achieved by using fuzzy integral method in decision level fusion. Vinola and Vimaladevi gave a detailed survey on human emotion recognition and listed many other multimodal datasets and methods~ _cite_ . \par To our best knowledge, there is no research work reported in the literature dealing with emotion recognition from multiple physiological signals using multimodal deep learning algorithms. In this paper, we propose a novel multimodal emotion recognition method using multimodal deep learning techniques. In Section _ref_, we will introduce the unimodal deep autoencoder and bimodal deep autoencoder. Section _ref_ contains contents about data pre-proessing, feature extraction and experiment settings. The experiment results are described in Section _ref_ . Following discusses in Section _ref_, conclusions and future work are represented in Section _ref_ .