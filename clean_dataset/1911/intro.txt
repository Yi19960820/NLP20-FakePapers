The study of events has long involved many disciplines, including philosophy, cognitive psychology, linguistics, computer science, and AI. The Gestalt school of philosophy characterized events as whole processes that emerge from the relations between their components. Cognitive psychologists, such as Tulving _cite_, recognized the importance of events by postulating a separate cognitive process called episodic memory . The representation of events in natural language has been studied from many different approaches, from formal logic and AI _cite_, and frames _cite_, to computational linguistics _cite_ . Combining perspectives from computer science, logic, and linguistics, some recent work suggests that events can be effectively modeled as programs within a dynamic logic (DITL) _cite_, enabling computer simulations of linguistic expressions _cite_ . In computer science, there is little consensus about how events should be modeled for learning. They can be represented atomically, i.e., entire events are predicted in a classification manner _cite_, or as combinations of more primitive actions _cite_, i.e., complex event types are learned based on recognition of combined primitive actions. For the former type of event representation, there are quantitative approaches based on low-level pixel features such as in _cite_ and qualitative approaches such as induction from relational states among event participants _cite_ . For the latter approach, systems such as _cite_, use state transition graphical models such as Dynamic Bayesian Networks (DBN) . While learning events as a whole works best for human motion signatures such as running, sitting etc., it poses a problem for event types that require distinctions in spatio-temporal relationships between objects. As pointed in _cite_, it is also difficult to model events as strict orderings of subevents, especially when there are overlapping or during relations between them. Moreover, if the purpose of event learning is to facilitate communication and interaction between human and computational agents, such as robots, to achieve some common goals, these agents need to keep track of multiple events at the same time, involving themselves, other humans, as well as the surrounding environment. From a practical point of view, this calls for a finer-grained treatment of event modeling. \iffalse Take the event as an example. Zooming into different parts of the event, we see different things. At the beginning, the performer reaches his hand to A, A starts to roll closer to B till some point of time that we can claim that A moves past B. A might continue rolling, with or without force from the performer's hand. \fi \iffalse For example, imagine a scenario in which a human agent takes a pedagogic role and a robotic agent needs to learn to perform gradually more complex activities. \fi It is also the case that a fine-grained analysis of events is strongly supported from a theoretical point of view. For example, it has long been known that event classification needs to take into account what is called extra-verbal factors . Event types should not be semantically defined only by a base verbal expression, such as running or walking, but need to incorporate other components of the expression compositionally, such as objects and adjuncts, which can change the event type of the overall verb phrase or sentence _cite_ . Motivated by those arguments, we suggest a different approach to event learning. Instead of treating events as whole, or as programs of subevents, we allow multiple interpretations of a motion capture by slicing over its temporal span and give a separate annotation for each slice. In particular, we use an event capture and annotation tool called ECAT _cite_, which employs Microsoft Kinect \circledR \to capture sessions of performers interacting with two types of objects, a cube (which can be slid on a flat surface) and a cylinder (which can be rolled) . Objects are tracked using markers fixed to their sides facing the camera. They are then projected into three dimensional space using depth of field. Performers are tracked using the Kinect API, which provides three dimensional inputs of a performer's joint points (e.g., wrist, palm, shoulder) . Sessions are first sliced, and each slice is annotated with a textual description using our event language. Our sequence learning algorithms (LSTM-CRF) will input sequences of feature vectors and output a representation of an event. The main contributions of our study are twofolds. Firstly we created a framework for event recording and annotation that takes into account their temporal dynamics, i.e., different interpretations of events on different temporal spans. Applying a flavor of the popular sequential learning method LSTM that accommodates to output constraints, we achieved good performance in our human-object interaction setup.