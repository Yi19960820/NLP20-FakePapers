There has been a strong interest toward obtaining highly efficient deep neural network architectures that maintain strong modeling power for different applications such as self-driving cars and smartphone applications where the available computing resources are practically limited to a combination of low-power, embedded GPUs and CPUs with limited memory and computing power. The optimal brain damage method~ _cite_ was one of the first approaches in this area, where synapses were pruned based on their strengths. ~ _cite_ proposed a network compression framework where vector quantization was leveraged to shrink the storage requirements of deep neural networks. Han {\it et al.} ~ _cite_ utilized pruning, quantization and Huffman coding to further reduce the storage requirements of deep neural networks. Hashing is another trick utilized by Chen {\it et al.} ~ _cite_ to compress the network into a smaller amount of storage space. Low rank approximation~ _cite_ and sparsity learning~ _cite_ are other strategies used to sparsify deep neural networks. Recently, Shafiee {\it et al.} ~ _cite_ tackled this problem in a very different manner by proposing a novel framework for synthesizing highly efficient deep neural networks via the idea of evolutionary synthesis. Differing significantly from past attempts at leveraging evolutionary computing methods such as genetic algorithms for creating neural networks~ _cite_, which attempted to create neural networks with high modeling capabilities in a direct but highly computationally expensive manner, the proposed novel evolutionary deep intelligence approach mimics biological evolution mechanisms such as random mutation, natural selection, and heredity to synthesize successive generations of deep neural networks with progressively more efficient network architectures. The architectural traits of ancestor deep neural networks are encoded via probabilistic `DNA' sequences, with new offspring networks possessing diverse network architectures synthesized stochastically based on the `DNA' from the ancestor networks and computational environmental factor models, thus mimicking random mutation, heredity, and natural selection. These offspring networks are then trained, much like one would train a newborn, and have more efficient, more diverse network architectures while achieving powerful modeling capabilities. An important aspect of evolutionary deep intelligence that is particular interesting and worth deeper investigation is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the way architectural traits are passed down from generation to generation and thus impact the quality of descendant deep neural networks. A more effective genetic encoding scheme can facilitate for better transfer of important genetic information from ancestor networks to allow for the synthesis of even more efficient and powerful deep neural networks in the next generation. As such, a deeper investigation and exploration into the incorporation of synaptic clustering into the genetic encoding scheme can be potentially fruitful for synthesizing highly efficient deep neural networks that are more geared for improving not only memory and storage requirements, but also be tailored for devices designed for highly parallel computations such as embedded GPUs. In this study, we introduce a new synaptic cluster-driven genetic encoding scheme for synthesizing highly efficient deep neural networks over successive generations. This is achieved through the introduction of a multi-factor synapse probability model where the synaptic probability is a product of both the probability of synthesis of a particular cluster of synapses and the probability of synthesis of a particular synapse within the synapse cluster. This genetic encoding scheme effectively promotes the formation of synaptic clusters over successive generations while also promoting the formation of highly efficient deep neural networks.