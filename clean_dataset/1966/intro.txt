has a remarkable ability to categorize complex scenes very accurately and rapidly. This ability is important for human to infer the current situations and navigate the environments _cite_ . Computer scene recognition and understanding aims at imitating this human ability by using algorithms to analyze input images. This is a fundamental problem in computer vision, and plays a crucial role on the success of numerous application areas like image retrieval, human machine interaction, autonomous driving, etc. The difficulties of scene recognition come from several aspects. Firstly, scene categories are defined not only by various image contents they contain, such as local objects and background environments, but also by global arrangements, interactions or actions between them, such as eating in restaurants, reading in library, watching in cinema. These cause a large diversity of the scene contents which imposes a huge number of scene categories and large within-class variations. These make it much more challenging than the task of object classification. Furthermore, scene images often include numerous fine-grained categories which exhibit very similar contents and structures, as shown in Fig. _ref_ . These fine-grained categories are hard to be discriminated by purely using the high-level FC-features of CNN, which often capture highly abstractive and global layout information. These difficulties make it challenging to develop a robust yet discriminative method that accounts for all types of feature cues for scene recognition. Deep learning models, i.e. CNN _cite_, have been introduced for scene representation and classification, due to their great successes in various related vision tasks _cite_ . Different from previous methods _cite_ that compute hand-crafted features or descriptors, the CNN directly learns high-level features from raw data with multi-layer hierarchical transformations. Extensive researches demonstrate that, with large-scale training data (such as ImageNet _cite_), the CNN can learn effective high-level features at top fully-connected (FC) layer. The FC-features generalize well for various different tasks, such as object recognition _cite_, detection _cite_ and segmentation _cite_ . However, it has been shown that directly applying the CNNs trained with the ImageNet _cite_ for scene classification was difficult to yield a better result than the leading hand-designed features incorporating with a sophisticated classifier _cite_ . This can be ascribed to the fact that the ImageNet data _cite_ is mainly made up of images containing large-scale objects, making the learned CNN features globally object-centric. To overcome this problem, Zhou trained a scene-centric CNN by using a large newly-collected scene dataset, called Places, resulting in a significant performance improvement _cite_ . In spite of using different training data, the insight is that the scene-centric CNN is capable of learning more meaningful local structures of the images (e.g. fine-scale objects and local semantic regions) in the convolutional layers, which are crucial to discriminate the ambiguous scenes _cite_ . Similar observation was also presented in _cite_ that the neurons at middle convolutional layers exhibit strong semantic information. Although it has been demonstrated that the convolutional features include the important scene cues, the classification was still built on the FC-features in these works, without directly exploring the mid-level features from the convolutional layers _cite_ . In CNN, the convolutional features are highly compressed when they are forwarded to the FC layer, due to computational requirement (i.e. the high-dimensional FC layer will lead to huge weight parameters and computational cost) . For example, in the celebrated AlexNet _cite_, the _inline_eq_ and _inline_eq_ convolutional layer have N, N and N, N nodes respectively, which are reduced considerably to N, N (about N/N or N/N) in the _inline_eq_ FC layer. And this compression is simply achieved by pooling and transformations with sigmod or ReLU operations. Thus there is a natural question: are the fine sematic features learned in the convolutional layers well preserved in the fully-connected layers? If not, how to rescue the important mid-level convolutional features lost when forwarded to the FC layers . In this paper, we explore the questions in the context of scene classification. Building on these observations and insightful analysis, this paper strives for a further step by presenting an efficient approach that both enhances and encodes the local semantic features in the convolutional layers of the CNN. We propose a novel Locally-Supervised Deep Hybrid Model (LS-DHM) for scene recognition, making the following contributions. Firstly, we propose a new local convolutional supervision (LCS) layer built upon the convolutional layers. The LCS layer directly propagates the label information to the low/mid-level convolutional layers, in an effort to enhance the mid-level semantic information existing in these layers. This avoids the important scene cues to be undermined by transforming them through the highly-compressed FC layers. Secondly, we develop the Fisher Convolutional Vector (FCV) that effectively encodes meaningful local detailed information by pooling the convolutional features into a fixed-length representation. The FCV rescues rich semantic information of local fine-scale objects and regions by extracting mid-level features from the convolutional layers, which endows it with strong ability to discriminate the ambiguous scenes. At the same time, the FCV discards explicit spatial arrangement by using the FV encoding, making it robust to various local image distortions. Thirdly, both the FCV and the FC-features are collaboratively explored in the proposed LS-DHM representation. We demonstrate that the FCV with LCS enhancement is strongly complementary to the high-level FC-features, leading to significant performance improvements. The LS-DHM achieves N \% and N \% accuracies on the MIT IndoorN _cite_ and SUNN _cite_, remarkably outperforming all previous methods. The rest of paper is organized as follows. Related studies are briefly reviewed in Section II. Then the proposed Locally-Supervised Deep Hybrid Model (LS-DHM), including the local convolutional supervision (LCS) layer and the Fisher Convolutional Vector (FCV), is described in Section III. Experimental results are compared and discussed in Section IV, followed by the conclusions in Section V.