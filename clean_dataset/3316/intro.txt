\noindent An estimated N million hip fractures occur annually and are associated with N, N deaths and N million disability adjusted life-years. _cite_ The chance of death in the three months following a hip fracture increases by fivefold for women and eightfold for men, relative to age-and sex-matched controls. _cite_ When a middle-aged or elderly patient presents with acute hip pain and fracture is suspected, clinical guidelines recommend first ordering a hip radiograph. _cite_ However, not all fractures are detectable on radiographs. _cite_ If a patient with high clinical suspicion of fracture has a negative or indeterminant radiograph, then it is usually appropriate to follow-up with a pelvic MRI. _cite_ Fractures are the most commonly missed diagnosis on radiographs of the spine and extremities, and the majority of these errors are perceptual (i.e., a radiologist not noticing some abnormality as opposed to misinterpreting a recognized anomaly) . _cite_ \bigskip \noindent Statistical learning models can both detect fractures and help radiologists detect fractures. Past studies used machine learning (ML) to identify combinations of hand-engineered features associated with fracture, and more recent studies used deep learning (DL) to discover hierarchical pixel patterns from many images with a known diagnosis. Most studies detect fracture in algorithm-only systems. _cite_ Kazai et al. performed a clinical trial to study how algorithms can augment radiologists and found radiologists were significantly better at detecting vertebral fractures when aided by an ML model that had a standalone sensitivity of N \%. _cite_ Convolutional Neural Networks (CNNs), the DL models best suited for image recognition, have recently been used to detect fracture in the appendicular skeleton including wrists _cite_, shoulders _cite_, and hands and feet _cite_ . Gale et al. developed the only previously-reported hip fracture detector using DL; their model achieved an area under the receiver operating curve (AUC) of N _cite_ These academic DL reports compared isolated image model performance against humans, but none tested whether algorithms could aid human diagnosis. In contrast, the company Imagen Technologies’ OsteoDetect DL system reported improving humans from an unaided AUC N to AUC N, according to a letter from the FDA (https: //www.accessdata.fda.gov/cdrh \_docs/pdfN/DENN.pdf) . Deep learning studies on various image-based fracture prediction tasks have been published, but they do not consider patient and hospital covariates or how algorithms can augment human decision processes. \bigskip \noindent Statistical learning algorithms sometimes learn unintended or unhelpful patterns contained in the model training data. Diverse examples for common DL applications include gender being differentially classified on photographs of the face depending on a person’s race _cite_ as well as gender detection on photographs of the outer eye leveraging disproportionate use of mascara. _cite_ Language processing algorithms learned to perpetuate human prejudices from text from the Web. _cite_ In medicine, retrospectively collected observational datasets may have patient and healthcare process biases. An ML study on medical record data found that hospital process variables were more predictive of patient mortality than biological signal. _cite_ Statistical learning can exploit biases that are pervasive in observational human datasets. \bigskip \noindent DL has previously been shown to detect patient demographics and clinical variables from fundoscopy images. _cite_ We previously showed that DL can learn individual hospital sources in a multi-site trial and leverage this for disease detection, which leads to inconsistent performance when deployed to new hospitals. _cite_ Here we perform the first comprehensive analysis of what patient and hospital process variables DL can detect in radiographs and whether they contribute to the inner workings of a fracture detection model. We group variables into disease (fracture), patient (sex, age, body mass index, pain, and recent fall), and hospital processes (e.g, department, device model, study date) . We use patient and hospital process data to develop multimodal models and to create matched patient cohorts to test image-only models. To assess the suitability for image-only models augmenting human interpretations, we experiment with Naive Bayes model ensembles. We re-analyze test data from the best published hip fracture model by Gale et al. and conclude by highlighting design of clinical experiments and strategies to mitigate the susceptibility of models to confounding variables.