One of the vital goals in mobile robotics is to develop a system that is aware of the dynamics of the environment. If the environment changes over time, the system should be capable of handling these changes. In this paper, we present an approach for pointwise semantic classification of a ND LiDAR scan into three classes: non-movable, movable and dynamic . Segments in the environment having non-zero motion are considered dynamic, a region which is expected to remain unchanged for long periods of time is considered non-movable, whereas the frequently changing segments of the environment is considered movable. Each of these classes entail important information. Classifying the points as dynamic facilitates robust path planning and obstacle avoidance, whereas the information about the non-movable and movable points can allow uninterrupted navigation for long periods of time. To achieve the desired objective, we use a Convolutional Neural Network (CNN) ~ _cite_ for understanding the distinction between movable and non-movable points. For our approach, we employ a particular type of CNNs called up-convolutional networks~ _cite_ . They are fully convolutional architectures capable of producing dense predictions for a high-resolution input. The input to our network is a set of three channel ND images generated by unwrapping _inline_eq_ ND LiDAR data onto a spherical ND plane and the output is the objectness score, where a high score corresponds to the movable class. Similarly, we estimate the dynamicity score for a point by first calculating pointwise ND motion using our previous method~ _cite_ and then comparing the estimated motion with the odometry to calculate the score. We combine the two scores in a Bayes filter framework for improving the classification especially for dynamic points. Furthermore, our filter incorporates previous measurements, which makes the classification more robust. In~ we show the classification results of our method. Black points represent non-movable points, whereas movable and dynamic points are shown in green and blue color respectively. Other methods~ _cite_ for similar semantic classification have been proposed for RGB images, however, a method solely relying on range data does not exist according to our best knowledge. For LiDAR data, separate methods exists for both object detection~ _cite_ and for distinguishing between static and dynamic objects in the scene~ _cite_ . The two main differences between our method and the other object detection methods is that the output of our method is a pointwise objectness score, whereas other methods concentrate on calculating object proposals and predict a bounding box for the object. Since our objective is pointwise classification, the need for estimating a bounding box is alleviated as a pointwise score currently suffices. The second difference is that we utilize the complete N _inline_eq_ field of view (FOV) of LiDAR for training our network in contrast to other methods which only use the points that overlap with the FOV of the front camera. The main contribution of our work is a method for semantic classification of a LiDAR scan for learning the distinction between non-movable, movable and dynamic parts of the scene. As mentioned above, these three classes encapsulate information which is critical for robust autonomous robotic system. A method for learning the same classes in LiDAR scans has not been proposed before, even though different methods exists for learning other semantic level information. Unlike other existing methods, we use the complete range of the LiDAR data. For training the neural network we use the KITTI object benchmark~ _cite_ and compare our results on this benchmark with the other methods. We also test our approach on the dataset by~ ~ _cite_ .