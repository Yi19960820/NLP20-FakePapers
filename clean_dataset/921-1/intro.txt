, state-of-the-art algorithms for computer vision tasks like image classification, object recognition and semantic segmentation employ some form of deep neural networks (DNNs) . The ease of design for such networks, afforded by numerous open source deep learning libraries _cite_, _cite_, has established DNNs as the go-to solution for many computer vision applications. Even challenging computer vision tasks like image classification _cite_, _cite_, _cite_, _cite_ and object recognition _cite_, _cite_, _cite_, which were previously considered to be extremely difficult, have seen great improvements in their state-of-the-art results due to the use of DNNs. An important factor contributing to the success of such deep architectures in computer vision tasks is the availability of large scale annotated datasets _cite_, _cite_ . The visual quality of input images is an aspect very often overlooked while designing DNN based computer vision systems. In most realistic computer vision applications, an input image undergoes some form of image distortion including blur and additive noise during image acquisition, transmission or storage. However, most popular large scale datasets do not have images with such artifacts. Dodge and Karam _cite_ showed that even though such image distortions do not represent adversarial samples for a DNN, they do cause a considerable degradation in classification performance. \figurename~ _ref_ shows the effect of image quality on the prediction performance of a DNN trained on high quality images devoid of distortions. Testing distorted images with a pre-trained DNN model for AlexNet _cite_, we observe that adding even a small amount of distortion to the original image results in a misclassification, even though the added distortion does not hinder the human ability to classify the same images (\figurename~ _ref_) . In the cases where the predicted label for a distorted image is correct, the prediction confidence drops significantly as the distortion severity increases. This indicates that features learnt from a dataset of high quality images are not invariant to image distortion or noise and cannot be directly used for applications where the quality of images is different than that of the training images. Some issues to consider while deploying DNNs in noise/distortion affected environments include the following. For a network trained on undistorted images, are all convolutional filters in the network equally susceptible to noise or blur in the input image? Are networks able to learn some filters that are invariant to input distortions, even when such distortions are absent from the training set? Is it possible to identify and rank the convolutional filters that are most susceptible to image distortions and recover the lost performance, by only correcting the outputs of such ranked filters? In our proposed approach called, we try to address these aforementioned questions through the following novel contributions: Applying our models for common vision tasks like image classification _cite_, object recognition _cite_ _cite_ and scene classification _cite_ significantly improves the robustness of DNNs against distorted images and also outperforms other alternative approaches, while training significantly lesser parameters. To ensure reproducibility of presented results, the code for is made publicly available at _url_ . The remainder of the paper is organized as follows. Section _ref_ provides an overview of the related work in assessing and improving the robustness of DNNs to input image perturbations. Section _ref_ describes the distortions, network architectures and datasets we use for analyzing the distortion susceptibility of convolutional filters in a DNN. A detailed description of our proposed approach is presented in Section _ref_ followed, in Section _ref_, by extensive experimental validation with different DNN architectures and multiple datasets covering image classification, object recognition and scene classification. Concluding remarks are given in Section~ _ref_ . \protect \vspace* {-Nex}