Training convolution neural networks becomes more difficult with the depth increasing and even the training accuracy deceases for very deep networks. Identity mappings or transformations, which are adopted as skip-connections in deep residual networks~ _cite_, ease the training of very deep networks and make the accuracy improved. Identity transformations lead to shorter connections between layers close to the input and those close to the output. It is shown that identity transformations improve information flow in both forward propagation and back-propagation because the product of identity matrices is still an identity matrix, in other words, no matter how many skip-connections there are. In this paper, we introduce two linear transformations and use them as skip-connections for improving information flow. The first one is an orthogonal transformation. Multiplying several orthogonal matrices, used to form the orthogonal transformations, yields an orthogonal matrix. The benefit is that information attenuation and explosion is avoided because the absolute values of the eigenvalues of an orthogonal matrix are always _inline_eq_ . The second one is an idempotent transformation, whose transformation matrix is an idempotent matrix which, when multiplied by itself, yields itself. A sequence of idempotent transformations with the same idempotent matrices is equivalent to a single idempotent transformation. We show that the success essentially comes from feature reuse and gradient reuse in forward and backward propagation for maintaining the information and eliminating the gradient vanishing problem because of the express way through skip-connections. The empirical results show that single-branch deep neural networks with idempotent and orthogonal transformations as skip-connections achieve perform similarly to those with identity transformations and that the performances are superior when applied to multi-branch networks.