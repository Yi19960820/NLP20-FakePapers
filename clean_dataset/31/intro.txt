Retrieving ND models from ND sketches has important applications in computer graphics, information retrieval, and computer vision _cite_ . Compared to the early attempts where keywords or ND shapes are used as queries _cite_, the sketch-based idea is very attractive because sketches by hand provide an easy way to input, yet they are rich enough to specify shapes. Directly matching ND sketches to ND models suffers from significant differences between the ND and ND representations. Thus, in many state of the art methods ND models are projected to multiple ND views, and a sketch matches a ND model if it matches one of its views. Fig. _ref_ shows a few examples of ND sketches and their corresponding ND models. One can immediately see the variations in both the sketch styles and ND models. In almost all state of the art approaches, sketch based ND shape retrieval amounts to finding the ``best views'' for ND models and hand-crafting the right features for matching sketches and views. First, an automatic procedure is used to select the most representative views of a ND model. Ideally, one of the viewpoints is similar to that of the query sketches. Then, ND models are projected to ND planes using a variety of line rendering algorithms. Subsequently, many ND matching methods can be used for computing the similarity scores, where features are always manually defined (e.g., Gabor, dense SIFT, and GALIF _cite_) . This stage-wise methodology appears pragmatic, but it also brings a number of puzzling issues. To begin with, there is no guarantee that the best views have similar viewpoints with the sketches. The inherent issue is that identifying the best views is an unsolved problem on its own, partially because the general definition of best views is elusive. In fact, many best view methods require manually selected viewpoints for training, which makes the view selection by finding ``best views'' a chicken-egg problem. Further, this viewpoint uncertainty makes it dubious to match samples from two different domains without learning their metrics. Take Fig. _ref_ for example, even when the viewpoints are similar the variations in sketches as well as the different characteristics between sketches and views are beyond the assumptions of many ND matching methods. Considering all the above issues arise when we struggle to seek the viewpoints for matching, can we bypass the stage of view selection? In this paper we demonstrate that by learning cross domain similarities, we no longer require the seemingly indispensable view similarity assumption. Instead of relying on the elusive concept of ``best views'' and hand-crafted features, we propose to define our views and learn features for views and sketches. Assuming that the majority of the models are upright, we drastically reduce the number of views to two per object for the whole dataset. We also make no selections of these two directions as long as they are significantly different. Therefore, we consider this as the minimalism approach as opposed to multiple best views. This upright assumption appears to be strong, but it turns out to be sensible for ND datasets. Many ND models are naturally generated upright (e.g., _cite_) . We choose two viewpoints because it is very unlikely to get degenerated views for two significantly different viewpoints. An immediate advantage is that our matching is more efficient without the need of comparing to more views than necessary. This seemingly radical approach triumphs only when the features are learned properly. In principle, this can be regarded as learning representations between sketches and views by specifying similarities, which gives us a semantic level matching. To achieve this, we need comprehensive shape representations rather than the combination of shallow features that only capture low level visual information. We learn the shape representations using Convolutional Neural Network (CNN) . Our model is based on the Siamese network _cite_ . Since the two input sources have distinctive intrinsic properties, we use two different CNN models, one for handling the sketches and the other for the views. This two model strategy can give us more power to capture different properties in different domains. Most importantly, we define a loss function to ``align'' the results of the two CNN models. This loss function couples the two input sources into the same target space, which allows us to compare the features directly using a simple distance function. Our experiments on three large datasets show that our method significantly outperforms state of the art approaches in a number of metrics, including precision-recall and the nearest neighbor. We further demonstrate the retrievals in each domain are effective. Since our network is based on filtering, the computation is fast. Our contributions include