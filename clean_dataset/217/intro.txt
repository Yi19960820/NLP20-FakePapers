Convolutional Neural Networks (CNNs) have seen great success in a range of computer vision tasks, including image classification _cite_, semantic segmentation _cite_, and style transfer _cite_ . However, these networks are typically designed to achieve only one particular task. For more complete vision systems in real-world applications, a network which can perform multiple tasks simultaneously is far more desirable than building a set of independent networks, one for each task. This is more efficient not only in terms of memory and inference speed, but also in terms of data, since related tasks may share informative visual features. This type of learning is called Multi-Task Learning (MTL) _cite_, and in this paper we present a novel architecture for MTL based on feature-level attention masks, which add greater flexibility to share complementary features. Compared to standard single-task learning, training multiple tasks whilst successfully learning a shared representation poses two key challenges: However, most prior MTL approaches focus on only one of these two challenges, whilst maintaining a standard implementation of the other. In this paper, we introduce a unified approach which addresses both challenges cohesively, by designing a novel network which (i) enables both task-shared and task-specific features to be learned automatically, and consequently (ii) learns an inherent robustness to the choice of loss weighting scheme. The proposed network, which we call the Multi-Task Attention Network (MTAN) (see Figure _ref_), is composed of a single shared network, which learns a global feature pool containing features across all tasks. Then for each task, rather than learning directly from the shared feature pool, a soft attention mask is applied at each convolution block in the shared network. In this way, each attention mask automatically determines the importance of the shared features for the respective task, allowing learning of both task-shared and task-specific features in a self-supervised, end-to-end manner. This flexibility enables much more expressive combinations of features to be learned for generalisation across tasks, whilst still allowing for discriminative features to be tailored for each individual task. Furthermore, automatically choosing which features to share and which to be task specific allows for a highly efficient architecture with far fewer parameters than multi-task architectures which have explicit separation of tasks _cite_ . MTAN can be built on any feed-forward neural network depending on the type of tasks. We first evaluate MTAN with SegNet _cite_, an encoder-decoder network on the tasks of semantic segmentation and depth estimation on the outdoor CityScapes dataset _cite_, and then with an additional task of surface normal prediction on the more challenging indoor dataset NYUvN _cite_ . We also test our approach with a different backbone architecture, Wide Residual Network _cite_, on the recently proposed Visual Decathlon Challenge _cite_, to solve N individual image classification tasks. Results show that MTAN outperforms several baselines and is competitive with the state-of-the-art for multi-task learning, whilst being more parameter efficient and therefore scaling more gracefully with the number of tasks. Furthermore, our method shows greater robustness to the choice of weighting scheme in the loss function compared to baselines. As part of our evaluation of this robustness, we also propose a novel weighting scheme, Dynamic Weight Average (DWA), which adapts the task weighting over time by considering the rate of change of the loss for each task.