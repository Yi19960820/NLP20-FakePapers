The problem of identifying the position of the camera that captured an image of a scene has been studied extensively for decades~ . With applications in domains such as robotics and autonomous driving, a considerable amount of engineering effort has been dedicated to developing systems for different versions of the problem. One formulation, often referred to as simply `localization', assumes that a map of the ND scene is provided in advance and the goal is to localize any new image of the scene relative to this map. A second formulation, commonly referred to as `Simultaneous Localization and Mapping' (SLAM), assumes that there is no prespecified map of the scene, and that it should be estimated concurrently with the locations of each observed image. Research on this topic has focused on different aspects and challenges including: estimating the displacement between frames in small time scales, correcting accumulated drifts in large time scales (also known as `loop closure'), extracting and tracking features from the observed images (e.g.~), reducing computational costs of inference (graph-based SLAM~) and more. Although this field has seen huge progress, performance is still limited by several factors~ . One key limitation stems from reliance on hand-engineered representations of various components of the problem (e.g. key-point descriptors as representations of images and occupancy grids as representations of the map), and it has been argued that moving towards systems that operate using more abstract representations is likely to be beneficial~ . Considering the map, it is typically specified as either part of the input provided to the system (in localization) or part of its expected output (in SLAM) . This forces algorithm designers to define its structure explicitly in advance, e.g. \either as ND positions of key-points, or an occupancy grid of a pre-specified resolution. It is not always clear what the optimal representation is, and the need to pre-define this structure is restricting and can lead to sub-optimal performance. In this work, we investigate the problem of localization with mapping, by considering the `re-localization' task, using learned models with no explicit form of map. Given a collection of `context' images with known camera poses, `re-localization' is defined as finding the relative camera pose of a new image which we call the `target'. This task can also be viewed as loop closure without an explicit map. We consider deep models that learn implicit representations of the map, where at the price of making the map less interpretable, the models can capture abstract descriptions of the environment and exploit abstract cues for the localization problem. In recent years, several methods for localization that are based on machine learning have been proposed. Those methods include models that are trained for navigation tasks that require localization. In some cases the models do not deal with the localization explicitly, and show that implicit localization and mapping can emerge using learning approaches~ . In other cases, the models are equipped with specially designed spatial memories, that can be thought of as explicit maps~ . Other types of methods tackle the localization problem more directly using models that are either trained to output the relative pose between two images ~, or models like PoseNet~, trained for re-localization, where the model outputs the camera pose of a new image, given a context of images with known poses. Models like PoseNet do not have an explicit map, and therefore have an opportunity to learn more abstract mapping cues that can lead to better performance. However, while machine learning approaches outperform hand-crafted methods in many computer vision tasks, they are still far behind in visual localization. A recent study~ shows that a method based on traditional structure-from-motion approaches~ significantly outperforms PoseNet. One potential reason for this, is that most machine learning methods for localization, like PoseNet, are discriminative, i.e. they are trained to directly output the camera pose. In contrast, hand-crafted methods are usually constructed in a generative approach, where the model encodes geometrical assumptions in the causal direction, concerning the reconstruction of the images given the poses of the camera and some key-points. Since localization relies heavily on reasoning with uncertainty, modeling the data in the generative direction could be key in capturing the true uncertainty in the task. Therefore, we investigate here the use of a generative learning approach, where on one hand the data is modeled in the same direction as hand-crafted methods, and on the other hand there is no explicit map structure, and the model can learn implicit and abstract representations. A recent generative model that has shown promise in learning representations for ND scene structure is the Generative Query Network (GQN) ~ . The GQN is conditioned on different views of a ND scene, and trained to generate images from new views of the same scene. Although the model demonstrates generalization of the representation and rendering capabilities to held-out ND scenes, the experiments are in simple environments that only exhibit relatively well-defined, small-scale structure, e.g. \a few objects in a room, or small mazes with a few rooms. In this paper, we ask two questions: N) Can the GQN scale to more complex, visually rich environments? N) Can the model be used for localization? To this end, we propose a dataset of random walks in the game Minecraft (figure~ _ref_), using the Malmo platform~ . We show that coupled with a novel attention mechanism, the GQN can capture the ND structure of complex Minecraft scenes, and can use this for accurate localization. In order to analyze the advantages and shortcomings of the model, we compare to a discriminative version of it, similar to previously proposed methods. In summary, our contributions are the following: N) We suggest a generative approach to localization with implicit mapping and propose a dataset for it. N) We enhance the GQN using a novel sequential attention model, showing it can capture the complexity of ND scenes in Minecraft. N) We show that GQN can be used for localization, finding that it performs comparably to our discriminative baseline when used for point estimates, and investigate how the uncertainty is captured in each approach.