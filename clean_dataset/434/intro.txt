Many key tasks in computer vision rely on the availability of dense and reliable ND reconstructions of the sensed environment. Due to high precision, low latency and affordable costs, passive stereo has proven particularly amenable to depth estimation in both indoor and outdoor set-ups. Following the groundbreaking work by Mayer _cite_, current state-of-the-art stereo methods rely on deep convolutional neural networks (CNNs) that take as input a pair of left-right frames and directly regress a dense disparity map. In challenging real-world scenarios, like the popular benchmarks _cite_, these networks turn out to be more effective, and sometimes faster, than algorithms. As recently highlighted in _cite_, learnable models suffer from loss in performance when tested on unseen scenarios due to the domain shift between training and testing data-often synthetic and real, respectively. Good performance can be regained by fine-tuning on few annotated samples from the target domain. Yet, obtaining groundtruth labels requires the use of costly active sensors (\eg, LIDAR) and noise removal by expensive manual intervention or post-processing _cite_ . Recent works _cite_ proposed to overcome the need for labels with unsupervised losses that require only stereo pairs from the target domain. Although effective, these techniques are inherently limited by the number of samples available at training time. Unfortunately, for many tasks, like autonomous driving, it is unfeasible to acquire, in advance, samples from all possible deployment domains (\eg, every possible road and/or weather condition) . We propose to address the domain shift issue by casting as a process whereby a stereo network can evolve based on the images gathered by the camera during its real deployment. We believe that the ability to continually adapt itself in real-time is key to any deep learning machinery intended to work in real scenarios. We achieve continuous online adaptation by: deploying one of the unsupervised losses proposed in literature (\ie, _cite_) ; computing error signals on the current frames; updating the whole network by back-propagation (from now on shortened as) ; and moving to the next pair of input frames. However, such adaptation reduces inference speed greatly. Therefore, to keep a high enough frame rate we propose a novel () architecture designed to be lightweight, fast and modular. This architecture exhibits accuracy comparable to DispNetC _cite_ using one-tenth parameters, runs at around _inline_eq_ FPS for disparity inference and performs an online adaptation of the whole network at around _inline_eq_ FPS. Moreover, to achieve an even higher frame rate during adaptation, at the cost of a slight loss in accuracy, we develop a () algorithm that leverages the modular architecture of in order to train sub-portions of the whole network independently. Using together with we can adapt our network to unseen environments without supervision at approximately _inline_eq_ FPS. _ref_ shows the disparity maps predicted by on three successive frames of a video sequence from the dataset _cite_: without undergoing any adaptation-row (b) ; by adapting online the network-row (c) ; and by our computationally efficient approach-row (d) . Rows (c) and (d) show how online adaptation can improve the quality of the predicted disparity maps significantly in as few as N frames (\ie, a latency of about N seconds for complete online adaptation and N seconds for) . Extensive experimental results support our three main novel contributions: To the best of our knowledge, the synergy between and realizes the first-ever real-time, self-adapting, deep stereo system.