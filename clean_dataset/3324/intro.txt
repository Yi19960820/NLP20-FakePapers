Deep Neural Networks have become important tools for modeling nonlinear functions in applications like computer vision, speech recognition, natural language processing, and computer games . However, inference and training of a DNN may involve up to billions of operations for inputs likes images . A DNN may also have large number of parameters, leading to large storage size and runtime memory usage. Such intensive resource requirements impede adoption of DNN in applications requiring real-time responses, especially on resource-limited platforms. To alleviate these requirements, many methods have been proposed, from both hardware and software perspective . For example, constraints may be imposed on the weights of DNN, like sparsity, circulant matrix, low rank, vector quantization, and hash trick etc., to reduce the number of free parameters and computation complexity. However, these methods use high bit-width numbers for computations, which require availability of high precision multiply-and-add instructions. Another line of research tries to reduce bit-width of weights and activations of a DNN by quantization to low bit-width numbers . Reducing bit-width of weights of a _inline_eq_-bit model to _inline_eq_ can shrink the storage size of model to _inline_eq_ of the original size. Similarly, reducing bit-widths of activations to _inline_eq_ can shrink the runtime memory usage by the same proportion. In addition, when the underlying platform supports efficient bitwise operations and _inline_eq_ that counts the number of bits in a bit vector, we can compute the inner product between bit vectors _inline_eq_ _inline_eq_ by the following formula: Consequently, convolutions between low bit-width numbers can be considerable accelerated on platforms supporting efficient execution of bitwise operations, including CPU, GPU, FPGA and ASIC. Previous works shows that using only N-bit weights and N-bit activation can achieve N \% top-N accuracy on ImageNet datasets . However, in contrast to the extensive study in compression and quantization of convolutional neural networks, little attention has been paid to reducing the computational resource requirements of RNN. claims that the weight binarization method does not work with RNNs, and introduces weight ternarization and leaves activations as floating point numbers. experiments with different combinations of bit-widths for weights and activations, and shows N-bit quantized CNN and RNN can achieve comparable accuracy as their N-bit counterpart. However, large performance degradation occurs when quantizing weights and activations to N-bit numbers. Though has their quantized CNN open-sourced, neither of the two works open-source their quantized RNNs. This paper makes the following contributions: