Deep learning~ _cite_ has garnered tremendous attention, driven by recent breakthroughs in a wide range of applications such as image categorization~ _cite_ and speech recognition~ _cite_ . Despite these successes, the increasing complexities of deep neural networks limit their widespread adoption in edge scenarios such as mobile and other consumer devices where computational, memory, bandwidth, and energy resources are scarce. There has in recent years been a noticeable increase in the exploration of strategies to improve the efficiency of deep neural networks, particularly on edge and mobile devices. One common approach explored is precision reduction~ _cite_, in which the data representation of a network is reduced from typical N-bit floating point precision to fixed-point or integer precision~ _cite_, N-bit precision~ _cite_, or even N-bit precision~ _cite_ . Another strategy explored is model compression~ _cite_, which involves traditional data compression methods such as weight thresholding, hashing, and Huffman coding, as well as teacher-student strategies involving a larger teacher network training a smaller student network. Finally, another technique that has been investigated targets the fundamental design of deep neural networks and involves leveraging architectural design principles to achieve more efficient deep neural network macroarchitectures~ _cite_ . In this study, we explore a very different idea: Can we learn generative machines to automatically generate deep neural networks with efficient network architectures? We introduce the idea of generative synthesis, which is premised on the intricate interplay between a generator-inquisitor pair that work in tandem to garner insights and learn to generate highly efficient deep neural networks that best satisfies operational requirements, such as those needed for on-device edge scenarios.