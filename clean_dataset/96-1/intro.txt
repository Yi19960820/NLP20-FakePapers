Depth field data encodes the distance between each recorded point and the camera plane. This allows for highly-accurate crowd dynamics analyses in real-world scenarios, i.e. outside of laboratory environments~ _cite_ . With this technology, for the first time, the ability to analyze between several thousands to few millions actual pedestrian trajectories has been achieved~ _cite_ . This enabled new statistical insights, unbiased by artificial laboratory conditions (e.g. need for participants to wear tracking hats or vests, and dynamics regulated by the experimenter instructions) ~ _cite_ . Furthermore, depth measurements do naturally protect the privacy of the pedestrians, since individuals remain unrecognizable. This is a requirement for real-life measurements, and a challenge for methods that use imaging rather than depth field data~ _cite_ . Pedestrian positioning from depth field data, acquired from sensors such as Microsoft Kinects~ _cite_, requires addressing two key tasks, namely: background subtraction and head localization. For a number of common cases these two tasks have a straight-forward solution. Since the camera takes a birds eye view, the background can be simply subtracted by removing all points beyond a depth threshold. The head localization can be approached similarly, in fact the points closest to the camera are part of the pedestrian heads. So far, these tasks have been tackled via hand-crafted approaches that rely on expert-tweaked depth-cloud clustering algorithms~ _cite_ (CL) . These approaches segment the different objects mainly based on the assumption that a cluster of neighboring points forms a pedestrian. This assumption typically holds when the dynamics on the scene involve low pedestrian densities (about _inline_eq_ ped./m _inline_eq_ max~ _cite_), assorted homogeneously (i.e. composed of adults of similar size, with no elements such as strollers, carts, and bikes) and developing in simple geometric settings (corridors in~ _cite_) . However, such designed approaches do not generalize well and are sensitive to a number of special case scenarios. This can be as simple as a raised hand that is interpreted as a head, but it becomes a larger issue when pedestrian density increases and CL algorithms experience difficulties in disentangling the individuals. Furthermore, these problems only increase when there are other (i.e. non-pedestrian) objects in the scene, such as moving doors, trolleys and obstacles. Figure~ _ref_ contains a sample depth-field map picturing N pedestrians, N of which are infant (notice the smaller size), and N static object. The CL based localization in Fig.~ _ref_ (A) shows some typical mistakes (cf. ground truth in Fig.~ _ref_ (B)): twice a couple formed by an adult and a child (top and left side of the scene) is detected as a unique individual. Based on the shape of the objects, an expert instead recognizes that the likely scenario involves two individuals standing close to each other. The CL algorithms also fails to disentangle a pedestrian at the bottom from a static obstacle in their neighborhood (even though they are not connected) . This follows from the small size of the object and the lack of ability of the CL approach to classify shapes. Detailing every exception and crafting appropriate rules to deal with such complexity becomes difficult and requires significant effort that does not transfer well across different measurement scenarios and implementations. On the other hand, Machine Learning (ML) methods for image analysis rely on training data rather than design of rules and exceptions. Given sufficient data and good features, these models tend to perform well and are robust to special cases. The success of these approaches has been particularly demonstrated with recent developments in Deep Learning (DL) . DL models obtain excellent performance and are currently state-of-the-art in image localization~ _cite_ . The major advantage that DL methods have over ML image analysis is that they incorporate automatic feature extraction (or representation learning) as part of the model training. However, one of the main disadvantages of these approaches is the difficulty in incorporating expert knowledge about the problem and hence the requirement of significant amount of annotations~ _cite_ . Therefore, to develop DL models for pedestrian localization, an expert needs to produce a large amount of hand-annotated images. These images and their annotations can then be used to train a DL model such as a Deep Convolution Neural Network (CNN) to produce the target model. As the number of annotations can be quite high, this becomes very labor intensive and diminishes the advantages of using CNNs. In this work we address this problem by proposing a method for efficient collection of expert annotations for pedestrian tracking using depth field images. Our contribution is twofold: The obtained model can be used for real time pedestrian detection in depth field maps, possibly on large areas exploiting Graphical Processing Units (GPU) . This paper is structured as follows: in Sect.~ _ref_ we provide some selected background on depth map based crowd recording setups. On this basis, in Sect.~ _ref_ we describe our synthetic data generation procedure as well as our neural network and its training method. In Sect.~ _ref_, we examine the detection performance. A final discussion section closes the paper.