Changes in land cover and land use are pervasive, rapid, and can have significant impact on humans, the economy, and the environment. Accurate land cover mapping is of paramount importance in many applications, e.g., in urban planning, forestry, natural hazard mapping, habitat mapping, assessment of land-use change effects on climate, etc.~ _cite_ . In practice, land cover maps are built by analyzing remotely sensed imagery, captured by satellites, airplanes or drones, using different classification methods. The accuracy of the results and their interpretation depends on the quality of the input data, e.g., spatial, spectral, and radiometric resolution of the images, and also on the classification methods used. The most used methods can be divided into two categories: pixel-based classifiers and Object-Based Image Analysis (OBIA) ~ _cite_ . Pixel-based methods, which use only the spectral information available for each pixel, are faster but ineffective especially for high resolution images~ _cite_ . Object-based methods take into account the spectral as well as the spatial properties of image objects, i.e., set of neighbor similar pixels. OBIA methods are more accurate but very expensive from a computational point of view, they require a high human supervision and number of iterations to obtain acceptable accuracies, and are not easily portable to other images (e.g., to other areas, seasons, extensions, radiometric calibrations or different spatial or spectral resolutions) . To detect a specific object in an input image, first, the OBIA method segments the image (e.g., by using a multi-resolution segmentation algorithm), and then classifies the segments based on their similarities (e.g., by using algorithms such as the k-nearest neighbor) . This procedure has to be repeated for each single input image and the knowledge acquired from one input image cannot be reutilized in another. In the last five years, deep learning and particularly supervised Convolutional Networks (CNNs) based models have demonstrated impressive accuracies in object recognition and image classification in the field of computer vision~ _cite_ . This success is due to the availability of larger datasets, better algorithms, improved network architectures, faster GPUs and also improvement techniques such as, transfer-learning and data-augmentation. This paper analyzes the potential of CNNs-based methods for plant species mapping using high-resolution Google Earth _inline_eq_ images and provides an objective comparison with the state-of-the-art OBIA-based methods. As case study, this paper addresses the challenging problem of detecting {\it Ziziphus lotus} shrubs, a species known by its role as the dominant plant that characterizes an ecosystem of priority conservation in the European Union with {\it Ziziphus}, which is experiencing a serious decline during the last decades. The complexity of this case is due to the fact that {\it Ziziphus lotus} individuals are scattered arborescent shrubs with variable shapes, sizes, and distribution patterns. In addition, distinguishing {\it Ziziphus lotus} shrubs from other neighbor plants in remote sensing images is complex for non-experts and for automatic classification methods. From our results, compared to OBIA, the detection model based on GoogLeNet network, in combination with data-augmentation, transfer-learning (fine-tuning) and pre-processing the input test images, achieves higher precision and balance between recall and precision in the problem of {\it Ziziphus lotus} detection. In addition, the detection process using GoogLeNet detector is faster, which implies a high user productivity in comparison with OBIA. In particular, the contributions of this work are: This paper is organized as follows. A review of related works is provided in Section~N. A description of the proposed CNN-methodology is given in Section~N. The considered study areas and how the dataset were constructed to train the CNN-based classifier can be found in Section~N. The experimental results of the detection using CNNs and OBIA are provided in Section~N and finally conclusions.