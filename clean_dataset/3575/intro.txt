Due to the development of depth sensors, ND human activity analysis _cite_ has attracted more interest than ever before. Recent manifold-based approaches are quite successful at ND human action recognition thanks to their view-invariant manifold-based representations for skeletal data. Typical examples include shape silhouettes in the Kendall's shape space _cite_, linear dynamical systems on the Grassmann manifold _cite_, histograms of oriented optical flow on a hyper-sphere _cite_, and pairwise transformations of skeletal joints on a Lie group _cite_ . In this paper, we focus on studying manifold-based approaches _cite_ to learn more appropriate Lie group representations of skeletal action data, that have achieved state-of-the-art performances for some ND human action recognition benchmarks. As studied in _cite_, Lie group feature learning methods often suffer from speed variations (i.e., temporal misalignment), which tend to deteriorate classification accuracy. To handle this issue, they typically employ dynamic time warping (DTW), as originally used in speech processing _cite_ . Unfortunately, such process costs additional time, and also results in a two-step system that typically performs worse than an end-to-end learning scheme. Moreover, such Lie group representations for action recognition tend to be extremely high-dimensional, in part because the features are extracted per skeletal segment and then stacked. As a result, any computation on such nonlinear trajectories is expensive and complicated. To address this problem, _cite_ attempt to first flatten the underlying manifold via tangent approximation or rolling maps, and then exploit SVM or PCA-like method to learn features in the resulting flattened space. Although these methods achieve some success, they merely adopt shallow linear learning schemes, yielding sub-optimal solutions on the specific nonlinear manifolds. Deep neural networks have shown their great power in learning compact and discriminative representations for images and videos, thanks to their ability to perform nonlinear computations and the effectiveness of gradient descent training with backpropagation. This has motivated us to build a deep neural network architecture for representation learning on Lie groups. In particular, inspired by the classical manifold learning theory _cite_, we equip the new network structure with rotation mapping layers, with which the input Lie group features are transformed to new ones with better alignment. As a result, the effect of speed variations can be appropriately mitigated. In order to reduce the high dimensionality of the Lie group features, we design special pooling layers to compose them in terms of spatial and temporal levels, respectively. As the output data reside on nonlinear manifolds, we also propose a Riemannian computing layer, whose outputs could be fed into any regular output layers such as a softmax layer. In short, our main contributions are: