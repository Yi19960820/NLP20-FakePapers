\begingroup `` '' {} \endgroup Automatically synthesizing realistic images has been an emerging research area in deep learning. Imagining an entire scene in the presence of discriminative properties of the scene such as ``sunny beach with mountains on the back'' is an ability that humans possess. As the most expressive artificial neural networks would presumably have human-like properties including imagination, automatic image generation research is a step towards this goal. Moreover, it is of practical interest as generated images would ideally augment data for various other tasks, e.g. image classification. Generating photo-realistic images of various object types has not yet been solved, however many successful attempts have been made. Generative Adversarial Nets (GANs) ~ _cite_ generated digits~ _cite_, faces~ _cite_, chairs~ _cite_, room interiors~ _cite_ and videos~ _cite_ . On the other hand, Variational Autoencoders (VAEs) ~ _cite_ are combined with visual attention~ _cite_ and have been extended to generating images based on textual descriptions~ _cite_ . Moreover, Pixel RNN~ _cite_ has been proposed as an alternative model for the same. Deep neural networks take their strength from the availability of large image collections which stabilizes the learning. However, in some domains with limited number of images various complementary sources of information has been proposed to stabilize the learning. Recently, for the domain of fine-grained image generation, GAN conditioned on detailed sentences synthesizes realistic bird images~ _cite_ where visual training data was limited. Moreover, integrating textual GAN with bounding box and keypoint conditionals~ _cite_ allows drawing bird at the desired location. On the other hand, conditioining VAEs on discriminative object properties, i.e. attributes, has generated faces~ _cite_ with different hair color, beard or glasses, at different ages. Apart from stabilizing the learning, conditioning variables also provide diversity to the generated images. Hence, we argue that the descriptive power of a generator network can be increased by conditioning it with respect to the object type, visual properties and location information. Object type conditioning teaches the network what to draw, visual properties specifies the visual details of the object and finally the location encodes where that object should be drawn. We propose a new GAN model architecture to generate realistic outdoor scenes, e.g. sea, mountain, urban scenes, conditioned on transient attributes, e.g. sunny, foggy, and on semantic layouts to determine the exact boundaries of where the object should be drawn. Our aim is to automatically generate outdoor scenes with various scene properties as shown in~ _ref_ . This problem has previously been tackled by designing hand-crafted procedures~ _cite_, however we propose to learn such transformations automatically through training deep convolutional networks. Towards this goal, we employ the recent ADENK dataset~ _cite_ that contains outdoor scenes with dense semantic layout annotations. To complement semantic layouts, we exploit a dataset of outdoor webcam sequences~ _cite_ that provides per-scene attribute annotations. We complement the missing spatial layouts of~ _cite_ with coarse semantic annotations of each scene and the missing attributes of~ _cite_ with attribute predictions. We will make these supplementary annotations and our code publicly available. Our contributions are summarized as follows. We propose a new conditioned GAN model that learns the content, i.e. transient attributes, to be drawn inside a scene layout. We show that our model generates realistic images of scenes with objects drawn within their own segments as well as transforming the scene by, for instance, imagining how a day scene would look like in the night.