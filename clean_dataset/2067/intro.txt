Single image depth prediction is a challenging task that aims to recover pixel-wise depth from monocular images, which plays an important role in many applications, such as ND modeling, autonomous driving, and ND-to-ND conversion. With the prosperity of deep convolutional neural networks (CNNs), many deep learning-based methods~ _cite_ have achieved state-of-the-art performance on various RGB-D datasets, such as NYUvN~ _cite_, ScanNet~ _cite_, and KITTI~ _cite_ . However, all these methods are trained individually on each dataset, which makes models be specific to certain domains. For example, ScanNet only focuses on indoor scenes, while KITTI considers outdoor views. The large differences between indoor and outdoor patterns limit the generalization ability of the model. That is, a model achieves remarkable performance on one dataset but performs poorly on the other one, as shown in Fig.~ _ref_ . In this paper, we study how to make the model robust so that it can perform well on diverse datasets. One challenge for robust depth prediction is how to extract discriminative features from diverse scenes. Compared with conventional depth prediction, robust depth prediction requires one single model to perform well in both indoor and outdoor scenes. Thus, the model should adapt to diverse scene layouts and can tackle complex depth patterns. Another challenge is the large difference of depth ranges between indoor and outdoor. For example, the depth range of ScanNet is from N _inline_eq_ to N _inline_eq_, while the depth range of KITTI is between N _inline_eq_ and N _inline_eq_ . It is worth noting that two datasets exist an overlapped depth range from N _inline_eq_ to N _inline_eq_ . In this range, different layouts belonging to indoor or outdoor scenes may correspond to the same depth value, which increases the difficulty of training models. In this paper, we present a deep attention-based classification network (DABC) to address these problems. In order to tackle diverse scenes, our model learns a universal RGB-to-depth mapping from both the ScanNet and KITTI datasets. Our model is based on a U-shape~ _cite_ structure, where skip-connections fuse multi-scale depth cues to generate accurate and high-resolution results. To extract discriminative features, we employ a global average pooling layer and a channel-wise attention mechanism to our model. Through the global average pooling layer, the model can capture the channel-wise statistics of features that represent the global information and visual characteristics of a scene. According to the channel-wise statistics, the attention mechanism assigns important channels with higher weights and updates the original features. In addition, to tolerate the significant difference of depth ranges, we formulate depth prediction as a multi-class classification problem. By discretizing continuous depth values into several intervals, we choose a softmax layer as the predictor. In this way, each neuron in the last layer only needs to activate for a specific depth interval rather than to predict the depth value in the whole depth range, which makes the model easy to train. Further, to reduce the influence of quantization error caused by depth discretization, we employ a soft-weighted sum inference method~ _cite_ to obtain the final continuous predictions. The contributions of this work can be summarized as follows: _inline_eq_ We present one of the first explorations on the task of robust depth prediction and propose a deep attention-based classification network (DABC) . _inline_eq_ For the task of depth prediction, we are the first to employ the channel-wise attention mechanism, and verify the effectiveness of the attention module on choosing scene-specific features. _inline_eq_ Our method achieves state-of-the-art performance on both the ScanNet and KITTI datasets, and won the N-nd place in the single image depth prediction entry of ROB N.