Deep neural networks, particularly deep Convolutional Neural Networks (CNNs), have provided significant improvement in visual tasks such as face recognition, attribute prediction and image classification _cite_ . Despite this advancement, designing a deep model to learn different tasks jointly while improving their performance by sharing learned parameters remains a challenging problem. Providing auxiliary information to a CNN-based face recognition model can improve its recognition performance; however, in some cases such information is available only during training and may not be available during the testing phase. Despite the potential advantages of using auxiliary data, these problems have diminished the popularity and flexibility of using both soft and hard modalities for biometric applications _cite_ . We propose a model which jointly predicts facial attributes and identifies faces while simultaneously leverages the predicted facial attributes as an auxiliary modality to improve face identification performance. We also show that when our model is trained jointly to recognize face images and predict facial attributes, the model performance on facial attribute prediction increases as well. In other words, in our model the two modalities improve each other's performance once they are trained jointly. We show that some soft biometric information, such as age and gender which on their own are not distinctive enough for face identification, but, nevertheless provide complementary information along with other primary information, such as the face images. Despite significant improvements in face recognition performance, it is still an ongoing problem in computer vision _cite_ . There are a number of approaches in the literature that use facial attributes for biometrics applications such as face recognition. For example, Wang et al _cite_ propose an attribute-constrained face recognition model for joint facial attributes prediction and face recognition. In this model, the parameters of the network are first updated for attributes prediction and then same network is fine-tuned for face recognition. While Ranjan et al _cite_ add other face related tasks to improve overall performance. Their model is a single multi-task CNN network for simultaneous face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and face recognition. Facial attributes as semantic features can be predicted from face images directly, or from other facial attributes indirectly _cite_ . Attribute prediction methods are generally classified into local or global approaches. Local methods consist of three steps; first they detect different parts of the object and then extract features from each part. Finally, these features are concatenated to train a classifier _cite_ . For example, Kumar et al's method _cite_ is based on extracting hand-crafted features from ten facial parts. Zhang et al _cite_ extract poselets aligning face parts to predict facial attributes. This method works improperly if object localization and alignment are not perfect. Global approaches, however, extract features from entire image disregarding object parts and then train a classifier on extracted features; these methods perform improperly if large face variations such as occlusion, pose and lighting are present in the image _cite_ . Attribute prediction has been improved in recent years. Bourdev et al _cite_ propose a part-based attribute prediction method which deploys semantic segmentation in order to transfer localization information from the auxiliary task of semantic face parsing to the facial attribute prediction task. Liu et al _cite_ use two cascaded CNNs; the first of which, LNet, is used for face localization, while the second, ANet, is used for attribute description. Zhong et al _cite_ first localize face images and then use an off-the-shelf architecture designed for face recognition to describe face attributes at different levels of a CNN. He et al _cite_ propose a multi-task framework for relative attribute prediction. The method uses a CNN to learn local context and global style information from the intermediate convolution and fully connected layers, respectively. Our network is inspired by multi-task network but we fuse the output of the attribute predictor into the face recognition layers which makes it different from other existing multi-task methods such as Wang et al's _cite_ approach. Our deep CNN model is constructed from two cascaded networks in which the final one consists of two branches, each of which are used for facial attribute prediction and face identification, respectively. Both these two branches communicate information together by sharing parameters of the first network in the model as well as fusing attribute branch with the last pooling layer of the face identification branch. In our model, all the parameters (i.e. the parameters of the two cascaded networks) are updated simultaneously in each training step. The Contributions of our work are summarized as follows: N) We design a new end to end CNN architecture that learns to predict facial attributes while simultaneously being trained with the objective of face identification. Our model shares learned parameters to train both tasks and also fuses attribute information and the face modality to improve face identification performance. N) Contrary to the existing multi-task methods that only use a shared CNN feature space to train these two tasks jointly, our model uses a feature level fusion approach to leverage facial attributes for improving face identification performance. Furthermore, we observe that our jointly trained network is a more capable face attribute predictor than one trained on facial attributes alone. The rest of this paper is organized as follows: The CNN architecture is described in section N, fusion of attribute and face modalities is described in section N, model training parameters are described in section N, and finally, results and concluding remarks are provided in sections N and N, respectively.