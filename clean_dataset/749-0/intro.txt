Deep residual networks are built by stacking multiple . Remarkable success has been achieved by deep residual networks for image segmentation, object localization, . The effectiveness of residual units is attributed to their adopted and . have explained the importance of the identity mapping in alleviating optimization difficulty. In this work, we focus on the residual functions. By analyzing various designs of residual functions, we propose a novel architecture with higher parameter efficiency that provides stronger learning capacity. When the residual network~ was first proposed, the residual function was designed as a three-layer bottleneck architecture consisting of N _inline_eq_ N, N _inline_eq_ N and N _inline_eq_ N convolutional filters per layer. The second layer has a less number of channels than the other two convolutional layers. The motivation behind such design is to increase the parameter efficiency by performing the complex N _inline_eq_ N convolution operations in a lower dimension space. Since then the residual function has been improved and developed into several different variations. proposed a Wide Residual Network (WDN), which increases the number of channels in the second N _inline_eq_ N convolutional layer. They found that WDN outperforms the ResNet-N model with N times fewer layers and offers significantly faster speed with roughly the same model size. Recently, proposed to divide the second N _inline_eq_ N convolution layer into several groups while keeping the number of parameters almost unchanged. The motivation is to enhance the parameter efficiency by increasing the learning capacity of each bottleneck-shape residual function using transformations aggregated from different paths. Besides, several works~ _cite_ introduce delicately designed inception architectures into the residual functions and build complex network topology structures with a less number of parameters. However, these inception-style residual functions lack modularity and contain many factors that require expertise knowledge to design. In this work, we focus on analyzing the various residual functions proposed in ~ that are highly modularized and widely used in different applications. For the first time, our analysis reveals that all of the aforementioned residual functions (that induce different network models) can be unified by viewing them through the lens of tensor analysis~ \textemdash~or more concretely a based on the conventional Block Term Decomposition~ _cite_ . With such tensor decomposition, a high order tensor operator (, a set of convolutional kernels operators) is decomposed by a summation of multiple low-rank Tucker operators. Varying the rank of the Tuckers instantiates different residual functions as mentioned above. Based on this new explanation on residual functions, we further propose a Collective Residual Unit (CRU) architecture that enables cross-layer knowledge sharing for different residual units through, illustrated in Figure~ _ref_ . With such a novel residual function induced unit, information from one residual unit can be reused when building others, leading to significant enhancement of the parameter efficiency in residual networks. We perform extensive experiments on the ImageNet and PlaceN datasets to compare the performance of residual networks built upon our proposed CRU and existing residual units. The results clearly verify the outstanding parameter efficiency of our proposed CRU architecture. The main contributions can be summarized as follows: N) We introduce a new perspective for explaining and understanding the popular convolutional residual networks and unify existing variants of residual functions into a single framework. N) Based on the analysis, we propose a novel Collective Residual Unite (CRU) which presents higher parameter efficiency compared with existing ResNet based models. N) Our proposed CRU Network achieves state-of-the-art performance on two large-scale benchmark datasets, This confirms sharing knowledge across the convolutional layers is promising for pushing the learning capacity and parameter efficiency of state-of-the-art network architectures.