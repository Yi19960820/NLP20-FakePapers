Autonomous urban driving is a long-studied and still under-explored task~ _cite_ particularly in the crowded urban environments~ _cite_ . A desirable system is required to be capable of solving all visual perception tasks (e.g. object and lane localization, drivable paths) and determining long-term driving strategies, referred as ``driving policy". Although visual perception tasks have been well studied by resorting to supervised learning on large-scale datasets~ _cite_, simplistic driving policies by manually designed rules in the modular pipeline is far from sufficient for handling diverse real-world cases as discussed in~ _cite_ . Learning a optimal driving policy that mimics human drivers is less explored but key to navigate in complex environments that requires understanding of multi-agent dynamics, prescriptive traffic rule, negotiation skills for taking left and right turns, and unstructured roadways. These challenges naturally lead people to machine learning approaches for discovering rich and robust planning strategies automatically. A line of researches~ _cite_ for learning policies follow the end-to-end imitation learning that directly maps sensor inputs to vehicle control commands via supervised training on large amounts of human driving data. However, these systems cannot be generalized to unseen scenarios and their performances are severely limited by the coverage of human driving data. For example, the model of Bojarski et al.~ _cite_ trained for road following fails for turning right/left. Moreover, it is difficult to pose autonomous driving with long-term goal-oriented navigation as a supervised learning problem as the autonomous vehicle needs to heavily interact with the environment including other vehicles, pedestrians and roadways. It is thus desirable to have a richer control policy which considers a large amount of feedbacks from the environment including self-states, collisions and off-road conditions for autonomous driving. Deep reinforcement Learning (RL) offers, in principle, a reasonable system to learn such policies from exploration~ _cite_ . However, the amount of exploration required for large action space (such as a sequence of continuous steer angles, brakes and speeds) has prohibited its use in real applications, leading to unsatisfactory results by recent efforts on RL-based driving policy learning~ _cite_ in complex real-world tasks. In this paper, we resolve this challenging planning task with our novel Controllable Imitative Reinforcement Learning (CIRL) that facilitates the continuous controllable deep-RL by exploiting the knowledge learned from demonstrations of human experts. The whole architecture is illustrated in Fig.~ _ref_ . Our CIRL is based on the Deep Deterministic Policy Gradient (DDPG) ~ _cite_ that is an off-policy replay-memory-based actor-critic algorithm. The conventional DDPG often falls into local optimal due to too much failed explorations for large action space. Our CIRL solves this issue by providing better exploration seeds for the search over the action space of the actor networks. Specifically, the actor networks are first warmed up by learned knowledge via imitation learning using human demonstrations in order to initialize the action exploration in a reasonable space. Then our CIRL incorporates DDPG to gradually boost the generalization capability of the learned driving policy guided by continuous reward signals sent back from the environment. Furthermore, to support the goal-oriented navigation, we introduce a controllable gating mechanism to selectively activate different branches for four distinct control signals (i.e. follow, straight, turn right, turn left) . Such gating mechanism not only allows the model to be controllable by a central planner or the drivers' intent, but also enhances the model's capability by providing tailored policy functions and reward designs for each command case. In addition, distinct abnormal steer angle rewards are further proposed to better guide policies of each control signal as auxiliary aggregated rewards. Our key contributions can be summarized as: N) we present the first successful deep-RL pipeline for vision-based autonomous driving that outperforms previous modular pipeline and other imitation learning on diverse driving tasks on the high-fidelity CARLA benchmark; N) we propose a novel controllable imitative reinforcement learning approach that effectively alleviates the inefficient exploration of large-scale continuous action space; N) a controllable gating mechanism is introduced to allow models be controllable and learn specialized policies for each control signal with the guidance of distinct abnormal steer-angle rewards; N) comprehensive results on public CARLA benchmark demonstrates our CIRL achieves state-of-the-art performance on a variety of driving scenarios and superior generalization capability by applying the same agent into unseen environments. More successfully driving videos are presented in Supplementary.