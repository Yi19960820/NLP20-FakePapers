Recent years have witnessed significant progress in object detection using deep convolutional neutral networks (CNNs) ~ _cite_ . The state-of-the-art object detection methods~ _cite_ mostly follow the paradigm since it is established in the seminal work R-CNN~ _cite_ . Given a sparse set of region proposals, object classification and bounding box regression are performed on each proposal . A post-processing step, non-maximum suppression (NMS), is then applied to remove duplicate detections. It has been well recognized in the vision community for years that contextual information, or between objects, helps object recognition~ _cite_ . Most such works are before the prevalence of deep learning. During the deep learning era, there is no significant progress about exploiting object relation for detection learning. Most methods still focus on recognizing objects separately. One reason is that object-object relation is hard to model. The objects are at arbitrary image locations, of different scales, within different categories, and their number may vary across different images. The modern CNN based methods mostly have a simple regular network structure~ _cite_ . It is unclear how to accommodate above irregularities in existing methods. Our approach is motivated by the success of attention modules in natural language processing field~ _cite_ . An attention module can effect an individual element (\eg, a word in the target sentence in machine translation) by aggregating information (or features) from a set of elements (\eg, all words in the source sentence) . The aggregation weights are automatically learnt, driven by the task goal. An attention module can model dependency between the elements, without making excessive assumptions on their locations and feature distributions. Recently, attention modules have been successfully applied in vision problems such as image captioning~ _cite_ . In this work, for the first time we propose an adapted attention module for object detection. It is built upon a basic attention module. An apparent distinction is that the primitive elements are objects instead of words. The objects have ND spatial arrangement and variations in scale/aspect ratio. Their locations, or geometric features in a general sense, play a more complex and important role than the word location in an ND sentence. Accordingly, the proposed module extends the original attention weight into two components: the original weight and a new geometric weight. The latter models the spatial relationships between objects and only considers the between them, making the module, a desirable property for object recognition. The new geometric weight proves important in our experiments. The module is called . It shares the same advantages of an attention module. It takes variable number of inputs, runs in parallel (as opposed to sequential relation modeling~ _cite_), is fully differentiable and is in-place (no dimension change between input and output) . As a result, it serves as a basic building block that is usable in any architecture flexibly. Specifically, it is applied to several state-of-the-art object detection architectures~ _cite_ and show consistent improvement. As illustrated in Figure~ _ref_, it is applied to improve the step and learn the step (see Section~ _ref_ for details) . For instance recognition, the relation module enables joint reasoning of all objects and improves recognition accuracy (Section~ _ref_) . For duplicate removal, the traditional NMS method is replaced and improved by a lightweight relation network (Section~ _ref_), resulting in (Section~ _ref_), to our best knowledge. In principle, our approach is fundamentally different from and would complement most (if not all) CNN based object detection methods. It exploits a new dimension: . The object relation module is general and not limited to object detection. We do not see any reason preventing it from finding broader applications in vision tasks, such as instance segmentation~ _cite_, action recognition~ _cite_, object relationship detection~ _cite_, caption~ _cite_, VQA~ _cite_, etc. Code is available at _url_ .