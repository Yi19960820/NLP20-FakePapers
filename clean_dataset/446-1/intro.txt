We consider the problem of approximating a binary matrix _inline_eq_ as the product of two other binary matrices _inline_eq_ and _inline_eq_ plus a third matrix _inline_eq_, \begRevTwo The problem of Binary Matrix Factorization (BMF) arises naturally in many problems, in particular in the so called ``association matrices'' where a _inline_eq_ indicates that some object _inline_eq_ belongs to group _inline_eq_ ~ _cite_, or some entity (e.g., a gene) _inline_eq_ is present in some species _inline_eq_ ~ _cite_, or are related to some type of disease~ _cite_ . The latter examples show the increasing relevance of this type of data in genomics. Other similar, growing applications include recommender systems (client-product preferences, etc.) . \endRevTwo The BMF problem dates back to at least the Ns~ _cite_ and has been treated extensively in the last three decades by various research communities, under quite different names. It was first studied as a combinatorial problem as a particular case of the classic problem (see~ _cite_ and references therein) . It then received great attention from the data mining community. \begRevTwo Long known to be an NP-hard problem~ _cite_ \endRevTwo, the earlier works in this field developed heuristics such as the or methods, where binary matrices are decomposed as Boolean or modulo-N superpositions of rectangular tiles~ _cite_ ; the BMF problem was later formulated as a matrix factorization problem in~ _cite_, with several works following that line since then. \begRevTwo Being an NP-hard problem, the quality of the decompositions _inline_eq_ is commonly measured in terms of their, that is whether the columns of _inline_eq_ and _inline_eq_ exhibit patterns that are intuitive in some sense, or reflect knowledge about the problem. For example, in data mining problems, where the pairs _inline_eq_ are interpreted as (for example, a group of clients--indicated by non-zeros in _inline_eq_--prefers a certain group of products--indicated by non-zeros in _inline_eq_) ; the examples in this paper are designed to show this interpretability in a visual way, by studying the results on visual patterns obtained on sets of images. \endRevTwo A thorough survey of BMF methods is beyond the scope of this paper; we refer the reader to~ _cite_ for a more in-depth review. We will however mention some works which are representative of the diversity of formulations and tools that surround the treatment of this problem, as well as the shortcomings that are common to the current state of the art and that motivate the development of the tools that we present in this work. We begin with the ASSO algorithm proposed in~ _cite_ . The method begins by constructing the so called _inline_eq_, which is a thresholded version of a particular normalization of the correlation matrix _inline_eq_ . It then produces a series of increasing rank approximations by adding to _inline_eq_ a column taken from _inline_eq_ and searching for a corresponding binary row _inline_eq_ whose outer product with _inline_eq_ minimizes the number of non-zeros in the current approximation error _inline_eq_ . This method can be efficiently implemented with bitwise and integer operations. It is also a popular and simple method with good performance. However, the complexity of each ASSO step is _inline_eq_, so that it does not scale well in applications where _inline_eq_ (the number of samples) is very large, something very common in current data science problems, which is the target of our work. \begRevTwo Many works approximate the BMF problem by a relaxed (non-convex) Non-negative Matrix Factorization (NMF) problem where _inline_eq_ and _inline_eq_ are allowed to take on real values, and then map the resulting (approximate) solution to the binary domain using some predefined rule. Examples of this are~ _cite_ . In particular, the work~ _cite_ develops a set of BMF conditions, that is, conditions under which the binary factorization of _inline_eq_ is unique (up to permutations) . However, as in~ _cite_, their solution is an approximation based on a local minima of the NMF problem, so there are no guarantees that the binarized pair _inline_eq_ obtained coincides with the unique solution even if the identifiability conditions are satisfied. Being based on non-linear optimization, the NMF methods are significantly more computationally demanding than binary methods such as ASSO. \endRevTwo The work~ _cite_ stands out as an interesting alternative to BMF which formulates the decomposition of _inline_eq_ as a Bayesian denoising problem with a particular prior on the unobserved matrix _inline_eq_ (_inline_eq_) and uses a algorithm to find the maximum a posteriori estimation of _inline_eq_ . Message Passing is a mature technique which in the form presented in this work can be quite computationally demanding. Approximate Message Passing _cite_ techniques have since been developed which may provide significant efficiency gains to the technique proposed in~ _cite_, but we are currently unaware of any development in this direction. An example of a tile-searching heuristic is the Proximus method~ _cite_, which approximates the first principal left and right binary components of the binary matrix _inline_eq_ . The method can be extended to produce a hierarchical representation of the matrix with further rank-_inline_eq_ components, although these do not coincide with additional factors in a rank-_inline_eq_ factorization. Contrary to the above methods, the focus of Proximus is on speed an scalability, and indeed is much faster and scales better than any of the above methods. Also, as we will see in the next subsection, finding the first principal component of a binary matrix is closely related to a crucial step in one of the main dictionary learning methods. e will describe this method in detail later in this document. \begRevTwo methods were first introduced in _cite_ and later adopted as a powerful extension of the concept, ubiquitous in signal processing since the introduction of Fourier analysis and their related discrete variants (DFT, DCT) . In this setting, the matrix _inline_eq_ consists of _inline_eq_ columns of dimension _inline_eq_, and the decomposition obtained _inline_eq_ is comprised of a _inline_eq_, a matrix of _inline_eq_, plus a residual matrix _inline_eq_ . Dictionary learning methods are adaptive: given sufficient data samples, they can be trained to efficiently represent such samples as a linear combination of very few basis elements or ``atoms'' (see~ _cite_ for a review) . Despite coming from a very different community, dictionary learning methods can be seen as matrix factorization methods which are specially tailored to the where _inline_eq_ is either extremely ``fat'' (_inline_eq_) or ``tall'' (_inline_eq_) . Furthermore, many of these methods can be implemented on-line, that is, they can process new data samples as they arrive, and adapt the dictionary along the way~ _cite_ . Another important aspect of dictionary learning methods is that they are not restricted to low-rank decompositions; if fact, the solutions can be, meaning that the number of columns _inline_eq_ in _inline_eq_ may be (much) larger than _inline_eq_, the dimension of the samples to be represented. As with all BMF methods, dictionary learning problems are non-convex and their solution cannot be obtained exactly. Nevertheless, similarly to what happens with BMF, their enormous practical success in a wide range of signal processing and machine learning problems, and their ability to produce human-interpretable patterns, has led to their widespread adoption. \endRevTwo As with any statistical model, the problem of model selection, (in this case, choosing _inline_eq_) is of paramount importance to BMF. Various works~ _cite_ have addressed this particular problem. In particular, ~ _cite_ and~ _cite_ are based on the Minimum Description Length (MDL) principle~ _cite_, which forms the basis of our model selection strategy as well. As a side note, the work~ _cite_ represents a recent example of the tiling approach. This problem has also been addressed in dictionary learning in~ _cite_ . \begRevTwo To the best of our knowledge, the works in the existing literature on BMF make no particular assumptions on the of the matrices to be decomposed. In particular, many methods which are efficient for the _inline_eq_ case do not scale well for _inline_eq_ and vice versa. Also, most methods deal with the analysis of readily available matrices, which makes them unsuitable to many recent data processing tasks involving adaptation of the models. The main motivation behind this work is the ability of dictionary learning methods to cope with the challenges mentioned above. We propose two dictionary learning-based BMF methods which are particularly suited to the treatment of extremely fat (or tall) matrices, and which are also suitable to online processing of samples. The first one, named Method of Binary Directions (MOB), is an adaptation of the method proposed in~ _cite_, itself an adaptation of the Method of Optimum Directions (MOD) ~ _cite_ (hence the name) . The second method is based on the idea of sequential rank-one updates of the K-SVD method~ _cite_ ; we adopt the Proximus~ _cite_ algorithm as a fast approximation to the mentioned operation; we thus bring together tools from two inherently related but otherwise disconnected fields to obtain a novel formulation to the binary matrix factorization. Finally, both methods rely on a third novel algorithm which we call Binary Matching Pursuit (BMP) . This is a binary adaptation of the Matching Pursuit method~ _cite_ for approximating the sparsest solution to a least squares regression problem. This adaptation, although conceptually analogous to MP, is non-trivial, as its efficiency relies on a careful combination of different algebraic operations. \endRevTwo Our methods construct binary dictionaries and binary coefficients matrices using efficient bitwise and a few integer operations. This is particularly relevant to the efficiency of our methods as recent processor architectures incorporate the ability to handle large number of bits through SIMD (Single Instruction Multiple Data) instructions. For example, a current off-the-shelf processor can perform a instruction (which counts the number of _inline_eq_ s in a binary array) on a N-bit register. This allows, for example, to compute the dot product between two binary vectors of dimension _inline_eq_ with just two processor instructions. \begRevTwo As we show in Section~ _ref_, our methods are also linear both in _inline_eq_ and _inline_eq_ and thus scale well for large matrices. \endRevTwo \begRevTwo The main objectives of this paper are to present our proposed methods, assess their interpretability on different datasets, to analyze their computational properties such as computational complexity and convergence rate, and to see how these properties are affected by the initial conditions. We thus focus our experiments on a small set of easily-interpretable datasets for which the patterns obtained can be easily recognized as salient features in the data. \endRevTwo The rest of this document is organized as follows: Section~ _ref_ provides the notation and background on the methods on which our methods are based. The proposed methods themselves are described in Section~ _ref_ . Section~ _ref_ describes the proposed MDL-based model selection algorithm for searching the best model order _inline_eq_ . We present and discuss our results in Section~ _ref_, and provide concluding remarks in Section~ _ref_ .