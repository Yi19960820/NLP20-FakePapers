Semantic segmentation for urban scenes is an important yet challenging task for a variety of vision-based applications, including autonomous driving cars, smart surveillance systems, etc. With the success of convolutional neural networks (CNNs), numerous successful fully-supervised semantic segmentation solutions have been proposed in recent years _cite_ . To achieve satisfactory performance, these methods demand a sufficiently large dataset with pixel-level labels for training. However, creating such large datasets is prohibitively expensive as it requires human annotators to accurately trace segment boundaries. Furthermore, it is difficult to collect traffic scene images with sufficient variations in terms of lighting conditions, weather, city and driving routes. To overcome the above-mentioned limitations, one can utilize the modern urban scene simulators to automatically generate a large amount of synthetic images with pixel-level labels. However, this introduces another problem, distributions mismatch between the source domain (synthesized data) and the target domain (real data) . Even if we synthesize images with the state-of-the-art simulators _cite_, there still exists visible appearance discrepancy between these two domains. The testing performance in the target domain using the network trained solely by the source domain images is severely degraded. The domain adaptation (DA) technique is developed to bridge this gap. It is a special example of transfer learning that leverages labeled data in the source domain to learn a robust classifier for unlabeled data in the target domain. DA methods for object classification have several challenges such as shifts in lighting and variations in object's appearance and pose. There are even more challenges in DA methods for semantic segmentation because of variations in the scene layout, object scales and class distributions in images. Many successful domain-alignment-based methods work for DA-based classification but not for DA-based segmentation. Since it is not clear what comprises data instances in a deep segmenter _cite_, DA-based segmentation is still far from its maturity. In this work, we propose a novel fully convolutional tri-branch network (FCTN) to solve the DA-based segmentation problem. In the FCTN, two labeling branches are used to generate pseudo segmentation ground-truth for unlabeled target samples while the third branch learns from these pseudo-labeled target samples. An alternating re-labeling and re-training mechanism is designed to improve the DA performance in a curriculum learning fashion. We evaluate the proposed method using large-scale synthesized-to-real urban scene datasets and demonstrate substantial improvement over the baseline network and other benchmarking methods.