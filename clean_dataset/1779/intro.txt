Despite their prevalent use, the effects of Batch Normalization (BN) _cite_ in Generative Adversarial Networks (GAN) have not been examined carefully. Popularized by the influential DCGAN architecture, the use of BN in GANs is typically justified by its perceived training speedup and stability, but the generated samples often suffer from visual artifacts and limited variations (mode collapse) . The lack of evidence that BN always improves GAN training is partly due to the unavailability of quality measures for GAN models. Being puzzled by this technique, we propose a methodical evaluation of GAN models and assess their abilities to generate large variations of samples (mode coverage) . The idea is to hold out a portion of the dataset as a test dataset and try to find the latent code that generates the closest approximation to these test images. For each test image, we optimize for the latent code by gradient descent for a fixed number of iterations. The average squared Euclidean distance between the test samples and the reconstructed ones is used as a measure of the quality of GANs. Our experiments show that the reconstruction error correlates with the visual quality of the generated samples, and while still time consuming, this approach is more efficient than existing log-likelihood-based evaluation methods. Our evaluation technique is therefore convenient for monitoring the progress during training. We show that BN generally accelerates training in early stages, and can increase the success rate of GAN training for certain datasets and network structures where a model without any normalization could often fail. In many cases though, BN can cause the stability and generalization power of the model to decrease drastically. Following the work of Salimans and Kingma and Arpit \etal, we introduce a modified Weight Normalization (WN) technique for GAN training. Using the same sets of experiments, we found that our WN approach can achieve faster and more stable training than BN, as well as generate equal or higher quality samples than GAN models without normalization. We believe that our proposed WN technique is superior than BN in the context of GANs.