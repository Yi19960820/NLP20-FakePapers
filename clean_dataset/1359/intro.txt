visual environment contains much more information than we are able to perceive at once. Attention allows us to select the most relevant parts of the visual scene and bring the high-resolution part of the retina, the fovea, onto them. The modeling of visual attention has been the topic of numerous studies in many different research fields, from neurosciences to computer vision. This interdisciplinary interest led to the publication of a large number of computational models of attention, with many different approaches, see _cite_ for an exhaustive review. In fact, being able to predict the most salient regions in a scene leads to a wide range of applications, like image segmentation _cite_, image quality assessment _cite_, image and video compression _cite_, image re-targeting _cite_, video summarization _cite_, object detection _cite_ and recognition _cite_, robot navigation _cite_, human-robot interaction _cite_, retinal prostheses _cite_, tumours identification in X-rays images _cite_ . Determining which location in a scene will capture attention-and hence the gaze of observers-requires finding the most salient subset of the input visual stimuli. Saliency relies on both stimulus-driven (bottom-up) and observer or task-related (top-down) features. Most visual attention models focus on bottom-up processes and are guided by the Feature Integration Theory (FIT) proposed by Treisman and Gelade _cite_ . They decompose a visual stimulus into several feature maps dedicated to specific visual features (such as orientations, spatial frequencies, intensity, luminance, contrast) _cite_ . In each map, the spatial locations that locally differ from their surroundings are emphasized (conspicuity maps) . Then, maps are combined into a master saliency map that points out regions the most likely to attract the visual attention, and the gaze, of observers. The visual features used in a model play a key role as they strongly affect its prediction performance. Over the years, authors refined their models by adding different features, such as center-bias _cite_, faces _cite_, text _cite_, depth _cite_, contextual information _cite_, or sound _cite_ . Other approaches, moving away from the FIT, have also been proposed. They often rely on machine-learning or statistical modeling techniques such as graphs _cite_, conditional random fields _cite_, Markov models _cite_, multiple kernel learning _cite_, adaptive boosting _cite_, Bayesian modeling _cite_, or information theory _cite_ . Major problems arise when attempting to merge feature maps having different dynamic ranges, stemming from different visual dimensions. Various fusion schemes have been used. In Fig. _ref_, we propose a classification of these methods according to four main approaches: image processing (eventually relying on cognitive priors), statistical modeling, machine-learning and learning from eye data. Note that some methods might belong to two classes, like "statistical modeling" and "learning from eye data": statistical models can be used to learn the feature map weights that best fit eye-tracking data _cite_ . In the following, we focus on schemes involving a weighted linear combination of feature maps: with S the master saliency map, _inline_eq_ the maps of K features extracted from the image or video frame being processed, and _inline_eq_ their associated weights. S and M are two-dimensional maps corresponding to the spatial dimensions of the visual scene. Non-linear fusion schemes based on machine-learning techniques such as multiple kernel learning can be efficient, but they often suffer from being used as a "black box". On the other hand, linear combinations are easier to interpret: the greater the weight, the more important the corresponding feature map in the master saliency map. Moreover, such linear fusion is biologically plausible _cite_ . In this linear context, we review two main approaches to determine the weights allocated to different feature maps. The first approach is based on priors (psychovisual or image-based), while the second is totally data-driven: the feature weights are learned to fit eye-tracking data. Itti et al. proposed an efficient normalization scheme that had been taken up many times _cite_ . First, all feature maps are normalized to the same dynamic range. Second, to promote feature maps having a sparse distribution of saliency and to penalize the ones having a more uniform spatial distribution, feature map regions are weighted by their local maximum. Finally, feature maps are simply averaged. Marat et al. apply the same normalization, and use a fusion taking advantage of the characteristics of the static (luminance) and the dynamic (motion) saliency maps _cite_ . Their weights are respectively equal to the maximum of the static saliency map and to the skewness of the dynamic saliency map. A similar approach is adopted in _cite_, where authors use a spatial competition mechanism based on the squared ratio of global maximum over average local maximum. This promotes feature maps with one conspicuous location to the detriment of maps presenting numerous conspicuous locations. Then feature maps are simply averaged. In _cite_, an analogous approach is proposed: each feature map is weighted by _inline_eq_, with m the number of local maxima that exceed a given threshold. In _cite_, authors propose a refinement of the normalization scheme introduced by Milanese et al. in _cite_ . First, the dynamic range of each feature map is normalized by using the theoretical maximum of the considered feature. The saliency map is obtained by a sum of maps representing the inter and intra-feature competition. Other saliency models simply weight feature maps with coefficients determined by testing various values, and keep the best empirical weights _cite_ . Visual saliency models are often evaluated by comparing their outputs against the regions of the scene actually looked at by humans during an eye-tracking experiment. In some cases they can be very efficient, especially when attention is mostly driven by bottom-up processes. However, visual exploration is not only shaped by bottom-up visual features, but is also heavily determined by numerous viewing behavioral biases, or systematic tendencies _cite_ . Some models solely based on viewing biases, i.e. blind to any visual information from the input visual scene have been even shown to outperform state-of-the-art saliency models _cite_ . For instance, observers tend to look more at the center of a scene rather than at the edges; this tendency is known as the center bias _cite_ . Some authors introduced this bias in their models through a static and centered ND Gaussian map, leading to significant improvements _cite_ . Many eye-tracking experiments have shown that the center bias is time-dependent: stronger at the beginning of an exploration than at the end _cite_ . Moreover, several studies have pointed out that, in many contexts, top-down factors such as semantic or task clearly take the precedence over bottom-up factors to explain gaze behavior _cite_ . Visual exploration also relies on many individual characteristics such as personality or culture _cite_ . Thus, time-independent fusion schemes only considering the visual features of the input often have a hard time accounting for the multivariate and stochastic nature of human exploration strategies _cite_ . To solve this issue, a few authors proposed to build models in a supervised manner, by learning a direct mapping from feature maps to the eye positions of several observers. Feature maps could encompass classic low-level visual features as well as maps representing viewing behavioral tendencies such as the center bias. The earliest learning-based approach was introduced by Peters \& Itti in _cite_ . They used linear least square regression with constraints to learn the weights of feature maps from eye positions. Formally, let _inline_eq_ be a set of K feature maps, _inline_eq_ the corresponding vector of weights and Y an eye position density map, represented as the recorded fixations convolved with an isotropic Gaussian kernel. The least square (LS) method estimates the weights _inline_eq_ by solving This method is repeated with success in _cite_ and _cite_ . Another method to learn the weights _inline_eq_ to linearly combine the visual feature maps is the Expectation-Maximisation algorithm _cite_ . It has first been applied in _cite_, and taken over in _cite_ . First, the eye position density map Y and the feature maps M are converted into probability density functions. After initializing the weights _inline_eq_, the following steps are repeated until convergence. Expectation: the current model (i.e. the current _inline_eq_) is hypothesized to be correct, and the expectation of the model likelihood is computed (via the eye position data) . Maximization: _inline_eq_ are updated to maximize the value found at the previous step. To be exhaustive, let us mention some other models that do not use a weighted linear combination of feature maps, but that are still trained on eye-tracking data. In _cite_, a saliency model is learnt from eye movements using a support vector machine (SVM) . In _cite_, the authors refine a region-based attention model with eye-tracking data using a genetic algorithm. Finally, Zhong et al. first use a Markov chain to model the relationship between the image feature and the saliency, and then train a support vector regression (SVR) from eye-tracking data to predict the transition probabilities of the Markov chain _cite_ .