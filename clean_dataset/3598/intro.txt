Comparing patches across images is probably one of the most fundamental tasks in computer vision and image analysis. It is often used as a subroutine that plays an important role in a wide variety of vision tasks. These can range from low-level tasks such as structure from motion, wide baseline matching, building panoramas, and image super-resolution, up to higher-level tasks such as object recognition, image retrieval, and classification of object categories, to mention a few characteristic examples. Of course, the problem of deciding if two patches correspond to each other or not is quite challenging as there exist far too many factors that affect the final appearance of an image _cite_ . These can include changes in viewpoint, variations in the overall illumination of a scene, occlusions, shading, differences in camera settings, \etc. In fact, this need of comparing patches has given rise to the development of many hand-designed feature descriptors over the past years, including SIFT _cite_, that had a huge impact in the computer vision community. Yet, such manually designed descriptors may be unable to take into account in an optimal manner all of the aforementioned factors that determine the appearance of a patch. On the other hand, nowadays one can easily gain access to (or even generate using available software) large datasets that contain patch correspondences between images _cite_ . This begs the following question: can we make proper use of such datasets to automatically learn a similarity function for image patches ? \ifdefined \USEIMAGES \fi The goal of this paper is to affirmatively address the above question. Our aim is thus to be able to generate a patch similarity function, \ie, without attempting to use any manually designed features but instead directly learn this function from annotated pairs of raw image patches. To that end, inspired also by the recent advances in neural architectures and deep learning, we choose to represent such a function in terms of a deep convolutional neural network _cite_ (Fig.~ _ref_) . In doing so, we are also interested in addressing the issue of what network architecture should be best used in a task like this. We thus explore and propose various types of networks, having architectures that exhibit different trade-offs and advantages. In all cases, to train these networks, we are using as sole input a large database that contains pairs of raw image patches (both matching and non-matching) . This allows to further improve the performance of our method simply by enriching this database with more samples (as software for automatically generating such samples is readily available _cite_) . To conclude this section, the paper's main contributions are as follows: (i) We learn directly from image data (\ie, without any manually-designed features) a general similarity function for patches that can implicitly take into account various types of transformations and effects (due to \eg, a wide baseline, illumination, \etc) . (ii) We explore and propose a variety of different neural network models adapted for representing such a function, highlighting at the same time network architectures that offer improved performance. as in _cite_ . (iii) We apply our approach on several problems and benchmark datasets, showing that it significantly outperforms the state-of-the-art and that it leads to feature descriptors with much better performance than manually designed descriptors (\eg, SIFT, DAISY) or other learnt descriptors as in _cite_ . Importantly, due to their convolutional nature, the resulting descriptors are very efficient to compute even in a dense manner.