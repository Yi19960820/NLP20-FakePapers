activity recognition in videos is an intensely researched area in computer vision due to its wide range of real-world applications in sports, health care, surveillance, robot vision, and human-computer interaction. Furthermore, the rapid growth of digital video data demands automatic classification and indexing of videos. Despite the increased interest, the state-of-the-art systems are still far from human-level performance, in contrast to the success in image classification _cite_ . This is partially due to the complex intra-class variations in videos, some obvious causes being the view point, background clutter, high dimensionality of data, lack of large datasets, and low resolution. Despite these reasons more-or-less affecting automatic image classification, it has been quite successful in recent years, largely owing to the rise of deep learning techniques. This is not the case in video classification, although deep learning is starting to be widely applied. Therefore, it is worthwhile investigating what is holding back video classification. In this study, we address three key areas: exploiting the underlying dynamics of sub-events for high-level action recognition, crafting self-explanatory, independent static and motion features---in which, motion features should capture micro-level actions of each actor or object independently, such as arm or leg movements---, and optimum fusing of static and motion features for better accuracy. A complex activity typically comprises several sub activities. The existing approaches try to classify a video treating it as a single, high-level activity _cite_ . As the action becomes complex, the behavior and the temporal evolution of its underlying sub-events become complicated. For example, cooking may involve sub-events: cutting, turning on the cooker, and stirring. It may not always preserve this same lower order for the same action class. Instead, they may contain a higher-order temporal relationship among them. For example, turning on the cooker and cutting may appear in reverse order in another video in the same class. Therefore, this temporal pattern of sub-events is not easy to capture through a simple time series analysis. These patterns can only be identified by observing many examples, using a system which has an infinite dynamic response. Therefore, it is important to model this higher-order temporal progression, and capture temporal dynamics of these sub-events for better recognition of complex activities. \IEEEpubidadjcol In contrast to image classification, the information contained in videos are not in a single domain. Both the motion patterns of the actors and objects, as well as the static information---such as, background and still objects that the actors interact with---are important for determining an action. For example, the body movements of a group of people fighting may closely relate to the body movements of a sports event, e.g., wrestling. In such a case, it is tough to distinguish between the two activities solely by looking at the motion patterns. Inspecting the background setting and objects is crucial in such a scenario. Therefore, it is necessary to engineer powerful features from both motion and static domains. In addition, these features must be complementary, and one feature domain should not influence the other domain. In other words, they should be self-explanatory, and mutually exclusive, as much as possible. Also, these motion features should be able to capture activities of each actor independently. If so, the distributional properties of these actions, over short and long durations, can be used to identify high level actions. In the proposed method, we craft static and motion features to have this property. Furthermore, how to optimally combine or fuse these motion and static descriptors remains a challenge for three key reasons: both static and motion information provide cues regarding an action in a video, the contribution ratio from each domain affects the final recognition accuracy, and optimum contribution ratio of static and motion information may depend on the richness of motion information in the video. The combined descriptor should contain the essence of both domains, and should not have a negative influence on each other. Recently, there has been attempts to answer this question _cite_, _cite_ . However, these existing methodologies lack the insight in to how much this ratio affects the final accuracy, and has no control over this ratio. One major requirement of the fusion method is to have a high-level intuitive interpretation on how much contribution each domain provides to the final descriptor, and can control level of contribution. This ability makes it possible to deeply investigate the optimum contribution of each domain for a better accuracy. In this study, we investigate this factor extensively. In this work, we focus on video activity recognition using both static and motion information. We address three problem areas: crafting mutually exclusive static and motion features, optimal fusion of the crafted features, and modeling temporal dynamics of sub-activities for high-level activity recognition. In order to examine the temporal evolution of sub-activities subsequently, we create video segments with constant overlapping durations. Afterwards, we combine static and motion descriptors to represent each segment. We propose motion tubes, a dense trajectory _cite_ based tracking mechanism, to identify and track candidate moving areas separately. This enables independent modeling of the activities in each moving area. In order to capture motion patterns, we use histogram oriented optic flows (HOOF) _cite_ inside motion tubes. Then we apply a bag-of-words (BoW) method on the generated features to capture the distributional properties of micro actions, such as body movements, and create high level, discriminative motion features. Inspired by the power of object recognition of convolutional neural networks (CNNs), we create a seven-layer deep CNN, and pre-train it on the popular ImageNet dataset _cite_ . Afterwards, we use this trained CNN to create deep features, to synthesize static descriptors. Then, using a computationally efficient, yet powerful, mathematical model, we fuse static and motion feature vectors. We propose three such novel methods in this paper: based on Cholesky decomposition, variance ratio of motion and static vectors, and principal components analysis (PCA) . The Cholesky decomposition based model provides the ability to precisely control the contribution of static and motion domains to the final fused descriptor. Using this intuition, we investigate the optimum contribution of each domain, experimentally. The variance ratio based method also provides us this ability, and additionally, lets us obtain the optimum contribution ratio mathematically. We show that the optimum contribution ratio obtained experimentally using the Cholesky based method, matches with the ratio obtained mathematically from the variance based method. Furthermore, we show that this optimum contribution may vary depending on the richness of motion information, and affects the final accuracy significantly. In order to model the temporal progression of sub events, we feed the fused vectors to a long short-term memory (LSTM) network. The LSTM network discovers the underlying temporal patterns of the sub events, and classifies high level actions. We feed the same vectors to a classifier which does not capture temporal dynamics to show that modeling temporal progression of sub events indeed contribute for a better accuracy. We carry out our experiments on the three popular action data sets, UCF-N _cite_, HollywoodN _cite_, and HMDBN _cite_ . The key contributions of this paper are as follows: With the proposed techniques we outperform the existing best results for the datasets UCF-N _cite_ and HollywoodN _cite_, and are on par for the dataset HMDBN _cite_ .