Driving involves different levels of scene understanding and decision making, ranging from detection and tracking of traffic participants, localization, scene recognition, risk assessment based on prediction and causal reasoning, to interaction. The performance of visual scene recognition tasks has been significantly boosted by recent advances of deep learning algorithms~ _cite_, and an increasing number of benchmark datasets~ _cite_ . However, to achieve an intelligent transportation system, we need a higher level understanding. Different vision-based datasets for autonomous driving~ _cite_ have been introduced and push forward the development of core algorithmic components. In core computer vision tasks, we have witnessed significant advances in object detection and semantic segmentation because of large scale annotated datasets~ _cite_ . Additionally, the Oxford RobotCar Dataset~ _cite_ addresses the challenges of robust localization and mapping under significantly different weather and lighting conditions. However, these datasets do not address many of the challenges in the higher level driving scene understanding. We believe detecting traffic participants and parsing scenes into the corresponding semantic categories is only the first step. Toward a complete driving scene understanding, we need to understand the interactions between human driver behaviors and the corresponding traffic scene situations~ _cite_ . To achieve the goal, we design and collect HDD with the explicit goal of learning how humans perform actions and interact with traffic participants. We collected N hours of real human driving in the San Francisco Bay Area using an instrumented vehicle. The recording consists of N sessions, and each session represents a navigation task performed by a driver. Further details about the dataset will be discussed in Section~ _ref_ . In each session, we decompose the corresponding navigation task into multiple predefined driver behaviors. Figure~ _ref_ illustrates the decomposition of a navigation task. The yellow trajectory indicates GPS positions of our instrumented vehicle. A N-layer annotation scheme is introduced to describe driver behaviors. The first layer is Goal-oriented action, colored green. In this example, the driver is making a left turn as shown in the upper left image of Figure~ _ref_ . The second layer is Stimulus-driven action, colored red and is shown in the lower left image. In this example, the driver makes a stop because of stopped car . The stop action corresponds to Stimulus-driven action layer and congestion belongs to Cause, which is designed to indicate the reason the vehicle makes a stop . The red bounding box localizes the Cause . While driving, human drivers are aware of surrounding traffic participants. We define the fourth layer called Attention, colored purple. In this example, the purple bounding box is used to indicate the traffic participant attended by the driver. Note that our annotation scheme is able to describe multiple scenarios happening simultaneously. In Figure~ _ref_, two different scenarios are illustrated. First, the driver intends to make a left turn but stops because of congestion. Second, the driver is making a U-turn while paying attention to a crossing pedestrian. A detailed description of our annotation methodology is presented in Section~ _ref_ . With the multimodal data and annotations, our dataset enables the following challenging and unique research directions and applications for intelligent transportation systems. First, detecting unevenly (but naturally) distributed driver behaviors in untrimmed videos is a challenging research problem. Second, interactions between drivers and traffic participants can be explored from cause and effect labels. Third, a multi-task learning framework for learning driving control can be explored. With the predefined driver behavior labels, the annotations can be used as an auxiliary task (i.e., classification of behavior labels) to improve the prediction of future driver actions. Fourth, a multimodal fusion for driver behavior detection can be studied. Learning how humans drive and interact with traffic scenes is a step toward intelligent transportation systems. In this paper, we start from a driver-centric view to describe driver behaviors. However, driving involves other aspects. Particularly, it involves predicting traffic participants' intentions and reasoning overall traffic situations for motion planning and decision making, which is not discussed in this work. Toward the goal of developing intelligent driving systems, a scalable approach for constructing a dataset is the next milestone.