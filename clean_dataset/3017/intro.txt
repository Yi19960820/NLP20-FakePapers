GB-D object detection attempts to localize and classify objects within an image with depth information. It is one of the core technologies in the field of robotics application and can be beneficial to many intelligent tasks, including pose estimation _cite_, content-based image retrieval _cite_ and robot task planning _cite_ . In recent years, the successful application of deep convolutional neural networks has pushed this research into a new phase and achieved very good results. Most CNN-based RGB-D object detection frameworks are extended from RCNN-based object detectors _cite_ for RGB images. R-CNN-Depth _cite_ is the first deep learning framework for RGB-D object detection that extends the R-CNN system~ _cite_ to take advantage of depth information by incorporating two parallel network streams for both RGB and depth modalities. This two-stream pipeline later became the basis for many visual perception tasks in RGB-D images _cite_ . In this framework, the features from the RGB and depth modalities are computed independently and concatenated after applying fully connected layers for final proposal classification. However, this pipeline has its own limitations: (N) ~Independent feature computation and simple feature concatenation ignore the correlation between the two modalities. (N) ~Only information inside the object proposal is used for object classification, which neglects the auxiliary role of context information outside the bounding box in object classification. In this paper, we propose a Cross Modal Attentional Context (CMAC) learning framework for RGB-D object detection that incorporates the consistency and complementary information between two diverse modalities~ (RGB and depth), as well as an attentional model for global context mining and discriminative object part discovery. To exploit the correlation between RGB and depth modalities, the CMAC model employs a cross-modal feature fusion component to fuse the features extracted from the output feature maps of the two fully convolutional networks~ (with different input sources) . Instead of directly applying fused features to classification and object location refinement, our proposed CMAC model further learns attentional context and explores discriminative object parts based on the fused features. We believe that both the attentional global context and the discriminative parts attended inside each possible object region~ (object proposal) are crucial for accurate RGB-D object detection. To capture the global context, our model employs a recurrent attention model that consists of multiple stacked Long Short-Term Memory (LSTM) units. The recurrent neural network is optimized to infer relevant regions for each given region proposal. As shown in Figure~ _ref_, the regions that are considered helpful for classification of the object proposal are highlighted. As can be seen, our proposed CMAC model can identify an adaptive global context for different object proposals~ (i.e., the regions of the keyboard, parts of the table around the target monitor as well as the other monitor are highlighted when the input region proposal contains a monitor. When the input region proposal contains a chair, the regions including parts of the table and other chairs are assigned higher weights in the final classification.) . Moreover, inspired by the fact that humans tend to quickly capture distinguishable parts for more accurate object classification judgment when observing objects with occluded regions, we propose to further incorporate a fine-grained object part attention module in our network framework. Considering the flexible attention mechanism and the excellent spatial manipulation ability of Spatial Transform Networks (STNs), we adopt multiple STNs in parallel to examine the discriminative parts located inside a specific object proposal for capturing local context. As illustrated in Figure~ _ref_, the CMAC model is able to successfully locate the most discriminative location that can differentiate an object's category~ (i.e., the main screen and the base of the monitors, as well as the back and legs of the chairs) . Acquiring such fine-grained object parts provides enhanced feature representations for region proposals. In summary, the main contributions of the proposed CMAC model can be listed as follows: