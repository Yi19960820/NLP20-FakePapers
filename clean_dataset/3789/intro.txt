There are a wide variety of applications where the ability to increase the resolution of an image adds to the user experience, from from surveillance and public security~ _cite_, business and entertainment~ _cite_ to remote sensing~ _cite_ . Single-image super resolution (SISR), the process of increasing the resolution of an image without additional information, has received significant attention~ as a result. Most early SISR methods focus on exploiting pixel statistics~ or the internal patch recurrence~ of HR images as priors. These methods typically do not generalise well, because even a small divergence between the properties of the real low-resolution image and the prior embodied in the heuristic causes visible artifacts in the reconstructed HR image. Recently, deep convolution neural network (DCNN) based learning methods~, have shown remarkable success in SISR, especially on some specific scaling factors (e.g., N-N) . Nevertheless, due to their very deep structures, these methods often exhibit significant memory and computing requirements, which necessitates powerful computational units (e.g., GPUs) thus limiting their application to the many real devices with limited computing power (and particularly hand-held devices including phones) . To address this problem, some efforts~ dedicate to customize specific lightweight network architectures. In this study, we revisit this problem in an orthogonal view and propose to develop an novel learning strategy to maximize the pixel-wise fitting capacity of a given lightweight architecture. To this end, we revisit the traditional training procedure for a SISR network, which seeks the optimal network parameters to minimize the average loss over all pixels in training images. Moreover, pixels of diffident reconstruction difficulty are mixed together to fed into the network for training. However, by doing this, complex pixels that are difficult to reconstruct will mislead the training procedure, which renders the network even failing to handle pixels that are easy to reconstruct, since the initial capacity of the lightweight network is very limited and vulnerable. This is similar to the cognitive process of human which is prone to be confused when starts with a compound of complex and easy tasks and considers them equally. For example, when receiving a compound of easy and hard words one time, a pupil may fail to remember those easy ones that should be well mastered. Alternatively, if he starts with some easy words and gradually attempts to remember more and more hard ones when these easy words have been well mastered, more words will be remembered. Therefore, the basic pattern of human cognitive process is to learn from easy to complex and gradually enhance the capacity of human. Recently, it has been empirically demonstrated that learning as such a paradigm can avoid bad local minima and generalize better~ . Therefore, it is promising to enhance the capacity of the lightweight SISR network with an appropriate easy-to-complex learning paradigm. Inspired by this, we present an adaptive importance learning scheme for SISR, which assigns importance (i.e., the probability of participating training and zero importance denotes removing the pixel during training) to each image pixel and dynamically updates the importance to control the network training following an easy-to-complex paradigm. To this end, we formulate the network training as well as the pixel-wise importance learning into a bi-convex optimization problem. With introducing a carefully designed importance penalty function, the importance of image pixels can be adaptively updated by solving a convex optimization problem. As a result, the importance is gradually increased according to the network reconstruction error on these pixels. By doing this, the network will start with pixels that are easy to reconstruct for training, and gradually be exposed to more and more complex pixels when its fitting capacity is enhanced. Furthermore, with the proposed importance learning scheme, the network can seamlessly assimilate the knowledge from a more powerful teacher network in the form of pixel importance initialization, which enables the network to generalize better. Through learning the network parameters and updating the pixel importance in an alternative way until convergence, the proposed learning scheme can obviously enhance the network capacity. With extensive experiments on four benchmark datasets and two seminal DCNN architectures for SISR, we demonstrate that the proposed adaptive importance learning scheme is able to enhance the performance of different scales of lightweight networks obviously. Moreover, due to not designing specific lightweight network architecture, it can be conveniently applied to any lightweight SISR networks for enhancement. In summary, this study mainly contributes in the following four aspects.