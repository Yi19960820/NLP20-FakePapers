Visual Question Answering (VQA) is an emerging interdisciplinary research field in the last few years, which attracts broad attention from both computer vision and natural language processing (NLP) communities. VQA aims to help computer automatically answer natural language question about an image. The question answers can be divided into the following types: yes/no answers, multi-choice answers, numeric answers, and open-ended word/phrase answers (for questions about what, where, who, ...) . VQA requires comprehensive image and language understanding, which constitutes a truly AI-complete task with similar spirit as Turing test _cite_ . Basically, VQA is formulated as a classification problem in most researches, in which images and questions are input, with answers as output categories (due to a limited number of possible answers) . As the VQA task was proposed after deep learning approaches had already gained wide popularity, almost all current VQA solutions use CNN to model image input and recurrent neural network (RNN) to model the question _cite_ . Attention mechanism has been heavily investigated in VQA. This includes visual attention~ _cite_ which focuses on handling the problem {where to look}, and question attention _cite_ which focuses on solving the problem {where to read} . As images and questions are two different modalities, it is straightforward to jointly embed two modalities together for a unified description of the image/question pair. Some works _cite_ even consider putting the attention mechanism and multi-modal joint embedding in one unified framework. However, VQA is significantly more complex than other vision and language tasks such as image captioning, since explicit information (middle-level recognition results such as objects, attributes, or even image captions, etc) are not enough for accurate VQA. For instance, we investigated the VQA N dataset _cite_, which is based on the COCO dataset _cite_ . As each image has N manual caption annotations, we match question answers to words in image captions, which only produces N, N exact matches, while the remaining nearly NK answers do not appear in captions. Basically, we could divide the question objectives of VQA into three categories: (a) Apparent objective which answers in the query image could be directly obtained from recognition results (objects, attributes, captions, etc) ; (b) Indiscernible objective which answer targets are usually too small or unclear in the query image, and thus requires supporting facts for correct answers; (c) Invisible objective which requires deduction of common sense, topic-specific or even encyclopedic knowledge about the content of the image. Please refer to _ref_ for some examples of these three cases. The external knowledge information at least could help VQA on the latter two categories. A supporting data is that we find more than N, N answers in VQA N dataset appear in knowledge base from the visual genome _cite_ . A few pioneering works~ _cite_ study the problem on how to reason with prior knowledge information for the VQA task. They only involve one supporting fact to help the decision, which may introduce knowledge ambiguity/inaccuracy due to the inaccurate knowledge extraction procedure, and further yield wrongly question answering. Before the VQA task appears, NLP community has extensively studied the text only question-answering (QA) problem _cite_ . Classical methods read documents directly and use information retrieval methods to find answers _cite_ . Thereafter, knowledge base (KB) such as Freebase _cite_ organizes information into structured triples: \textless _inline_eq_, _inline_eq_, _inline_eq_, where _inline_eq_ is the subject, _inline_eq_ is the target, and _inline_eq_ is the relation between _inline_eq_ and _inline_eq_ . Then, question answering is converted into a database query problem _cite_ . Most recently, memory networks have been proposed to combine document reading and knowledge base retrieval _cite_ for accurate QA. Inspired by the development of memory networks based text QA methods, this paper proposes visual knowledge memory network (VKMN) for accurate reasoning with a pre-built visual knowledge base. _ref_ illustrates how the proposed VKMN model works on visual question answering. VKMN extracts multiple related knowledge facts from the question, jointly embeds knowledge facts with visual attentive features into visual knowledge attentive features, and stores them in a key-value pair for easy and efficient reading from memory. The memory reading will make the visual-question addend to the highly relevant/correlated knowledge facts, and thus gives much more accurate question answering. The major contributions of this paper are: