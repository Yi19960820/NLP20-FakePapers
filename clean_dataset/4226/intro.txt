Cancer cells originate from the irreversible injuring of respiration of normal cells. Part of the injured cells could succeed in replacing the lost respiration energy by fermentation energy, but will therefore convert into undifferentiated and widely growing cells (cancer cells) _cite_ . Tumors develop from such abnormal cell/tissue growth, which is associated with cell invasion and mass-effect _cite_ . Cell invasion is characterized by the migration and penetration of cohesive groups of tumor cells to surrounding tissues, and mass-effect by the distension and outward pushing of tissues induced by tumor growth (Fig. _ref_) . In most previous model _cite_, both cell invasion and mass-effect are accounted for, since they are inter-related, mutually reinforcing factors _cite_ . Cell invasion is often modeled by the reaction-diffusion equations _cite_, and mass-effect by the properties of passive material (mainly isotropic materials) and active growth (biomechanical model) _cite_ . While these methods yield informative results, most previous tumor growth models are independently estimated from the target patient without considering the tumor growth pattern of population trend. Furthermore, the small number of model parameters (e.g., N in _cite_) may be insufficient to represent the complex characteristics of tumor growth. Apart from these mathematical modeling methods, a different idea based on voxel motion is proposed _cite_ . By computing the optical flow of voxels over time, and estimating the future deformable field via an autoregressive model, this method is able to predict entire brain MR scan. However, the tumor growth pattern of population trend is still not involved. Moreover, this method might over-simplify the tumor growth process, since it infers the future growth in a linear manner (most tumor growth are nonlinear) . Data-driven statistical learning is a potential solution to incorporate the population trend of tumor growth into personalized tumor modeling. The pioneer study in _cite_ attempts to model the glioma growth patterns as a classification problem. This model learns tumor growth patterns from selected features at patient, tumor, and voxel levels, and achieves a prediction accuracy (both precision and recall) of N \%. However, this study only learns population trend of tumor growth without incorporating subject-specific personalization related to the tumor natural history. Besides this problem, this early study is limited by the feature design and selection components. Specifically, hand-crafted features are extracted to describe each isolated voxel (without context information) . These features could be compromised by the limited understanding of tumor growth, and some of them are obtained in an unsupervised manner. Furthermore, some features may not be generally effective for other tumors, e.g., the tissue type features (cerebrospinal fluid, white and grey matter) in brain tumors _cite_ are not fit for liver or pancreatic tumors. Moreover, considering that the prediction of tumor growth pattern is challenging even for human experts, the low-level features used in this study may not be able to represent complex discriminative information. Deep neural networks _cite_ are high capacity trainable models with a large set of (_inline_eq_ N M) parameters. By optimizing the massive amount of network parameters using gradient backpropagation, the network can discover and represent intricate structures from raw data without any type of feature-engineering. In particular, deep convolutional neural networks (ConvNets) _cite_ have significantly improved performance in a variety of traditional medical imaging applications _cite_, including lesion detection _cite_, anatomy segmentation _cite_, and pathology discrimination _cite_ . The basic idea of these applications is using deep learning to determine the current status of a pixel or an image (whether it belongs to object boundary/region, or certain category) . The ConvNets have also been successfully used in prediction of future binary labels at image level, such as survival prediction of patients with brain and lung cancer _cite_ . whether deep ConvNets are capable of predicting the future status at the pixel/voxel level for medical problem. More generally, in computer vision and machine learning community, the problem of modeling spatio-temporal information and predicting the future have attracted lots of research interest in recent years. The spatio-temporal ConvNet models _cite_, which explicitly represent the spatial and temporal information as RGB raw intensity and optical flow magnitude _cite_, respectively, have shown outstanding performance for action recognition. To deal with the modeling of future status, recurrent neural network (RNN) and ConvNet are two popular methods. RNN has a ``memory" of the history of previous inputs, which can be used to influence the network output _cite_ . RNN is good at predicting the next word in a sequence _cite_, and has been used to predict the next image frames in video _cite_ . ConvNet with fully convolutional architecture can also be directly trained to predict next images in video _cite_ by feeding previous images to the network. However, the images predicted by both RNN and ConvNet are blurry, even after re-parameterizing the problem of predicting raw pixels to predicting pixel motion distribution _cite_, or improving the predictions by multi-scale architecture and adversarial training _cite_ . Actually, directly modeling the future raw intensities might be an over-complicated task _cite_ . Therefore, predicting the future high-level object properties, such as object boundary _cite_ or semantic segmentation _cite_, has been exploited recently. It is also demonstrated in _cite_ that the ConvNet-based method can produce more accurate boundary prediction in compared to the RNN-based method. In addition, ConvNet has shown its strong ability to predict the next status at image-pixel level--as a key component in AlphaGo _cite_, ConvNets are trained to predict the next move (position of the _inline_eq_ Go game board) of Go player, given the current board status, with an accuracy of N \%. Therefore, in this paper, we investigate whether ConvNets can be used to directly represent and learn the two fundamental processes of tumor growth (cell-invasion and mass-effect) from multi-model tumor imaging data at multiple time points. Moreover, given the current state information in the data, we determine whether the ConvNet is capable of predicting the future state of the tumor growth. Our proposed ConvNet architectures are partially inspired by the mixture of policy and value networks for evaluating the next move/position in game of Go _cite_, as well as the integration of spatial and temporal networks for effectively recognizing action in videos _cite_ . In addition to _inline_eq_ and _inline_eq_ direction optical flow magnitudes (i.e., N-channel image input) used in _cite_, we add the flow orientation information to form a N-channel input, as the optical flow orientation is crucial to tumor growth estimation. In addition, we apply a personalization training step to our networks which is necessary and important to patient-specific tumor growth modeling _cite_ . Furthermore, we focus on predicting future labels of tumor mask/segmentation, which is found to be substantially better than directly predicting and then segmenting future raw images _cite_ . Finally, considering that the longitudinal tumor datasets spanning multiple years are very hard to obtain, the issue of small dataset is alleviated by patch oversampling strategy and pixel-wise ConvNet learning (e.g., only a single anatomical MR image is required to train a ConvNet for accurate brain image segmentation _cite_), in contrast to the fully ConvNet used in _cite_ which is more efficient but may lose labeling accuracy. The main contributions of this paper can be summarized as: N) To the best of our knowledge, this is the first time to use learnable ConvNet models for explicitly capturing these two fundamental processes of tumor growth. N) The invasion network can make its prediction based on the metabolic rate, cell density and tumor boundary, all derived from the multi-model imaging data. Mass-effect--the mechanical force exerted by the growing tumor--can be approximated by the expansion/shrink motion (magnitude and orientation) of the tumor mass. This expansion/shrink cue is captured by optical flow computing _cite_, based on which the expansion network is trained to infer tumor growth. N) To exploit the inherent correlations among the invasion and expansion information, we study and evaluate three different network architectures, named: early-fusion, late-fusion, and end-to-end fusion. N) Our proposed ConvNet architectures can be both trained using population data and personalized to a target patient. Quantitative experiments on a pancreatic tumor dataset demonstrate that the proposed method substantially outperforms a state-of-the-art model-based method _cite_ in both accuracy and efficiency. The new method is also much more efficient than our recently proposed group learning method _cite_ while with comparable accuracy.