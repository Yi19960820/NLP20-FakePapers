In many computer vision and computational photography applications, images captured under different imaging modalities are used to supplement the data provided in color images. Typical examples of other imaging modalities include near-infrared _cite_ and dark flash _cite_ photography. More broadly, photos taken under different imaging conditions, such as different exposure settings _cite_, blur levels _cite_, and illumination _cite_, can also be considered as cross-modal _cite_ . Establishing dense correspondences between cross-modal image pairs is essential for combining their disparate information. Although powerful global optimizers may help to improve the accuracy of correspondence estimation to some extent _cite_, they face inherent limitations without help of suitable matching descriptors _cite_ . The most popular local descriptor is scale invariant feature transform (SIFT) _cite_, which provides relatively good matching performance when there are small photometric variations. However, conventional descriptors such as SIFT often fail to capture reliable matching evidences in cross-modal image pairs due to their different visual properties _cite_ . Recently, convolutional neural networks (CNNs) based features _cite_ have emerged as a robust alternative with high discriminative power. However, CNN-based descriptors cannot satisfactorily deal with severe cross-modality appearance differences, since they use shared convolutional kernels across images which lead to inconsistent responses similar to conventional descriptors _cite_ . Furthermore, they do not scale well for dense correspondence estimation due to their high computational complexity. Though recent works _cite_ propose an efficient method that extracts dense outputs through the deep CNNs, they do not extract dense CNN features for all pixels individually. More seriously, their methods were usually designed to perform a specific task only,, semantic segmentation, not to provide a general purpose descriptor like ours. To address the problem of cross-modal appearance changes, feature descriptors have been proposed based on local self-similarity (LSS) _cite_, which is motivated by the notion that the geometric layout of local internal self-similarities is relatively insensitive to imaging properties. The state-of-the-art descriptor for cross-modal dense correspondence, called dense adaptive self-correlation (DASC) _cite_, makes use of LSS and has demonstrated high accuracy and speed on cross-modal image pairs. However, DASC suffers from two significant shortcomings. One is its limited discriminative power due to a limited set of patch sampling patterns used for modeling internal self-similarities. In fact, the matching performance of DASC may fall well short of CNN-based descriptors on images that share the same modality. The other major shortcoming is that the DASC descriptor does not provide the flexibility to deal with non-rigid deformations, which leads to lower robustness in matching. In this paper, we introduce a novel descriptor, called deep self-convolutional activations (DeSCA), that overcomes the shortcomings of DASC while providing dense cross-modal correspondences. This work is motivated by the observation that local self-similarity can be formulated in a deep convolutional architecture to enhance discriminative power and gain robustness to non-rigid deformations. Unlike the DASC descriptor that selects patch pairs within a support window and calculates the self-similarity between them, we compute self-convolutional activations that more comprehensively encode the intrinsic structure by calculating the self-similarity between randomly selected patches and all of the patches within the support window. These self-convolutional responses are aggregated through spatial pyramid pooling in a circular configuration, which yields a representation less sensitive to non-rigid image deformations than the fixed patch selection strategy used in DASC. To further enhance the discriminative power and robustness, we build hierarchical self-convolutional layers resembling a deep architecture used in CNN, together with nonlinear and normalization layers. For efficient computation of DeSCA over densely sampled pixels, we calculate the self-convolutional activations through fast edge-aware filtering. DeSCA resembles a CNN in its deep, multi-layer, and convolutional structure. In contrast to existing CNN-based descriptors, DeSCA requires no training data for learning convolutional kernels, since the convolutions are defined as the local self-similarity between pairs of image patches, which yields its robustness to cross-modal imaging. illustrates the robustness of DeSCA for image pairs across non-rigid deformations and illumination changes. In the experimental results, we show that DeSCA outperforms existing area-based and feature-based descriptors on various benchmarks.