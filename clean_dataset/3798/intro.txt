Person re-identification _cite_ is to match pedestrians observed from non-overlapping camera views based on image appearance. It has important applications in video surveillance such as human retrieval, human tracking, and activity analysis. It saves a lot of human efforts on exhaustively searching for a person from large amounts of images and videos. Nevertheless, person re-identification is a very challenging task. A person observed in different camera views undergoes significant variations on viewpoints, poses, and illumination, which make intra-personal variations even larger than inter-personal variations. Image blurring, background clutters and occlusions also cause additional difficulties. Variations of viewpoints and poses commonly exist in person re-identification, and cause misalignment between images. In Figure _ref_, the lower right region of _inline_eq_ is a red bag, while a leg appears in this region in _inline_eq_ ; the central region of _inline_eq_ is an arm, while it becomes a backpack in _inline_eq_ . Most existing methods _cite_ match pedestrian images by first computing the difference of feature vectors and then the similarities based on such difference vectors, which is problematic due to the spatial misalignment. In our work, patch matching is employed to handle misalignment, and it is integrated with saliency matching to improve the discriminative power and robustness to spatial variation. Salient regions in pedestrian images provide valuable information in identification. However, if they are small in size, saliency information is often overwhelmed by other features when computing similarities of images. In this paper, means regions with attributes that N) make a person from their candidates, and N) are in finding the same person across camera views. In many cases, humans can easily recognize matched pedestrian pairs because they have distinct features. For example, in Figure _ref_, person _inline_eq_ takes a red bag, _inline_eq_ dresses bright white skirt, _inline_eq_ takes a blue bag, and _inline_eq_ carries a red folder in arm. These features are discriminative in distinguishing one person from others. Intuitively, if a body part is salient in one camera view, it usually remains salient in another camera view. Therefore, saliency also has view invariance. Salient regions are not limited to body parts (such as clothes and trousers), but also include accessories (such as baggages, folders and umbrellas as shown in Figure _ref_), which are often considered as outliers and removed in existing approaches. Our computation of saliency is based on the comparison with images from a large scale reference dataset rather than a small group of persons. Therefore, it is quite stable in most circumstances. We observe that images of the same person captured from different camera views have some invariance property on their spatial distributions of saliency, like pair _inline_eq_ in Figure _ref_ . Since the person in image _inline_eq_ shows saliency in her dress while others _inline_eq_-_inline_eq_ are salient in blouses, they can be well distinguished simply from the spatial distributions of saliency. Therefore, not only the visual features from salient regions are discriminative, the spatial distributions of human saliency also provide useful information in person re-identification. Such information can be encoded into patch matching. If two patches from two images of the same person are matched, they are expected to have similar saliency values; otherwise such matching brings penalty on saliency matching. In the second row in Figure _ref_, the query image _inline_eq_ shows a similar saliency distribution as those of gallery images. In this case, visual similarity needs to be considered. This motivates us to relate saliency matching penalty to the visual similarity of two matched patches.