Due to the existence of turbid medium (e.g., dusk, smoke, and other particles) in the atmosphere, images taken in such atmospheric phenomena are subject to visible quality degradation, such as contrast and saturation loss. Taking these degraded images as input, many vision-based systems, originally designed with the assumption of clean capture environments, may be easily troubled with drastic performance decrease. Given that, image dehazing has been extensively studied to restore the clean image from the corrupted input, to serve as the preprocessing step of the aforementioned systems. In this literature, the hazing processing is often represented with the physical corruption model: where _inline_eq_ and _inline_eq_ are the degraded hazy image and the target haze-free scene radiance respectively. _inline_eq_ is the global atmospheric light, and _inline_eq_ is the medium transmission map, which is dependent on the unknown depth information. Most previous dehazing methods first estimate the transmission map _inline_eq_ or the atmospheric light _inline_eq_, then try to recover the final clean image _inline_eq_ . But the first step is a very challenging problem because both the transmission map _inline_eq_ and the atmospheric light _inline_eq_ are often unknown in the real scenarios. To compensate for the lost information during the corruption procedure, many traditional methods _cite_ leverage some image priors and visual cues to estimate the transmission maps and atmospheric light. For example, _cite_ maximizes the local contrast of the target image by using the prior that the contrast of degraded images is often drastically decreased. _cite_ proposes the dark channel prior based on the assumption that image patches of outdoor haze free images often have low-intensity values. _cite_ relies on the assumption that haze-free image colors are well approximated by a few hundred distinct colors and proposes a non-local prior-based dehazing algorithm. However, these priors do not always hold, so they may not work well in certain real cases. With the latest advances of deep learning, many CNN-based methods _cite_ are proposed by leveraging a large scale training datasets. Compared to traditional methods as described above, CNN-based methods attempt to directly regress the intermediate transmission map or the final clean image, and achieve superior performance and robustness. _cite_ presents an end-to-end network to estimate the intermediate transmission map. _cite_ reformulates the atmospheric scattering model to predict the final clean image through a light-weight CNN. _cite_ creates three different derived input images from the original hazy image and fuses the dehazed results out of these derived inputs. _cite_ incorporates the physical model in into the network design and uses two sub-networks to regress the transmission map and atmospheric light respectively. In this paper, we propose a new end-to-end gated context aggregation network (denoted as " GCANet ") for image dehazing. Since dilated convolution is widely used to aggregate context information for its effectiveness without sacrificing the spatial resolution _cite_, we also adopt it to help obtain more accurate restoration results by covering more neighbor pixels. However, the original dilated convolution will produce so-called "gridding artifacts" _cite_, because adjacent units in the output are computed from completely separate sets in the input when the dilation rate is larger than one. Recently, _cite_ analyzes the dilation convolution in a compositional way and proposes to smooth the dilated convolution, which can greatly reduce this gridding artifacts. Hence, we also incorporate this idea in our context aggregation network. As demonstrated in _cite_, fusing different levels of features is often beneficial for both low-level and high-level tasks. Inspired by it, we further propose a gated sub-network to determine the importance of different levels and fuse them based on their corresponding importance weights. _cite_ also uses a gated fusion module in their network, but they directly fuse the dehazing results of different derived input images rather than the intermediate features. To validate the effectiveness of the proposed GCANet, we compare it with previous state-of-the-art methods on the recent dehazing benchmark dataset RESIDE _cite_ . Experiments demonstrate that our GCANet outperforms all the previous methods both qualitatively and quantitatively by a large margin. Furthermore, we conduct comprehensive ablation studies to understand the importance of each component. To show the generality of the proposed GCANet, we have also applied it to the image deraining task, which can also obtain superior performance over previous state-of-the-art image deraining methods. To summarize, our contributions are three-fold as below: The remainder of the paper is organized as follows. We will first summarize related work in Section N, then give our main technical details in Section N. Finally, we will provide comprehensive experiments results and ablation studies in Section N and conclude in Section N.