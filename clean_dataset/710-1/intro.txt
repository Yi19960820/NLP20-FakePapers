As the Internet becomes more pervasive, information overload becomes increasingly more severe. Even with the help of search engines such as Google and Yahoo, people cannot easily understand a series of coherent news events. For example, a person who desires to learn about the N Presidential Election needs iteratively search through several keywords many times and review numerous news documents so that he or she can generate a cohesive picture, e.g., knowledge graph. Storytelling is an efficient way to solve this issue of information overload. By inferring the entity nodes connections, the original documents can be represented through a knowledge graph which consists of a set of storylines. Current works _cite_ mainly focus on this task but employ strict assumptions that may not capture meaningful stories. For instance, _cite_ argues that maximizing the weakest link makes good storylines, which can be reduced to density-based clustering. However, to model users' preferred stories, it requires to understand the evolution patterns, not merely to keep strong coherence. Specifically, existing research in this area suffers from several shortcomings: (N) Strong assumptions can lead to poor storylines. Most related works manually design coherence metrics which are directly assumed to be associated with good storylines. However, a high consistency does not guarantee story quality, since good story might have other properties such interest, novelty, or user-preferred style. Therefore, the coherence based metric is not sufficient for modeling storylines. (N) Lack of multimodal learning . Existing works only focus on unimodal data such as Text Storytelling or Visual Storytelling, overlooking the cross-modality information. Multimodal learning can find entity linkages that a unimodal may miss. (N) Absence of benchmark dataset: Few prior works are reported to provide a publicly available dataset for imitation storytelling. This paper focuses on directly imitating user-provided storylines rather than designing any indirect measures. The basic idea is to learn the connectivity features and structure in storylines so that the agent can reveal similar stories in other domains. This approach is illustrated in Figure _ref_ . The September N attack event contains a storyline which shows the key entities related to the cause, perpetrators, victims, and the aftermath. Similarly, the event Charlie Hebdo Attack also has a story of a similar type. We argue that the two similar storylines share the same structure in a certain embedding space. Therefore, we can reveal similar stories in another event domain (Mexico Iguala mass kidnapping at the bottom) . The inherent benefit of utilizing multimodal data is that humans often make inferences between the images and texts so as to resolve ambiguities. Deep reinforcement learning can learn multi-step decisions. However, a critical difficulty in reinforcement learning is designing a reward function for optimizing the agent. Unlike game application in which there exists a responsive environment, it is dramatically difficult to design a reward function for storytelling due to the unavailability of such responses. Instead of proposing reward function, we introduce a typical Inverse Reinforcement Learning (IRL), imitation learning, to learn the latent policy. Imitating from demonstrations is a strategy in which an agent learns a hidden policy for a dynamic environment by observing demonstrations delivered by an expert agent. While IRL is often with two issues: instability and implicit policy. To solve these problems, Generative Adversarial Networks (GAN) _cite_ mechanism is employed to solve the instability issue. GAN yields an internal generator model which can output policy explicitly after training, which addresses the second issue. Therefore, it is promising to integrate IRL with GAN to learn policy from users' demonstrations. Different from previous work, our study treats storytelling as an imitation learning. Specifically, a policy is acquired from one event domain, and then transfer the policy to learn a storyline from another event. Furthermore, our work takes full advantage of multimodal data to improve the imitation performance. In this paper, we define a Multimodal Imitation Storytelling Task (MIST), and then propose a multimodal generative adversarial method that derives latent policy behind users' demonstrations without explicitly designing a reward function. The main contributions as follows: The rest of this paper is organized as follows. Section _ref_ reviews the related works. A detailed description of the proposed method is given in Section _ref_ . Experiments on multiple public datasets and case study are presented in Section _ref_ . We conclude the paper in Section _ref_ .