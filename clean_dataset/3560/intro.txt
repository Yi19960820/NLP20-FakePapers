Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision~ _cite_ . One of the leading approaches is based on topic modelling, the most popular model being latent Dirichlet allocation or LDA~ _cite_ . LDA is a generative model for documents that originates from the natural language processing community, but has had great success in computer vision~ _cite_ . LDA models a document as a multinomial distribution over topics, where a topic is itself a multinomial distribution over words. While the distribution over topics is specific for each document, the topic-dependent distributions over words are shared across all documents. Topic models can thus extract a meaningful, semantic representation from a document by inferring its latent distribution over topics from the words it contains. In the context of computer vision, LDA can be used by first extracting so-called ``visual words'' from images, convert the images into visual word documents and training an LDA topic model on the bags of visual words. To deal with multimodal data, some variants of LDA have been proposed recently~ _cite_ . For instance, Correspondence LDA (Corr-LDA) ~ _cite_ was proposed to discover the relationship between images and annotation modalities, by assuming each image topic must have a corresponding text topic. Multimodal LDA~ _cite_ generalizes Corr-LDA by learning a regression module relating the topics from the different modalities. Multimodal Document Random Field Model (MDRF) ~ _cite_ was also proposed to deal with multimodal data, which learns cross-modality similarities from a document corpus containing multinomial data. Besides the annotation words, the class label modality can also be embedded into LDA, such as in supervised LDA (sLDA) ~ _cite_ . By modeling the image visual words, annotation words and their class labels, the discriminative power of the learned image representations could thus be improved. At the heart of most topic models is a generative story in which the image's latent representation is generated first and the visual words are subsequently produced from this representation. The appeal of this approach is that the task of extracting the representation from observations is easily framed as a probabilistic inference problem, for which many general purpose solutions exist. The disadvantage however is that as a model becomes more sophisticated, inference becomes less trivial and more computationally expensive. In LDA for instance, inference of the distribution over topics does not have a closed-form solution and must be approximated, either using variational approximate inference or MCMC sampling. Yet, the model is actually relatively simple, making certain simplifying independence assumptions such as the conditional independence of the visual words given the image's latent distribution over topics. Another approach to model the statistical structure of words is through the use of distributed representations modeled by artificial neurons. In the realm of document modeling, proposed a so-called Replicated Softmax (RS) model for bags of words. The RS model was later used for multimodal data modeling~ _cite_, where pairs of images and text annotations were modeled jointly within a deep Boltzmann machine (DBM) ~ _cite_ . This deep learning approach to the generative modeling of multimodal data achieved state-of-the-art performance on the MIR Flickr data set~ _cite_ . On the other hand, it also shares with LDA and its different extensions the reliance on a stochastic latent representation of the data, requiring variational approximations and MCMC sampling at training and test time. Another neural network based state-of-the-art multimodal data modeling approach is Multimodal Deep Recurrent Neural Network (MDRNN) ~ _cite_ which aims at predicting missing data modalities through the rest of data modalities by minimizing the variation of information rather than maximizing likelihood. Recently, an alternative generative modeling approach for documents was proposed in . In this work, a Document Neural Autoregressive Distribution Estimator (DocNADE) is proposed, which models directly the joint distribution of the words in a document by decomposing it as a product of conditional distributions (through the probability chain rule) and modeling each conditional using a neural network. Hence, DocNADE doesn't incorporate any latent random variables over which potentially expensive inference must be performed. Instead, a document representation can be computed efficiently in a simple feed-forward fashion, using the value of the neural network's hidden layer. also show that DocNADE is a better generative model of text documents than LDA and the RS model, and can extract a useful representation for text information retrieval. In this paper, we consider the application of DocNADE to deal with multimodal data in computer vision. More specifically, we first propose a supervised variant of DocNADE (SupDocNADE), which can be used to model the joint distribution over an image's visual words, annotation words and class label. The model is illustrated in Figure~ _ref_ . We investigate how to successfully incorporate spatial information about the visual words and highlight the importance of calibrating the generative and discriminative components of the training objective. Our results confirm that this approach can outperform other topic models, such as the supervised variant of LDA. Moreover, we propose a deep extension of SupDocNADE, that learns a deep and discriminative representation of pairs of images and annotation words. The deep version of SupDocNADE, which is illustrated in Figure~ _ref_, outperforms its shallow one and achieves state-of-the-art performance on the challenging MIR Flickr data set.