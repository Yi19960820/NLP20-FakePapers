Supervised hashing techniques aim to learn compact and similarity-preserving binary representations from labeled data, such that similar inputs are mapped to nearby binary hash codes in the Hamming space, and information retrieval can be efficiently and effectively done in large-scale databases. A large category of these methods seek to learn a set of hyperplanes as linear hash functions, such as Iterative Quantization (ITQ) _cite_, supervised Minimal Loss Hashing (MLH) _cite_, Semi-Supervised Hashing (SSH) _cite_, and FastHash _cite_ . Several kernel-based hashing methods like Binary Reconstructive Embedding (BRE) _cite_ and Kernel-Based Supervised Hashing (KSH) _cite_ have also been proposed. It is well recognized that deep models are able to learn powerful image representations in a latent space where samples with different properties can be well separated. In this context convolutional Neural Networks (CNN) based hashing schemes have been developed _cite_ . Hash codes learned from these latent spaces have been shown to significantly improve the retrieval performance on many benchmark datasets. Nevertheless, the efficacy of deep learning in applications such as hashing hinges on the ability to efficiently train deep models _cite_ . Back propagation (or ``backprop'') _cite_ is currently the most widely-used training method in deep learning due to its simplicity. Backprop is known to suffer from the so called vanishing gradient issue _cite_, where gradients in the front layers of an _inline_eq_-layer network decrease exponentially with _inline_eq_ . This directly impacts computational efficiency, which in turn limits the size of the networks that can be trained. For instance, the training of VGG's very deep features _cite_ for ILSVRCN with N convolutional layers takes approximately one month using N GPUs. We propose a {\em very deep supervised hashing (VDSH)} algorithm by training very deep neural networks for hashing. Our method can take in any form of vector input, such as raw image intensities, traditional features like GIST _cite_, or even CNN features _cite_ . Given training data with class labels, our network learns a data representation tailored for hashing, and outputs binary hash codes with varying lengths. VDSH can easily train large very deep networks within hours on a single GPU. Our learning objective is to generate optimal hash codes for linear classification. To this end we minimize the least square between the weighted encoding features (\ie the output of our last hidden layer) and their label vectors with regularization on model parameters to prevent overfitting. Rather than using backprop, we propose a novel computationally efficient training algorithm for VDSH inspired by alternating direction method of multipliers (ADMM) _cite_ . We represent DNN features in a recursive way by introducing an auxiliary variable to model the output of each hidden layer for each data sample. Then we apply the augmented Lagrangian to incorporate our learning objective with equality constraints, where another set of auxiliary variables are introduced to store the network weights between every pair of adjacent layers locally for efficient update. Empirically we demonstrate smooth convergence and computational efficiency for VDSH. Our training complexity is linearly proportional to the number of connections between nodes in the network. We train DNNs with up to N hidden layers and N nodes per layer for supervised learning of hash codes within about N hours on a single GTX TITAN GPU, while achieving state-of-the-art results on several benchmark datasets. Learning high-level feature representations by building deep hierarchical models have shown great potential in various applications. Researchers have been adopting deep models to jointly learn image representations and hash codes from data. Kang \etal _cite_ proposed a deep multi-view hashing (DMVH) algorithm to learn hash codes with multiple data representations. Xia \etal _cite_ proposed learning image representations for supervised hashing by approximating the data affinity matrix with CNN features. Zhao \etal _cite_ proposed a Deep Semantic Ranking Hashing (DSRH) method to preserve multilevel semantic similarity between multi-label images. Erin Liong \etal _cite_ proposed a deep hashing method to explore the nonlinear relationships among data. Zhang \etal _cite_ proposed a Deep Regularized Similarity Comparison Hashing (DRSCH) method to allow the length of output bits to be scalable. Most of the works learn hash functions on top of a deep CNN architecture. In contrast, VDSH can be built from arbitrary vector representations. When CNN features are used, our method can be viewed as fine-tuning these networks for supervised hashing. Besides, the scale and depth of our DNNs are much larger than previous methods, which pose harder challenges for training. In the literature, many different DNN architectures (\eg LeNet _cite_, AlexNet _cite_, GoogLeNet _cite_ and VGG-VD _cite_) and weighting structures (\eg sparse network _cite_, circulant structure _cite_, low-rank approximation _cite_) have been proposed. Several techniques have been proposed to improve the generalization of networks such as dropout _cite_ and dropconnet _cite_, which can be viewed as better regularization. Some techniques for speeding-up the training have been proposed as well such as distributed training _cite_ and batch normalization _cite_ . These architectures and methods, however, are trained using backprop, suffering from the same issues such as vanishing gradients. Ongoing efforts to overcome issues in backprop include variational Bayesian autoencoder _cite_, auto-encoding target propagation _cite_, and difference target propagation _cite_ . Carreira-Perpin {\'a} n and Wang _cite_ recently proposed a method for training deeply nested systems. Their method of auxiliary coordinates (MAC) breaks down the dependency in nested systems into equality constraints, so that the quadratic penalty method can be utilized as an efficient solver. Shen \etal _cite_ proposed a Supervised Discrete Hashing (SDH) method based on MAC which achieved the state-of-the-art on supervised hashing. Carreira-Perpin {\'a} n and Raziperchikolaei _cite_ proposed learning binary autoencoders for hashing as well using MAC. In contrast our ADMM-based method is more suitable and efficient for solving regularized loss minimization as has been shown in the Block-Splitting algorithm _cite_ . ADMM solves optimization (possibly nonconvex) problems with equality constraints by decomposing an objective into several disjoint sub-objectives using new auxiliary variables so that the original objective can be optimized iteratively using coordinate descent. With small additional computational cost we circumvent the need for relaxation of penalty related parameters as required in this context _cite_ .