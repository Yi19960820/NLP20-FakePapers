People now are trying to make machines work like humans. Researchers are attempting to teach machines to comprehend natural languages, to understand the content in images, etc. After understanding the content, we also want machines to describe what they see. For example, in video caption generation~ _cite_, machine describes what contents are in the video after watching it. In addition, we also want machines to have the ability to imagine. In the task of text-to-image _cite_, machine can turn text descriptions into images. In this paper, we want machines to imagine the scenes by listening to sounds. We hope that when hearing sounds, machine can draw the object that is making sounds and the scene that the sound is made. For instance, after hearing the sparrows crow, machine can draw a picture of sparrows with probably trees or grass as background. In recent years, there are lots of generative models using generative adversarial networks (GANs) ~ _cite_ to generate images. Besides generating images randomly, there is also a large number of researches using conditional GANs~ _cite_, in which the generators take some conditions as input and generate corresponding images. In the previous work, their conditions are the text description of images~ _cite_ or the classes of the images to be generated _cite_ _cite_ . Based on conditional GANs, if we can provide enough sounds and their corresponding images, machines are supposed to learn how to generate images that include the objects making sounds. As far as we know, there is little image generative model that is conditioned on sound. The technology we use to learn an audio-to-image generator is based on GAN. In this paper, we fuse several advanced techniques of conditional GANs including spectral normalization~ _cite_, hinge loss~ _cite_ _cite_, projection discriminator~ _cite_ and auxiliary classifier~ _cite_ into one model. Machine learns the relationships between audio and visual information from watching videos. We create a dataset from SoundNet Dataset _cite_ by using pretrained image classification and sound classification models to apply data cleaning. After training, the audio-to-image generator can produce recognizable images, and the advanced techniques of conditional GAN achieve better Inception score~ _cite_ _cite_ than the naive conditional GAN. In addition, we show that our model learns the relationship between sounds and images by inputting the same sound with different volume levels.