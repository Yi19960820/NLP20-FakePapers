Novelty detection is the process of identifying the new or unexplained set of data to determine if they are within the norm (\ie, inliers) or outside of it (\ie, outliers) . Novelty refers to the unusual, new observations that do not occur regularly or is simply different from the others. Such problems are especially of great interest in computer vision studies, as they are closely related to outlier detection _cite_, image denoising _cite_, anomaly detection in images _cite_ and videos _cite_ . Novelty detection can be portrayed in the context of one-class classification _cite_, which aims to build classification models when the negative class is absent, poorly sampled or not well defined. As such, the negative class can be considered as the novelty (\ie, outlier or anomaly), while the positive (or target) class is well characterized by instances in the training data. To accurately chart the intrinsic geometry of the positive class, the first step is to efficiently represent the data in a way that can entangle more or less the different explanatory factors of variation in the data. Recently, deep learning approaches have gained immense success in representing visual data for various vision-based applications _cite_, especially in cases that they are trained in an end-to-end fashion. However, for novelty detection or one-class classification applications, due to unavailability of data from the negative class, training an end-to-end deep network is not straightforward. Some efforts have been made, in recent years, to benefit from deep features in learning one-class classifiers _cite_, few of which could train an end-to-end feature learning and classification model. Inspired by the recent developments in generative adversarial networks (GANs) _cite_, we propose an end-to-end model for one-class classification and apply it to different applications including outlier detection, novelty detection in images and anomaly event detection in videos. The proposed architecture, similar to GANs, comprises two modules, which compete to learn while collaborating with each other for the detection task. The first module (denoted as _inline_eq_) refines the input and gradually injects discriminative material into the learning process to make the positive and novelty samples (\ie, inliers, and outliers) more separable for the detector, the second module (referred to as _inline_eq_) . These two networks are adversarially and unsupervisedly learned using the training data, which is composed of only the target class. Specifically, _inline_eq_ learns to reconstruct the positive samples and tries to fool the detector (\ie, _inline_eq_) . Whereas, _inline_eq_ learns to distinguish original (positive) samples from the reconstructed ones. In this way, _inline_eq_ learns merely the concept characterized by the space of all positive samples, and hence it can be used for distinguishing between positive and novelty classes. On the other hand, _inline_eq_ learns to efficiently reconstruct the positive samples, while for negative (or novelty) samples it is unable to reconstruct the input accurately, and hence, for negative samples it acts as a decimator (or informally a distorter) . In the testing phase, _inline_eq_ operates as the actual novelty detector, while _inline_eq_ improves the performance of the detector by adequately reconstructing the positive or target samples and decimating (or distorting) any given negative or novelty samples. Fig. _ref_ depicts example inputs and outputs of both _inline_eq_ and _inline_eq_ networks for a model trained to detect images of Penguins. In summary, the main contributions of this paper are as follows: (N) We propose an end-to-end deep network for learning one-class classifier learning. To the best of our knowledge, this article is one of the firsts to introduce an end-to-end network for one-class classification. (N) Almost all approaches based on GANs in the literature _cite_ discard either the generator or the discriminator (analogous to _inline_eq_ and _inline_eq_ Ÿê, respectively, in our architecture) after training. Only one of the trained models is used, while our setting is more efficient and benefits from both trained modules to collaborate in the testing stage. (N) Our architecture learns the model in the complete absence of any training samples from the novelty class and achieves state-of-the-art performance in different applications, such as outlier detection in images and anomaly event detection in videos.