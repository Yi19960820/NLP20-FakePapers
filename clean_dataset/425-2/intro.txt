Tactile understanding is important for a wide variety of tasks. For example, humans constantly adjust their movements based on haptic feedback during object manipulation _cite_ . Similarly, robot performance is likely to improve on a diverse set of tasks if robots can understand the haptic properties of surfaces and objects. A robot might adjust its grip when manipulating a fragile object, avoid surfaces it perceives to be wet or slippery, or describe the tactile qualities of an unfamiliar object to a human. In this work, we explore methods to classify haptic properties of surfaces. Using our proposed methods for haptic classification, we believe haptic information can be more effectively harnessed for a wide array of robot tasks. Humans rely on multiple senses to make judgments about the world. A well known example of this multisensory integration is the McGurk effect _cite_, in which humans perceive different phonemes based on the interaction of visual and auditory cues. Similarly, humans rely on both tactile and visual understanding to interact with their environment. Humans use both haptic and vision to correctly identify objects _cite_, and fMRI data demonstrates that haptic and visual signals are processed in a multi-sensory fashion during object recognition _cite_ . Motivated by the cross-modal processing inherent in the human brain, we build a model that processes both haptic and visual input and demonstrate that this combination achieves higher performance than using either the haptic or visual input alone. In order to effectively learn from haptic and visual data, we train deep neural networks, which have led to dramatic improvements in disparate learning tasks such as object recognition _cite_ and automatic speech recognition _cite_ . As opposed to hand-designing features for particular modalities, neural networks provide a unified framework to learn features directly from data. With our neural network model, we learn haptic features that outperform previous proposed features with little haptic domain knowledge. Additionally, when training models on visual features, we transfer learned models on the related task of material classification to haptic classification. Consequently, we can train a large visual model with less than N, N training instances. Our contributions are as follows. First, we demonstrate that neural networks serve as a unifying framework for signal classification, allowing us to learn rich features on both visual and haptic data with little domain knowledge. We believe similar methods can be used to learn models for other signals that robots need to understand. Furthermore, we demonstrate that visual data from a different yet related task, material classification, easily transfers to haptic classification. Finally, we show that haptic and visual signals are complementary, and combining modalities further improves classification.