Recently, Deep Convolutional Neural Networks (DCNNs) have attracted much research attention in visual recognition, largely due to their excellent performance _cite_ . It has been discovered that the activation of a DCNN trained on a large dataset, such as ImageNet _cite_, can be employed as a universal image descriptor, and applying this descriptor to many visual classification and retrieval problems delivers impressive performance~ _cite_ . This discovery quickly sparked significant interest and inspired many extensions, including _cite_ . A fundamental issue with these kinds of methods is how to generate an image representation from a pre-trained DCNN. Most current solutions take activations of a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we show that we can build a powerful image representation using the activations from two consecutive convolutional layers. We name our method cross-convolutional layer pooling (or cross-layer pooling for short) . This new method relies on two crucial components: (N) we extract local features from one convolutional layer (N) we pool extracted local features by using activations from its successive convolutional layer as guidance. The first component is motivated by recent work _cite_ which has shown that DCNN activations are not translation invariant and that it is beneficial to extract fully connected layer activations from a DCNN to describe local regions and create the image representation by pooling multiple regional DCNN activations. In this paper, we view those regional CNN activations as a newly added convolutional layer (named as the augmented convolutional layer as discussed in section _ref_) . Inspired by this view, we also extract local features from the original convolutional layers of the pre-trained CNN. The second component is motivated by the parts-based pooling method _cite_ which was originally proposed for fine-grained image classification. This method creates one pooling channel for each detected part region while the final image representation is obtained by concatenating pooling results from multiple channels. We generalize this idea to DCNNs and avoid the need for annotating predefined parts. More specifically, we deem the feature map of each filter in a convolutional layer as the detection score map of a part detector and apply the feature map to weight regional descriptors extracted from the previous convolutional layer in the pooling process. The final image representation is obtained by concatenating pooling results from multiple channels with each channel corresponding to one feature map. Note that in contrast to existing regional-DCNN based methods _cite_, the proposed method does not require additional dictionary learning and encoding steps at either the training or testing stage once the convolutional layer activations become available. To further reduce the memory use in storing image representations, we also experiment with a coarse `feature sign quantization' compression scheme and show that the discriminative power of the proposed representation can be largely maintained after compression. Besides image classification, we explore the use of cross-layer pooling for image retrieval. To overcome the high computational cost of the direct implementation of cross-layer pooling, we propose to employ feature binarization and adaptive pooling channel selection schemes to reduce the computational cost. We conduct extensive experiments on three popular visual classification datasets, and three popular image retrieval datasets. Experimental results suggest that the proposed method achieves significantly better performance than competitive methods in most cases. Further ablation studies provide insight into the importance of various components of our approach. A preliminary version of this paper has been published in _cite_ . In this paper, we have made a significant extension. The major differences are threefold. \noindent Preliminary: Two network structures are considered in this paper. One is the AlexNet _cite_ and another one is the VGG very deep (VGGVD in short) network _cite_ . Both networks are composed by the cascade of convolutional layers and fully connected layers. At each convolutional layer, multiple filters are applied, and it results in multiple feature maps, one for each filter. In this paper, we use the term `feature map' to indicate the convolutional result (after applying the ReLU) of one filter and the term `convolutional layer activations' to indicate feature maps of all filters in a convolutional layer.