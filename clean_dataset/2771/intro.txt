In the era of big data, large-scale visual search is vital for accessing and processing a huge amount of images and this is of great importance in many fields of computer vision. Compared to tree based approaches, hashing based methods utilize several hash functions to project image features into binary codes and are more suitable for large-scale visual search due to compact representations _cite_ . Therefore, the hashing approaches are becoming appealing for dealing with high-dimensional data. Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods _cite_ and data-dependent methods _cite_ . Data-independent methods basically project the data into a hamming space by random hash functions whereas data-dependent methods (also referred to as learning based hashing) usually learn hash functions from training datasets by optimizing an objective function. In the case of data-independent methods, the random projections require many binary bits to achieve good performance. With the growing size of data, these methods tend to suffer from memory constraints. The learning based methods, on the other hand, can learn more discriminative hash functions with different objective functions while the hash bits are not linearly growing with the data size. Hence, learning based hashing is clearly more suitable for large-scale data and is the focus of recent research. It appears that most hashing approaches use linear models to map the data into binary codes. Hence, most existing methods using learning techniques do not capture well the nonlinear relationships within images. Although several improvements have been proposed, e.g. by adding kernelization _cite_, it is still challenging to select an appropriate kernel function for specific data. As deep learning techniques are shown to capture well the nonlinear relationships within data, a deep architecture can effectively boost the learning of hash functions. In this paper, we propose a novel deep hashing method to learn hierarchy and nonlinear hash functions for obtaining compact binary codes. Our proposed deep architecture includes two heterogeneous layers: autoencoder layers and an RBM (a Restricted Boltzmann Machine) layer. The autoencoder layers are used to generate the initial binary codes whereas the RBM layer is utilized to reduce the dimensionality of the binary codes. For learning the deep autoencoders and RBM, we introduce new objective functions minimizing the reconstruction error and energy function under the constraints of balanced and uncorrelated bits. Extensive experimental analysis on the problem of large-scale visual search demonstrates the validity and competitiveness of our proposed approach compared to state-of-the-art methods.