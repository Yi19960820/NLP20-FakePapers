In this paper, our objective is to both localize and recognize human actions in video clips. One reason that human actions remain so difficult to recognize is that inferring a person's actions often requires understanding the people and objects around them. For instance, recognizing whether a person is `listening to someone' is predicated on the existence of another person in the scene saying something. Similarly, recognizing whether a person is `pointing to an object', or `holding an object', or `shaking hands'; all require reasoning jointly about the person and the animate and inanimate elements of their surroundings. Note that this is not limited to the context at a given point in time: recognizing the action of `watching a person', after the watched person has walked out of frame, requires reasoning over time to understand that our person of interest is actually looking at someone and not just staring into the distance. Thus we seek a model that can determine and utilize such contextual information (other people, other objects) when determining the action of a person of interest. The Transformer architecture from Vaswani {\it et al.} ~ _cite_ is one suitable model for this, since it explicitly builds contextual support for its representations using self-attention. This architecture has been hugely successful for sequence modelling tasks compared to traditional recurrent models. The question, however, is: how does one build a similar model for human action recognition? Our answer is a new video action recognition network, the {\bf}, that uses a modified Transformer architecture as a `head' to classify the action of a person of interest. It brings together two other ideas: (i) a spatio-temporal IND model that has been successful in previous approaches for action recognition in video~ _cite_--this provides the base features; and (ii) a region proposal network (RPN) ~ _cite_--this provides a sampling mechanism for localizing people performing actions. Together the IND features and RPN generate the query that is the input for the Transformer head that aggregates contextual information from other people and objects in the surrounding video. We describe this architecture in detail in section~ _ref_ . We show in section~ _ref_ that the trained network is able to learn both to track individual people and to contextualize their actions in terms of the actions of other people in the video. In addition, the transformer attends to hand and face regions, which is reassuring because we know they have some of the most relevant features when discriminating an action. All of this is obtained without explicit supervision, but is instead learned during action classification. We train and test our model on the Atomic Visual Actions (AVA) ~ _cite_ dataset. This is an interesting and suitable testbed for this kind of contextual reasoning. It requires detecting multiple people in videos semi-densely in time, and recognizing multiple basic actions. Many of these actions often cannot be determined from the person bounding box alone, but instead require inferring relations to other people and objects. Unlike previous works~ _cite_, our model learns to do so without needing explicit object detections. We set a new record on the AVA dataset, improving performance from N \%~ _cite_ to N \% mAP. The network only uses raw RGB frames, yet it outperforms all previous work, including large ensembles that use additional optical flow and sound inputs. At the time of submission, ours was the top performing approach on the ActivityNet leaderboard~ _cite_ . However, we note that at N \% mAP, this problem, or even this dataset, is far from solved. Hence, we rigorously analyze the failure cases of our model in Section~ _ref_ . We describe some common failure modes and analyze the performance broken down by semantic and spatial labels. Interestingly, we find many classes with relatively large training sets are still hard to recognize. We investigate such tail cases to flag potential avenues for future work.