As we demand more from our robots the need arises for them to operate in increasingly complex, dynamic environments where scenes--and objects of interest--are often only partially observable. However, successful decision making typically requires complete situational awareness. Commonly this problem is tackled by a processing pipeline which uses separate stages of object detection and tracking, both of which require considerable hand-engineering. Classical approaches to object tracking in highly dynamic environments require the specification of plant and observation models as well as robust data association. In recent years neural networks and deep learning approaches have revolutionised how we think about classification and detection in a number of domains _cite_ . The often unreasonable effectiveness of such approaches is commonly attributed to both an ability to learn relevant representations directly from raw data as well as a vastly increased capacity for function approximation afforded by the depth of the networks _cite_ . In this work we propose a framework for, which effectively provides an off-the-shelf solution for learning the dynamics of complex environments directly from raw sensor data and mapping it to an intuitive representation of a complete and unoccluded scene around the robot as illustrated in Figure~ _ref_ . To achieve this we leverage Recurrent Neural Networks (RNNs), which were recently demonstrated to be able to effectively handle sequence data _cite_ . In particular we consider a complete yet uninterpretable hidden state of the world and then train the network end-to-end to update its belief of this state using a sequence of partial observations and map it back into an interpretable unoccluded scene. This gives the network a freedom to optimally define the content of the hidden state and the considered operations without the need of any hand-engineering. We demonstrate that such a system can be trained in an entirely manner based only on raw sensor data and without the need for supervisor annotation. To the best of our knowledge this is the first time such a system has been demonstrated and we believe our work will provide a new paradigm for an end-to-end tracking. In particular, our contributions can be summarized as follows: