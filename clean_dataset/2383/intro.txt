Neural network is widely used in different tasks in society which is profoundly changing our life. Good algorithm, adequate training data, and computing power make neural network supersede human in many tasks, such as face recognition. Face recognition can be used to determine which one the face images belong to or whether the two face images belong to the same one. Applications based on this technology are gradually adopted in some important tasks, such as identity authentication in a railway station and for payment. Unfortunately, it has been shown that face recognition network can be deceived inconspicuously by mildly changing inputs maliciously. The changed inputs are named adversarial examples which implement adversarial attacks on networks. Szegedy {\it et al.} ~ _cite_ present that adversarial attacks can be implemented by applying an imperceptible perturbation which is hard to be observed for human eyes for the first time. Following the work of Szegedy, many works focus on how to craft adversarial examples to attack neural networks. Neural network is gradually under suspicion. The works on adversarial attacks can promote the development of neural network. Akhtar {\it et al.} ~ _cite_ review these works' contributions in the real-world scenarios. Illuminated by predecessor's works, we also do some research about adversarial attack. Most of adversarial attacks aim at misleading classifier to a false label, not a determined specific label. Besides, attacks on image classifier can not be against face recognition networks. Existing works produce perturbation on the images~ _cite_, do some makeup to faces and add eyeglass, hat or occlusions~ _cite_ to faces. And their adversarial examples are fixed by the algorithms which are not flexible for attacks. These algorithms can not accept any images as inputs. Our goal is to generate face images which are similar to the original images but can be classified as the target person shown in Fig.~ _ref_ . The method manipulating the intensity of input images directly is intensity-based. Our work uses geometry-based method to generate adversarial examples. In our work, we use generative adversarial net (GAN) ~ _cite_ to produce adversarial examples which are not limited by data, algorithms or target networks. It can accept any faces as inputs and convert them to adversarial examples for attacks. To generate adversarial examples, we present _inline_eq_ to produce the fake image whose appearance is similar to the origin but is able to be classified as the target person. In face verification domain, whether the two faces belong to one person is based on the cosine distance between feature map in the last layer not based on the probability for each category. So _inline_eq_ pays more attention to the exploration of feature distribution for faces. To get the instance information, we introduce a conditional variational autoencoder to get the latent code from the target face, and meanwhile, attentional modules are provided to capture more feature representation and facial dependencies of the target face. For adversarial examples, _inline_eq_ adopts two discriminators--one for estimating whether the generated faces are real called {\it normal discriminator}, another for estimating whether the generated faces can be classified as the target person called {\it instance discriminator} . Meanwhile, cosine loss is introduced to promise that the fake images can be classified as the target person by the target model. Our main contributions can be summarized into three-fold: