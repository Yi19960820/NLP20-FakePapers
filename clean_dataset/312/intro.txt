Deep Neural Networks (DNNs) have demonstrated significant performance improvements in a wide range of computer vision tasks~ _cite_, such as image classification~ _cite_, object detection~ _cite_ and semantic segmentation~ _cite_ . DNNs often stack tens or even hundreds of layers with millions of parameters to achieve the promising performance. Therefore, DNN based systems usually need considerable storage and computation power. This hinders the deployment of DNNs to some resource limited scenarios, especially low-power embedded devices in the emerging Internet-of-Things (IoT) domain. Many works have been proposed to reduce model parameter size or even computation complexity due to the high redundancy in DNNs~ _cite_ . Among them, low-bit deep neural networks~ _cite_, which aim for training DNNs with low bitwidth weights or even activations, attract much more attention due to their promised model size and computing efficiency. In particular, in BinaryConnect (BNN) ~ _cite_, the weights are binarized to _inline_eq_ and _inline_eq_ and multiplications are replaced by additions and subtractions to speed up the computation. In~ _cite_, the authors proposed binary weighted networks (BWN) with weight values to be binarized plus one scaling factor for each filter, and extended it to XNOR-Net with both weights and activations binarized. Moreover, Ternary Weight Networks (TWN) ~ _cite_ incorporate an extra _inline_eq_ state, which converts weights into ternary values with _inline_eq_-bits width. However, low-bit DNNs are challenged by the non-negligible accuracy drop, especially for large scale models (\eg, ResNet~ _cite_) . We argue that the reason is due to that they quantize the weights of DNNs to low-bits all together at each training iteration. The quantization error is not consistently small for all elements/filters. It may be very large for some elements/filters, which may lead to inappropriate gradient direction during training, and thus makes the model converge to relatively worse local minimum. Besides training based solutions for low-bit DNNs, there are also post-quantization techniques, which focus on directly quantizing the pre-trained full-precision models _cite_ . These methods have at least two limitations. First, they work majorly for DNNs in classification tasks, and lack the flexibility to be employed for other tasks like detection, segmentation, etc. Second, the low-bit quantization can be regarded as a constraint or regularizer~ _cite_ in training based solutions towards a local minimum in the low bitwidth weight space. However, it is relatively difficult for post-quantization techniques (even with fine-tuning) to transfer the local minimum from a full-precision weight space to a low bitwidth weight space losslessly. In this paper, we try to overcome the aforementioned issues by proposing the Stochastic Quantization (SQ) algorithm to learn accurate low-bit DNNs. Inspired by stochastic depth _cite_ and dropout _cite_, the SQ algorithm stochastically select a portion of weights in DNNs and quantize them to low-bits in each iteration, while keeping the other portion unchanged with full-precision. The selection probability is inversely proportional to the quantization error. We gradually increase the SQ ratio until the whole network is quantized. We make a comprehensive study on different aspects of the SQ algorithm. First, we study the impact of selection granularity, and show that treating each filter-channel as a whole for stochastic selection and quantization performs better than treating each weight element separately. The reason is that the weight elements in one filter-channel are interacted with each other to represent a filter structure, so that treating each element separately may introduce large distortion when some elements in one filter-channel are low-bits while the others remain full-precision. Second, we compare the proposed roulette algorithm to some deterministic selection algorithms. The roulette algorithm selects elements/filters to quantize with the probability inversely proportional to the quantization error. If the quantization error is remarkable, we probably do not quantize the corresponding elements/filters, because they may introduce inaccurate gradient information. The roulette algorithm is shown better than deterministic selection algorithms since it eliminates the requirement of finding the best initial partition, and has the ability to explore the searching space for a better solution due to the exploitation-exploration nature of stochastic algorithms. Third, we compare different functions to calculate the quantization probability on top of quantization errors. Fourth, we design an appropriate scheme for updating the SQ ratio. The proposed SQ algorithm is generally applicable for any low-bit DNNs including BWN, TWN, etc. Experiments show that SQ can consistently improve the accuracy for different low-bit DNNs (binary or ternary) on various network structures (VGGNet, ResNet, etc) and various datasets (CIFAR, ImageNet, etc) . For instance, TWN trained with SQ can even beat the full-precision models in several testing cases. Our main contributions are: