Sparse representation of signals has attracted tremendous interest and has been successfully applied to many computer vision applications _cite_ . According to sparse representation theory, signals can be well-approximated by linear combination of a few columns of some appropriate basis or dictionary. The dictionary, which should faithfully and discriminatively represent the encoded signal, plays an important role in the success of sparse representation _cite_ and it has been shown that learned dictionaries significantly outperform pre-defined ones such as Wavelets _cite_ . The last few years have witnessed fast development on dictionary learning (DL) approaches and great success has been demonstrated in different computer vision applications such as image classification. Moreover, in many areas of computer vision and pattern recognition, data are characterized by high dimensional feature vectors; however, dealing with high-dimensional data is challenging for many tasks such as DL. High-dimensional data are not only inefficient and computationally intensive, but the sheer number of dimensions often masks the discriminative signal embedded in the data _cite_ . Therefore, finding a low-dimensional projection seems to be a natural solution. In general, dimensionality reduction (DR) techniques map data to a low-dimensional space such that non-informative and irrelevant information of data are discarded _cite_ . Usually, DR is performed first to the training samples and the dimensionality reduced data are used for DL. However, recent studies reveal that the pre-learned DR matrix neither fully promotes the underlying structure of data _cite_, nor preserves the best features for DL _cite_ . Intuitively, the DR and DL processes should be jointly conducted for a more effective classification. Only a few works have discussed the idea of jointly learning the transformation of training samples and dictionary. Some of these techniques such as _cite_, _cite_ assume that the dictionary is given and cannot help the process of learning the dictionary. By addressing this issue, _cite_ presented a simultaneous projection and DL method using a carefully designed sigmoid reconstruction error. The data is projected to an orthogonal space where the intra-and inter-class reconstruction errors are minimized and maximized, respectively for making the projected space discriminative. However, _cite_ showed that the dictionary learned in the projected space is not more discriminative than the one learned in the original space. JDDLDR method _cite_ jointly learns a DR matrix and a discriminative dictionary and achieves promising results for face recognition. The discrimination is enforced by a Fisher-like constraint on the coding coefficients, but the projection matrix is learned without any discrimination constraints. Nguyen \etal \, _cite_ proposed a joint DR and sparse learning framework by emphasizing on preserving the sparse structure of data. Their method, known as sparse embedding (SE) can be extended to a non-linear version via kernel tricks and also adopts a novel classification schema leading to great performance. Nevertheless, it fails to consider the discrimination power among the separately learned class-specific dictionaries, such that it is not guaranteed to produce improved classification performance _cite_ . Ptucha \etal \, _cite_ integrated manifold-based DR and sparse representation within a single framework and presented a variant of the K-SVD algorithm by exploiting a linear extension of graph embedding (LGE) . The LGE concept is further leveraged to modify the K-SVD algorithm for co-optimizing a small, yet over-complete dictionary, the projection matrix and the coefficients. Most recently, Liu \etal \, _cite_ proposed a joint non-negative projection and DL method. The discrimination is achieved by imposing graph constraints on both projection and coding coefficients that maximises the intra-class compactness and inter-class separability. Although, some of the aforementioned methods perform well for different classification and recognition tasks, the performance of these methods deteriorates when the training data are contaminated heavily because of occlusion, disguise, lighting variations or pixel corruption. In the recent years, low-rank (LR) matrix recovery, which efficiently removes noise from corrupted observations, has been successfully applied to a variety of computer vision applications, such as subspace clustering _cite_, background subtraction _cite_ and image classification _cite_ . Accordingly, some DL methods have been proposed by integrating rank minimization into sparse representation that have achieved impressive results, especially when corruption exists _cite_, _cite_ . In this paper, we propose a novel framework, called joint projection and dictionary learning using low-rank regularization and graph constraints (JPDL-LR), which brings the strength of both DL and LR together for an efficient DR. The algorithm learns a discriminative structured dictionary in the reduced space, whose atoms have correspondence to the class labels and Fisher discrimination criterion is imposed on the coding vectors to enhance class discrimination. Simultaneously, we consider optimizing the input feature space by jointly learning a feature projection matrix. In particular, a supervised nearest neighbor graph is built to encode the local structure information of data; consequently, the desirable relationship among training samples is preserved. To learn effective features from noisy data, we incorporate LR regularization into JPDL-LR objective function and impose a LR constraint on sub-dictionaries to make them robust to noise. This joint framework empowers our algorithm with several important advantages: (N) Learning in the reduced dimensions with lower computational complexity, (N) Ability to handle noisy and corrupted observations, (N) Maintaining both global and local structure of data, and (N) Promoting the discriminative ability of the learned projection and dictionary that is highly desired when the ultimate goal is classification. Extensive experimental results validate the effectiveness of our method for DL and DR and its applicability to image classification task, especially for noisy observations. The remainder of the paper is organized as follows. Section _ref_ presents the proposed JPDL-LR method. The optimization algorithms are described in Section _ref_ and the classification scheme is explained in Section _ref_ . Section _ref_ shows experimental results on different datasets and we draw conclusions in Section _ref_ .