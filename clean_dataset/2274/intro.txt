A recent endeavour of computer vision research is to scale the visual recognition problem to large-scale. This is made possible by the emergence of large-scale datasets such as ImageNet _cite_ and the advances in deep learning techniques _cite_ . However, scalability remains an issue because beyond daily objects, collecting image samples for rare and fine-grained object categories is difficult even with modern image search engines. Taking the ImageNet dataset for example, the popular large-scale visual recognition challenge (ILSVRC) _cite_ mainly focuses on the task of recognising NK classes, a rather small subset of the full ImageNet dataset consisting of N, N classes with NM images. This is because many of the NK object classes are only composed of a handful of images including N classes with only one image. Humans can identify approximately N, N basic object categories~ _cite_ and many more sub-classes, e.g. breeds of dogs and combination of attributes and objects. Importantly, humans are very good at recognising objects without seeing any visual samples. In machine learning, this is considered as the problem of {\em zero-shot learning} (ZSL) . For example, a child would have no problem recognising a ``zebra'' if he/she has seen horses before and also learned that a ``zebra'' is like a horse with black-and-white stripes. Inspired by humans' ZSL ability, there is a recent surge of interest in machine learning based ZSL for scaling up visual recognition to unseen object classes without the need for additional data collection _cite_ . Zero-shot recognition relies on the existence of a labelled training set of seen classes and the knowledge about how each unseen class is semantically related to the seen classes. Seen and unseen classes are usually related in a high dimensional vector space, which is called semantic embedding space. Such a space can be a semantic attribute space _cite_ or a semantic word vector space _cite_ . In the semantic embedding space, the names of both seen and unseen classes are embedded as vectors called class prototypes _cite_ . The semantic relationships between classes can then be measured by a distance, e.g. the prototypes of zebra and horse should be close to each other. Importantly, the same space can be used to project a feature representation of an object image, making visual recognition possible. Specifically, most existing ZSL methods learn a projection (mapping) function from a visual feature space to a semantic embedding space using the labelled training visual data consisting of seen classes only. At test time for recognising unseen objects, this mapping function is then used to project the visual representation of an unseen class image into the same semantic space where both seen and unseen classes reside. The task of unseen class recognition is then realised by a simple nearest neighbour (NN) search--the class label of the test image is assigned to the nearest unseen class prototype in the projected semantic space. The training seen classes and testing unseen classes are different. Although they can be considered as two overlapping domains with some degrees of shared semantics, there exists significant domain differences, e.g. the visual appearance of the same attributes can be fairly different in unseen classes. Existing ZSL models mostly suffer from the projection domain shift problem _cite_ . This is, if the projection for visual feature embedding is learned {\em only} from the seen classes, the projections of unseen class images are likely to be misplaced (shifted) due to the bias of the training seen classes. Sometimes this shift could be far away from the correct corresponding unseen class prototypes, making the subsequent NN search inaccurate. In this work, we present a novel approach to zero-shot learning based on the encoder-decoder paradigm _cite_ . Specifically, an encoder projects a visual feature representation of an image into a semantic representation space such as an attributes space, similar to a conventional ZSL model. However, we also consider the visual feature projection as an input to a decoder which aims to reconstruct the original visual feature representation. This additional reconstruction task imposes a new constraint in learning the visual _inline_eq_ semantic projection function so that the projection must also preserve all the information contained in the original visual features, i.e. they can be recovered by the decoder _cite_ . We show that this additional constraint is very effective in mitigating the domain shift problem. This is because although the visual appearance of attributes may change from seen classes to unseen classes, the demand for more truthful reconstruction of the visual features is generalisable across seen and unseen domains, resulting in the learned project function less susceptible to domain shift. More precisely, we formulate a semantic autoencoder with the simplest possible encoder and decoder model architecture (Fig.~ _ref_): Both have one linear projection to or from a shared latent embedding/code layer, and the encoder and decoder are symmetric so that they can be represented by the same set of parameters. Such a design choice is motivated by computational efficiency--the true potential of a ZSL model is when applied to large-scale visual recognition tasks where computational speed is essential. Even with this simple formulation, solving the resultant optimisation problem efficiently is not trivial. In this work, one such solver is developed whose complexity is independent of the training data size therefore suitable for large-scale problems. Our semantic autoencoder differs from conventional autoencoder _cite_ in that the latent layer has clear semantic meaning: It corresponds to the semantic space and is subject to strong supervision. Therefore our model is not unsupervised. Beyond ZSL learning, it can also be readily used for solving other problems where a discriminative low-dimensional representation is required to cluster visually similar data points. To demonstrate its general applicability, our SAE model is formulated for the supervised clustering problem _cite_ . Our contributions are: (N) A novel semantic encoder-decoder model is proposed for zero-shot learning. (N) We formulate a semantic autoencoder which learns a low-dimensional semantic representation of input data that can be used for data reconstruction. An efficient learning algorithm is also introduced. (N) We show that the proposed semantic autoencoder can be applied to other problems such as supervised clustering. Extensive experiments are carried out on six benchmarks for ZSL which show that the proposed SAE model achieves state-of-the-art performance on all the benchmarks.