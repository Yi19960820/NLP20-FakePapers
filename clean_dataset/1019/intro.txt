\noindent Human faces convey a large range of semantic meaning through facial expressions, which reflect both actions e.g. talking, eye-blinking, and emotional states such as happy (smiling), sad (frowning) or surprised (raising eyebrows) . Over the years, much research has been dedicated to the task of facial expression editing, in order to transfer the semantic expression from a target to a source face, with impressive results~ _cite_ . In general, these state-of-the-art techniques assume that a pair of source-target images is available, and there exists a pair of matching ND or ND facial meshes in both images for texture warping and rendering. Additionally, recent work by Thies et al.~ _cite_ and Cao et al.~ _cite_ require a set of source images in order to learn a statistical representation, that can be used to create a source instance at runtime. The above requirement limits the application of these techniques to certain settings, where source data is abundant. In~ _cite_, the authors propose to directly transfer expressions from the target image to the source face, forgoing the need of prior statistics of the source subject. However, there are situations in which the target face to drive facial deformation of the source does not exist, instead, facial expression can be inferred from other input modalities, such as speech~ _cite_, or explicitly specified by user as vector of facial action unit (AU) intensities~ _cite_ . In this work, we are interested in mid-level facial expression manipulation by directly animating a human portrait given only AU coefficients, thereby enabling a whole new level of flexibility to the facial expression editing task. Particularly, our proposed GATH model is able to modify a frontal face portrait of arbitrary identity and expression at pixel level, hallucinating a novel facial image whose expressiveness mimics that of a real face that has similar AU attributes. In other words, our model learns to extract identity features to preserve individual characteristic of the portrait, facial enactment to animate the portrait according to values of AU coefficients and texture mapping, all in an end-to-end deep neural network. Learning identity features requires a large number of training images from thousands of subjects, which are readily available in various public datasets. On the other hand, the amount of publicly available emotional videos such as~ _cite_, from which we could collect a wide range of AU coefficients, is rather limited. A deep net trained on such a small number of subjects would not generalize well to unseen identity. To address this shortcoming, we propose to train the deep net with separate source and target sets, i.e. the animated facial image of subject A in the source set does not have an exact matching target image, but there exists an image of subject B in the target set that has similar expression to the synthesized image of A, and their expressiveness similarity is measured by an auxiliary function. Inspired by recent advances in image synthesis with adversarial learning ~ _cite_, we jointly train the deep face generator with a discriminator in a minimax game, in which the generator gradually improves the quality of its synthesis to try to fool the discriminator in believing that its output is from the real facial image distribution. Furthermore, taking advantage of the availability of subject class labels in the source set, we jointly train a classifier to recognize the subject label of the generated output, therefore encouraging the generator to correctly learn identity features, and producing better synthesis of the input subject. Our main contributions are as follows: