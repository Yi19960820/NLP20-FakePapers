The annually held ILSVRC competition has seen state-of-the-art classification accuracies by deep networks such as AlexNet by _cite_, VGG by _cite_, GoogleNet and ResNet . These networks contain millions of parameters and require billions of arithmetic operations. Various solutions have been offered to reduce the resource-requirement of CNNs. Fixed point arithmetic is less resource hungry compared to floating point. Moreover, it has been shown that fixed point arithmetic is adequate for neural network computation . This observation has been leveraged recently to condense deep CNNs. _cite_ show that networks on datasets like CIFAR-N (N images classes) can be trained in N-bit. Further trimming of the same network uses as low as N-bit multipliers . Another approach by _cite_ uses binary weights and activations, again on the same network. The complexity of deep CNNs can be split into two parts. First, the convolutional layers contain more than N \% of the required arithmetic operations. By turning these floating point operations into operations with small fixed point numbers, both the chip area and energy consumption can be significantly reduced. The second resource-intense layer type are fully connected layers, which contain over N \% of the network parameters. As a nice by-product of using bit-width reduced fixed point numbers, the data transfer to off-chip memory is reduced for fully connected layers. In this paper, we concentrate on approximating convolutional and fully connected layers only. Using fixed point arithmetic is a hardware-friendly way of approximating CNNs. It allows the use of smaller processing elements and reduces the memory requirements without adding any computational overhead such as decompression. Even though it has been shown that CNNs perform well with small fixed point numbers, there exists no thorough investigation of the delicate trade-off between bit-width reduction and accuracy loss. In this paper we present Ristretto, which automatically finds a perfect balance between the bit-width reduction and the given maximum error tolerance. Ristretto performs a fast and fully automated trimming analysis of any given network. This post-training tool can be used for application-specific trimming of neural networks.