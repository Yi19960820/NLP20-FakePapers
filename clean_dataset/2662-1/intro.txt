Large-scale distributed training improves the productivity of training deeper and larger models . Synchronous stochastic gradient descent (SGD) is widely used for distributed training. By increasing the number of training nodes and taking advantage of data parallelism, the total computation time of the forward-backward passes on the same size training data can be dramatically reduced. However, gradient exchange is costly and dwarfs the savings of computation time, especially for recurrent neural networks (RNN) where the computation-to-communication ratio is low. Therefore, the network bandwidth becomes a significant bottleneck for scaling up distributed training. This bandwidth problem gets even worse when distributed training is performed on mobile devices, such as federated learning . Training on mobile devices is appealing due to better privacy and better personalization, but a critical problem is that those mobile devices suffer from even lower network bandwidth, intermittent network connections, and expensive mobile data plan. Deep Gradient Compression (DGC) solves the communication bandwidth problem by compressing the gradients, as shown in Figure _ref_ . To ensure no loss of accuracy, DGC employs and on top of the gradient sparsification to maintain model performance. DGC also uses and to overcome the staleness problem caused by reduced communication. We empirically verified Deep Gradient Compression on a wide range of tasks, models, and datasets: CNN for image classification (with CifarN and ImageNet), RNN for language modeling (with Penn Treebank) and speech recognition (with Librispeech Corpus) . These experiments demonstrate that gradients can be compressed up to N _inline_eq_ without loss of accuracy, which is an order of magnitude higher than previous work .