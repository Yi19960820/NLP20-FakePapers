With an increasing demand of intelligent cellphones and digital cameras, people today take more photos to jot down daily life and stories. Such an overwhelming trend is generating a desperate demand for smart tools to recognize the same person (known as query), across different time and space, among thousands of images from personal data, social media or Internet. Previous work _cite_ has demonstrated that person recognition in such unconstrained settings remains a challenging problem due to many factors, such as non-frontal faces, varying light and illumination, the variability in appearance, texture of identities, etc. The recently proposed PIPA dataset _cite_ contains thousands of images with complicated scenarios and similar appearance among persons. The illumination, scale and context of the data varies a lot and many instances have partial or even no faces. Figure _ref_ shows some samples from both the training and test sets. Previous work _cite_ resort to identifying the same person via a multi-cue, multi-level manner where the training set is used only for extracting features and the follow-up classifier (SVM or neural network) is trained on the set . The recognition system is evaluated on the set. We argue that such a practice is infeasible and ad hoc in realistic application since the second training on is auxiliary and needs re-training if new samples are added. Instead, we aim at providing a set of robust and well generalized feature representations, which is trained directly on the training set, and at identifying the person by measuring the feature similarity between two splits on test set. There is no need to train on and the system still performs well even if new data comes in. As shown in the bottom of Figure _ref_, the woman in green box wears similar scarf as does the person in light green box. Her face is shown partially or does not appear in some cases. To obtain robust feature representations, we train several deep models for different regions and combine the similarity score of features from different regions to have the prediction of one's identity. Our key observation is that during training, the cross-entropy loss does not guarantee the similarity among samples within a category. It magnifies the difference across classes and ignore the feature similarity of the same class. To this end, we propose a congenerous cosine loss, namely COCO, to enlarge the inter-class distinction as well as narrow down the inner-class variation. It is achieved by measuring the cosine distance between sample and its cluster centroid in a cooperative manner. Moreover, we also align each region patch to a pre-defined base location to further make samples within a category be more closer in the feature space. Such an alignment strategy could also make the network less prone to overfitting. Figure _ref_ illustrates the training pipeline of our proposed algorithm at a glance. Each instance in the image is annotated with a ground truth head and we train a face and human body detector respectively, using the RPN framework _cite_ to detect these two regions. Then a human pose estimator _cite_ is applied to detect key parts of the person in order to localize the upper body region. After cropping four region patches, we conduct an affine transformation to align different patches from training samples to a `base' location. Four deep models are trained separately on the PIPA training set using the COCO loss to obtain a set of robust features. To sum up, the contributions in this work are as follows: