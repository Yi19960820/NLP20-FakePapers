Deep Convolutional Neural Networks (CNNs) are very successful at performing visual tasks such as image classification _cite_, object detection _cite_, and semantic segmentation _cite_ . Despite these successes we lack understanding of why CNNs work so well. For example, when CNNs are trained to detect objects it seems likely that the object semantic parts are represented by the internal activity of the filters/neurons. But what is the form of these internal representation? Are semantic parts represented by individual neurons _cite_ or perhaps by populations of activity _cite_ ? Understanding the nature of these representations not only makes it easier to understand deep networks, and hence may suggest ways to improve them, but also serves as an unsupervised method for learning object semantic parts. Note, we use the term ``semantic parts" to mean object parts, like wheels and windows of cars, which are defined in terms of the three-dimensional object. We address this problem by studying the internal activity of CNNs when looking at six classes of objects (car, aeroplane, bike, motorbike, bus, and train) . Our hypothesis is that object semantic parts are represented by populations of filter/neural activity which we can find by clustering algorithms. We call each cluster a ``visual concept". Although these visual concepts are defined in terms of the feature activity of the CNN they also correspond to a set of image patches (those patches whose feature activity lie within the cluster) . We show visually that visual concepts (i.e. the image patches associated with them) correspond well to parts of objects (e.g., the wheels and windows of cars) . These clusters are also visually tight, in the sense that image patches corresponding to the same visual concept tend to look very similar. Moreover, the visual concepts give a fairly dense description of each object, in the sense that for most parts of the object we can find a visual concept that looks visually similar. We proceed to understand and quantify the correspondence between the visual concepts and the semantic parts of the objects. The correspondence is non-trivial because the scale of the visual concepts and semantic parts may differ, hence several visual concepts may correspond to different, or overlapping, regions of the same semantic part. Conversely, some semantic parts may look visually similar (e.g., the bike wheel center and the bike chain ring) . In general, we expect the correspondence between visual concepts and semantic parts to be many-to-many. To study this correspondence we convert each visual concept into a part detector and evaluate them in terms of part detection and localization. We first compute the distance between the center of the visual concept and the feature response/activity to an input image patch. If the distance is below a threshold we treat this as a detection of that visual concept. We then evaluate the visual concepts for detecting keypoints in the PASCALND + dataset _cite_ . This gives the promising result that for each keypoint there is typically a visual concept that detects the keypoint fairly well, as measured by average precision (AP), and compared with detection using single filters or a supervised baseline method. But this study is limited because the keypoints are very sparse and only cover a small portion of each object, so many visual concepts do not correspond to them. To overcome this difficulty we create a new dataset, ImageNetPart, by annotating the six object classes (car, aeroplane, bike, motorbike, bus, and train) in PASCALND + _cite_ with dense semantic parts and also determining the background regions. We use the ImageNet images provided in PASCALND + dataset. This is a total of roughly N, N images and requires roughly N times as many labels as the keypoints. As before, we treat the single visual concepts as part detectors and show that for most semantic parts there is a visual concept that detects it fairly well (as measured by AP) . But this only tells us what a subset of visual concepts are doing, because the number of visual concepts is much larger than the number of semantic parts (by a factor of N or more) . By further analysis we show that the remaining visual concepts fall into three classes: (i) those that detect several semantic parts (two to four) which are visually similar, (ii) those that detect background regions of images (e.g., sky for aeroplanes, railway tracks for trains), and (iii) a few which have no obvious correspondence (perhaps due to limitations of the clustering method and CNN feature) . We obtain better APs in evaluation when taking into account the fact that visual concepts may respond to a small subset of semantic parts. We argue that our work gives some understanding of CNNs and, in particular, of how they encode internal representations of objects by visual concepts, encoded by populations of filter/neuron activity. From another perspective, our approach can be thought of as an unsupervised way to learn part detectors of objects.