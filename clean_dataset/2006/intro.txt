Human pose estimation refers to the task of recognizing postures by localizing body keypoints (head, shoulders, elbows, wrists, knees, ankles, {\em etc.}) from images. We focus on the problem of single-person pose estimation from a single RGB image with the input of a rough bounding box of a person, while the pose and the activity of the person can be arbitrary. The task is challenging due to the large variability of human body appearances, lighting conditions, complex background and occlusions, body physique and posture structures of the activities performed by the subject. The inference is further sophisticated when the case extends to multi-person scenarios. Human pose estimation has been studied extensively _cite_ . Traditional methods rely on hand-craft features _cite_ . With the prosperity of Deep Neural Networks (DNN), Convolutional Neural Networks (CNN) _cite_, in particular the {\em hourglass} models _cite_ and their variants _cite_ have demonstrated remarkable performance in human pose estimation. The repeated bottom-up and top-down processing within the hourglass modules can reliably extract posture features across scales and viewing variabilities, and thus effectively localize body keypoints for pose estimation. Although great progress has been made, state-of-the-art DNN-based pose estimation methods still suffer from several problems (Fig.~ _ref_): \noindent (N) {\bf Scale instability:} Slight perturbation of the input bounding box from the person detector (such as the SSD _cite_) can cause abrupt changes in the pose estimation, due to the influence of such dominating scales. Such scale instability causes unreliable pose estimations, and even the latest hourglass methods (_cite_) tend to overfit body keypoints in a particular scale (out of all scales in the deconv pyramid), which results in a domination of a single scale. Current practice to handle this scale instability ({\em e.g.} widely used in the MPII pose estimation challenge _cite_), is to repeatedly performing pose estimations in multiple trials of various scales, and output the result with the highest score. This clearly shows the lack of a consistent scale representation in limitations of the existing methods. This will be addressed in this work in _inline_eq_ ~ _ref_ and _inline_eq_ ~ _ref_ . \noindent (N) {\bf Insufficient structural priors:} The second issue is how to effectively incorporate the structure of human body as priors in the deep network for pose estimation. Such priors can provide key information to solve challenges of pose estimation in real-world scenarios with complex multi-person activities and cluttered backgrounds, where body keypoint occlusions and matching ambiguities are the bottlenecks. In these challenge cases, accurate keypoint localization is not the only factor for successful pose estimation, as there will be questions on how best to associate the keypoints (invisible, or multiple visible ones among possibilities) to infer the global pose configuration. Known body structural priors can provide valuable cues to infer the locations of the hidden body parts from the visible ones. We propose to model the skeleton with an {\em intermediate structural loss} (_inline_eq_ ~ _ref_) and through the use of a global regression network at the end (_inline_eq_ ~ _ref_) . We further develop a keypoint masking scheme to improve the training of our network on challenging cases of severely occluded keypoints (_inline_eq_ ~ _ref_) . In this paper, we propose a holistic framework to effectively address the drawbacks in the existing state-of-art hourglass networks. Our method is based on two neural networks: the {\bf multi-scale supervision network} (MSS-net) and the {\bf multi-scale regression network} (MSR-net) . In MSS-net, a layer-wise loss term is added at each deconv layer to allow explicit supervision of scale-specific features in each layer of the network. This multi-scale supervision enables effective learning of multi-scale features that can better capture local contextual features of the body keypoints. In addition, coarse-to-fine deconvolution along the resolution pyramid also follows a paradigm similar to the {\em attention mechanism} to focus on and refine keypoint matches. The MSR-net takes output from multiple stacks of MSS-nets to perform a global keypoint regression by fusing multiple scales of keypoint heatmaps to determine the pose output. In addition to the MSS-net and MSR-net which can jointly learn to match keypoints across multiple scales of features, we explicitly match connected keypoint pairs based on the connectivity and structure of human body parts. For example, the connectivity from the elbow to the lower-arm and to the wrist can be leveraged in the inference of an occluded wrist, when the elbow and lower-arm are visible. Hence, we add a {\em structure-aware loss} aims to improve the capacities of the current deep networks in modeling structural priors for pose estimation. This structure loss improves the estimations of occluded keypoints in complex or crowded scenarios. Lastly, our {\em keypoint masking training} scheme serves as an effective data augmentation approach to enhance the learning of the MSS-net and MSR-net together, to better recognize occluded poses from difficult training samples. The main contributions of this paper can be summarized as follows: Experimental evaluations show that our method achieves state-of-the-art results on the MPII pose challenge benchmark.