A good image description is often said to ``paint a picture in your mind's eye.'' The creation of a mental image may play a significant role in sentence comprehension in humans _cite_ . In fact, it is often this mental image that is remembered long after the exact sentence is forgotten _cite_ . What role should visual memory play in computer vision algorithms that comprehend and generate image descriptions? Recently, several papers have explored learning joint feature spaces for images and their descriptions _cite_ . These approaches project image features and sentence features into a common space, which may be used for image search or for ranking image captions. Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) _cite_, recursive neural networks _cite_, or deep neural networks _cite_ . While these approaches project both semantics and visual features to a common embedding, they are not able to perform the inverse projection. That is, they cannot generate novel sentences or visual depictions from the embedding. In this paper, we propose a bi-directional representation capable of generating both novel descriptions from images and visual representations from descriptions. Critical to both of these tasks is a novel representation that dynamically captures the visual aspects of the scene that have already been described. That is, as a word is generated or read the visual representation is updated to reflect the new information contained in the word. We accomplish this using Recurrent Neural Networks (RNNs) _cite_ . One long-standing problem of RNNs is their weakness in remembering concepts after a few iterations of recurrence. For instance RNN language models often find difficultly in learning long distance relations _cite_ without specialized gating units _cite_ . During sentence generation, our novel dynamically updated visual representation acts as a long-term memory of the concepts that have already been mentioned. This allows the network to automatically pick salient concepts to convey that have yet to be spoken. As we demonstrate, the same representation may be used to create a visual representation of a written description. We demonstrate our method on numerous datasets. These include the PASCAL sentence dataset _cite_, Flickr NK _cite_, Flickr NK _cite_, and the Microsoft COCO dataset _cite_ . When generating novel image descriptions, we demonstrate state-of-the-art results as measured by both BLEU _cite_ and METEOR _cite_ on PASCAL NK. Surprisingly, we achieve performance only slightly below humans as measured by BLEU and METEOR on the MS COCO dataset. Qualitative results are shown for the generation of novel image captions. We also evaluate the bi-directional ability of our algorithm on both the image and sentence retrieval tasks. Since this does not require the ability to generate novel sentences, numerous previous papers have evaluated on this task. We show results that are better or comparable to previous state-of-the-art results using similar visual features.