Our paper is motivated by 'Learning to Learn' as was discussed in the NIPS-N workshop, where interest was shown in applying previously gained knowledge to a new domain for learning new tasks . The learner uses a series of tasks learned in the source domain, to improve and learn the target tasks. In our paper, a frame taken from the target game is mapped to the analogous state in the source game and the trained policy learned from the source game knowledge is used to play the target game. We also rely on transferring knowledge using transfer learning methods that have shown to improve performance and stability _cite_ . Expert performances can be achieved on several games and with the model complexity of a single expert. Significant improvements in learning speeds in target games are also seen _cite_ . The same learned weights can be generalized from a source game to several new target games. This treatment of weights from a previously trained model has implications in Safe Reinforcement Learning _cite_ as we can easily learn from already learned stable agents. We find underlying similarities between the source and the target domains i.e. \different Atari Games to represent common knowledge using Unsupervised Image-to-image Translation (UNIT) Generative adversarial networks (GANs) _cite_ . Recent progress in the applications of Reinforcement Learning (RL) using deep networks has led us to policy gradient methods like the ANC algorithm, that can autonomously achieve human-like performance in Atari games _cite_ . In an ANC network, several agents are executed in parallel, with different starting policies and the global state updated intermittently by the agents. ANC has a smaller computation cost and we do not have to calculate the Q value for every action in the action space to find the maximum. propose a training method where two games are simultaneously trained where each game within the neural network competes for representational space. In our paper, the target game competes with its visual representation obtained after using the UNIT GAN as a visual mapper between the source and target game.