Image classification involves describing images with pre-determined labels. One of the first breakthroughs towards solving this problem was the bag-of-visual-words (BOV) ~ _cite_ . While the BOV simply involves counting the number of occurrences of quantized local features, approaches that encode higher order statistics such as the the Fisher Vector (FV) ~ _cite_ led to state-of-the-art image classification results~ _cite_ . Especially, such higher-order encodings were used by the leading teams in the N and N editions of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ~ _cite_ . FV-based approaches were however outperformed in N by the work of Krizhevsky \etal~ _cite_ based on Convolutional Networks (ConvNets) ~ _cite_ trained in a supervised fashion on large amounts of labeled data. These models are feed-forward architectures involving multiple computational layers that alternate linear operations, \eg convolutions, and non-linear operations, \eg rectified linear units (ReLU) . The end-to-end training of the large number of parameters inside ConvNets from pixels to the specific end-task is a key to their success. Since then, ConvNets, including improved architectures~ _cite_, have consistently outperformed all other alternatives in subsequent editions of ILSVRC. Also, ConvNets have remarkable when used as ``universal'' feature extractors~ _cite_: if one feeds an image to a ConvNet, the output of intermediate layers might be used as a representation of this image and typically fed to linear classifiers. To the best of our knowledge, this heuristic is not based on a strong theoretical ground, but has been experimentally shown to work well in practice~ _cite_ . Although ConvNets and FV approaches differ significantly, several works tried to combine their benefits~ _cite_ . Our work also attempts to . Our {\bf primary contribution} is a novel approach to extract a representation of an image given a pre-trained ConvNet. We draw inspiration from the FV, which is based on the theoretically well-founded Fisher Kernel (FK) proposed by Jaakkola and Haussler~ _cite_ . The FK involves deriving a kernel from an underlying generative model of the data by taking the gradient of the log-likelihood with respect to the model parameters. In a similar manner, given an unlabeled image, we propose to compute the . This gradient with respect to the parameters of the fully connected layers yields very high-dimensional representations (\cf Figure~ _ref_) . Our {\bf second contribution} consists in leveraging the special structure of this gradient representation to design an efficient kernel. We show that our representation actually corresponds to a rank-N matrix, for which the trace kernel can be efficiently computed. Furthermore, this kernel decomposes in our case into the product of two simpler kernels: the standard one on forward-pass features, and a second one on quantities efficiently computed by back-propagation. The remainder of this article is organized as follows. In section~ _ref_, we review related works. In section~ _ref_, we provide more background on the FK and ConvNets. In section ~ _ref_, we introduce our novel hybrid ConvNet-gradient representation as well as our associated efficient kernel. Finally, we provide experimental results on the PASCAL VOC N and N benchmarks in section~ _ref_, showing that our representation consistently transfers better than the standard forward pass features.