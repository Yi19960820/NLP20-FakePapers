Live cell microscopy imaging is a key component in the biological research process. However, without the proper analysis tools, the raw images are a diamond in the rough. One must obtain the segmentation of the raw images defining the individual cells prior to calculation of the cells' properties. Manual segmentation is infeasible due to the large quantity of images and cells per image. Automatic segmentation tools are available and roughly split into two groups, supervised and unsupervised methods. The methods vary and include: automatic gray level thresholding _cite_, the watershed algorithm _cite_ and Active Contours _cite_ . Another approach is to support the segmentation algorithm with temporal information from tracking algorithms as was proposed by _cite_ . All these methods assume some structure in the data that may not fit every case. Supervised methods, on the other hand, do not assume any structure rather aim to learn it from the data. Classic machine learning methods generally require two independent steps, feature extraction and classification. In most cases the feature extraction is based either on prior knowledge of the image properties such as in _cite_ or general image properties such as smoothing filters, edge filters, etc. A widely used toolbox which takes a pixel classification approach is Ilastik _cite_, using a random forest classifier trained on predefined features extracted from a user's scribbles on the image. Recent developments in the computer vision community have shown the strength Convolutional Neural Networks (CNNs) which surpass state of the art methods in object classification _cite_, semantic segmentation _cite_ and many other tasks. Recent attempts at cell segmentation using CNNs include _cite_ . The common ground of all CNN methods is the need for an extensive training set alongside a predefined loss function such as the cross-entropy (CE) . In this work we present a novel approach for microscopy cell segmentation inspired by the GAN _cite_ and extension thereof _cite_ . The GAN framework is based on two networks, a generator and a discriminator, trained simultaneously, with opposing objectives. This allows the discriminator to act as an abstract loss function in contrast to the common CE and _inline_eq_ losses. We propose a pair of adversarial networks, an estimator and a discriminator for the task of microscopy cell segmentation. Unlike the original GAN _cite_, we do not generate images from random noise vectors, rather estimate the underlaying variables of an image. The estimator learns to output some segmentation of the image while the discriminator learns to distinguish between expert manual segmentations and estimated segmentations given the associated image. The discriminator is trained to a classification loss on two classes, manual and estimated, i.e. minimizing the similarity between the two. The estimator, on the other hand, is trained to the discriminator's loss and effectively, maximize the similarity. In _cite_, semantic segmentation of natural images are generated for a set of predefined class. However, the main difference lays in our need to separate instances of a single class (cells) and not to separate different classes. The method also differs in choice of discriminator architecture and training method. Our contribution is three-fold. We expand the concept of the GAN for the task of cell segmentation and in that reduce the dependency on a selection of loss function. We propose a novel architecture for the discriminator, referred to as the ``Rib Cage'' architecture (See section _ref_), which is adapted to the problem. The ``Rib Cage'' architecture includes several cross connections between the image and the segmentation, allowing the network to model complicated correlation between the two. Furthermore we show that accurate segmentations can be achieved with a low number of training examples therefore dramatically reducing the manual workload. The rest of the paper is organized as follows. Section~ _ref_ defines the problem and elaborates on the proposed solution. Section~ _ref_ presents the results for both a common adversarial and non-adversarial loss compared to the proposed method, showing promising initial results. Section~ _ref_ summarizes and concludes the work thus far.