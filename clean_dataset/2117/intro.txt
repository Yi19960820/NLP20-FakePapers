Semantic concept and complex event recognition in unconstrained consumer videos has received much research interest in recent years _cite_ due to both the applicability of technology and real application needs. Thanks to the popularity of video capturing devices such as smart phones and multimedia sharing sites such as Youtube, people nowadays generate and share a large volume of videos over the internet everyday. It becomes an urgent need to automatically organize and manage these videos, and video recognition is a promising first step for intelligent video management systems. Many previous works focus on recognition in very specific domains such as action recognition _cite_, and the videos usually come from limited sources such as news, movies or recorded in lab environment _cite_ . Recognizing complex events in consumer video is a more challenging task, because it requires identifying various semantic concepts that may appear in various spatio-temporal locations in the video, in contrast to the temporally local action recognition. Also, the videos are captured in more diverse environments, which require more discriminative representations for recognition. Extensive efforts have been made to design video features for video recognition. One common approach is to utilize image-based features to capture appearance information in each frame and then aggregate multiple frames with video-specific pooling method _cite_ . In addition to image-based features, effort has been made to develop video specific features that capture information encoded in the temporal dimension _cite_ . These features have improved video recognition performance in certain domains, but they also introduce significant computation overhead. Moreover, the performance gain of using video specific features is task and dataset dependent. In fact, some empirical results show that image-based features may perform as well as or even more robust than spatio-temporal features, especially when detecting complex events or high level semantic concepts in the video _cite_ . Therefore, designing discriminative video features remains the key challenge for video recognition. Motivated by recent successes of DCN in concept recognition and object detection on static image, we aim to exploit DCN to extract better video representation. Deep Learning as a learning paradigm has been shown effective in various domains including natural language processing _cite_, speech recognition _cite_, etc. In computer vision, the great success of DCNs on the ImageNet dataset _cite_ has attracted extensive research interest, and many further efforts have made DCN the state-of-the-art algorithm for various visual recognition tasks in static images. However, there remains several difficulties for applying DCNs on visual recognition in practice. The first difficulty is its dire need for extremely large amount of labeled training data. For example, the ImageNet dataset that is widely used as the test bed as well as the source for pre-training DCNs contains more than a million images, all labeled by human. Collecting such a large dataset with complete ground truth is very challenging and may not be possible in some domains. This is especially true for video recognition, where most existing video corpora are one to two order smaller than those used for static image recognition. Although efforts have been made to collect million scale video recognition benchmarks _cite_, the label of the dataset is still in video level. Because irrelevant contents usually interleave with important information in user generated videos, the video level annotation can be considered as a noisy label for visual content in the video and impose additional difficulty for learning DCNs. Frame level or even pixel level annotation is more precise and informative, but it requires extensive human labor and is unlikely to be scalable. The second difficulty is the large number of meta-parameters in DCN. While models such as SVM and logistic regression usually have only two to three meta-parameters in practice and can be optimized easily using cross validation grid search, the number of meta-parameters in DCN is at least an order larger. Combined with the extensive computation requirement, grid search for meta-parameters is generally infeasible, and many previous works simply apply the setting of existing models which yield sub-optimal performance. In this work, we focus on solving the two problems to enable practical video recognition algorithm using DCN. To overcome the lack-of-training-data problem and avoid overfitting, we apply two transfer learning approaches to transfer knowledge from static image to video. The transfer learning approach learns important visual patterns from static images and improves the generalizability of DCNs on unseen videos. To determine the network configuration, we perform extensive experiments on independent image datasets. These experiments lead to better understanding about the correlation between meta-parameters and DCN performance, which can serve as the basis for heuristic search in both our experiments and any future research in DCN. The workflow of our video recognition algorithm is shown in Fig.~ _ref_ . Motivated by the competitive performance of static image feature in previous research _cite_, we apply DCN on frame basis and aggregate frame-wise result using late fusion. We use the image dataset to initialize the network with meaningful visual pattern in static image and augment the video dataset. These transfer learning approaches lead to better DCN models and improve recognizer performance. While some recent works exploit various aggregation methods such as recurrent neural network (RNN) _cite_ over DCN, their models are still based on frame-wise DCN and can benefit from better DCN models obtained by our transfer learning approach. We verify the efficacy of the transfer learning approach on two video datasets that are popular for high level semantic concept recognition. We also show that important knowledge can be extracted from image corpora without any human labeling effort. While fully human annotated image corpora lead to better performance, even image datasets containing only weak supervision collected from the Internet can significantly boost the video recognition rate. In practice, we can learn robust video recognizers using unlabeled image dataset and Nk annotated videos labeled in frame-level. The annotation effort is much less than existing works on image recognition. Our main contribution is that we successfully learn a DCN with reasonable performance using only scarce training data. With the transfer learning approach, we can apply DCN in more general visual recognition problems and datasets. Our systematic study on the meta-parameters (e.g., image resolution, depth, training data diversity) of DCN also provides better knowledge about how to configure the networks for future researches. The rest of the paper is organized as follows. In Section~ _ref_, we review the related works of DCN. In Section~ _ref_, we describe the proposed method for transfer learning. In Section~ _ref_, we describe the datasets we used in this work and their properties, and we summarize the DCN architectures we used in Section~ _ref_ . Our preliminary studies on the DCN architectures are in Section~ _ref_ . Experiment results for transfer learning are in Section~ _ref_ . Finally, we summarize the work in Section~ _ref_ .