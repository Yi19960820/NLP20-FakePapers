MAGE and text both contain very rich semantics but reside in heterogeneous modalities. Comparing to information retrieval within the same modality, matching image-text poses extra critical challenges, mapping images and text onto one shared feature space. For example, a model needs to distinguish between the ``black dog'', ``gray dog'' and ``two dogs'' in the text, and understand the visual differences in images depicting ``black dog'', ``gray dog'' and ``two dogs''. In this paper, given an unseen image (text) query, we aim to measure its semantic similarity with the text (image) instances in the database and retrieve the true matched texts (images) to the query. Considering the testing procedure, this task requires connecting the two modalities with robust representations. In the early times, some relatively small datasets were used, \eg, Wikipedia _cite_ and Pascal Sentence _cite_, which contain around N, N and N, N image-text pairs, respectively. In recent years, several large-scale datasets with more than N, N images, including MSCOCO _cite_ and FlickrNk _cite_, have been introduced. Each image in these datasets is annotated with around five sentences. These large datasets allow deep architectures to learn robust representations and provide challenging evaluation scenarios. During the past few years, ranking loss is commonly used as the objective function _cite_ for image-text representation learning. The ranking loss aims to make the distance between positive pairs smaller than that between negative pairs by a predefined margin. In image-text matching, every training pair contains a visual feature and a textual feature. The ranking loss focuses on the distance between the two modalities. Its potential drawback is that it does not explicitly consider the feature distribution in a single modality. For example, when using ranking loss during training which does not distinguish between the slight differences in images, then given two testing images with slightly different semantics, the model may output similar descriptors for the two images. This is clearly undesirable for image / text matching considering the extremely fine granularity of this task. In our experiment, we observe that using the ranking loss alone in end-to-end training may cause the network to be stuck in a local minimum. What motivates us is the effectiveness of class labels in earlier years of cross-media retrieval _cite_ . In these works, the class labels are annotated manually and during testing, the aim is to retrieve image / text belonging to the same class to the query. In light of this early practice, this paper explores the feasibility of ``class labels'' in image / text matching, which is an instance retrieval problem. Two differences exist between cross-media retrieval on the category level _cite_ and on the instance level (considered in this paper) . First, the true matches are those with the same category, and those with the exact same content with the query, respectively. That is to say, instance-level retrieval has a more strict matching criteria than category-level retrieval. Second, instance-level retrieval does not assume the existence of class labels. In this field of research, only image / text pairs are utilized during training. Given the intrinsic differences between the two tasks, it is non-trivial to directly transfer the experience from using class labels in category-level retrieval to instance-level retrieval. Without annotated class labels, how can we initiate the investigation of the underlying data structures in the image / text embedding space? In this paper, we name an image and its associated sentences an ``image / text group''. Our key assumption is that each ``image / text'' group is different from the others, and can be viewed as a distinct class (see Fig. _ref_) . So we propose a classification loss called instance loss to classify the image / text groups. Using this unsupervised class labels as supervision, we aim to enforce the model to discriminate each two images and two sentences (from different groups) . It helps to investigate the fine-grained difference in single modality (intra-modal) and provides a good initialization for ranking loss which is a driving force for end-to-end retrieval representation learning. In more details, using such an unsupervised assumption, we train the network to classify every image / text group with the softmax loss. In the experiment, we show that the instance loss which classifies a large number of classes, \ie, N, N image / text groups on MSCOCO _cite_, is able to converge without any hyper-parameter tuning. Improved retrieval accuracy can be observed as a result of instance loss. In addition, we notice in the field of image-text matching that most recent works employ off-the-shelf deep models for image feature extraction _cite_ . The fine-tuning strategy commonly seen in other computer vision tasks _cite_ is rarely adopted. A drawback of using off-the-shelf models is that these models are usually trained to classify objects into semantic categories _cite_ . The classification models are likely to miss image details such as color, number, and environment, which may convey critical visual cues for matching images and texts. For example, a model trained on ImageNet _cite_ can correctly classify the three images as ``dog''; but it may not tell the difference between and, or between and . The ability to convey critical visual cues is a necessary component in instance-level image-text matching. Similar observations have been reported with regards to image captioning _cite_ . Moreover, for the text feature, _cite_ is a popular choice in image-text matching _cite_ . Aiming to model the context information, the model is learned through a shallow network to predict neighboring words. However, the model is initially trained on GoogleNews, which differs substantially from the text in the target dataset. As such, instead of using the off-the-shelf model, we explore the possibility of fine-tuning the model using image-text matching datasets. Briefly, inspired by the effectiveness of class labels in early-time cross-media retrieval, we propose a similar practice in image-text matching called ``instance loss''. Instance loss works by providing better weight initialization for the ranking loss, thus producing more discriminative and robust image / text descriptions. Next, we also note that the pretrained CNN models may not meet the fine-grained requirement in image / text matching. So we construct a dual path CNN to extract image and text features directly from data rather. The network is end-to-end trainable and yields superior results to using features extracted from off-the-shelf models as input. Our contributions are summarized as follows: We note that Ma \etal also apply the CNN structure for text feature learning _cite_ . The main difference between our method and _cite_ is two-fold. First, Ma \etal _cite_ use the ranking loss alone. In our method, we show that the proposed instance loss can further improve the result of ranking loss. Second, in _cite_, four text CNN models are used to capture different semantic levels \ie, word, short phrase, long phrase and sentence. In this paper, only one text CNN model is used and the word-level input is considered. Our model uses the residual block shown in Fig. _ref_, which combines low level information \ie, word, as well as high level inference to produce the final feature. In experiment (Table _ref_ and Table _ref_), we show that using on the same image CNN (VGG-N), our method (with one text CNN) is superior to _cite_ with text model ensembles by a large margin. The rest of this paper is organized as follows. Section _ref_ reviews and discusses the related works. Section _ref_ describes the proposed Image-Text CNN Structure in detail, followed by the objective function in Section _ref_ . Training policy is described in Section _ref_ . Experimental results and comparisons are discussed in Section _ref_ and conclusions are in Section _ref_ . Furthermore, some qualitative results are included in Appendix.