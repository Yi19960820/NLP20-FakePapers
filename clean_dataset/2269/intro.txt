Dense image labeling is a problem of paramount importance in the computer vision community as it encompasses many low or high level vision tasks including stereo matching~ _cite_, optical flow~ _cite_, surface normals estimation~ _cite_, and semantic segmentation~ _cite_, to mention a few characteristic examples. In all these cases the goal is to assign a discrete or continuous value for each pixel in the image. Due to its importance, there is a vast amount of work on this problem. Recent methods can be roughly divided into three main classes of approaches. The first class focuses on developing independent patch classifiers/regressors ~ _cite_ that would directly predict the pixel label given as input an image patch centered on it or, in cases like stereo matching and optical flow, would be used for comparing patches between different images in order to pick pairs of best matching pixels ~ _cite_ . Deep convolutional neural networks (DCNNs) ~ _cite_ have demonstrated excellent performance in the aforementioned tasks thanks to their ability to learn complex image representations by harnessing vast amount of training data~ _cite_ . However, despite their great representational power, just applying DCNNs on image patches, does not capture the structure of output labels, which is an important aspect of dense image labeling tasks. For instance, independent feed-forward DCNN patch predictors do not take into consideration the correlations that exist between nearby pixel labels. In addition, feed-forward DCNNs have the extra disadvantages that they usually involve multiple consecutive down-sampling operations (i.e. max-pooling or strided convolutions) and that the top most convolutional layers do not capture factors such as image edges or other fine image structures. Both of the above properties may prevent such methods from achieving precise and accurate results in dense image labeling tasks. Another class of methods tries to model the joint dependencies of both the input and output variables by use of probabilistic graphical models such as Conditional Random Fields (CRFs) ~ _cite_ . In CRFs, the dense image labeling task is performed through maximum a posteriori (MAP) inference in a graphical model that incorporates prior knowledge about the nature of the task in hand with pairwise edge potential between the graph nodes of the label variables. For example, in the case of semantic segmentation, those pairwise potentials enforce label consistency among similar or spatially adjacent pixels. Thanks to their ability to jointly model the input-output variables, CRFs have been extensively used in pixel-wise image labelling tasks~ _cite_ . Recently, a number of methods has attempted to combine them with the representational power of DCNNs by getting the former (CRFs) to refine and disambiguate the predictions of the later one~ _cite_ . Particularly, in semantic segmentation, DeepLab~ _cite_ uses a fully connected CRF to post-process the pixel-wise predictions of a convolutional neural network while in CRF-RNN~ _cite_, they unify the training of both the DCNN and the CRF by formulating the approximate mean-field inference of fully connected CRFs as Recurrent Neural Networks (RNN) . However, a major drawback of most CRF based approaches is that the pairwise potentials have to be carefully hand designed in order to incorporate simple human assumptions about the structure of the output labels _inline_eq_ and at the same time to allow for tractable inference. A third class of methods relies on a more data-driven approach for learning the joint space of both the input and the output variables. More specifically, in this {case} a deep neural network gets as input an initial estimate of the output labels and (optionally) the input image and it is trained to predict a new refined estimate for the labels, thus being implicitly enforced to learn the joint space of both the input and the output variables. The network can learn either to predict new estimates for all pixel labels ~ (transform-based approaches) _cite_, or alternatively, to predict residual corrections w.r.t. the initial label estimates (residual-based approaches) ~ _cite_ . We will hereafter refer to these methods as . These are, loosely speaking, related to the CRF models in the sense that the deep neural network is enforced to learn the joint dependencies of both the input image and output labels, but with the advantage of being less constrained about the complexity of the input-output dependencies that it can capture. Our work {belongs to this} last category of dense image labeling {approaches, thus} it is not constrained on the complexity of the input-output dependencies that it can capture. However, {here we argue that prior approaches in this category use a sub-optimal strategy.} For instance, the transform-based approaches (that always learn to predict new label estimates) often have to learn something more difficult than necessary since they must often simply learn to operate as identity transforms in case of correct initial labels, yielding the same label in their output. On the other hand, for the residual based approaches it is easier to learn to predict zero residuals in the case of correct initial labels, but it is more difficult for them to refine ``hard'' mistakes that deviate a lot from the initial labels (see figure~ _ref_) . Due to the above reasons, in our work we propose a deep joint input-output model that decomposes the label estimation/refinement process as a sequence of the following easier to execute operations: N) of errors in the input labels, N) of the erroneous labels with new ones, and finally N) an overall of all output labels in the form of residual corrections. Each of the described operations in our framework is executed by a different component implemented with a deep neural network. Even more, those components are embedded in a unified architecture that is fully differentiable thus allowing for an end-to-end learning of the dense image labeling task by only applying the objective function on the final output. As a result of this, we are also able to explore a variety of novel deep network architectures by considering different ways of combining the above components, including the possibility of performing the above operations iteratively, as it is done in~ _cite_, thus enabling our model to correct even large, in area, regions of incorrect labels. It is also worth noting that the error detection component in the proposed architecture, by being forced to detect the erroneous pixel labels (given both the input and the initial estimates of the output labels), it implicitly learns the joint structure of the input-output space, which is an important requirement for a successful application of any type of structured prediction model. To summarize, our contributions are as follows: The remainder of the paper is structured as follows: We first describe our structured dense label prediction framework in \S _ref_ and its implementation w.r.t. the dense disparity estimation task (stereo matching) in \S _ref_ . Then, we provide experimental results in \S _ref_ and we finally conclude the paper in \S _ref_ .