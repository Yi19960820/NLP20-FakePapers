RGB-D based action recognition has attracted much attention in recent years due to the advantages that depth information brings to the combined data modality. For example, depth is insensitive to illumination changes and has rich ND structural information of the scene. However, depth alone is often insufficient for recognizing some actions. In the task of recognizing human-object interactions where texture is vital for successful recognition, depth does not capture the necessary texture context. To exploit the complementary nature of the two modalities, methods~ _cite_ have been proposed to combine the two modalities for RGB-D action recognition and demonstrated the effectiveness of modality fusion. However, most of these methods are based on shallow hand-crafted features and tend to be dataset-dependent. The advent of deep learning has led to the development of methods~ _cite_ based on Convolutional Neural Network (ConvNet) or Recurrent Neural Network (RNN) . These methods take as input either RGB or depth or both of them as independent channels and fuse the recognition scores of individual modalities. It is noteworthy that none of these methods address the problem of using heterogeneous inputs (such as RGB and depth) in a cooperative manner to train a single network for action recognition. This cooperative training paradigm allows the powerful representation capability of deep neural network to be fully leveraged and to explore the complementary information in the two modalities using one single network architecture. The need for independent processing channels is thus obviated. Motivated by this observation, in this paper, we propose to adopt deep cooperative neural networks for recognition from the RGB and depth modalities. One typical challenge in deep learning based action recognition is how a RGB-D sequence could be effectively represented and fed to deep neural networks for recognition. For example, one can conventionally consider it as a sequence of still images (RGB and depth) with some form of temporal smoothness, or as a subspace of images or image features, or as the output of a neural network encoder. Which one among these and other possibilities would result in the best representation in the context of action recognition is not well understood. In addition, it is not clear either how the two heterogeneous RGB and depth channels can be represented and fed into a single deep neural network for the cooperative training. Inspired by the promising performance of the recently introduced rank pooling machine~ _cite_ on RGB videos, the rank pooling method is adopted to encode both RGB and depth sequences into compatible dynamic images. A dynamic image contains the temporal information of a video sequence and keeps the spatio-temporal structured relationships of the video; this has been demonstrated to be an effective video descriptor~ _cite_ . Based on this pair of dynamic images, namely, RGB visual dynamic images (VDIs) and depth dynamic images (DDIs), a cooperatively trained convolutional neural networks (c-ConvNet) is proposed to exploit the two modality features and enhance the capability of ConvNets for cases in which the features arise from either or both sources. There are two key issues in using a single c-ConvNet for heterogeneous modalities. First, how to enhance the discriminative power of ConvNets and second, how to reduce the modality discrepancy. Specifically, in most classification cases, the conventional ConvNets can learn separable features but they are often not compact enough to be discriminative~ _cite_ . Modality discrepancy arises because the feature variations in different modalities pose a challenge for a single network to learn modality-independent features for classification. To handle these two issues, we propose to jointly train a ranking loss and a softmax loss for action recognition. The ranking loss consists of two intra-modality and cross-modality triplet losses, which reduces variations in both intra-modality and cross-modality. Together with the softmax loss, the signal intra-modality triplet loss enables the c-ConvNet to learn more discriminative features, while the inter-modality triplet loss weakens or eliminates the modalities distribution variations and only focuses on inter-action variations. Moreover, in this way, the correlations between RGB and depth data are embedded in the c-ConvNet, and can be retrieved and contribute to the recognition even in the case where only one of the modalities is available. Furthermore, due to the image structure of dynamic images, the proposed c-ConvNet can be fine-tuned on the pre-trained networks on ImageNet, thus making it possible to work on small datasets. The c-ConvNet was evaluated extensively on three datasets: two large datasets, ChaLearn LAP IsoGD~ _cite_ and NTU RGB + D~ _cite_ datasets, and one small one, SYSU ND HOI~ _cite_ dataset. Experimental results achieved are state-of-the-art.