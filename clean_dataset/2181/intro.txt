The visual grounding of language plays an indispensable role in our daily lives. We use language to name, refer, and describe objects, their properties and generally, visual concepts. Distributional semantics (\eg, global word embeddings~ _cite_) based on large-scale corpora have shown great success in modeling the functionality and correlation of words in the natural language domain. This further contributes to the success in numerous natural language processing (NLP) tasks such as language modeling~ _cite_, sentiment analysis~ _cite_, and reading comprehension ~ _cite_ . However, effective and efficient grounding of distributional embeddings remains challenging. Being ignorant of the corresponding visual concepts, pure textual embeddings demonstrate inferior performances when incorporating with visual inputs. A set of typical tasks includes image/video captioning, multi-modal retrieval/understanding, and visual reasoning, some of which are further extensively studied in the paper. Visual concept and its link with textual semantics, as a cognitive alignment, provide rich supervision to learning systems. Introduced in, Visual-Semantic Embedding (VSE) aims at building the bridge between natural language and the underlying visual world by jointly optimize and align the embedding spaces of both images and descriptive texts (captions) . Nevertheless, even for large-scale datasets such as MS-COCO~ _cite_, the number of image-caption pairs are far less than the number of possible constitutions of real-world semantics, making the dataset inevitably sparse and biased. To reveal this, we begin with constructing textual adversarial samples to attack the state-of-the-art system VSE + + ~ _cite_ . Specifically, we study the composition of sentences from two aspects: (N) content words including nouns and numerals and (N) prepositions indicating spatial relations (\eg, in, on, above, below) . As shown in Figure~ _ref_, we manipulate the original caption to construct hard negative captions with similar structure but completely contradictory semantics. We found that the models easily get confused, suffering a noticeable drop in confidence or even wrong predictions in the caption retrieval task. We propose VSE-C, which enforces the learning of correlation and correspondence between textual semantics and visual concepts by providing contrastive adversarial samples during the training procedure, incorporating with an intra-pair hard negative sample mining. Instead of defending adversarial attacks, we focus on the study of limitations of current visual-semantic datasets and the transferability of learned embeddings. To fulfill the large gap between the number of parallel image-caption pairs and the expressiveness of natural languages, we augment the data by employing a set of heuristic rules to generate large sets of contrastive negative captions, as demonstrated in Figure~ _ref_ . The candidates are selectively used for training by an intra-pair hard-example mining technique. VSE-C alleviate the bias of dataset and provide rich and effective samples on par with original image captions. This strengthen the link between text and visual concepts by requiring models to detect a mismatch on the level of some precise concepts. VSE-C learns discriminative and visually-grounded word embeddings on the MS-COCO dataset~ _cite_ . It is extensively compared with existing works with rich experiments and analyses. Most importantly, we explore the transferability of the learned embeddings on several real-world applications both qualitatively and quantitatively, including image-to-text retrieval and bidirectional word-to-concept retrieval. Furthermore, VSE-C demonstrates a general framework for augmenting textual inputs considering semantical consistency. The introduction human priors and knowledge bases alleviates the sparsity and non-contiguity of languages. We release our codes and data at _url_ .