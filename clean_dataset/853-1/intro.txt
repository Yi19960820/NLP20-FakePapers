The recent advances in deep Convolutional Neural Networks (CNNs) have demonstrated high capability in visual recognition. For instance, an ensemble of residual nets _cite_ achieves N \% top-N error on the ImageNet test set, which is even lower than N \% of the reported human-level performance. The achievements have relied on the fact that learning deep CNNs requires large quantities of annotated data. As a result, the standard optimization of deep CNNs does not offer a satisfactory solution for learning new categories from very little data, which is generally referred to as ``One-Shot or Few-Shot Learning" problem. One possible way to alleviate this problem is to capitalize on the idea of transfer learning _cite_ by fine-tuning a pre-trained network from another task with more labelled data. However, as pointed out in _cite_, the benefit of a pre-trained network will greatly decrease especially when the network was trained on the task or data which is very different from the target one, not to mention that the very little data may even break down the whole network due to overfitting. More importantly, the general training procedure which contains a number of examples per category in each batch does not match inference at test time when only a single or very few examples of a new category is given. This discrepancy affects the generalization of the learnt deep CNNs from prior knowledge. We propose to mitigate the aforementioned two issues in our one-shot learning framework. First, we induce from a single or few examples per category to form a small set of labelled images (support set) in each batch of training. The optimization of our framework is then performed by recognizing other instances (unlabelled images) from the categories in the support set correctly. As such, the training strategy is amended particularly for one-shot learning so as to match inference in the test stage. Moreover, a memory module is leveraged to compress and generalize the input set into slots in the memory and produce the outputs holistically on the whole support set, which further enhances the recognition. Second, we feed the memory slots into one Recurrent Neural Networks (RNNs), as a contextual learner, to predict the parameters of CNNs for the unlabelled images. As a result, the contextual learner captures both long-term memory across all the categories in the training and short-term knowledge specified on the categories at test time. Note that our solution does not require a fine-tuning process and computes the parameters on the fly. In addition, the memory is an uniform medium which could convert different size of support sets into common memory slots, making it very flexible to train an unified model irrespective of the number of shots and categories. By consolidating the idea of learning a learner to predict parameters in networks and matching training and inference strategy, we present a novel Memory Matching Networks (MM-Net) for one-shot image recognition, as shown in Figure _ref_ . Specifically, a single or few examples per category are fed into a batch every time as a support set of labelled images in training. A deep CNNs is exploited to learn image representations, which update the memory through a write controller. A read controller enhances the image representations with the memory across all the categories to produce feature embeddings of images in the support set. Meanwhile, we take the memory slots as a sequence of inputs to a contextual learner, i.e., bidirectional Long Short-Term Memory (bi-LSTM) networks, to predict the parameters of the convolutional layers in the CNNs. The outputs of CNNs are regarded as embeddings of unlabelled images. As such, the contextual relations between categories are also explored in learning network parameters. The dot product between the embeddings of a given unlabelled image and each image in the support set is computed as the similarity and the label of the nearest one is assigned to this unlabelled image. The whole deep network is end-to-end optimized by minimizing the error of predicting the labels in the batch conditioned on the support set. It is also worth noting that we could form each batch with different number of shots and categories in training stage to learn an unified architecture for performing inference on any one-shot learning scenarios. At inference time, the support set is then replaced by the examples from new categories and there is no any change in the procedure. The main contribution of this work is the proposal of Memory Matching Networks for addressing the issue of one-shot learning in image recognition. The solution also leads to the elegant views of how the discrepancy between training and inference in one-shot learning should be amended and how to make the parameters of CNNs computable on the fly in the context of very little data, which are problems not yet fully understood in the literature.