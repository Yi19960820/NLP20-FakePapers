Image-to-image translation attempts to convert the image appearance from one domain to another while preserving the intrinsic image content. Many computer vision tasks can be formalized as a certain image-to-image translation problem, such as super-resolution~ _cite_, image colorization~ _cite_, image segmentation _cite_, and image synthesis _cite_ . However, conventional image-to-image translation methods are all task specific. A common framework for universal image-to-image translation remains as an emerging research subject in the literature, which has gained considerable attention in recent studies _cite_ . Isola \etal~ _cite_ leveraged the power of generative adversarial networks (GANs) _cite_, which encourage the translation results to be indistinguishable from real images in the target domain, to learn image-to-image translation from image pairs in a supervised fashion. However, obtaining pairwise training data is time-consuming and heavily relies on human labor. Recent works _cite_ explore tackling the image-to-image translation problem without using pairwise data. Under the unsupervised setting, besides the traditional adversarial loss used in supervised image-to-image translation, a cycle-consistent loss is introduced to restrain the two cross-domain transformations _inline_eq_ and _inline_eq_ to be the inverses of each other (\ie, _inline_eq_ and _inline_eq_) . By constraining both of the adversarial and cycle-consistent losses, the networks learn how to accomplish cross-domain transformations without using pairwise training data. Despite the progress mentioned above, existing unsupervised image-to-image translation methods may generate inferior results when two image domains are of significant appearance differences or the image resolution is high. As shown in Figure _ref_, the result of CycleGAN~ _cite_ in translating a Cityscapes semantic layout to a realistic picture lacks details and remains visually unsatisfactory. The reason for this phenomenon lies in the significant visual gap between the two distinct image domains, which makes the cross-domain transformation too complicated to be learned by running a single-stage unsupervised approach. Jumping out of the scope of unsupervised image-to-image translation, many methods have leveraged the power of multi-stage refinements to tackle image generation from latent vectors~ _cite_, caption-to-image~ _cite_, and supervised image-to-image translation~ _cite_ . By generating an image in a coarse-to-fine manner, a complicated transformation is broken down into easy-to-solve pieces. Wang \etal~ _cite_ successfully tackled the high-resolution image-to-image translation problem in such a coarse-to-fine manner with multi-scale discriminators. However, their method relies on pairwise training images, so cannot be directly applied to our studied unsupervised image-to-image translation task. To the best of our knowledge, there exists no attempt to exploit stacked networks to overcome the difficulties encountered in learning unsupervised image-to-image translation. In this paper, we propose the stacked cycle-consistent adversarial networks (SCANs) aiming for unsupervised learning of image-to-image translation. We decompose a complex image translation into multi-stage transformations, including a coarse translation followed by multiple refinement processes. The coarse translation learns to sketch a primary result in low-resolution. The refinement processes improve the translation by adding details into the previous results to produce higher resolution outputs. We adopt a conjunction of an adversarial loss and a cycle-consistent loss in all stages to learn translations from unpaired image data. To benefit more from multi-stage learning, we also introduce an adaptive fusion block in the refinement processes to learn the dynamic integration of the current stage's output and the previous stage's output. Extensive experiments demonstrate that our proposed model can not only generate results with realistic details, but also enable us to learn unsupervised image-to-image translation in higher resolution. In summary, our contributions are mainly two-fold. Firstly, we propose SCANs to model the unsupervised image-to-image translation problem in a coarse-to-fine manner for generating results with finer details in higher resolution. Secondly, we introduce a novel adaptive fusion block to dynamically integrate the current stage's output and the previous stage's output, which outperforms directly stacking multiple stages.