imaging is a cornerstone of modern healthcare providing unique diagnostic, and increasingly therapeutic, capabilities that affect patient care. The range of medical imaging modalities is wide but in essence they provide anatomical and functional information about structure and physiopathology. The multi-modality _inline_eq_ F-Fluorodeoxyglucose (FDG) positron emission tomography and computed tomography (PET-CT) scanner is regarded as the imaging device of choice for the diagnosis, staging, and assessment of treatment response in many cancers~ _cite_ . PET-CT combines the sensitivity of PET to detect regions of abnormal function and anatomical localization from CT~ _cite_ . With PET, sites of disease usually display greater FDG uptake (glucose metabolism) than normal structures. The spatial extent of the disease within a particular structure, however, cannot be accurately determined due to the inherent lower resolution of PET when compared to CT and MR imaging, tumor heterogeneity, and the partial volume effect~ _cite_ . CT provides the anatomical localization of sites of abnormal FDG uptake in PET and so adds precision to image interpretation~ _cite_ . One example clinical domain that has benefited greatly from PET-CT imaging is the evaluation of non-small cell lung cancer (NSCLC), the most common type of lung cancer. In NSCLC, the extent of the disease at diagnosis is the most important determinant of patient outcome: whether it is restricted to the lung parenchyma, involved the lung plus regional lymph nodes at the pulmonary hilum and/or mediastinum, extends into the mediastinum or chest wall directly, or has spread beyond the thorax. PET-CT is able to detect sites of disease where there are no abnormalities in the underlying structure on CT, hence its value in patient management~ _cite_ . The role of PET-CT in cancer care has provoked extensive research into methods to detect, classify, and retrieve PET-CT images~ _cite_ . These methods can be separated into those that: (i) process each modality separately and then combine the modality-specific features~ _cite_, and (ii) combine or fuse complementary features from each modality~ _cite_ . Methods that process each modality separately are inherently limited when the intent is to consider both the function and anatomical extent of disease. For example, in a chest CT depicting a lung tumor that is causing collapse in adjacent lung tissue, both the tumor and the collapse can appear identical. Similarly, some areas of high FDG uptake on PET images may be linked to normal physiological uptake, such as in the heart, and these regions need to be filtered out based upon knowledge about anatomical characteristics from CT to differentiate them from abnormal PET regions~ _cite_ . In contrast, methods that fuse information from the two modalities often use knowledge about characteristics of the different modalities to `prioritize' information from one of the two modalities for different tasks. Alternatively, they may fuse information using a representation that models relationships between the two modalities. The fusion is particularly necessary in cases where different imaging modalities identify different attributes of the same region of interest (ROI), with no one modality capturing the entire ROI~ _cite_ . Bagci et al.~ _cite_ proposed a method to simultaneously delineate ROIs in PET, PET-CT, PET-MR imaging, and fused MR-PET-CT images using a random walk segmentation algorithm with an automated similarity-based seed selection process. Zhao et al.~ _cite_ combined dynamic thresholding, watershed segmentation, and support vector machine (SVM) classification to classify solitary pulmonary nodules on the basis of CT texture features and PET metabolic features. Similarly, Lartizien et al.~ _cite_ used texture feature selection and SVM classification for staging of lymphoma patients. Y. Song et al.~ _cite_, Q. Song et al.~ _cite_, and Ju et al.~ _cite_ used the context of PET and CT regions to characterize tumors with spatial and visual consistency. Han et al.~ _cite_ segmented tumors from PET-CT images, formulating the problem as a Markov Random Field with modality-specific energy terms for PET and CT characteristics. In our prior work~ _cite_, we used multi-stage discriminative models to classify ROIs in the thoracic PET-CT images and in full-body lymphoma studies. In our PET-CT retrieval research~ _cite_, we have also derived a graph-based model that attempts to bridge the semantic gap by modeling the spatial characteristics that are important for lung cancer staging~ _cite_ . These methods are highly dependent upon an external predefined specification of the relationship between the features from both modalities. Hence, the ability to derive an application-specific fusion would reduce this dependency. For multi-modality medical imaging, fusion is necessary for computer aided diagnosis tasks such as image visualization, lesion detection, lesion segmentation, and disease classification. For example, Li et al.~ _cite_ designed a fusion technique for denoising and enhancing the visual details in multi-modality medical images, while Tong et al.~ _cite_ fused information from MR and PET images with clinical and genetic biomarkers to predict Alzheimer's disease. Current image fusion strategies in the general (non-medical) domain derive spatially varying fusion weights from the local visual characteristics of the different image data~ _cite_ . Features such as pixel variance, contrast, and color saturation are used to derive task-specific fusion ratios for different regions of interest (ROIs) within the images~ _cite_ . These fusion methods can thus adapt to and prioritize different content at different locations in the images according to the underlying image features that are relevant to the different images being analyzed. This results in the capacity to enhance specific information from different image data for different tasks. In the case of cancer, a disease that can spread throughout the body, a spatially varying fusion may enhance analysis of the multi-modality medical image data~ _cite_ . This may enable greater scrutiny of heterogeneous tumors, better investigation of tumours at and across different tissue boundaries (e.g., tumor invasion of adjacent anatomical structures) ~ _cite_, and through integration into visualisation pipelines improve clinicians' interpretation of a patientâ€™s data (e.g., for disease staging) ~ _cite_ . Our hypothesis is that the derivation of an appropriate spatially varying fusion of multi-modality image data should be learned from the underlying visual features of the individual images, as this will enable a better integration of the complementary information within each modality. The state-of-the-art in feature learning, selection, and extraction are deep learning methods~ _cite_ . Convolutional neural networks (CNNs) ~ _cite_ are deep learning methods for object detection, classification, and analysis of image data. CNNs have achieved better results than non-deep learning methods in many benchmark tasks, e.g., the ImageNet Large Scale Visual Recognition Challenge~ _cite_ . This dominance relates to the ability of CNNs to implicitly learn image features that are `meaningful' for a given task directly from the image data. Initial research in medical image analysis used CNN approaches that `transferred' features learned from a non-medical domain and tuned them to a specific medical task~ _cite_, e.g., classification of the modality of the medical images depicted in research literature~ _cite_, and the localization of planes in fetal ultrasound images~ _cite_ . Later studies designed new CNNs for specific clinical challenges, such as the classification of interstitial lung disease in CT images~ _cite_ . Transfer learning approaches have also been used for multi-modality image classification~ _cite_ . Bi et al.~ _cite_ used domain-transferred CNNs to extract PET features for PET-CT lymphoma classification. Bradshaw et al.~ _cite_ fine-tuned a CNN (pre-trained on ImageNet data) for PET-CT images in a multi-channel input approach, using a CT slice and two maximum intensity projections of the PET data as the inputs. CNNs have also been specifically trained for a number of multi-modality medical image analysis applications. One area of focus are brain MR images obtained with different sequences that are often treated as multi-modality images with the reasoning that the different MR images showed different aspects of the same anatomical structure~ _cite_ . Zhang et al.~ _cite_ designed a CNN-based segmentation approach for brain MR images based upon this reasoning. Similarly, Tseng et al.~ _cite_ segmented ROIs with complementary features that were learned via a convolution across the different MR images. Van Tulder and de Bruijne~ _cite_ used an unsupervised approach to learn a shared data representation of MR images, which acted as a robust feature descriptor for classification applications. In the wider, multi-modality image domain, Liu et al.~ _cite_ used a convolutional autoencoder to detect air, bone, and soft tissue for attenuation correction in PET-MR images. Teramoto et al.~ _cite_ used a CNN as a second stage classifier to determine if candidate lung nodules in PET-CT were false positives. Xu et al.~ _cite_ cascaded two V-Nets~ _cite_ to detect bone lesions, using CT alone as the input to the first V-Net and a pre-fused PET-CT image for the second. Zhao et al.~ _cite_ also used V-Nets in a multi-branch paradigm for lung tumor segmentation. Similarly, Zhong et al.~ _cite_ trained one U-Net~ _cite_ for PET and one for CT, combining the results using a graph cut algorithm. Li et al.~ _cite_ reported a variational model that integrated PET pixel intensity with a CNN-derived CT probability map to segment lung tumors. In general, CNNs have been applied to multi-modality image data as feature extractors and classifiers without consideration of how the features from each modality were combined, relying either on pre-fusion of the input data or independent processing of each input modality. In addition, recent research on CNN-based PET-CT lung tumor segmentation~ _cite_ learned from image patches centered around the tumor and did not consider variation of PET and CT image features for tumors occurring in different anatomical locations. Our aim was to improve fusion of the complementary information in multi-modality images for automatic medical image analysis. In particular, we focus on image data that depict disease across multiple anatomical locations. We present a new CNN that learns to fuse complementary anatomical and functional data from PET-CT images in a spatially varying manner. The novelty of our CNN is its ability to produce a fusion map that explicitly quantifies the fusion weights for the features in each modality. This is in contrast to CNNs that use multi-channel inputs~ _cite_, where modalities are implicitly fused, or modality-specific encoder branches~ _cite_, where the modality-specific features are concatenated at a later stage. Our co-learning CNN is intended as a general approach for integrating PET and CT information, with components that can be leveraged and optimized for a number of different medical image analysis tasks, such as visualization, classification, and segmentation. To demonstrate the efficacy of our CNN, we conducted experimental comparisons with baseline fusion and tumor segmentation methods on PET-CT lung cancer images.