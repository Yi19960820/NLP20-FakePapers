This paper addresses the problem of how to take pictures of the best moment using mobile phones. With the recent advances in hardware, such as Dual-Lens camera on iPhone N Plus, the quality of the pictures taken on mobile phones has been dramatically improved. However, capturing a great ``moment" is still quite challenging for common users, because anticipating the subject movements patiently while keeping the scene framed in viewfinder requires lots of practices and professional training. For example, taking spontaneous shots for children could be extremely hard as they may easily run out of the frame by the time you press the shutter. As a result, one may not only miss the desired moment, but also get a blurry photo due to the camera or subject motion. Taking another common example in portrait photography, keeping a perfect facial expression for long time without blinking eyes is nearly impossible. Therefore, it is likely that one has to replicate his pose and expression multiple times in order to capture a perfect shot, or one can use the burst mode to shot dozens of photos and then manually select the best one to keep and discard the rest. Although this method works for some people, it is less efficient due to the fact of wasting storage space and intensive manual selection. In this paper, we introduce a real-time system that automates the best frame (great moment) selection process ``during" the capture stage without any post-capture manual operations. Specifically, we propose to buffer a few frames before and another few frames after the shutter press, we then apply an efficient photo ranking model to surface the best moment and automatically remove the rest of them to save storage space. We argue that having a real-time capture system would dramatically lower the bar of high quality moment capture for memory keeping or social sharing. To our best knowledge, there is no prior work in academia that directly targets at building automatic moment capture system during the capture stage, not to say on mobile phones. This is mainly due to the following challenges. First, such a system needs to run during the capture stage in the viewfinder, the ranking model has to be compact enough to be deployed on mobile phones and fast enough to run in real-time. Second, learning such an efficient and robust ranking model is challenging because the visual differences within a sequence of burst images are usually very subtle, yet the criteria for relative ranking could range from low-level image quality assessment, such as blur and exposure, to high-level image aesthetics, such as the attractiveness of facial expression or body pose, requiring a holistic way of learning all such representations in one unified model. Last but not least, due to the uniqueness of this problem, there is no available burst image sequences to serve as our training data, and it is also unclear how to collect such supervision signals in an effective way. For the same reasons, we cannot leverage related works developed for automatic photo selection from personal photo albums, because their photo selection criteria primarily focus on absolute attributes such as low-level image quality _cite_, memorability _cite_, popularity _cite_, interestingness _cite_, and aesthetics _cite_ . In contrast, we are more interested in learning relative attributes that can rank a sequence of burst images with subtle differences. To address these challenges, we first created a novel burst dataset by manually capturing Nk bursts covering a rich set of common categories including selfies, portrait, landscaping, pets, action shots and so on. We sample image pairs from each burst and then conducted crowd-sourcing through Amazon Mechanical Turk (AMT) to get their overall relative goodness label (i.e., which looks better?) for each image pair. We consolidate the label information by a simple average voting. Second, considering a pair of images sampled from a burst, the visual content is largely overlapped, indicating the high-level features of a convolution network pre-trained for image classification may not be suitable for relative ranking, because classification network generally tries to achieve certain translation and rotation invariance and be robust to certain degree of image quality variations for the same object. However, those variances are the key information used for photo ranking. Therefore, in order to leverage the transfer learning from an existing classification net, one can only borrow the weights of the backbone net and must re-design a new head to tailer for our photo ranking problem. In addition to this, we observed that the relative ranking between a pair of images is determined by a few relative attributes such as sharpness, eye close or open, attractiveness of body pose or overall composition. And the overall ranker should be an aggregation of all such relative attributes. To enforce this observation, also inspired by recent advances in Generative Adversarial Networks (GANs) _cite_, we introduce another generator (denoted as ``G'') that can enhance the representation of the latent attributes so as to augment more training pairs in the feature space for improving the ranking model. Although we do not have the attribute level label information during the training, we expect the ranking network with a novel head can learn latent attribute values implicitly, so that it can minimize the ranking loss more easily. Motivated by the above facts and observations, we explored various choices for the backbone network and head (the final multi-layer module for ranking) design, and proposed a compact fully convolution network that can achieve good balance among model size, runtime speed, and accuracy. To sum up, we made the following contributions: