Sparseness is a concept of efficiency in neural networks, and exists in two variants in that context . The property means that only a small fraction of neurons is active at any time. The property means that each neuron is connected to only a limited number of other neurons. Both properties have been observed in mammalian brains and have inspired a variety of machine learning algorithms. A notable result was achieved through the sparse coding model of . Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex . Another example is the optimal brain damage method of, which can be used to prune synaptic connections in a neural network, making connectivity sparse. Although only a small fraction of possible connections remains after pruning, this is sufficient to achieve equivalent classification results. Since then, numerous approaches on how to measure sparseness have been proposed, see for an overview, and how to achieve sparse solutions of classical machine learning problems. The _inline_eq_ pseudo-norm is a natural sparseness measure. Its computation consists of counting the number of non-vanishing entries in a vector. Using it rather than other sparseness measures has been shown to induce biologically more plausible properties . However, finding of optimal solutions subject to the _inline_eq_ pseudo-norm turns out to be NP-hard . Analytical properties of this counting measure are very poor, for it is non-continuous, rendering the localization of approximate solutions difficult. The Manhattan norm of a vector is a convex relaxation of the _inline_eq_ pseudo-norm, and has been employed in a vast range of applications. This sparseness measure has the significant disadvantage of not being scale-invariant, so that an intuitive notion of sparseness cannot be derived from it. A normalized sparseness measure _inline_eq_ based on the ratio of the _inline_eq_ or Manhattan norm and the _inline_eq_ or Euclidean norm of a vector has been proposed by, where higher values indicate more sparse vectors. _inline_eq_ is well-defined because _inline_eq_ holds for all _inline_eq_ . As _inline_eq_ for all _inline_eq_ and all _inline_eq_, _inline_eq_ is also scale-invariant. As composition of differentiable functions, _inline_eq_ is differentiable on its entire domain. This sparseness measure fulfills all criteria of except for Dalton's fourth law, which states that the sparseness of a vector should be identical to the sparseness of the vector resulting from multiple concatenation of the original vector. This property, however, is not crucial for a proper sparseness measure. For example, sparseness of connectivity in a biological brain increases quickly with its volume, so that connectivity in a human brain is about _inline_eq_ ~times more sparse than in a rat brain . It follows that _inline_eq_ features all desirable properties of a proper sparseness measure. A sparseness-enforcing projection operator, suitable for projected gradient descent algorithms, was proposed by for optimization with respect to _inline_eq_ . For a pre-defined target degree of sparseness _inline_eq_, the operator finds the closest vector of a given scale that has sparseness _inline_eq_ given an arbitrary vector. This can be expressed formally as Euclidean projection onto parameterizations of the sets The first set is for achieving unrestricted projections, whereas the latter set is useful in situations where only non-negative solutions are feasible, for example in non-negative matrix factorization problems. The constants _inline_eq_ are target norms and can be chosen such that all points in these sets achieve a sparseness of _inline_eq_ . For example, if _inline_eq_ was set to unity for yielding normalized projections, then _inline_eq_ can be easily derived from the definition of _inline_eq_ . Hoyer's original algorithm for computation of such a projection is an alternating projection onto a hyperplane representing the _inline_eq_ norm constraint, a hypersphere representing the _inline_eq_ norm constraint, and the non-negative orthant. A slightly modified version of this algorithm has been proved to be correct by in the special case when exactly one negative entry emerges that is zeroed out in the orthant projection. However, there is still no mathematically satisfactory proof for the general case. This paper improves upon previous work in the following ways. Section~ _ref_ proposes a simple algorithm for carrying out sparseness-enforcing projections with respect to Hoyer's sparseness measure. Further, an improved algorithm is proposed and compared with Hoyer's original algorithm. Because the projection itself is differentiable, it is the ideal tool for achieving sparseness in gradient-based learning. This is exploited in Section~ _ref_, where the sparseness projection is used to obtain a classifier that features both sparse activity and sparse connectivity in a natural way. The benefit of these two key properties is demonstrated on a real-world classification problem, proving that sparseness acts as regularizer and improves classification results. The final sections give an overview of related concepts and conclude this paper. On the theoretical side, a first rigorous and mathematically satisfactory analysis of the properties of the sparseness-enforcing projection is provided. This is lengthy and technical and therefore deferred into several appendixes. Appendix~ _ref_ fixes the notation and gives an introduction to general projections. In Appendix~ _ref_, certain symmetries of subsets of the Euclidean space and their effect on projections onto such sets is studied. The problem of finding projections onto sets where Hoyer's sparseness measure attains a constant value is addressed in Appendix~ _ref_ . Ultimately, the algorithms proposed in Section~ _ref_ are proved to be correct. Appendix~ _ref_ investigates analytical properties of the sparseness projection and concludes with an efficient algorithm that computes its gradient. The gradients for optimization of the parameters of the architecture proposed in Section~ _ref_ are collected in the final Appendix~ _ref_ .