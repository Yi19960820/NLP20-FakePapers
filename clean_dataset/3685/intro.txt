Recent advances in deep learning have led to its wide adoption in various challenging tasks such as image classification. However, the current state of the art has been shown to be vulnerable to, small perturbations of the original inputs, often indistinguishable to a human, but carefully crafted to misguide the learning models into producing incorrect outputs. Recent results have shown that generating these adversarial examples are inexpensive~ _cite_ . Moreover, as safety critical applications such as autonomous driving increasingly rely on these tasks, it is imperative that the learning models be reliable and secure against such adversarial examples. Prior work has yielded a lot of attack methods that generate adversarial samples, and defense techniques that improve the accuracy on these samples (see related work for details) . However, defenses are often specific to certain attacks and cannot adaptively defend against any future attack and some general defense techniques have been shown to be ineffective against more powerful novel attacks. More generally, attacks and defenses have followed the cat-and-mouse game that is typical of many security settings. Further, traditional machine learning theory assumes a fixed stochastic environment hence accuracy in the traditional sense is not enough to measure performance in presence of an adversarial agent. In this paper, with the goal of generality, we pursue a principled approach to attacks and defense. Starting from a theoretical robustness definition, we present a attack and a defense that learns to generate adversarial examples against any given classifier and learns to defend against any attack respectively. Based on formal intuition, we categorize known attacks into high and low perturbation attacks. Our attack is a high perturbation attack and analogously our defense technique defends against high perturbation attack. For low perturbation attacks, we provide a approach that defends against such attacks. Our two defense techniques can be combined to defend against multiple types of attacks. While our guiding principle is general, this paper focuses on the specific domain of adversarial examples in image classification. Our is a definition of in presence of an adversarial agent. Towards the definition, we define the exploitable space by the adversary which includes data points already mis-classified (errors) by any given classifier and any data points that can be perturbed by the adversary to force mis-classifications. Robustness is defined as the probability of data points occurring in the exploitable space. We believe our definition captures the essence of the multi-agent defender-adversary interaction, and is natural as our robustness is a strictly stronger concept than accuracy. We also analyze why accuracy fails to measure robustness. The formal set-up also provides an intuition for all the techniques proposed in this paper. Our is an (ALN) . ALN is motivated by the fact that adversarial examples for a given classifier _inline_eq_ are subsets of the input space that the _inline_eq_ mis-classifies. Thus, given a data distribution with data points _inline_eq_ and a classifier _inline_eq_ trained on such data, we train a feed forward neural network _inline_eq_ with the goal of generating output points _inline_eq_ in the mis-classified space. Towards this end, we re-purpose an autoencoder to work as our ALN _inline_eq_ with a special choice of loss function that aims to make (N) the classifier _inline_eq_ mis-classify _inline_eq_ and (N) minimize the difference between _inline_eq_ and _inline_eq_ . Our are two defense techniques: (DLN) and (NAC) . Following the motivation and design of ALN, we motivate DLN _inline_eq_ as a neural network that, given any classifier _inline_eq_ attacked by an attack technique _inline_eq_, takes in an adversarial example _inline_eq_ and aims to generate benign example _inline_eq_ that lie in the mis-classified space of _inline_eq_ . The DLN is prepended to the classifier _inline_eq_ acting as a sanitizer for _inline_eq_ . Again, similar to the ALN, we re-purpose an autoencoder with a special loss function suited for the goal of the DLN. For non-adversarial inputs the DLN is encouraged to reproduce the input as well as make the classifier predict correctly. We show that DLN allows for attack and defense to be set up as a repeated competition leading to more robust classifiers. Next, while DLN works efficiently for attacks that produces adversarial examples with high perturbation, such as fast gradient sign method~ _cite_ (FGSM), it is not practical for low perturbation attacks (discussed in details in Section~ _ref_) such as Carlini-Wagner~ _cite_ (CW) . For low perturbation attacks, we present NAC which masks the classifier boundary by adding a very small noise at the logits output of the neural network classifier. The small noise added affects classification in rare cases, thereby ensuring original accuracy is maintained, but also fools low perturbation attacks as the attack is mislead by the logits. DLN and NAC can work together to defend simultaneously against both high and low perturbation attacks. We tested our approach on two datasets: MNIST and CIFAR-N. Our ALN based attack was able to attack all classifiers we considered and achieve performance comparable to other high perturbation attacks. Our defense approach made the resultant classifier robust to the FGSM and CW. Detailed experiments are presented in Section~ _ref_ and~ _ref_ . Missing proofs are in an online appendix (see footnote) .