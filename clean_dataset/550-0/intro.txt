Every day, human beings interact with the surrounding environments via perception and action. Through interacting with our environments, we gradually draw on our understanding of the functions that areas of the scene bear. For example, a round knob like object indicates that an action of spherical grasping and perhaps turning could be applied onto, and a faucet like entity with metallic texture indicates that an action of turning on or off water could be applied onto. As human beings, we recognize such functional areas in our daily environments with vision, so we can perform various of actions in order to finish a task. As robots (such as Baxter) begin to collaborate with humans in domestic environments, they will also need to recognize functional areas in order to develop a functional understanding of the scene. Imagine a Baxter robot with a mobile base enters an arbitrary kitchen, trying to clean up the mess from a dinner party (Fig.~ _ref_) . Before the robot can perform any dexterous movements, the robot needs to recognize functional areas of the scene such as where it can indicate actions like getting power for a vacuum machine, getting water, etc. Modern Computer Vision technologies allows robots to recognize objects from a known set of hundreds of categories. However, in the previous situation, a brutal applying of object detectors either does not suffice (for example, detecting cabinet does not indicate where and how to open), or is an overkill (for example, it is not necessary to differentiate between paper towel with towel as long as the robot understands a pinch grasp could be applied to get them) . Instead, we argue that more importantly the robot needs to know which part of the scene corresponds to which functionality, or to address the inevitable self-questioning: what can I do around here? As Gibson remarked, ``If you know what can be done with a [n] object, what it can be used for, you can call it whatever you please'' _cite_ . In this paper, we address the novel problem of {\bf localizing and identifying functional areas of an arbitrary indoor scene}, so that a robot can have a functional understanding of the visual scene in order to perform actions accordingly, and generalize this knowledge into novel scenes. Example outputs of the presented system are shown in Fig.~ _ref_: without recognizing the big salad bowl, the robot understands that a two-hand raise-and-move action (black bounding boxes) can be applied in a specific area of the kitchen; without seeing a specific type of handle bar before. However, from its appearance it should be able to infer that a wrap grasp could be applied to pull the bar (green bounding boxes), etc. Knowing ``what can I do here'' can serve as the first step for many applications in Robotics. For example, the presented pipeline could serve as a functional attention mechanism at the first glance of a novel scene for the robot. The output of the system can guide robot motion planning to move towards target functional area. Dexterous actions can then be planned in these attended areas. Also the usage of our system is not limited in the field of robotics. One potential application could be to provide verbalized functional instructions for the blind. From a computer vision and robotic vision perspective, recognizing functional areas from a static scene (in our experiments, we use static images to simulate the scene seen by a robot), is a challenging task since areas with unique visual appearance may indicate the same functionality. Also, unlike concepts such as objects, functionality itself can only be meaningful while a certain action could be applied on or leads to a certain consequence. Therefore we need an ontology for scene functionality. Furthermore, our goal is to provide a general functional scene understanding model for unconstrained indoor scene from real scenarios, and we have to overcome the huge variance naturally imposed from real world image. The main contributions of this paper are as follows: N) a scene functional area ontology for indoor domain; N) the first two stage, deep network based recognition approach is developed and presented for scene functional understanding, which is proved to be effective and efficient on two scene functional area datasets that are augmented from publicly available datasets; N) the first scene functionality dataset is compiled and made publicly available, which contains more than {\bf N, N} annotated training samples and two sets of testing images from different indoor scenes . .