Natural perception can extract complete interpretations of sensory data in a coherent and efficient manner. By contrast, machine perception remains a collection of disjoint algorithms, each solving specific information extraction sub-problems. Recent advances such as modern convolutional neural networks have dramatically improved the performance of machines in individual perceptual tasks, but it remains unclear how these could be in the same seamless way as natural perception does. In this paper, we consider the problem of learning data representations for integrated perception. The first question we ask is whether it is possible to learn that can be used to solve all sub-problems of interest. In computer vision, fine-tuning or retraining has been show to be an effective method to transfer deep convolutional networks between different tasks~ _cite_ . Here we show that, in fact, it is possible to learn a single, shared representation that performs well on several sub-problems, often as well or even better than specialised ones. A second question, complementary to the one of feature sharing, is how different perceptual subtasks should be combined. Since each subtask extracts a partial interpretation of the data, the problem is to form a coherent picture of the data as a whole. We consider an incremental interpretation scenario, where subtasks collaborate in parallel or sequentially in order to gradually enrich a shared interpretation of the data, each contributing its own ``dimension'' to it. Informally, many computer vision systems operate in this stratified manner, with different modules running in parallel or in sequence (e.g. \object detection followed by instance segmentation) . The question is how this can be done end-to-end and systematically. In this paper, we develop an architecture, (Fig.~ _ref_), that provides an answer to such questions. Multinet builds on the idea of a shared representation, called an integration space, which reflects both the statistics extracted from the data as well as the result of the analysis carried by the individual subtasks. As a loose metaphor, one can think the integration space as a ``canvas'' which is progressively updated with the information obtained by solving sub-problems. The representation distills this information and makes it available for further task resolution, in a recurrent configuration. Multinet has several advantages. First, by learning the latent integration space automatically, synergies between tasks can be discovered automatically. Second, tasks are treated in a symmetric manner, by associating to each of them encoder, decoder, and integrator functions, making the system modular and easily extensible to new tasks. Third, the architecture supports incremental understanding because tasks contribute back to the latent representation, making their output available to other tasks for further processing. Finally, while multinet is applied here to a image understanding setting, the architecture is very general and could be applied to numerous other domains as well. The new architecture is described in detail in section~ _ref_ and an instance specialized for computer vision applications is given in section~ _ref_ . The empirical evaluation in section~ _ref_ demonstrates the benefits of the approach, including that sharing features between different tasks is not only economical, but also sometimes better for accuracy, and that integrating the outputs of different tasks in the shared representation yields further accuracy improvements. Section~ _ref_ summarizes our findings. Multiple task learning (MTL): Multitask learning~ _cite_ methods have been studied over two decades by the machine learning community. The methods are based on the key idea that the tasks share a common low-dimensional representation which is jointly learnt with the task specific parameters. While MLT trains many tasks in parallel, Mitchell and Thrun~ _cite_ propose a sequential transfer method called Explanation-Based Neural Nets (EBNN) which exploits previously learnt domain knowledge to initialise or constraint the parameters of the current task. Breiman and Freidman~ _cite_ devise a hybrid method that first learns separate models and then improves their generalisation by exploiting the correlation between the predictions. Multi-task learning in computer vision: MTL has been shown to improve results in many computer vision problems. Typically, researchers incorporate auxiliary tasks into their target tasks, jointly train them in parallel and achieve performance gains in object tracking~ _cite_, object detection~ _cite_, facial landmark detection~ _cite_ . Differently, Dai~ et al. ~ _cite_ propose multi-task network cascades in which convolutional layer parameters are shared between three tasks and the tasks are predicted sequentially. Unlike _cite_, our method can train multiple tasks in parallel and does not require a specification of task execution. Recurrent networks: Our work is also related to recurrent neural networks (RNN) ~ _cite_ which has been successfully used in language modelling~ _cite_, speech recognition~ _cite_, hand-written recognition~ _cite_, semantic image segmentation~ _cite_ and human pose estimation~ _cite_ . Related to our work, Carreira~ et al. ~ _cite_ propose an iterative segmentation model that progressively updates an initial solution by feeding back error signal. Najibi~ et al. ~ _cite_ propose an efficient grid based object detector that iteratively refine the predicted object coordinates by minimising the training error. While these methods~ _cite_ are also based on an iterative solution correcting mechanism, our main goal is to improve generalisation performance for multiple tasks by sharing the previous predictions across them and learning output correlations.