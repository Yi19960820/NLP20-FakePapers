Deep Neural Networks (DNNs) have demonstrated state-of-the-art and sometimes human-competitive performance in numerous vision-related tasks~ _cite_, including image classification~ _cite_, object detection~ _cite_ and image/video captioning~ _cite_ . With such success, DNNs have been integrated into various intelligent systems as a key component, \eg., autonomous car~ _cite_, medical image analysis~ _cite_, financial investment~ _cite_, \etc. The high-performance of DNNs highly lies on the fact that they often stack tens of or even hundreds of nonlinear layers, and encode knowledge as numerical weights of various node-to-node connections. Although DNNs offer tremendous benefits to various applications, they are often treated as ``black box'' models because of their highly nonlinear functions and unclear working mechanism~ _cite_ . Without a clear understanding of what a given neuron in the complex models has learned and how it interacts with others, the development of better models typically relies on trial-and-error~ _cite_ . Furthermore, the effectiveness of DNNs is partly limited by its inabilities to explain the reasons behind the decisions or actions to human users. It is far from enough to provide eventual outcomes to the users especially for highly regulated environments, since they may also need to understand the rationale of the decisions. For example, a driver of an autonomous car is eager to recognize why obstacles are reported so that he/she can decide whether to trust it; and radiologists also require a clearly interpretable outcome from the system such that they can integrate the decision with their standard guideline when they make diagnosis. As an extreme case in~ _cite_, a DNN can be easily fooled, \ie, it is possible to produce images that DNNs believe to be recognizable objects with nearly certain confidence but are completely unrecognizable to humans. In summary, the counter-intuitive properties and the black-box nature of DNNs make it almost impossible for one to reason about what they do, foreseen what they will do, and fix the errors when potential problems are detected. Therefore, it is imperative to develop systems with good interpretability, which is an essential property for users to clearly understand, appropriately trust, and effectively interact with the systems. Recently, many research efforts have been devoted to interpreting hidden features of DNNs~ _cite_, and have made several steps towards interpretability, \eg, the de-convolutional networks~ _cite_ to visualize the layers of convolutional networks, and the activation maximization~ _cite_ to associate semantic concepts with neurons of a CNN. A few attempts have also been made to explore the effectiveness of various gates and connections of recurrent neural networks (RNNs) ~ _cite_ . Interpretability also bring us some benefits like weakly supervised detection~ _cite_ . However, these works often focus on analyzing relatively simple architectures such as AlexNet~ _cite_ for image classification. There still lack interpretation techniques for more complex architectures that integrates both CNN and RNN, in which the learned features are difficult to interpret and visualize. More importantly, these methods perform interpretation and visualization after the training process. It means that they can only explain a given model, but are unable to learn an interpretable model. Such a decoupling between learning and interpretation makes it extremely hard (if possible at all) to get humans to interact with the models (\eg, correct errors) . In this paper, we address the above limitations by presenting a method that incorporates the interpretability of hidden features as an essential part during the learning process. A key component of our method is to measure the interpretability and properly regularize the learning. Instead of pursuing a generic solution, we concentrate our attention on the video captioning task~ _cite_, for which DNNs have proven effective on learning highly predictive features while the interpretability remains an issue as other DNNs do. In this task, we leverage the provided text descriptions, which include rich information, to guide the learning. We first extract a set of semantically meaningful topics from the corpus, which cover a wide range of visual concepts including objects, actions, relationships and even the mood or status of objects, therefore suitable to represent semantic information. Then we parse the descriptions of each video to get a latent topic representation, \ie, a vector in the semantic space. We integrate the topic representation into the training process by introducing an interpretive loss, which helps to improve the interpretability of the learned features. To further interpret the learned features, we present a prediction difference maximization algorithm. We also present a human-in-the-loop learning procedure, through which users can easily revise false predictions and the model based on the good interpretation of the learned features. Our results on real-world datasets demonstrate the effectiveness.