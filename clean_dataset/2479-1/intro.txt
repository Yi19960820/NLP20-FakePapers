Invariant representations are core for vision, audio, and language models because they abstract our data. For example, we desire viewpoint and scale invariance in vision, reverberation and background noise invariance in audio, and synonym and grammar invariance in language. Discriminative, invariant representations learned from large datasets have enabled machines to understand unconstrained situations to huge success _cite_ . The goal of this paper is to create representations that are robust in another way: we learn representations that are aligned across modality. Consider the sentence ``she jumped into the pool.'' This same concept could also appear visually or aurally, such as the image of a pool or the sound of splashing. Representations are robust to modality if the there is alignment in the representation across modalities. The pool image, the splashing sound, and the above sentence should have similar representations. We believe aligned cross-modal representations will have a large impact in computer vision because they are fundamental components for machine perception to understand relationships between modalities. Cross-modal perception plays key roles in the human perceptual system to recognize concepts in different modalities _cite_ . Cross-modal representations also have many practical applications in recognition and graphics, such as transferring learned knowledge between modalities. In this paper, we learn rich deep representations that are aligned across the three major natural modalities: vision, sound, and language. We present a deep convolutional network that accepts as input either a sound, a sentence, or an image, and produces a representation shared across modalities. We capitalize on large amounts of in-the-wild data to learn this aligned representation across modalities. We develop two approaches that learn high-level representations that can be linked across modalities. Firstly, we use an unsupervised method that leverages the natural synchronization between modalities to learn an alignment. Secondly, we design an approach to transfer discriminative visual models into other modalities. Our experiments and visualizations show that a representation automatically emerges that detects high-level concepts independent of the modality. Figure _ref_ visualizes this learned representation: notice how units in the upper layers have learned automatically to detect some objects agnostic of the modality. We experiment with this representation for several multi-modal tasks, such as cross-modal retrieval and classification. Moreover, although our network is only trained with image + text and image + sound pairs, our representation can transfer between text and sound as well, a transfer the network never saw during training. Our primary contribution is showing how to leverage massive amounts of synchronized data to learn a deep, aligned cross-modal representation. While the methods in the paper are standard, their application on a large-scale to the three major natural modalities is novel to our knowledge. In the remainder of this paper, we describe the approach and experiments in detail. In section N, we discuss our datasets and modalities. In section N, we present a model for learning deep aligned cross-modal representations. In section N, we present several experiments to analyze our representations. Vision and Sound: Understanding the relationship between vision and sound has been recently explored in the computer vision community. One of the early works, _cite_, explored the cross-modal relations between ``talking head'' images and speech through CCA and cross-modal factor analysis. _cite_ applied CCA between visual and auditory features, and used common subspace features for aiding clustering in image-audio datasets. _cite_ explored interaction between visual and audio modalities through human behavior analysis using Kernel-CCA and Multi-view Hidden CRF. _cite_ investigates RBM auto-encoders between vision and sound. _cite_ investigated the relations between materials and their sound in a weakly-paired settings. Recent work _cite_ has capitalized on material properties to learn to regress sound features from video, learn visual representations _cite_, and _cite_ analyzes small physical vibrations to recover sounds in video. We learn cross-modal relations from large quantities of unconstrained data. Sound and Language: Even though the relation between sound and language is mostly studied in the line of speech recognition _cite_, in this paper we are interested in matching sentences with auditory signals. This problem is mainly studied in the audio retrieval setting. Early work _cite_ performs semantic audio retrieval by aligning sound clusters with hierarchical text clusters through probabilistic models. _cite_ applies a passive-aggressive model for content-based audio retrieval from text queries. _cite_ uses probabilistic models for annotating novel audio tracks with words and retrieve relevant tracks given a text-based query. However, we seek to learn the relationship between sound and language using vision as an intermediary, i.e. we do not use audio + text pairs. Language and Vision: Learning to relate text and images has been extensively explored in the computer vision community. Pioneering work _cite_ explore image-captioning as a retrieval task. More recently, _cite_ developed deep large-scale models to generate captions from images. In this paper, rather than generating sentences, we instead seek to learn a representation that is aligned with images, audio, and text. _cite_ explores aligned representations, but does not learn the representation with a deep architecture. Moreover, rather than using recurrent networks _cite_, we use convolutional networks for text. _cite_ learns to align books and moviesl. _cite_ learn joint image-tag embeddings through several CCA variations. We instead seek to align three natural modalities using readily-available large-scale data. While _cite_ harnesses clusters of tags as a third view of the data, we instead obtain clusters from images through state-of-the-art visual categorization models. This is crucial since only the image modality is shared in both image + sound and image + text pairs.