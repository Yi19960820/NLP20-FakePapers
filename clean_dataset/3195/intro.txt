Generative adversarial networks (GANs) are capable of producing sharp and realistic images by learning the generative process, instead of explicitly estimating the data distribution with variational bounds or strict model constraints. GANs _cite_ is composed of two networks, discriminator and generator, and they adversarially compete each other to approximate _inline_eq_ using _inline_eq_: the discriminator distinguishes real samples from fake samples produced by the generator, while the generator aims to create the sample as real as possible so that the discriminator cannot recognize it as the fake sample. The objective function of this adversarial learning process in _cite_ is defined by the following minimax game, {\footnotesize \} \ + _ {z \sim _ {}} \left \, \]} where _inline_eq_ denotes expectation, _inline_eq_ and _inline_eq_ are random variables for data and latent vector, where their probability distributions are _inline_eq_ and _inline_eq_, respectively. Most GAN algorithms learn an unidirectional mapping function from _inline_eq_ to _inline_eq_ for a single domain. Unlike those, our algorithm learns two mapping functions for two domains simultaneously; one associates _inline_eq_ to _inline_eq_, and the other maps the same _inline_eq_ to _inline_eq_ . Throughout this paper, we denote two different domains by _inline_eq_ and _inline_eq_, respectively. When _inline_eq_ and _inline_eq_ are samples generated from the same latent _inline_eq_, we aim to accomplish two objectives; N) two data obey their own data distribution, and N) two data hold shareable characteristics as similar as possible. For example, faces of human and those of cat represent different species, technically different domains. Hence, they have different shapes and structures. However, their posture, hair color, or facial expression can be similar in both domains, thus those attributes can be regarded as shareable characteristics. Then, our research goal is to generate a pair of human face and cat face that faithfully produce their domain characteristics, and at the same time both faces show the similar attributes such as pose, hair style, or facial color. Note that our algorithm is categorized as an unsupervised GAN because we do not rely on correspondences between two different domains. Also, our approach does not condition on additional input data (i.e., unconditional GAN) . Thus, our algorithm is categorized into an unsupervised unconditional GAN for generating two domain data simultaneously. Recent studies learn the relationship between two different data domain for domain transfer. They include cycleGAN _cite_, DiscoGAN _cite_, and dualGAN _cite_ . Because they aim to establish an image-to-image translation, they require to have the input image as the given condition so to generate the output image in different domain. This problem is inherently analogous to the problem of conditional GAN; it is a different problem from unconditinoal GAN where the data is generated from _inline_eq_ . CoGAN _cite_ made the first attempt toward the unsupervised unconditional GAN for generating two domains, as same as our study. They formulate this problem by learning the joint distribution _inline_eq_ . Suppose the joint probability distribution is factorized as _inline_eq_ . CoGAN assumes that _inline_eq_ is related to high-level semantics while _inline_eq_ and _inline_eq_ are related to low-level details. Based on this assumption, they suggests a new GAN architecture of two generators. To model _inline_eq_, first several layers of two generators are coupled by a weight-sharing constraint. The last remaining layers for both generators are designed to learn _inline_eq_ and _inline_eq_, respectively. It is because the first layers decode high-level semantics and the last layers decode low-level details in the generator. Although each generator is trained with samples for a single data domain, two generators are enforced to share high-level representations during training because of the shared layers. This weight-sharing constraint works well when there is the high structural similarity between two domains. However, with the low structural similarity, the network constraint for CoGAN is too restrictive to achieve the factorization; two generated samples may not show similar attributes. Unlike CoGAN, the proposed GAN, namely Resembled GAN, does not explicitly design the network architecture to enforce structural similarity. Instead, our approach employs the feature statistics as an additional constraint, thus it is naturally more flexible to handle a wide range of structural similarities between two data domains. The main contribution of this study is to define a new objective function of discriminators that leads generators to model the joint distribution, _inline_eq_ . To factorize this joint distribution, we propose a feature statistic matching algorithm. Suppose that we derive a feature space where all samples are representative. On this feature space, we assume that the feature distribution of all training data for each domain forms a multivariate Gaussian distribution. After that, we regard a mean vector of each Gaussian as the independent component (i.e. domain specific characteristics), associated with _inline_eq_ or _inline_eq_ . On the other hand, the covariance matrix represents the dependent component (i.e. shareable attributes), associated with _inline_eq_ . Under this assumption, we enforce that two feature distributions have similar feature covariance matrices, effectively leveraging the covariance of two feature distributions. Using our algorithm, different levels of structural similarity is accounted by the feature covariance; lower the similarity, greater the difference of covariance matrices. As the results, even two domain data are structurally quite different, we can maintain the quality of data generation as well as the attribute similarity.