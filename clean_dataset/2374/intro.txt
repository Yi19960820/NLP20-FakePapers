Supervised learning with backpropagation at its core works extremely well on an immense diversity of complicated tasks _cite_ . Using conventional techniques, the earliest layers of neurons in deep neural networks learn connections that yield neuron's receptive fields qualitatively described as feature detectors. In visual tasks, the resemblance of some of the features found by backpropagation in convolutional neural networks to the simple observed selectivities of the response of neurons in early visual processing areas in the brains of higher animals is intriguing _cite_ . For simplicity, we will always describe the task to be performed as a visual task, but none of our methods have any explicit limitation to vision. All the methods discussed below are pixel and color permutation invariant. In concept, the learning rule that shapes the early artificial neural network responses through supervised learning, e.g. backpropagation or stochastic gradient decent (SGD), and the learning rules which describe the development of the early front-end neural processing in neurobiology are unrelated. There are two conceptual reasons for this. First, in real biological neural networks, the neuron responses are tuned by a synapse-change procedure that is physically local, and thus describable by local mathematics. Consider the synaptic connection _inline_eq_ between an input neuron _inline_eq_ and an output neuron _inline_eq_ . In SGD training the alteration of _inline_eq_ depends not only on the activities of neurons _inline_eq_ and _inline_eq_, but also on the labels and the activities of the neurons in higher layers of the neural network, which are not directly knowable from the activities of neurons _inline_eq_ and _inline_eq_ . Thus, the learning rule is non-local, i.e. requires information about the state of other specific neurons in the network in addition to the two neurons that are connected by the given synapse. In biology, the synapse update depends on the activities of the presynaptic cell and the postsynaptic cell and perhaps on some global variables such as how well the task was carried out (state of animal attention, arousal, fear, etc.), but not on the activities other specific neurons. Second, higher animals require extensive sensory experience to tune the early (in the processing hierarchy) visual system into an adult system. This experience is believed to be predominantly observational, with few or no labels, so that there is no explicit task. The learning is said to be unsupervised. By contrast, training a deep artificial neural network (ANN) with SGD requires a huge amount of labeled data. The central question that we investigate in this paper is the following: can good (i.e. useful) early layer representations be learned in an ANN context without supervision and using only a local ``biological'' synaptic plasticity rule? We propose a family of learning rules that have conceptual biological plausibility and make it possible to learn early representations that are as good as those found by end-to-end training with SGD on MNIST. On CIFAR-N the performance of our algorithm is slightly poorer than end-to-end training, but it still outperforms previously published benchmarks on biological learning. We demonstrate these points by training the early layer representations using our algorithm, then freezing the weights in that layer and adding another layer on top of it (that is trained with labels using conventional methods of SGD) to perform the classification task. Backpropagation can also be used to generate useful early representations without using labeled data. A feed-forward network with a small bottleneck structure can be trained to carry out a self-mapping task on input patterns _cite_ . The bottleneck serves the role of the labels in providing information to be back-propagated to determine connection patterns in early processing layers. The bottleneck generates virtual labels which are non-locally propagated backward. This is very different from our system which requires no back-propagation of signals, receives all its information directly from forward-propagating signals, and has a local rule of synapse update. For completeness, we note that diverse efforts have been made to describe a procedure equivalent to backpropagation for training an ANN without grossly violating the ``rules'' of neurobiology. In principle this is possible, but only with considerable addition to the present simple network structure and/or invoking particular special aspects of neuronal biophysics. The direction of such efforts is well summarized in _cite_ . What is meant by ``biological plausibility'' for synapse change algorithms in ANN? We take as fundamental the idea coming from psychologists that the change of the efficacy of synapses is central to learning, and that the most important aspect of biological learning is that the coordinated activity of a presynaptic cell _inline_eq_ and a post-synaptic cell _inline_eq_ will produce changes in the synaptic efficacy of the synapse _inline_eq_ between them _cite_ . In the years since Hebb's original proposal, a huge amount of neurobiology research has fleshed out this idea. We now know that neurons make either excitatory or inhibitory outgoing synapses (but not both) _cite_, that for excitatory synapses the change in efficacy can either increase or decrease depending on whether the coordinated activity of the post-synaptic cell is strong or weak (LTP = long term potentiation and LTD = long term depression) _cite_, that homeostatic processes regulate overall synaptic strength, that pre-post timings of neuronal action potentials can be very important (STDP) _cite_, that changes in synaptic efficacy may be quantized _cite_, that there are somewhat different rules for changing the strength of excitatory and inhibitory synapses _cite_, etc. For the ANN purposes, we will subsume most such detail with four ideas: