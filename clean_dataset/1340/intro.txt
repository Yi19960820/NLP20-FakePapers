While deep learning-based metrics _cite_ demonstrate great promise for solving difficult registration problems, they are thought to require well-registered training data, which can be a serious drawback. Consider the registration of abdominal MRI and Ultrasound (US), an important and difficult problem. There is strong application pull for such registration, as diagnostic MRI has good contrast for tumors, while US is routinely used for interventional guidance. Unfortunately, for this application, it is difficult to obtain well-registered training data, because the scans can not be obtained simultaneously, and the anatomy will shift between scans. While manual registration is a possibility, it is laborious and technically challenging. For these reasons, there is great practical advantage if we can eliminate or reduce the requirement for well-registered training data for learning deep metrics for registration. In this paper we show how this may be achieved. Many registration systems can be decomposed into two components, an image agreement metric (or objective function), and an optimizer. Human designed agreement metrics such as mutual information (MI) have been successful in multimodal image registration _cite_ . However, for difficult problems, single pixel statistics may not capture all the information that is needed. In addition, using manually constructed features also limits the capacity to learn the information that is shared among images. Convolutional Neural Networks (CNNs) have proven remarkably powerful for image classification, and other image processing tasks, presumably because they are able to learn and manipulate effective representations of image contents at multiple levels of abstraction. They are now gaining traction on difficult medical imaging problems. A recent survey on deep learning methods in medical image analysis _cite_ reports two common themes where deep neural networks have been applied to image registration: N) estimation of similarity measures _cite_ ; N) estimation of transformation parameters, between the images _cite_ . Here, we will focus on the first category as our work is also estimation of similarity metrics. Wu et al. _cite_ explored unsupervised learning methods to extract deep features from the input patches and used the learned features vectors, instead of hand-crafted features, in an existing registration method. Cheng et al. _cite_ proposed learning such a metric by training on corresponding and non-corresponding patches from CT and MR in a multi-modal stacked denoising autoencoder framework. Simonovsky et al. _cite_, trained a CNN classifier to distinguish between pairs of corresponding and non-coresponding patches. After training, gradients of the deep metric were used to compute the updates for the transformation parameters in an iterative manner. A limitation of this approach appears to be that well-registered training data is a requirement for training such a classifier. In this paper we focus on the training requirements of deep metrics for registration. We demonstrate that well-registered training data is actually not required. With misregistered training data there is a risk that the objective function will be biased, however, we demonstrate that with suitable data augmentation, including a novel ``dithering" approach, the effect of misregistration is to broaden the objective function, while eliminating its bias, in comparison to the non-augmented case. While there may be some loss of accuracy due to the broadening of the objective function, we show that a multi-shot approach can be used whereby the broadened response function is used to improve the registration of the training data, and the process repeated. This leads to a well-registered training data set where the ultimate trained network would perform as well as the one presented in _cite_ . We envision that our training approach will be used once for a new application domain, subsequently registrations in the domain will only require the trained network.