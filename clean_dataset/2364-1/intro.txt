In order to label point clouds, many methods have been proposed which can be grouped into two main categories. The first category is the set of algorithms that are applied directly to the point clouds without changing the ND nature of the data. We will refer to such methods as "Direct Methods". On the other hand, the second set of methods transform the input point cloud into a different representation such as images or volumes in which various semantic segmentation methods are applied. We will refer to the second set as "Transformation Methods". In this section, we will review methods correspond to both categories from the remote sensing as well as the computer vision communities. In these methods, algorithms perform semantic labeling by directly consuming the point clouds whether by using simple point-wise discriminative models or by incorporating contextual information for better results. For example, used covariance features at multiple scales found using the eigenentropy-based scale selection method (_cite_) . Then, they evaluated four different classifiers using the ISPRS benchmark on ND semantic labeling without contextual relationships. Their best performing model was using a Linear Discriminant Analysis (LDA) classifier combined with a variety of geometric features. While their method depends on various handcrafted ND features, in our work we show that only using the location information along with three spectral bands is sufficient to obtain a high classification accuracy. In (_cite_), the authors proposed the use of spectral data and points coordinates directly forming a per point vector of (_inline_eq_, _inline_eq_, _inline_eq_, _inline_eq_, _inline_eq_, _inline_eq_) components. To label every point, first, they filtered the scene into a ground and non-ground points using the method presented in (_cite_) . Then, a ND region growing segmentation method applied to both sets of points is used to generate object proposal. By using several geometrical and spectral features, each segment is then classified into a set of five classes by combining or omitting classes from the main ISPRS ND labeling challenge. However, while the authors stated the use of several features, details about such features were not provided. Also, even though their method combines spectral information with LiDAR, their goal was to improve the results by introducing more features. On the other hand, by only using spectral data along with the ND point coordinates, we aim to show that without additional handcrafted features or ND segmentation, we were able to achieve near state-of-the-art results when using all of nine classes presented in the main challenge. Several other non-contextual methods were also reported in the literature such as the work by (_cite_) which classified full-waveform LiDAR data using point-wise multiclass SVM, and (_cite_) who used Random Forests (RF) for airborne feature selection and urban classification. To allow for spatial dependencies between object classes by considering the labels of the neighborhood, (_cite_), proposed a contextual classification method based on Conditional Random Fields (CRF) to label each ND point into one of seven categories. In the first part of their work, the authors implemented and compared two CRF models, where the first CRF is based on a linear model, while the second CRF is based on Random Forests (RF) for both the unary and the pairwise potentials. In the second part of their work, they focused on building detection, where another CRF model was introduced. During the training process in the first part, they randomly sampled N point per class for training, while testing was carried out using the remaining points. While the authors reported promising results, they had to use multiple complex contextual models to capture the interactions of individual points with their surroundings. Such complex interactions resulted in promising results at the cost of test time speed of N minutes using linear models for the CRF potentials, and N minutes when using RF adding to that the time needed to estimate the N dimensional feature vector per point before testing. {\mxy Compared to our work, we don't explicitly use contextual model, instead we learn global features per region during the training process using only simple input features} . In their recent work, the authors extended their contextual classification model to use hierarchical high order CRF (_cite_) . They introduced a two-layer CRF in which spatial and semantic context is incorporated. First, segments are generated using the first layer which operates on a point level and utilizes high order cliques and a Robust _inline_eq_ Potts model known as the point-layer. Features at this stage are derived directly from the points and their neighborhood using the eigenentropy-based scale selection method that is known to be time consuming. The second CRF layer operates on the generated segments which incorporates a larger spatial scale. Features at this stage included intensity-based and normal-based features in addition to two road-related features namely the {\it distance to a road} and the {\it orientation with respect to the closest road} (_cite_) . By iteratively propagating the information between they layers, the method allowed for revision of wrong classifications at later stages. This method showed promising results on the ND ISPRS challenge. However, this method had two major disadvantages. First, the method consists of multiple algorithms where each is designed separately. For example, the feature estimation process, the probabilistic classifier of the unary potential, and the minimization of the CRF energy function are separate algorithms that are learned or trained individually. Having multiple algorithms introduces the difficulty of optimizing the whole method at once. Second, the use of inference methods such as graph-cut and Loopy Belief Propagation (LBP) at different stages, in addition to the time needed for estimating the features, increases the inference time during testing. Such disadvantages call for faster test time methods with an end-to-end learning mechanism instead of relying on multiple individually trained components. Most of the transformation methods are focused on the use of deep learning. Recently, the computer vision field noticed a substantial improvement in various domains such as image labeling (_cite_), object detection (_cite_), semantic segmentation (_cite_), and target tracking (_cite_) to name a few. Such advances were possible due to the reintroduction of the Convolutional Neural Networks (CNNs) (_cite_), the availability of large scale datasets (_cite_), in addition to the affordable high performance compute resources such as GPUs. With the recent improvements of CNNs (_cite_) and their ability to learn local and global features in an end-to-end fashion (_cite_), an interest to apply such successful frameworks to large ND datasets emerged. However, the nature of ND point clouds being nonuniform and irregular created challenges as how to extend the ND CNN architectures to handle such data. Several recent attempts by the computer vision community were proposed. In (_cite_), they generate N rendered views by placing N virtual cameras around the ND shape every _inline_eq_ . Then, each rendered view is passed though a replica of a common CNN separately. Next, the resulted representation are aggregated using a view-pooling layer. The pooled representations are then passed to another CNN where the categories are learned. During training, the weights are initialized using VGG-M network (_cite_) . Several other methods use the multiview approach with various modification to the rendered views. For example, in (_cite_) instead of just generating regular ND images, they generate depth view images in which each pixel represents the depth of the shape at that location. Other methods include generating a common signature from multiple view features found using a fine-tuned VGG-N (_cite_) model, or projecting a ND shape into N channels and modify AlexNet (_cite_) to handle such input. For more details regarding the specific implementations of the previous algorithms, we refer the reader to (_cite_) . While previous methods apply multiview approaches to ND CAD shapes, other researches took the same approach to ground-based LiDAR. (_cite_) applied the multiview approach to label points from the large scale point cloud classification benchmark (_cite_) . First, they decimated the data and generated point features such as normals and local noise. Then, they generated a mesh to allow for multiview generation using a ND mesh viewer. Using the mesh and the point attributes, they generated two types of views, RGB and a N-channel depth composite. Given multiple N-channel images, a two stream SegNet (_cite_) network fused with residual correction (_cite_) is used to label corresponding pixels in both streams. Finally, they back project the ND labels to the corresponding mesh area then to the point cloud. (_cite_) proposed a deep learning approach for road detection using unstructured LiDAR data. First, multiple top-view images encoding several features such as elevation and density are created to form a six-channel image representing the scene. Then, using a Fully Convolutional Network (FCN) (_cite_) for a single scale binary semantic segmentation trained on the KITTI dataset (_cite_), the multi-channel image is classified into classes. This work requires rendering the data unnecessarily ND, in which it has to be gridded to form a ND map. Such gridding introduces void location that have to be replaced with values different from the data itself. This forces the network to learn implicitly an additional task to understand that such points don't belong to any of the classes. While one can get away by assigning them to the negative class in a binary task, in multi-class problems, such points will have to be assigned a separate class which in turns increases the complexity and may reduce the performance of the network. While the previous algorithms share the common theme of generating multiple views of the ND shape, others took the volumetric approach to handle points clouds while using deep learning methods. In (_cite_), the author presents a method for vehicle detection in ground-based point clouds. First he discretized the point cloud using square grids, then represented the data by a ND-array with dimensions correspond to length, width, height, and channels. By using only a single channel, binary values were used to represent the availability or the absence of a point. Then, a ND FCN is trained to produce two maps representing the objectness and the bounding box scores.The method was evaluated on the KITTI dataset. Similar approach to label ground based LiDAR is taken by (_cite_), where the data is passed through a voxelization process that generates occupancy voxel grids. Then, the occupancy voxels are labeled by the label that corresponds to the center point within a voxel. Next, a ND CNN is trained to semantically label each voxel into one of seven classes. The individual points are then labeled by back propagating the labels of each voxel to the points that formed the voxel. In VoxNet (_cite_), the authors explore three types of occupancy grids, binary, density, and hit grid. In a binary grid, each voxel is assumed to have a binary state, occupied or unoccupied. In the density grid model, each voxel is assumed to have a continuous density corresponding to the probability that the voxel would block a sensor beam. Finally, the hit grid only consider hits and ignore the difference between unknown and free space. Using each occupancy model individually, a ND CNN with NxNxN grid inputs was trained. To handle multi-resolution inputs, they trained two separate networks each receiving a occupancy grid with different resolution. (_cite_), the authors noticed a gap in performance between multiview and volumetric methods. To close such a gap, they proposed two volumetric CNN architectures which improved the results on the ND classification task. The first network introduced auxiliary learning tasks by classifying parts of the object, which in turn helped to implicitly learn deeper details od the ND shape. The second ND CNN used long anisotropic kernels that projects the ND volume to a ND representation that can be processed later using an image-based CNN. For the ND CNN part they adapted the Network In Network (NIN) architecture {\mxy NIN} . To combine the multiview approach with propsed volumetric methods, they rotated the object in ND to create various orientations. Each individual orientation is processed individually by the same network to generate ND representations that are pooled together and passed later to the image-based CNN. {\mxy say this another approach, and talk about the multiview ground-based method.} In ({_cite_}), a semantic segmentation method using a combination of spectral and LiDAR data is presented. Instead of working directly on the LiDAR data by extracting the spectral data from georeferenced images, the authors decided to generate DSM images encoding the height per pixel as a separate channel and tackle the problem as a ND semantic classification task. In order to fuse the information from the LiDAR and the spectral modalities, they estimated two probability maps suing two different methods. First, they used a pre-trained FCN to estimate the first probability map using the spectral data. Then, by handcrafting another set of features from both the spectral and the LiDAR data, a logistic regression is then applied to generate a second set of probability maps. At the end of the two stream process, they combined the two probability maps using a high order Conditional Random Fields (CRF) to label every pixel into one of six categories. {\mxy In contrast, we show that instead of creating two streams to account for the additional depth information, we can extend our single ND method to work with ND semantic segmentation with different modalities by a simple modification to the data preparation process} . By closely following the previous methods, one can notice the following: