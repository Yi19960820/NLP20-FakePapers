Recently, several deep multi-modal learning tasks have emerged. There are image captioning _cite_, text conditioned image generation _cite_, object tagging _cite_, text to image search _cite_, and so on. For all these works, how to achieve semantic multi-modal representation is the most crucial part. \newline Therefore, there were several works for multi-modal representation learning _cite_ . And all of these works require image-text pair information. Their assumption is, image-text pair has similar meaning, so if we can embed image-text pair to similar points of multi-modal space, we can achieve semantic multi-modal representation. \newline But pair information is not always available in several situations. Image and text data usually not exist in pair and if they are not paired, manually pairing them is an impossible task. But tag or category information can exist separately for image and text. And also, does not require paired state and can be manually labeled separately. \newline And learning multi-modal representation from image-text pair information can be a narrow approach. Because, their training objective focuses on adhering image and text in same image-text pair and doesn't care about adhering image and text, that are semantically similar, but in different pair. So some image and text can have not similar multi-modal feature even though they are semantically similar. In addtion, resolving every pair relations can be a bottleneck with large training dataset. \newline To deal with above problems, for multi-modal representation learning, we bring concept from ganin's work _cite_ which does unsupervised image to image domain adaptation by adversarial backpropagation. They use adversarial learning concept which is inspired by GAN (Generative Adversarial Network) _cite_ to achieve category discriminative and domain invariant feature. We extend this concept to image-text multi-modal representation learning. \newline We think image and text data are in covariate shift relation. It means, image and text data has same semantic information or labelling function in high level perspective but they have different distribution shape. So we regard, multi-modal representation learning process is adapting image and text distribution to same distribution and retain semantic information at the same time. \newline In contrast with previous multi-modal representation learning works, we don't exploit image-text pair information and only use category information. Our focus is on achieving category discriminative, domain (image, text) invariant and semantically universal multi-modal representation from image and text. \newline With above points of view, we did multi-modal embedding with category predictor and domain classifier with gradient reversal layer. We use category predictor for achieving discriminative power of multi-modal feature. And using domain classifier with grdient reversal layer, which makes adversarial relationship with embedding network and domain classifier, for achieving domain (image, text) invariant multi-modal feature. Domain invariant means image and text have same distribution in multi-modal space. \newline We show that our multi-modal feature distribution is well mixed about domain, which means image and text multi-modal feature's distributions in multi-modal space are similar, and also well distributed by t-SNE _cite_ embedding visualization. And comparison classification performance of multi-modal feature and uni-modal (Image only, Text only) feature shows, there exists small information loss within multi-modal embedding process and still multi-modal feature has category discriminative power even though it is domain invariant feature after multi-modal embedding. And our sentence to image search result (Figure _ref_) with multi-modal feature shows our multi-modal feature has universal semantic information, which is more than category information. It means, within multi-modal-embedding process, extracted universal information from WordNVec _cite_ and VGG-VeryDeep-N _cite_ is not removed. \newline In this paper, we make the following contributions. First, we design novel image-text multi-modal representation learning method which use adversarial learning concept. Second, in our knowledge, this is the first work that doesn't exploit image-text pair information for multi-modal representation learning. Third, we verify image-text multi-modal feature's quality in various perspectives and various methods. \newline Our approach is much generic as it can be easily used for any different domain (e.g. sound-image, video-text) multi-modal representation learning works with backpropagation only.