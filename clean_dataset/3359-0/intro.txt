Robotic grasping lags far behind human performance and is an unsolved problem in the field of robotics. When humans see novel objects, they instinctively know how they would grab them to pick them up. There has been a lot of work done related to robotic grasping and manipulation ~ _cite_, but the problem of real-time grasp detection and planning is still a challenge. Even the current state-of-the-art grasp detection techniques fail to detect a potential grasp in real-time. The robotic grasping problem can be divided into three parts: grasp detection, trajectory planning, and execution. Grasp detection is essentially a visual recognition problem, where the robot uses sensors to gather information of the surroundings and detects graspable objects from the scene. ND vision systems or RGB-D cameras are most commonly used to perceive the robot's environment. The key task is to predict potential grasps from sensor information and to map the pixel values to real world coordinates and orientation for a potential grasp. This is the most important step in performing a grasp as the subsequent steps are dependent on the coordinates calculated in this step. The calculated real world coordinates are then transformed to position and orientation for the robot's end-of-arm tooling (EOAT) . An optimal trajectory for the robotic arm is then planned to reach the target grasp position. Subsequently, the planned trajectory for the robotic arm is executed using either an open-loop or a closed loop controller. In contrast to an open-loop controller, a closed-loop controller receives a feedback from the vision system during the entire grasping task. A feedback based system is computationally more expensive and can drastically affect the performance and speed of the task. In this paper, we target the problem of detecting a 'good grasp' from the RGB-D data of a scene. Figure _ref_ shows a five-dimensional grasp representation for a potential good grasp of a spoon. This five-dimensional representation gives the position and orientation of a parallel plate gripper before grasp is executed on an object. Although it is a simplification of the seven-dimensional grasp representation introduced by Jiang \etal ~ _cite_, Lenz \etal showed that a good five-dimensional grasp representation can be projected back to a seven-dimensional grasp representation and can be used by a robot to perform a grasp~ _cite_ . In addition to low computational cost, this reduction in dimension will allow us to detect grasps using N-D images. We introduce a novel approach to detecting good robotic grasps for parallel plate grippers using a five-dimensional representation. A pre-trained N layer deep convolutional neural network architecture is used to extract features from the input RGB image, followed by a binary classifier. A multi-modal grasp predictor is also presented, which consists of two independent ResNet-N models running in parallel to extract features from a multi-modal (RGB and Depth) input image. The extracted features are given to a shallow convolutional neural network for predicting graspability. We explore different variations of our models and test them on the standard Cornell Grasp Dataset. Fig.~ _ref_ shows a small sample of images from the Cornell Grasp Dataset. Our experiments show that the proposed architecture outperforms the current state-of-the-art results in terms of both accuracy and speed.