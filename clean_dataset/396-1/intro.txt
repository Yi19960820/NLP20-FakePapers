has shown superior performance on several categories of machine learning problems, especially classification task. These (DNN) learn models from large training data to efficiently classify unseen samples with high accuracy. However, recent works have demonstrated that DNNs are vulnerable to, which are maliciously created by adding imperceptible perturbations to the original input by attackers. Adversarially perturbed examples have been deployed to attack image classification service~ _cite_, speech recognition system~ _cite_ and autonomous driving system~ _cite_ . Heretofore, numerous algorithms have been proposed to generate adversarial examples for ND images. When model parameters are known, a paradigm called includes methods based on calculating the gradient of the network, such as FGSM ~ _cite_, IGSM ~ _cite_ and JSMA ~ _cite_, and based on solving optimization problems, such as L-BFGS ~ _cite_, Deepfool ~ _cite_ and Carlini \& Wagner (C \&W) attack ~ _cite_ . In the scenario where access to the model is not available, called . Since the robustness of DNNs to adversarial examples is a critical feature, that target to increase robustness against adversarial example are urgently considered and can be classified into three main categories: input transformations~ _cite_, adversarial training~ _cite_ and gradient masking~ _cite_ . In addition to defense, of adversarial examples before they are fed into the networks is another approach to resist attacks, such as MagNet~ _cite_ and steganalysis based detection~ _cite_ . The popularity of ND sensors such as LiDAR and RGBD cameras draws many research concerns with ND vision. An increasing number of accessible data motivates data-driven deep learning methods practical to be used in many areas, including autopilot~ _cite_, robotics~ _cite_ and graphics~ _cite_ . In particular, point cloud is one of the most natural data structures to represent the ND geometry. After the difficult problem of irregular data format was addressed by DeepSets ~ _cite_, PointNet ~ _cite_ and its variants ~ _cite_, point cloud data can be directly processed by DNNs, and has become a promising data structure for ND computer vision tasks. Hua \etal ~ _cite_ propose a pointwise convolution operator that can output features at each point, which can offer competitive accuracy while being easy to implement. Yang \etal~ _cite_ construct losses based on mesh shape and texture to generate adversarial examples, which aim to project the optimized ``adversarial meshes'' to ND with a photorealistic renderer, and still able to mislead different DNNs. Xiang \etal~ _cite_ attack point clouds built upon C \&W loss and point cloud-specific perturbation metric with high success rate. Zheng \etal~ _cite_ propose a malicious point-dropping method to generate adversarial point clouds based on learning a saliency map for a whole point cloud, which assigns each point a score reflecting its contribution to the model-recognition loss. Liu \etal~ _cite_ propose several iterative gradient based attack methods and input restoration based defenses. Yang \etal~ _cite_ propose point-detach strategy utilizing the critical point property to attack neural network based classification system, and introduce several perturbation defenses. Adversarial examples do well in ND point cloud classification, and probably cause inconvenient issues even security problems. Consequently, research on defense of ND point cloud adversarial example is in urgent need. Based on the above reasoning, in this paper, we propose a defense method against adversarial point cloud by training a Denoiser and UPsampler Network (DUP-Net) to mitigate adversarial effects. As far as we know, this is the first work that demonstrates the effectiveness of point-dropping and point-adding operations at inference time on mitigating adversarial effects on the ND dataset, \eg, ModelNetN. We summarize the key contributions of our work as follows: We conduct comprehensive experiments to test the effectiveness of our defense method against point shifting/dropping/adding attacks from ~ _cite_ . The results in Section~ _ref_ demonstrate that the proposed DUP-Net can significantly mitigate adversarial effects.