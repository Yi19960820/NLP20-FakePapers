When training deep neural networks, we must confront the challenges of general nonconvex optimization problems. Local gradient descent methods that most deep learning systems rely on, such as variants of (SGD), have no guarantee that the optimization algorithm will converge to a global minimum. It is well known that an ensemble of multiple instances of a target neural network trained with different random seeds generally yields better predictions than a single trained instance. However, an ensemble of models is too computationally expensive at inference time. To keep the exact same computational complexity for inference, several training techniques have been developed by adding additional networks in the training graph to boost accuracy without affecting the inference graph, including auxiliary training _cite_, multi-task learning _cite_, and knowledge distillation _cite_ . Auxiliary training is introduced to improve the convergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers _cite_ . However, auxiliary classifiers require specific new designs for their network structures in addition to the target network. Furthermore, it is found later _cite_ that auxiliary classifiers do not result in obvious improved convergence or accuracy. Multi-task learning is an approach to learn multiple related tasks simultaneously so that knowledge obtained from each task can be reused by the others _cite_ . However, it is not useful for a single task use case. Knowledge distillation is introduced to facilitate training a smaller network by transferring knowledge from another high-capacity model, so that the smaller one obtains better performance than that trained by using labels only _cite_ . However, distillation is not an end-to-end solution due to having two separate training phases, which consume more training time. In this paper, we propose a framework of collaborative learning that trains several classifier heads of the same network simultaneously on the same training data to cope with the above challenges. The method acquires the advantages from auxiliary training, multi-task learning, and knowledge distillation, such as, appending the exact same network as the target one in the training graph for a single task, sharing intermediate-level representation (ILR), learning from the outputs of other heads (peers) besides the ground-truth labels, and keeping the inference graph unchanged. Experiments have been performed with several popular deep neural networks on different datasets to benchmark performance, and their results demonstrate that collaborative learning provides significant accuracy improvement for image classification problems in a generic way. There are two major mechanisms collaborative learning benefits from: N) The consensus of multiple views from different classifier heads on the same data provides supplementary information and regularization to each classifier. N) Besides computational complexity reduction benefited from ILR sharing, backpropagation rescaling aggregates the gradient flows from all heads in a balanced way, which leads to additional performance enhancement. The per-layer network weight distribution shows that ILR sharing reduces the number of “dead” filter weights in the bottom layers due to the vanishing gradient issue, thereby enlarging the network capacity. The major contributions are summarized as follows. N) Collaborative learning provides a new training framework that for any given model architecture, we can use the proposed collaborative training method to potentially improve accuracy, with no extra inference cost, with no need to design another model architecture, with minimal hyperparameter re-tuning. N) We introduce ILR sharing into co-distillation that not only enhances training time/memory efficiency but also improves generalization error. N) Backpropagation rescaling we propose to avoid gradient explosion when the number of heads is big is also proven able to improve accuracy when the number of heads is small. N) Collaborative learning is demonstrated to be robust to label noise.