has been enormous growth in research around reinforcement learning since the development of Deep Q-Networks (DQNs) ~ _cite_ . DQNs apply Q-learning to deep networks so that complicated reinforcement tasks can be learnt. However, as with most distributed models, DQNs can suffer from Catastrophic Forgetting (CF) ~ _cite_ . This is where a model has the tendency to forget previous knowledge as it learns new knowledge. Pseudo-rehearsal is a method for overcoming CF by rehearsing randomly generated examples of previous tasks, while learning real data from a new task. Although pseudo-rehearsal methods have been widely used in image classification, they have been virtually unexplored in reinforcement learning. Solving CF is essential if we want to achieve artificial agents that can continuously learn. Continual learning is important to neural networks because CF limits their potential in numerous ways. For example, if a network has been trained on a particular task, but since training, the function of the neural network needs to be extended or partially changed, the typical solution would be to train the neural network on all of the previously learnt data (that was still relevant) along with the data to learn the new function. This can be an expensive operation because previous datasets (which can be extremely large, as is often the case in deep learning) would need to be stored and retrained. However, if a neural network could effectively perform continual learning, it would only be necessary for it to directly learn data representing the changes that should be made to the function of the network. Furthermore, continual learning is also desirable because it allows the solution to multiple tasks to be compressed into a single network where weights common to both tasks may be shared. This can also benefit the speed at which new tasks are learnt because useful features may already be present in the network. Our Reinforcement-Pseudo-Rehearsal model (which we call RePR) achieves continual learning in the reinforcement domain. It does so by utilising a dual memory system where a freshly initialised DQN is trained on the new task and then knowledge from this short-term network is transferred to a separate DQN containing long-term knowledge of all previously learnt tasks. A generative model is used to produce short sequences of data representative of previous tasks which can be rehearsed while transferring knowledge of the new task. For each new task, the generative model is trained on data generated from the previous generative model and data from the new task. Therefore, the system can prevent CF without the need for a large memory store holding data from all previous tasks. The main contributions of this paper are: In Deep Q-learning~ _cite_, the neural network is taught to predict the discounted reward that would be received from taking each of the possible actions given the current state. More specifically, the loss function used in deep Q-learning is: where there exist two _inline_eq_ functions, a deep predictor network and a deep target network. The predictor's parameters _inline_eq_ are updated continuously by stochastic gradient descent and the target's parameters _inline_eq_ are infrequently updated with the values of _inline_eq_ . _inline_eq_ is the state, action, reward, terminal and next state for a given time step _inline_eq_ drawn uniformly from a large record of previous experiences, known as an experience replay. The simplest way of solving the CF problem is to use a rehearsal strategy, where previously learnt items are practised alongside the learning of new items. However, rehearsal is not ideal as it requires a buffer containing previously learnt items across all tasks, not just a record of recently learnt items from the current task as stored by the experience replay. Researchers have proposed extensions to this method such as utilising previous examples' gradients during learning~ _cite_, picking a subset of previous samples which best represents the population~ _cite_ and using a variational auto-encoder to compress stored items~ _cite_ . Such rehearsal methods are cognitively implausible and therefore, do not shine light on how mammal brains might efficiently solve the CF problem. Pseudo-rehearsal was proposed as a solution to CF which does not require storage of a large dataset of previously learnt input items~ _cite_ . Originally, pseudo-rehearsal involved constructing a pseudo-dataset by generating random inputs, passing them through the original network and recording their output. This meant that when a new dataset was learnt, the pseudo-dataset could be rehearsed alongside it, resulting in the network learning the data with minimal changes to the previously modelled function. There is psychological research that suggests that mammal brains use an analogous method to pseudo-rehearsal to prevent CF in memory consolidation. Memory consolidation is the process of transferring memory from the hippocampus, which is responsible for short-term knowledge, to the cortex for long-term storage. The hippocampus and sleep have both been linked as important components for retaining previously learnt information~ _cite_, even in tasks which do not require the hippocampus to learn~ _cite_ . The hippocampus has been observed to replay patterns of activation that occurred during the day while sleeping~ _cite_, similar to the way that pseudo-rehearsal generates previous experiences. Therefore, we believe that a similar concept will solve the CF problem in deep reinforcement learning. Although pseudo-rehearsal works for neural networks with relatively small input spaces, it does not scale well to datasets with large input spaces such as image datasets~ _cite_ . This is because the probability of a randomly generated input example representing a plausible input item is essentially zero. This is where Deep Generative Replay~ _cite_ and Pseudo-Recursal~ _cite_ have leveraged the generative abilities of a Generative Adversarial Network (GAN) ~ _cite_ to randomly generate pseudo-items representative of previously learnt items. A GAN has two components; a generator and a discriminator. The discriminator is trained to distinguish between real and generated images, whereas the generator is trained to generate images which fool the discriminator. When a GAN is used alongside pseudo-rehearsal, the GAN is also trained on the task so that its generator learns to produce items representative of the task's input items. Then, when a second task needs to be learnt, pseudo-items can be generated randomly from the GAN's generator and used in pseudo-rehearsal. More specifically, the loss function for pseudo-rehearsal is: where _inline_eq_ is a loss function, such as cross-entropy, _inline_eq_ is a neural network with weights _inline_eq_ while learning task _inline_eq_ . _inline_eq_ is the input-output pair for the current task, whereas _inline_eq_ is a pseudo-item generated to represent the previous task _inline_eq_ and its target output is calculated by _inline_eq_ . This technique can be applied to multiple tasks using only a single GAN by doing pseudo-rehearsal on the GAN as well. Thus, the GAN learns to generate items representative of the new task while still remembering to generate items representative of the previous tasks (by rehearsing the pseudo-items it generates) . This technique has been shown to be very effective for remembering a chain of multiple image classification tasks without ever using real data to rehearse a previously learnt task~ _cite_ .