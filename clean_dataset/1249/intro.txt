Before the rise of deep learning and its success in end-to-end feature learning, manual feature engineering was arguably one of the most important steps in the machine learning workflow, but also very time-consuming and tedious. With an abundance of choices in designing the architecture of deep neural networks, manual feature engineering has nowadays to a certain degree been replaced by manual tuning of architectures. Recent work on ~ automates this choice of network architecture. On some benchmarks, these NAS methods have indeed led to new state-of-the-art performance~, although only at extreme computational costs on the order of N GPUs for two weeks. Many prominent NAS methods~ follow a two step process, which we argue is inefficient. During their main architecture search phase, they evaluate architectures with a fixed set of hyperparameters and a relatively small number of epochs (e.g., N), and only after the search has finished, they optimize hyperparameters for the end result and evaluate it with a large number of epochs (e.g., N) . This process is suboptimal for various reasons: To combat these problems, we propose to use an approach for joint neural architecture and hyperparameter search that is anytime and gradually increases the computational budget for the best fraction of networks at lower compute budgets in order to yield a far more computationally efficient optimization procedure. Specifically, our contributions are as follows: