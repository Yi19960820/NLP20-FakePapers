The first year of the life is the most dynamic stage of the human brain development, which is characterized by rapid tissue growth and development of various cognitive and motor functions~ _cite_ . Non-invasively imaging the infant brain through magnetic resonance imaging (MRI) helps gauge the degree of maturation of the infant's brain. It also aids in assessing risks of the infant developing neuro-developmental and neuropsychiatric disorders in the future~ _cite_ . There is immense clinical advantage to developing tools for automated and objective analysis of the infant brain MRIs, especially for tissue segmentation. This can help the radiologist / pediatrician identify and recognize cues that are not visually apparent and also enables large-scale volumetric studies of the infant brain, such as volume computations, analysis of cortical folding patterns etc. ~ _cite_ . In this paper, we particularly focus on infant brain segmentation at the iso-intense stage (N to N months postnatal) using multi-modal MR images (specifically TN and TN-weighted) . \noindent Prior art: Segmentation of adult human brain through deep learning has gained a lot of attention ~ _cite_ . Their extension to infant brain segmentation is not trivial due to significant anatomical differences between the two age groups. Task-specific approaches in the direction of iso-intense infant brain segmentation include use of handcrafted multi-modal feature fusion approach~ _cite_, patch-based multi-modal convolutional neural network (CNN) model ~ _cite_, to name a few. However, these solutions are either not end-to-end~ _cite_ or have slow segmentation speed as individual voxels are segmented in sequence. \noindent Challenge: Segmenting this volumes is fraught with several challenges including low Signal-to-Noise ratio (SNR), motion artifacts, poor tissue contrast between the gray and white matter, intensity inhomogeneities etc. ~ _cite_ . In addition to these, the high degree of intra-subject variability (due to progressive maturational changes) coupled with enormous inter-subject heterogeneity pose additional challenges for automated segmentation. Learning end-to-end networks for segmentation for the application-at-hand can help learn representations that are robust to these task-specific challenges. As the segmentation is performed with multi-modal data, an effective information fusion scheme is needed. \noindent Approach: Recently, Fully convolutional neural networks (F-CNNs) have been effectively used for semantic segmentation both for computer vision~ _cite_ and medical imaging~ _cite_ . These F-CNN models leverage the context of whole image for prediction, and provides labels for all pixels simultaneously, making its deployment very fast.In this paper, we propose a variant of the F-CNN model, termed InfiNet, with two encoder arms to process multi-modal data separately. These extracted features from the two modalities are fused at higher levels of abstraction into a joint decoder arm that terminates in a classification layer to get the final segmentation (shown in Fig.~ _ref_) . We train networks dedicated for segmenting across each anatomical view (coronal, sagittal and axial) and the final volume level segmentation is obtained by aggregating across the multiple views (shown in Fig.~ _ref_) .