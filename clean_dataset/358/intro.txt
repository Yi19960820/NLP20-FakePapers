We have witnessed a significant revolution in computer vision brought by the deep Convolutional Neural Networks (CNNs) . With powerful computational resources ({\em e.g.}, GPUs) and a large amount of labeled training data ({\em e.g.}, _cite_), a hierarchical structure containing different levels of visual concepts is constructed and trained~ _cite_ to produce impressive performance on large-scale visual recognition tasks~ _cite_ . A pre-trained deep network is also capable of generating deep features for various tasks, such as image classification~ _cite_ _cite_, image retrieval~ _cite_ _cite_ and object detection~ _cite_ _cite_ . CNN is composed of several stacked layers, each of which contains a number of neurons. We argue that modeling the co-occurrence of neuron responses is important, whereas less studied in the previous work. For this, we define a set of {\em geometric neural phrases} on the basis of the hidden neurons, and propose the {\bf Geometric Neural Phrase Pooling} (GNPP) algorithm to encode them efficiently. GNPP can be regarded as a new type of layer, and inserted into a network with little computational overhead ({\em e.g.}, _inline_eq_ and _inline_eq_ extra time and memory costs in the experiments on {\bf ImageNet}) . We explain the behavior of GNPP by noting that it punishes the isolated neuron responses, and that the isolated responses are often less reliable than clustered ones, especially in the high-level network layers. Experimental results show that adding GNPP layers boosts image classification accuracy significantly and consistently. Later, we will discuss the benefits brought by the GNPP layer from different points of view, showing that GNPP produces better internal representation, builds latent connections, and accelerates the network training process. The remainder of this paper is organized as follows. Section~ _ref_ briefly introduces related work. Section~ _ref_ introduces the GNPP layer, and Section~ _ref_ shows experimental results. We discuss the benefits brought by adding GNPP layers in Section~ _ref_ . Finally, we conclude this work in Section~ _ref_ .