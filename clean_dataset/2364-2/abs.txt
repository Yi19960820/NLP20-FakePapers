When classifying point clouds, a large amount of time is devoted to the process of engineering a reliable set of features which are then passed to a classifier of choice. Generally, such features--usually derived from the ND-covariance matrix--are computed using the surrounding neighborhood of points. While these features capture local information, the process is usually time-consuming, and requires the application at multiple scales combined with contextual methods in order to adequately describe the diversity of objects within a scene. In this paper we present a ND-fully convolutional network that consumes terrain-normalized points directly with the corresponding spectral data, if available, to generate point-wise labeling while implicitly learning contextual features in an end-to-end fashion. Our method uses only the ND-coordinates and three corresponding spectral features for each point. Spectral features may either be extracted from ND-georeferenced images, as shown here for Light Detection and Ranging (LiDAR) point clouds, or extracted directly for passive-derived point clouds, \ie~ from muliple-view imagery. We train our network by splitting the data into square regions, and use a pooling layer that respects the permutation-invariance of the input points. Evaluated using the ISPRS ND Semantic Labeling Contest, our method scored second place with an overall accuracy of N \%. We ranked third place with a mean FN-score of N \%, surpassing the FN-score of the method with highest accuracy by N \%. In addition to labeling ND-point clouds, we also show that our method can be easily extended to ND-semantic segmentation tasks, with promising initial results.