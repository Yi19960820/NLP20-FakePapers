This paper addresses efficient test-time computation of deep convolutional neural networks (CNNs) _cite_ . Since the success of CNNs _cite_ for large-scale image classification, the accuracy of the newly developed CNNs _cite_ has been continuously improving. However, the computational cost of these networks (especially the more accurate but larger models) also increases significantly. The expensive test-time evaluation of the models can make them impractical in real-world systems. For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets mostly have CPUs or low-end GPUs only; some recognition tasks like object detection _cite_ are still time-consuming for processing a single image even on a high-end GPU. For these reasons and others, it is of practical importance to accelerate the test-time computation of CNNs. There have been a few studies on approximating deep CNNs for accelerating test-time evaluation _cite_ . A commonly used assumption is that the convolutional filters are approximately low-rank along certain dimensions. So the original filters can be approximately decomposed into a series of smaller filters, and the complexity is reduced. These methods have shown promising speedup ratios on a single _cite_ or a few layers _cite_ with some degradation of accuracy. The algorithms and approximations in the previous work are developed for reconstructing linear filters _cite_ and linear responses _cite_ . However, the nonlinearity like the Rectified Linear Units (ReLU) _cite_ is not involved in their optimization. Ignoring the nonlinearity will impact the quality of the approximated layers. Let us consider a case that the filters are approximated by reconstructing the linear responses. Because the ReLU will follow, the model accuracy is more sensitive to the reconstruction error of the positive responses than to that of the negative responses. Moreover, it is a challenging task of accelerating the whole network (instead of just one or a very few layers) . The errors will be accumulated if several layers are approximated, especially when the model is deep. Actually, in the recent work _cite_ the approximations are applied on a single layer of large CNN models, such as those trained on ImageNet _cite_ . It is insufficient for practical usage to speedup one or a few layers, especially for the deeper models which have been shown very accurate _cite_ . In this paper, a method for accelerating convolutional networks is proposed. It is based on minimizing the reconstruction error of responses, subject to a low-rank constraint that can be used to reduce computation. To solve the challenging constrained optimization problem, we decompose it into two feasible subproblems and iteratively solve them. We further propose to minimize an asymmetric reconstruction error, which effectively reduces the accumulated error of multiple approximated layers. We evaluate our method on a N-convolutional-layer model trained on ImageNet. We investigate the cases of accelerating each single layer and the whole model. Experiments show that our method is more accurate than the recent method of Jaderberg \etal's _cite_ under the same speedup ratios. A speedup ratio of N _inline_eq_ is demonstrated, and its degradation is merely N \%. When our model is accelerated to have a comparably fast speed as the ``AlexNet'' _cite_, our accuracy is N \% higher.