Zero-shot visual recognition, or more generally, zero-shot learning (ZSL), recognizes novel classes that are unseen at training stage. The community has reached a consensus that ZSL is all about transferring knowledge from seen classes to unseen classes; Despite that there are fruitful ZSL methods, the transfer still follows the simple but intuitive mechanism: although ``raccoon'' is unseen, we can recognize it by checking if it satisfies the ``raccoon signature'', \eg, visual attributes ``striped tail''~ _cite_, classeme ``fox-like''~ _cite_, or ``raccoon'' word vectors~ _cite_ . These attributes can be modeled at training stage and are expected to be sharable in both seen and unseen classes at test stage. After a decade of progress, the transfer has evolved from primitive attribute classifiers~ _cite_ to semantic embedding based framework~ _cite_, which is prevailing due to its simple and effective paradigm (cf. Figure~ _ref_ (a)): first, it maps images from visual space to semantic space where all the classes reside; then, ZSL is reduced to a simple nearest neighbor search---the image is assigned to the nearest class in embedding space. The semantic transfer ability of this embedding-based ZSL framework is limited by the semantic loss problem. As shown in Figure~ _ref_, discarding the low-variance attributes (\ie, less discriminative) is beneficial to classification at training; However, due to the semantic discrepancy between seen and unseen classes, these attributes would be discriminative at test time, resulting in a lossy semantic space that is problematic for unseen class recognition. The main reason is that although the class embedding has rich semantic meanings, it is still a lonely point in the semantic space, where the mappings of many images will inevitably collapse to it~ _cite_ . One may consider the extreme case that all the class embeddings are one-hot label vectors, degenerating to the traditional supervised classification, therefore, no semantics can be transfered. An arguably possible solution is to preserve semantics by reconstruction---the embedded semantic vector from one image should be able to map the image back, where any two semantic embeddings are expected to preserve sufficient semantics to be apart, otherwise the reconstruction would fail~ _cite_ . However, reconstruction and classification are essentially two conflicting objectives: the former aims to preserve as many image details as possible while the latter focuses on suppressing irrelevant ones. For example, using only ``head'' and ``torso'' attributes might be sufficient for ``person'' recognition while the color attributes ``red'' and ``white'' are indeed disturbing. To further illustrate this, as shown in Figure~ _ref_ (b), suppose _inline_eq_: _inline_eq_ and _inline_eq_: _inline_eq_ are two mapping transformations between the visual and semantic spaces. For classification, we want _inline_eq_ of the same class to be mapped to close semantic embeddings _inline_eq_, \ie, _inline_eq_ ; For reconstruction, we want _inline_eq_ and _inline_eq_, which is difficult to be satisfied as _inline_eq_ . Therefore, joint training of the two objectives is ineffective to preserve semantics (\eg, SAE~ _cite_) . For example, as illustrated in Figure~ _ref_ (b), if we want to achieve good classification performance, the reconstruction will fail generally. To resolve this conflict, we propose a novel visual-semantic embedding framework: Semantics-Preserving Adversarial Embedding Network (SP-AEN) . As illustrated in Figure~ _ref_ (c), we introduce a new mapping _inline_eq_: _inline_eq_ and an adversarial objective~ _cite_ where the discriminator _inline_eq_ and encoder _inline_eq_ try to make _inline_eq_ and _inline_eq_ indistinguishable. There are two benefits of introducing _inline_eq_ and _inline_eq_ to help _inline_eq_ preserve semantics: N) Semantic Transfer . Even though the semantic loss is inevitable by _inline_eq_, we can avoid it using _inline_eq_ by borrowing ingredients from _inline_eq_ of other classes, and the discriminator _inline_eq_ will eventually transfer semantics from _inline_eq_ to _inline_eq_ by tailoring the two semantic embeddings into the same distribution. For example, for a ``bird'' image where the attribute ``spotty'' in _inline_eq_ is lost, we can retain it by using _inline_eq_ because ``spotty'' is a discriminative and preserved attribute in ``leopard'' images. N) Disentangled Classification and Reconstruction . As the reconstruction is only imposed to _inline_eq_ and _inline_eq_, _inline_eq_ is disentangled to focus on classification. In this way, the conflict between classification and reconstruction is resolved because the constraint _inline_eq_ and _inline_eq_ is relaxed to _inline_eq_ and _inline_eq_, as _inline_eq_ and _inline_eq_ are not necessarily to be close with each other to comply with the discriminative objective as _inline_eq_ . As shown in Figure~ _ref_ (b), compared to the reconstruction style in Figure~ _ref_ (b) ~ _cite_, our visual-semantic embedding _inline_eq_ can reconstruct photo-realistic images, suggesting that the semantic is better preserved. We can deploy state-of-the-art network structures for SP-AEN in a flexible plug-and-play and end-to-end fine-tune fashion, \eg, _inline_eq_ may use the powerful model for classification~ _cite_, _inline_eq_ and _inline_eq_ may use the encoder and decoder of the image generation architecture~ _cite_ . The overall architecture is illustrated in Figure~ _ref_ and will be detailed in Section~ _ref_ . We validate the effectiveness of SP-AEN on four popular benchmarks: CUB~ _cite_, AWA~ _cite_, SUN~ _cite_, and aPY~ _cite_, surpassing the state-of-the-art performances~ _cite_ by N \%, N \%, N \%, and N \% in harmonic mean values, respectively. To the best of our knowledge, SP-AEN is the first ZSL model that empowers photo-realistic image generation from the semantic space. We hope that it will facilitate the ZSL community for better visual investigations of knowledge transfer.