Driven by the large number of videos that are being produced every day, video summarization~ plays an important role in extracting and analyzing key contents within videos. Video summarization techniques have recently gained increasing attention in an effort to facilitate large-scale video distilling~ due to its promising significance. They aim to generate summaries by selecting a small set of key frames/shots in the video while still conveying the whole story, and thus can improve efficiency of key information extraction and understanding. Essentially, video summarization techniques need to address two key challenges in order to provide effective summarization results: N) how to exploit a good key-frame/key-shot selection policy that can take into account the long-range temporal correlations embedded in the whole video to determine the uniqueness and importance of each frame/shot; N) from a global perspective, how to ensure that the resulting short summary can capture all key contents of the video with a minimal number of frames/shots, that is, how to ensure video information completeness and compactness. Previous works have made some attempts toward solving these challenges. For instance, video summarization methods have to a large extent made use of Long Short-Term Memory (LSTM) ~ and determinantal point process (DPP) ~ in order to address the first challenge and learn temporal dependencies. However, due to the fact that memories in LSTMs and DPPs are limited, we believe that there is still room to better exploit long-term temporal relations in the videos. The second challenge is often addressed by utilizing feature-based approaches, i.e. instance motion features learning~, to encourage diversity between the frames included in the summary. However, this cannot ensure the information completeness and compactness of summaries, leading to redundant frames and less informative results. Generative Adversarial Networks (GANs) ~ have been widely used in many computer vision tasks due to its effectiveness. Instead of only relying on the more traditional neural network approach that is only trained by Mean Squared Error (MSE) between the prediction and the ground-truth, the usage of GANs adds additional regularization. During training, the discriminator is encouraged to learn a complex loss function that encodes the higher order statistics of what a summary consists of, which in practice cannot be explicitly formulated by hand. A recent work~ utilizing adversarial neural networks reduces redundancy by minimizing the distance between training videos and the distribution of summaries, but it encodes all different information into one fixed-length representation, which reduces the model learning capabilities given different length of video sequences. To better address the above two core challenges in the video summarization task, namely modeling of long-range temporal dependencies and information completeness and compactness, we propose a novel dilated temporal relational generative adversarial network (DTR-GAN) . Figure~ _ref_ shows an overview of the proposed method. The generator, which consists of Dilated Temporal Relational (DTR) units and a Bidirectional LSTM (Bi-LSTM) ~, takes the real summary and the video representation as the input. DTR units aim to exploit long-range temporal dependencies complementing the commonly used LSTMs. The discriminator takes three pairs of input: generated summary pair, real summary pair and random summary pair and optimizes a three-player loss during training. To better ensure the completeness and compactness, we further introduce a supervised generator loss during adversarial training as a form of regularization. Specifically, DTR units integrate context among frames at multi-scale time spans, in order to enlarge the modelâ€™s temporal field-of-view and thereby effectively model temporal relations among frames. We use three layers of DTR units, each modeling four different time spans, to capture short-term, mid-term and long-term dependencies. Bi-LSTM can function on every time step and benefit both long and short time dependencies by addressing the gradient problem commonly found in traditional non-gated Recurrent Neural Networks (RNNs) ~ . Since our DTR units act on some certain time scales for efficiently capturing long and short temporal dependencies, Bi-LSTM can help with temporal modeling in parallel. In this way, combining DTR units with the LSTMs ensures that the generator can have better generating ability. The discriminator takes three pairs of input:, and, and optimizes a three-player loss during training. It is cast to discriminate real summary from the generated summary, which further enhances the ability of the generator. At the same time, it ensures that the video representations are not learned from a trivial randomly shorten sequence. We further introduce a supervised generator loss during adversarial training to better ensure the completeness and compactness. Our approach essentially achieves better model capability with DTR units by exploiting the global multi-scale temporal context. Further, the three-player loss-based adversarial network also provides more effective regularization to improve the discriminator's ability to recognize real summaries from fake ones. This, in turn, leads to better generated summaries. Evaluation results on three public benchmark datasets ~, ~ and ~ demonstrate the effectiveness of our proposed method. In summary, this paper makes the following contributions: A preliminary version of this method appeared in~ . Here we extend our work by: N) placing our work into a broader context and providing a thorough literature background discussion, N) providing a more thorough description of the methodology N) extending the experimental evaluation to two additional datasets, namely the SumMe and YouTube datasets; and N) including a thorough experimental analysis in form of ablation and visualization studies. The rest of the paper is organized as follows. In Section N, we review the related work. We present our proposed approach for video summarization in Section N and report and analyze the experimental results in Section N. Finally, Section N draws conclusions and points to future research directions.