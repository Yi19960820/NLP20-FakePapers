Dynamic or video textures are movies that are stationary both in space and time. Common examples are movies of flame patterns in a fire or waves in the ocean. There exists a long history in synthesising dynamic textures (e.g. _cite_) and recently spatio-temporal Convolutional Neural Networks (CNNs) were proposed to generate samples of dynamic textures _cite_ . In this note we introduce a much simpler approach based on feature spaces of a CNN trained on object recognition _cite_ . We demonstrate that our model leads to comparable synthesis results without the need to train a separate network for every input texture.