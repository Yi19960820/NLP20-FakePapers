In the field of pathology, staining types determine which parts or targets in the tissue are highlighted with specific colors. Tissue staining materials and procedures can be time consuming, expensive, and typically require special expertise. These limitations usually reduce the number of examinations and stainings performed on a sample. This can limit clinicians' ability to obtain all relevant information from a patient biopsy. In many cases information exists in the stained slide image about targets and objects not specifically targeted by the stain. For example, pathologists have the ability to identify lymphocytes in a Hematoxylin and Eosin (H \&E) image _cite_ even without directly staining them for lymphocyte specific markers. This fact motivated the research in the direction of generating virtually stained slides from other modalities _cite_ . Recently, supervised deep learning based methods have been applied in the task of virtual staining generation _cite_ . As supervised training methods are based on coupled pairs of aligned images, all the aforementioned methods require additional accurate registration steps between dataset image pairs. In this work we propose to virtually generate FAP-CK stained slide images from KiN-CDN stained slide images. These input and output stainings were chosen for several reasons. First, information about tumor characteristics in FAP-CK could be encoded in the form of proliferation and tumor infiltrating lymphocytes in KiN-CDN. Furthermore, KiN-CDN is one of the classical Immunohistochemistry (IHC) stainings used in histopathology while FAP-CK is a new duplex IHC protocol allowing to characterize tumor and to advance research in the direction of drug development. Additionally, generating virtual FAP-CK stained slide images from KiN-CDN allows the creation of a virtual multiplexed brightfield image, i.e. having N target stains on the same whole slide coordinate system, which is technically challenging using classical staining methods. In this paper, we present an unsupervised deep learning method based on Cycle-Consistent Adversarial Networks (CycleGAN) _cite_ . This allows avoiding the slide registration process for training datasets and facilitates dealing with variability present in sets of slide images due to different lab protocols, scanners and experiment conditions. We further present a method aimed at reducing the tiling artifact caused by tile-wise processing of large images, a common problem in image style transfer encountered when high resolution testing images can not fit into memory _cite_ . Finally, we validate the results of our method by comparing quantification of tumor cells and FAP in virtual slides with a real stained slide taken from the same tissue block.