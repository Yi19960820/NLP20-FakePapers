Accurate ND human pose estimation from monocular images and videos is the key to unlock several applications in robotics, human computer interaction, surveillance, animation and virtual reality. These applications require and ND pose estimation from monocular image or video under challenging variations of clothing, lighting, view-point, self-occlusions, activities, background clutter etc.~ _cite_ . With the advent of recent advances in deep learning, compute hardwares and, most importantly, large-scale datasets (ImageNet~ _cite_, MS COCO~ _cite_, CityScapes~ _cite_ etc.), computer vision systems have witnessed dramatic improvements in performance. Human-pose estimation has also benefited from synthetic and real-world datasets such as MS COCO~ _cite_, MPII Pose~ _cite_, N _cite_, MPI-INF-NDHP~ _cite_, and SURREAL~ _cite_ . Especially, ND pose prediction has witnessed tremendous improvement due to large-scale in-the-wild datasets~ _cite_ . However, ND pose estimation still remains challenging due to severely under-constrained nature of the problem and absence of any real-world ND annotated dataset. A large body of prior art either directly regresses for ND joint coordinates~ _cite_ or infers ND from ND joint-locations in a two-stage approach~ _cite_ . These approaches perform well on synthetic ND benchmark datasets, but lack generalization to the real-world setting due to the lack of ND annotated in-the-wild datasets. To mitigate this issue, some approaches use synthetic datasets~ _cite_, green-screen composition~ _cite_, domain adaptation~ _cite_, transfer learning from intermediate ND pose estimation tasks~ _cite_, and joint learning from ND and ND data~ _cite_ . Notably, joint learning with ND and ND data has shown promising performance in-the-wild owing to large-scale real-world ND datasets. We seek motivation from the recently published joint learning framework of Zhou et al.~ _cite_ and present a novel structure-aware loss function to facilitate training of Deep ConvNet architectures using both ND and ND data to accurately predict the ND pose from a single RGB image. The proposed loss function is applicable to ND images during training and ensures that the predicted ND pose does not violate anatomical constraints, namely joint-angle limits and left-right symmetry of the human body. We also present a simple learnable temporal pose model for pose-estimation from videos. The resulting system outperforms the best published system by N \% on both N and MPI-INF-NDHP and runs at Nfps on commodity GPU. Our proposed structure-aware loss is inspired by anatomical constraints that govern the human body structure and motion. We exploit the fact that certain body-joints cannot bend beyond an angular range; e.g. the knee (elbow) joints cannot bend forward (backward) . We also make use of left-right symmetry of human body and penalize unequal corresponding pairs of left-right bone lengths. Lastly, we also use the bone-length ratio priors from~ _cite_ that enforces certain pairs of bone-lengths to be constant. It is important to note that the illegal-angle and left-right symmetry constraints are complementary to the bone-length ratio prior, and we show that they perform better too. One of our contributions lies in formulating a loss function to capture joint-angle limits from an inferred ND pose. We present the visualization of the loss surfaces of the proposed losses to facilitate a deeper understanding of their workings. The three aforementioned structure losses are used to train our . Joint-angle limits and left-right symmetry have been used previously in the form of optimization functions~ _cite_ . To the best of our knowledge we are the first ones to exploit these two constraints, in the form of differentiable and tractable loss functions, to train ConvNets directly. Our structure-aware loss function outperforms the published state-of-the-art in terms of Mean-Per-Joint-Position-Error (MPJPE) by N \% and N \% on N and MPI-INF-NDHP, respectively. We further propose to learn a temporal motion model to exploit cues from sequential frames of a video to obtain anatomically coherent and smoothly varying poses, while preserving the realism across different activities. We show that a moving-window fully-connected network that takes previous _inline_eq_ poses performs extremely well at capturing temporal as well as anatomical cues from pose sequences. With the help of carefully designed controlled experiments we show the temporal and anatomical cues learned by the model to facilitate better understanding. We report an additional N \% improvement on N with the use of our temporal model and demonstrate real-time performance of the full pipeline at Nfps. Our final model improves the published state-of-the-art on N _cite_ and MPI-INF-NDHP~ _cite_ by N \% and N \%, respectively.