The ability to generalize across datasets is one of the holy grails of computer vision. Designing models that can perform well on datasets sharing similar characteristics such as classes, but also presenting different underlying data structures (for instance different backgrounds, colorspaces, or acquired with different devices) is key in applications where labels are scarce or expensive to obtain. However, traditional learning machines struggle in performing well out of the datasets (or) they have been trained with. This is because models generally assume that both training (or) and test (or) data are issued from the same generating process. In vision problems, factors such as objects position, illumination, number of channels or seasonality break this assumption and call for adaptation strategies able to compensate for such shifts, or strategies~ _cite_ . In a first rough subdivision, domain adaptation strategies can be separated into and domain adaptation: the former assumes that no labels are available in the target domain, while the latter assumes the presence of a few labeled instances in the target domain that can be used as reference points for the adaptation. In this paper, we propose a contribution for the former, more challenging case. Let _inline_eq_ be the source domain examples with the associated labels _inline_eq_ . Similarly, let _inline_eq_ be the target domain images, but with unknown labels. The goal of the unsupervised domain adaptation is to learn the classifier _inline_eq_ in the target domain by leveraging the information from the source domain. To this end, we have access to a source domain dataset _inline_eq_ and a target domain dataset _inline_eq_ with only observations and no labels. Early unsupervised domain adaptation research tackled the problem as the one of finding a common representation between the domains, or a latent space, where a single classifier can be used independently from the datapoint's origin~ _cite_ . In~ _cite_, the authors propose to use discrete optimal transport to match the shifted marginal distributions of the two domains under constraints of class regularity in the source. In~ _cite_ a similar logic is used, but the joint distributions are aligned directly using a coupling accounting for the marginals and the class-conditional distributions shift . However, the method has two drawbacks, for which we propose solutions in this paper: N) first, the JDOT method in _cite_ scales poorly, as it must solve a _inline_eq_ coupling, where _inline_eq_ and _inline_eq_ are the samples to be aligned; N) secondly, the optimal transport coupling _inline_eq_ is computed between the input spaces (and using a _inline_eq_ distance), which is a poor representation to be aligned, since we are interested in matching more semantic representations supposed to ease the work of the classifier using them to take decisions. We solve the two problems above by a strategy based on deep learning. On the one hand, using deep learning algorithms for domain adaptation has found an increasing interest and has shown impressive results in recent computer vision literature~ _cite_ . On the other hand (and more importantly), a Convolutional Neural Network (CNN) offers the characteristics needed to solve our two problems: N) by gradually adapting the optimal transport coupling along the CNN training, we obtain a scalable solution, an approximated and stochastic version of JDOT; N) by learning the coupling in a deep layer of the CNN, we align the representation the classifier is using to take its decision, which is a more semantic representation of the classes. In summary, we learn jointly the embedding between the two domains and the classifier in a single CNN framework. We use a domain adaptation-tailored loss function based on optimal transport and therefore call our proposition . We test DeepJDOT on a series of visual domain adaptation tasks and compare favorably against several recent state of the art competitors.