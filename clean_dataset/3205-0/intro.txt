If we desire robots capable of functioning in arbitrary home and office settings---environments that exhibit huge amounts of variation---we require dramatic improvement to our perceptual systems. It is no longer feasible to rely on operating in known settings with previously encountered objects; generally-intelligent robots require the ability to generalize from objects they have previously observed to similar but novel objects. Prior work has identified three critical inference capabilities required by most robots interacting with objects: classification, pose estimation, and ND completion _cite_ . For a single observed object, that entails determining the object's type, its position in the world, and its full ND geometry. These capabilities are critical for many common robot tasks and constitute the perceptual foundation upon which complex object-centric behavior may be built. For instance, a household robot should be able to reason about a new and somewhat oddly shaped coffee mug, recognizing it as a coffee cup, inferring that it likely has a handle protruding from the rear (even if that portion is unseen), and estimating its pose. This work presents a novel framework for object representation: Hybrid Bayesian Eigenobjects (HBEOs) . HBEOs, like BEOs _cite_ which preceded them, are designed to generalize knowledge from previously encountered objects to novel ones. HBEOs are trained from a set of ND meshes with known class and pose; they allow a novel object---observed via a depth image from a single viewpoint---to have its class, pose, and full ND shape estimated. HBEOs employ Variational Bayesian Principal Component Analysis (VBPCA) to learn a linear subspace in which objects lie. The critical insight is that the space spanned by all possible ND structures is far larger than the---still quite sizable---space spanned by ND objects encountered in the everyday world. By explicitly learning a compact basis for objects we wish our robot to reason about, we constrain the inference problem significantly. This representation allows a ND object to be expressed as a low dimensional set of VBPCA coefficients, facilitating efficient pose estimation, classification, and completion without operating directly in high-dimensional object space. HBEOs use a learned non-linear method---specifically, a deep convolutional network _cite_---to determine the correct projection coefficients for a novel partially observed object. By combining linear subspace methods with deep convolutional inference, HBEOs draw from the strengths of both approaches. Previous work on ND shape completion employed either deep architectures which predict object shape in full ND space (typically via voxel output) _cite_ or linear methods which learn linear subspaces in which objects tend to lie _cite_ . Both approaches have had some success, but also have significant weaknesses. End-to-end deep methods suffer from the high dimensionality of object space; the data and computation requirements of regressing into _inline_eq_ or even million dimensional space are severe. Linear approaches, on the other hand, perform their predictions in an explicit low dimensional subspace. This approach is fast and quite data efficient but requires partially observed objects be voxelized before inference can occur, and lacks the expressiveness of a non-linear deep network. Unlike existing approaches, which are either fully linear or do not leverage explicit object spaces, HBEOs have the flexibility of nonlinear methods without requiring expensive regression directly into high-dimensional space. Additionally, because HBEOs perform inference directly from a depth image, they do not require voxelizing a partially observed object, a process which requires estimating a partially observed object's full ND extents and pose prior to voxelization. Empirically, we show our hybrid approach outperforms competing methods when performing joint pose estimation, classification, and ND completion of novel objects.