Domain mapping or image-to-image translation, which targets at translating an image from one domain to another, has been intensively investigated over the past few years. Let _inline_eq_ denote a random variable representing source domain images and _inline_eq_ represent target domain images. According to whether we have access to a paired sample _inline_eq_, domain mapping can be studied in a supervised or unsupervised manner. While several works have successfully produced high-quality translations by focusing on supervised domain mapping with constraints provided by cross-domain image pairs _cite_, the progress of unsupervised domain mapping is relatively slow. Unluckily, obtaining paired training examples is expensive and even infeasible in some situations. For example, if we want to learn translators between Monet's paintings and Photographs, how can we collect sufficient well-defined () pairs for model training? By contrast, collecting unpaired sets is often convenient since infinite images are available online. From this viewpoint, unsupervised domain mapping has great potential for real-world applications in the long term. In unsupervised domain mapping, from a probabilistic modeling perspective, our goal is to model the joint distribution _inline_eq_ given samples drawn from the marginal distributions _inline_eq_ and _inline_eq_ in individual domains. Since the two marginal distributions can be inferred from an infinite set of possible joint distributions, it is difficult to guarantee that an individual input _inline_eq_ and the output _inline_eq_ are paired up in a meaningful way without additional assumptions or constraints. To address this problem, recent approaches have exploited the cycle-consistency assumption, {\it i.e} ., a mapping _inline_eq_ and its inverse mapping _inline_eq_ should be bijections _cite_ . Specifically, when feeding an example _inline_eq_ into the networks _inline_eq_, the output should be a reconstruction of _inline_eq_ and vise versa for _inline_eq_, {\it i.e} ., _inline_eq_ and _inline_eq_ . Further, DistanceGAN _cite_ showed that maintaining the distances between images within domains allows one-sided unsupervised domain mapping rather than simultaneously learning both _inline_eq_ and _inline_eq_ . Existing constraints overlook the special properties of images that simple geometric transformations (global geometric transformations without shape deformation) do not change the image's semantic structure. Here, semantic structure refers to the information that distinguishes different object/staff classes, which can be easily perceived by humans regardless of trivial geometric transformations such as rotation. Based on this property, we develop a geometry-consistency constraint, which helps in reducing the search space of possible solutions while still keeping the correct set of solutions under consideration, and results in a geometry-consistent generative adversarial network (GcGAN) for unsupervised domain mapping. Our geometry-consistency constraint is motivated by the fact that a given geometric transformation _inline_eq_ between the input images should be preserved by related translators _inline_eq_ and _inline_eq_, if _inline_eq_ and _inline_eq_ are the domains obtained by applying _inline_eq_ on the examples of _inline_eq_ and _inline_eq_, respectively. Mathematically, given a random example _inline_eq_ from the source domain _inline_eq_ and a predefined geometric transformation function _inline_eq_, geometry consistency can be expressed as _inline_eq_ and _inline_eq_, where _inline_eq_ is the inverse function of _inline_eq_ . Because it is unlikely that _inline_eq_ and _inline_eq_ always fail in the same location, _inline_eq_ and _inline_eq_ co-regularize each other by the geometry-consistency constraint and thus correct each others' failures in local regions of their respective translations (see Figure~ _ref_ for an illustrative example) . Our geometry-consistency constraint allows one-sided unsupervised domain mapping, {\it i.e} ., _inline_eq_ can be trained independently from _inline_eq_ . In this paper, we employ two simple but representative geometric transformations as examples, {\it i.e} ., vertical flipping () and N degrees clockwise rotation (), to illustrate geometry consistency. Quantitative and qualitative comparisons with the baseline (GAN alone) and the state-of-the-art methods including CycleGAN _cite_ and DistanceGAN _cite_ demonstrate the effectiveness of our model in generating realistic images.