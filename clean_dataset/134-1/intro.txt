\noindent Population ecologists use camera traps to monitor animal population sizes and manage ecosystems around the world. Camera traps were first introduced in N, and in N, Karanth demonstrated their usefulness for population ecology by re-identifying tigers (Panthera tigris) in Nagarahole, India using a formal mark and recapture model _cite_ . The popularity of the camera trap methodology grew rapidly thereafter, with a N \% annual growth using the technique as a tool to estimate population sizes _cite_ . Camera traps respond to motion, which generally corresponds with an animal entering the frame. Camera trap data analyses involve manually quantifying the species and number of individuals in thousands of images. Automating this process has obvious advantages, including a reduction in human labour, an unbiased estimate across analyses, and the availability of species identification without domain expertise. \newline In this work, we focus on utilizing deep learning based approaches for object detection to identify, quantify, and localize animal species within camera trap images. Camera trap data provides a robust measure of the capabilities of deep learning for species classification, as the images are often `messy', with animals being partly obstructed, positioned at varying distances, cropped out of the image, or extremely close to the camera _cite_ . These obstacles are in addition to the traditional difficulties of computer vision tasks, such as variable lighting, photos taken at day and night, and species exhibiting a variety of poses. \newline Deep learning methods have demonstrated near perfect accuracy for computer vision tasks when trained on large labled datasets; however, labeled ecological data is notorious for being sparse and intermittent _cite_ . We aim to test the bounds of deep learning for realistic ecological applications, demonstrating the usefulness of the technique for researchers to train their own classifiers on their own ecosystem of interest, instead of relying on large public data sets which may not fit their niche of study. We considered the Reconyx Camera Trap data set, which contains N labeled images with N species classifications and bounding box coordinates, as well as the Gold Standard Snapshot Serengeti data set, which contains N, N labeled images of N species classifications _cite_ . Current methods for object detection require the bounding box coordinates for training, and as a result, we hand-labeled the bounding box coordinates for the Gold Standard Snapshot Serengeti data set and offer it to the camera trap and deep learning community. \newline We compare two methods for object detection using deep learning, Faster Region-Convolutional Neural Network and You-Only-Look-Once N (hereafter referred to as Faster R-CNN and YOLO, respectively) _cite_ . These two approaches are generally considered by the trade-off of data efficiency versus speed, as YOLO can be used in real time, but requires additional training data _cite_ . Our results demonstrate Faster R-CNN shows promise for accurate and autonomous analysis of camera trap data, while YOLO fails to perform. These results demonstrate that ecologists should consider utilizing Faster R-CNN or its successors as the method of object detection to autonomously extract ecological information from camera trap images.