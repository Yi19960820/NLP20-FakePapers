Robotic grasping lags far behind human performance and is an unsolved problem in the field of robotics. When humans see novel objects, they instinctively know how to grasp to pick them up. A lot of work has been done related to robotic grasping and manipulation ~ _cite_, but the problem of real-time grasp detection and planning is still a challenge. Even the current state-of-the-art grasp detection techniques fail to detect a potential grasp in real-time. The robotic grasping problem can be divided into three sequential phases: grasp detection, trajectory planning, and execution. Grasp detection is a visual recognition problem in which the robot uses its sensors to detect graspable objects in its environment. The sensors used for perceiving the robot's environment are typically N-D vision systems or RGB-D cameras. The key task is to predict potential grasps from sensor information and map the pixel values to real world coordinates. This is a critical step in performing a grasp as the subsequent steps are dependent on the coordinates calculated in this step. The calculated real world coordinates are then transformed to position and orientation for the robot's end-of-arm tooling (EOAT) . An optimal trajectory for the robotic arm is then planned to reach the target grasp position. Subsequently, the planned trajectory for the robotic arm is executed using either an open-loop or a closed loop controller. In contrast to an open-loop controller, a closed-loop controller receives continuous feedback from the vision system during the entire grasping task. The additional processing needed to handle the feedback is computationally expensive and can drastically affect the speed of the task. In this paper, we target the problem of detecting a `good grasp' from RGB-D imagery of a scene. Fig. _ref_ shows a five-dimensional grasp representation for a potential good grasp of a toner cartridge. This five-dimensional representation gives the position and orientation of a parallel plate gripper before the grasp is executed on an object. Although, it is a simplification of the seven-dimensional grasp representation introduced by Jiang \etal ~ _cite_, Lenz \etal ~showed that a good five-dimensional grasp representation can be projected back to a seven-dimensional grasp representation that can be used by a robot to perform a grasp~ _cite_ . In addition to low computational cost, this reduction in dimension allows us to detect grasps using RGB-D images. In this work, we use this five-dimensional grasp representation for predicting the grasp pose. We introduce a novel approach for detecting good robotic grasps for parallel plate grippers using the five-dimensional representation. Our approach uses two N-layer deep convolutional residual neural networks running in parallel to extract features from RGB-D images, with one network analyzing the RGB component and the other analyzing the depth channel. The outputs of these networks are then merged, and fed into another convolutional network that predicts the grasp configuration. We compare this approach to others in the literature, as well as a uni-modal variation of our model that uses only the RGB component. Our experiments are done on the standard Cornell Grasp Dataset. Example images from the dataset are shown in Fig.~ _ref_ . Our experiments show that the proposed architecture outperforms the current state-of-the-art methods in terms of both accuracy and speed.