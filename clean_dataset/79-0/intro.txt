Human communication consists of much more than verbal elements, words and sentences. Facial expressions (FE) play a significant role in inter-person interaction. They convey emotional state, truthfulness and add context to the verbal channel. Automatic FE recognition (AFER) is an interdisciplinary domain standing at the crossing of behavioral science, psychology, neurology, and artificial intelligence. \smallskip \smallskip \noindent The analysis of human emotions through facial expressions is a major part in psychological research. Darwin's work in the late N's _cite_ placed human facial expressions within an evolutionary context. Darwin suggested that facial expressions are the residual actions of more complete behavioral responses to environmental challenges. When in disgust, constricting the nostrils served to reduce inhalation of noxious or harmful substances. Widening of the eyes in surprise increased the visual field to better see an unexpected stimulus. Inspired by Darwin's evolutionary basis for expressions, Ekman \etal _cite_ introduced their seminal study about facial expressions. They identified seven primary, universal expressions where universality related to the fact that these expressions remain the same across different cultures _cite_ . Ekman labeled them by their corresponding emotional states, that is,, and, see Figure _ref_ . Due to its simplicity and claim for universality, the primary emotions hypothesis has been extensively exploited in cognitive computing. In order to further investigate emotions and their corresponding facial expressions, Ekman devised the (FACS) _cite_ . FACS is an anatomically based system for describing all observable facial movements for each emotion, see Figure _ref_ . Using FACS as a methodological measuring system, one can describe any expression by the (AU) one activates and its activation intensity. Each action unit describes a cluster of facial muscles that act together to form a specific movement. According to Ekman, there are _inline_eq_ facial AUs, describing actions such as ``open mouth'', ``squint eyes'' etc., and _inline_eq_ other AUs were added in a _inline_eq_ revision of the FACS manual _cite_, to account for head and eye movement. \smallskip \smallskip \noindent The ability to automatically recognize facial expressions and infer the emotional state has a wide range of applications. These included emotionally and socially aware systems _cite_, improved gaming experience _cite_, driver drowsiness detection _cite_, and detecting pain in patients _cite_ as well as distress _cite_ . Recent papers have even integrated automatic analysis of viewers' reaction for the effectiveness of advertisements _cite_ . Various methods have been used for (FER or AFER) tasks. Early papers used geometric representations, for example, vectors descriptors for the motion of the face _cite_, active contours for mouth and eye shape retrieval _cite_, and using ND deformable mesh models _cite_ . Other used appearance representation based methods, such as Gabor filters _cite_, or local binary patterns (LBP) _cite_ . These feature extraction methods usually were combined with one of several regressors to translate these feature vectors to emotion classification or action unit detection. The most popular regressors used in this context were support vector machines (SVM) and random forests. For further reading on the methods used in FER, we refer the reader to _cite_ \smallskip \smallskip \noindent Over the last part of this past decade, (CNN) _cite_ and (DBN) have been used for feature extraction, classification and recognition tasks. These CNNs have achieved state-of-the-art results in various fields, including object recognition _cite_, face recognition _cite_, and scene understanding _cite_ . Leading challenges in FER _cite_ have also been led by methods using CNNs _cite_ . Convolutional neural networks, as first proposed by LeCun in N _cite_, employ concepts of receptive fields and weight sharing. The number of trainable parameters is greatly reduced and the propagation of information through the layers of the network can be simply calculated by convolution. The input, like an image or a signal, is convolved through a filter collection (or map) in the to produce a feature map. Each feature map detects the presence of a single feature at all possible input locations. In the effort of improving CNN performance, researchers have developed methods of exploring and understanding the models learned by these methods. _cite_ demonstrated how saliency maps can be obtained from a ConvNet by projecting back from the fully connected layers of the network. _cite_ showed visualizations that identify patches within a dataset that are responsible for strong activations at higher layers in the model. Zeiler \etal _cite_ describe using as a way to visualize a single unit in a feature map of a given CNN, trained on the same data. The main idea is to visualize the input pixels that cause a certain neuron, like a filter from a convolutional layer, to maximize its output. This process involves a step, where we stream the input through the network, while recording the consequent activations in the middle layers. Afterwards, one fixes the desired filter's (or neuron) output, and sets all other elements to the neutral elements (usually N) . Then, one ``back-propagates'' through the network all the way to the input layer, where we would get a neutral image with only a few pixels set-those are the pixels responsible for max activation in the fixed neuron. Zeiler \etal. found that while the first layers in the CNN model seemed to learn Gabor-like filters, the deeper layers were learning high level representations of the objects the network was trained to recognize. By finding the maximal activation for each neuron, and back-propagating through the deconvolution layers, one could actually view the locations that caused a specific neuron to react. Further efforts to understand the features in the CNN model, were done by Springenberg \etal. who devised _cite_ . With some minor modifications to the deconvolutional network approach, they were able to produce more understandable outputs, which provided better insight into the model's behavior. The ability to visualize filter maps in CNNs improved the capability of understanding what the network learns during the training stage. \smallskip \noindent The main contributions of this paper are as follows.