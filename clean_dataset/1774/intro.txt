Automatically predicting and executing a sequence of actions given a specific task is an ability that is quite expected for intelligent robots~ _cite_ . For example, to complete the task of ``make tea'' under the scene shown in Figure _ref_, an agent needs to plan and successively execute a number of steps, e.g., ``move to the tea box'' and ``grasp the tea box''. In this paper, we aim to train a neural network model to enable this capability, which has rarely been addressed in computer vision research. We regard the aforementioned problem as semantic task planning, i.e., predicting a sequence of atomic actions toward accomplishing a specific task. Furthermore, we consider an atomic action to be a primitive action operating on an object, denoted by a two-tuple _inline_eq_ . Therefore, the prediction of action sequences depends on not only the task semantics (i.e., how the task is represented and planned) but also the visual scene image parsing (e.g., recognizing object categories, states and their spatial relations in the scene) . Considering the task of a robot pouring a cup of water from a pot, the predicted sequence varies according to the properties of the objects in the scene such as the relative distances among the agent, cup and pot and the state of the cup (empty or not) . If the robot is located far from the cup, it should first move close to the cup and then grasp the cup. If the cup is full of water, the robot will have to pour the water out before filling the cup with water from the pot. Since recent advanced deep convolutional neural networks (CNNs) _cite_ have achieved great successes in object categorization _cite_ and localization _cite_, we assume that objects are correctly located and that the initial states of the objects are known in the given scene in this work. However, this problem remains challenging due to the diversity of action decomposition and ordering, long-term dependencies among atomic actions, and large variations in object states and layouts in the scene. In this work, we develop a recurrent long short-term memory (LSTM) _cite_ network to address the problem of semantic task planning because LSTM networks have been demonstrated to be effective in capturing long-range sequential dependencies, especially for tasks such as machine translation _cite_ and image/video captioning _cite_ . These approaches generally adopt an encoder-decoder architecture, in which an encoder first encodes the input data (e.g., an image) into a semantic-aware feature representation and a decoder then decodes this representation into the target sequence (e.g., a sentence description) . In this work, we transform the input image into a vector that contains the information about the object categories and locations and then feed this vector into the LSTM network (named Action-LSTM) with the specified task. This network is capable of generating the action sequence through the encoder-decoder learning. In general, large numbers of annotated samples are required to train LSTM networks, especially for complex problems such as semantic task planning. To overcome this issue, we present a two-stage training method by employing a knowledge and-or graph (AOG) representation _cite_ . First, we define the AOG for task description, which hierarchically decomposes a task into atomic actions according to their temporal dependencies. In this semantic representation, an and-node represents the chronological decomposition of a task (or sub-task), an or-node represents the alternative ways to complete the certain task (or sub-task), and leaf nodes represent the pre-defined atomic actions. The AOG can thus contain all possible action sequences for each task by embodying the expressiveness of grammars. Specifically, given a scene image, a specific action sequence can be generated by selecting the sub-branches at all of the or-nodes in a depth-first search (DFS) manner. Second, we train an auxiliary LSTM network (named AOG-LSTM) to predict the selection at the or-nodes in the AOG and thus produce a large number of new valid samples (i.e., task-oriented action sequences) that can be used for training the Action-LSTM network. Notably, training the AOG-LSTM network requires only a few manually annotated samples (i.e., scene images and the corresponding action sequences) because making a selection in the context of task-specific knowledge (represented by the AOG) is seldom ambiguous. Note that a preliminary version of this work has been presented at a conference _cite_ . In this paper, we inherit the idea of integrating task-specific knowledge via a two-stage training method, and we extend the initial version from several aspects to strengthen our method. First, we extend the benchmark to involve more tasks and include more diverse scenarios. Moreover, because the automatically augmented set includes some difficult samples with uncertain and even incorrect labels, we further incorporate curriculum learning _cite_ to address this issue by starting the training with only easy samples and then gradually extending to more difficult samples. Finally, more detailed comparisons and analyses are conducted to demonstrate the effectiveness of our proposed model and to verify the contribution of each component. The main contributions of this paper are two-fold. First, we present a new problem called semantic task planning and create a benchmark (that includes N daily tasks and N, N scene images) . Second, we propose a general approach for incorporating complex semantics into the recurrent neural network (RNN) learning, which can be generalized to various high-level intelligent applications. The remainder of this paper is organized as follows. Section II provides a review of the most-related works. Section III presents a brief overview of the proposed method. We then introduce the AOG-LSTM and Action-LSTM modules in detail in Sections IV and V, respectively, with thorough analyses of the network architectures, training and inference processes of these two modules. Extensive experimental results, comparisons and analyses are presented in Section VI. Finally, Section VII concludes this paper.