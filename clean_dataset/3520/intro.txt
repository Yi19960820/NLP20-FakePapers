Sparse representation has been shown to be a very powerful model for many real-world signals, leading to impressive results in various restoration tasks such as denoising _cite_, deblurring _cite_, inpainting _cite_, super-resolution _cite_ and recognition _cite_, to name a few. The core assumption of this model is that signals can be expressed as a linear combination of a few columns, also called atoms, taken from a matrix _inline_eq_ termed a dictionary. Concretely, for a signal _inline_eq_, the model assumption is that _inline_eq_, where _inline_eq_ is a noise vector with bounded energy _inline_eq_ . This allows for a slight deviation from the model and/or may account for noise in the signal. The vector _inline_eq_ is the sparse representation of the signal, obtained by solving the following optimization problem _cite_: where _inline_eq_ denotes the _inline_eq_ pseudo-norm that counts the number of non-zeros in the representation. Solving this optimization problem, known as the pursuit stage, is generally NP hard, but under certain conditions _cite_, the solution of problem (_ref_) can be approximated using greedy algorithms such as Orthogonal Matching Pursuit (OMP) _cite_ or convex relaxation algorithms such as Basis Pursuit (BP) _cite_ . Over the years, various methods have been proposed to adaptively learn the model parameters from real data. Such dictionary learning methods attempt to find _inline_eq_ that best represents the set of signals at hand. Prime examples are K-SVD _cite_, MOD _cite_, Double sparsity _cite_, Online dictionary learning _cite_, Trainlets _cite_, and more. When dealing with high-dimensional signals, learning the dictionary suffers from the curse of dimensionality, and this process becomes computationally infeasible. To cope with this problem, many algorithms suggest training a local model on fully-overlapping patches taken from the signal _inline_eq_, and processing these patches independently. This patch-based dictionary learning technique has gained much popularity over the years due to its simplicity and high-performance _cite_ . Yet, patch-based approaches are known to be sub-optimal as they ignore the relations between neighboring patches _cite_ . An alternative approach to meet this challenge is posed by the Convolutional Sparse Coding (CSC) model. This model assumes that the signal can be represented as a superposition of a few local filters, convolved with sparse feature-maps. The CSC model handles the signal globally, and yet pursuit and dictionary learning are feasible due to the specific structure of the dictionary involved. This model has been shown to be useful in tackling some limitations of the patch-based model, and led to superior performance in several applications, such as super-resolution _cite_, inpainting _cite_, image separation _cite_, source separation _cite_, image fusion _cite_ and audio processing _cite_, albeit with room for improvement. Contemporary CSC based algorithms often rely on the ADMM _cite_ formulation in order to extract the signal-representation of the model and train its corresponding filters. While the majority of works employ ADMM in the Fourier domain _cite_, a recent approach proposed by Papyan at el. _cite_, coined Slice Based Dictionary Learning (SBDL), adopts a local point of view and trains the filters in terms of only local computations in the signal domain. This local-global approach, and the decomposition that it induces, follows a recent work _cite_ that presented a novel theoretical analysis of the global CSC model, providing guarantees which stem from localized sparsity measures. The SBDL algorithm demonstrates state-of-the-art performance compared to the Fourier-based methods, while still relying on the ADMM algorithm. As such, this approach requires the introduction of _inline_eq_ auxiliary variables, increasing the memory requirements, it can only be deployed in a batch-learning mode, its convergence is questionable, and strongly depends on the ADMM parameter, which is application-dependent. In this work we propose intuitive and easy-to-implement algorithms, based on the block coordinate descent approach, for solving the global pursuit and the CSC filter learning problems, all done with local computations in the original domain. The proposed algorithms operate without auxiliary variables nor extra parameters for tuning in the pursuit stage. We call this algorithm Local Block Coordinate Descent (LoBCoD) . In addition, we introduce a stochastic gradient descent variant of LoBCoD for training the convolutional filters. This algorithm leverages the benefits of online learning, while being applicable even to a single training-image. The LoBCoD algorithm and its stochastic version show faster convergence and achieve a better solution to the CSC problem compared to the previous ADMM-based methods (global or local) . We should note that a very recent work by Moreau at el. _cite_ also proposes a coordinate descent based algorithm for the pursuit task in the CSC model. Their algorithm, like ours, operates locally and without necessitating additional parameters. However, the algorithm in _cite_ is restricted in two important ways compared to ours: (i) Their method is specifically tailored to ND signals, and does not harness the ND structure of the CSC model for images; (ii) their algorithm is limited to pursuit only, with no treatment for the filter learning. The rest of this paper is organized as follows: Section _ref_ provides an overview of the CSC model and discusses previous methods. The proposed pursuit algorithm and its derivation are presented in Section _ref_ . In Section _ref_ we discuss dictionary update methods and introduce the stochastic LoBCoD algorithm. We compare these methods with previously published approaches in section _ref_ . Section _ref_ shows how our method can be employed to tackle the tasks of image inpainting and multi-focus image fusion, and later, in Section _ref_, we demonstrate this empirically. Section _ref_ concludes this work.