The existence of haze, due to the presence of aerosols such as dust, mist, and fumes, adds complicated noise to the images captured by cameras. It dramatically degrades the visibility of outdoor images, where contrasts are reduced and surface colors become faint. Moreover, a hazy image will put the effectiveness of many subsequent high-level computer vision tasks in jeopardy, such as object detection and recognition. The dehazing algorithms have thus been widely considered, as a challenging instance of (ill-posed) image restoration and enhancement. Similar to other problems like image denoising and super-resolution _cite_, earlier dehazing work~ _cite_ assumed the availability of multiple images from the same scene. However, the haze removal from one single image has now gained the dominant popularity, since it is more practical for realistic settings _cite_ . This paper focused on the problem of single image dehazing . As a prior knowledge to be exploited for dehazing, the hazy image generation follows a well-received physical model (see Section _ref_ for details) . Apart from estimating a global atmospheric light magnitude, the key to achieve haze removal has been recognized to be the recovery of the transmission matrix . _cite_ proposed a physically-grounded method by estimating the albedo of the scene. _cite_ discovered the effective dark channel prior (DCP) to more reliably calculate the transmission matrix, followed by a series of works _cite_ . _cite_ enforced the boundary constraint and contextual regularization for sharper restored images. An accelerated method for the automatic recovery of the atmospheric light was presented in~ _cite_ . _cite_ developed a color attenuation prior and created a linear model of scene depth for the hazy image, and then learned the model parameters in a supervised way. _cite_ illustrate the method to jointly estimate scene depth and recover the clear latent image from a foggy video sequence. _cite_ proposed an algorithm based on the non-local prior (haze-line), based on the assumption that each color cluster in the clear image becomes a haze-line in RGB space. All above methods hinge on the physical model and various sophisticated image statistics assumptions. However, since the estimation of physical parameters from a single image is often inaccurate, the dehazing performance of the above methods appears not always satisfactory. Lately, as Convolutional Neural Networks (CNNs) have witnessed prevailing success in computer vision tasks, they have been introduced to image dehazing as well. DehazeNet~ _cite_ proposed a trainable model to estimate the transmission matrix from a hazey image. ~ _cite_ further exploited a multi-scale CNN (MSCNN), that first generated a coarse-scale transmission matrix and later refined it. Most deep learning approaches for image restoration and enhancement have fully embraced end-to-end modeling: training a model to directly regress the clean image from the corrupted image. Examples include image denoising _cite_, deblurring _cite_, and super resolution _cite_ . In comparison, there has been no end-to-end deep model for dehazing so far, that directly regresses a clean image from a hazy one . While that might appear weird at the first glance, one needs to realize that haze essentially brings in non-uniform, signal-dependent noise: the scene attenuation of a surface caused by haze is correlated with the physical distance between the surface the camera (i.e., the pixel depth) . That is different from most image degradation models that assume signal-independent noise, in which case all signals go through the same parameterized degradation process. Hence their restoration models could be easily modeled with one static mapping function. The same is not directly applicable to dehazing: the degradation process varies by signals, and the restoration model has to be input-adaptive as well. Existing methods share the same belief, that in order to recover a clean scene from haze, it is the key to estimate an accurate medium transmission map _cite_ . The atmospheric light is calculated separately by empirical rules, and the clean image is recovered based on the physical model. Albeit being intuitive, such a procedure does not directly measure or minimize the reconstruction distortions. The errors in the two separate steps for estimating transmission matrix and atmospheric light will accumulate and potentially amplify each other. As a result, the traditional separate pipeline gives rise to the sub-optimal image restoration quality. Currently, dehazing models rely on two sets of evaluation criteria: (N) for syntehtic hazy images where their groundtruth clean images are known, PSNR and SSIM are typically computed to measure the restoration fidelity; (N) for real natural hazy images with unknown groundtruth, the only available comparison of dehazing results is on the subjective visual quality. However, unlike image denoising and super resolution results whose suppression effects of visual artifacts are visible (e.g., on textures and edges), the visual differences between state-of-the-art dehazing models _cite_ typically manifest in the global illumination and tone, and are often too subtle to tell. General image restoration and enhancement, known as part of low-level vision tasks, are usually thought as the pre-processing step for mid-level and high-level vision tasks. It has been known that the performance of high-level computer vision tasks, such as object detection and recognition, will deteriorate in the presence of various degradations, and is then largely affected by the quality of image restoration and enhancement. However, up to our best knowledge, there has been no exploration to correlate the dehazing algorithms and results with the high-level vision task performance. In this paper, we propose the All-in-One Dehazing Network (AOD-Net), a CNN-based dehazing model with two key innovations in response to the above two challenges: AOD-Net is trained on synthetic hazy images, and tested on both synthetic and real natural images. Experiments demonstrate the superiority of AOD-Net over several state-of-the-art methods, in terms of not only PSNR and SSIM (see Figure~ _ref_), but also visual quality (see Figure~ _ref_) . As a lightweight and efficient model, AOD-Net costs as low as N second to process one _inline_eq_ image with a single GPU. When concatenated with Faster R-CNN~ _cite_, AOD-Net notably outperforms other dehazing models in improving the object detection performance over hazy images, and the performance margin is boosted even more when we jointly tune the pipeline of AOD-Net and Faster R-CNN from end to end. This paper is extended from a previous conference version _cite_ . The most notable improvement of this current paper lies in Section _ref_, where we present an in-depth discussion on evaluating and enhancing dehazing on object detection, and introduce the joint training part with abundant details and analysis. We also provide a more detailed and thorough analysis on the architecture of AOD-Net (e.g. Section _ref_) . Besides, we have included more extensive comparison results.