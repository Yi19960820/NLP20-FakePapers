Deep learning has witnessed a series of remarkable successes in computer vision. In particular, Convolutional Neural Networks (CNNs) ~ _cite_ have turned out to be effective for visual tasks in image domain, such as image classification~ _cite_, object detection~ _cite_, and semantic segmentation~ _cite_ . Deep models have been also introduced into video domain for action recognition~ _cite_, and obtain comparable or better recognition accuracy to those traditional methods with hand-crafted representations~ _cite_ . However, the progress of architecture design and representation learning in video domain is much slower, partially due to its inherent complexity and higher dimension. Video could be viewed as the temporal evolution of a sequence of static images. It is generally assumed that two visual cues are crucial for video classification and understanding: (N) {\em static appearance} in each frame, and (N) {\em temporal relation} across multiple frames. Therefore, an effective deep architecture should be able to capture both information to achieve excellent classification accuracy. There are three kinds of successful architectures or frameworks for video classification~ _cite_: (N) two-stream CNNs~ _cite_, (N) ND CNNs~ _cite_, and (N) ND CNNs with temporal models on top such as LSTM~ _cite_, temporal convolution~ _cite_, sparse sampling and aggregation~ _cite_, and attention modeling~ _cite_ . Two-stream CNNs capture appearance and motion information with different streams, which turn out to be effective for video classification. Yet, it is time-consuming to train two networks and calculate optical flow in advance. To overcome this limitation, ND CNNs employ ND convolutions and ND pooling operations to directly learn spatiotemporal features from stacked RGB volumes. However, the performance of ND CNNs is still worse than two-stream CNNs, and it is still unclear whether this straightforward ND extension over ND convolution could efficiently model static appearance and temporal relation. ND CNNs with temporal models usually focus on capturing coarser and long-term temporal structure, but lack capacity of representing finer temporal relation in a local spatiotemporal window. In this paper, we address the problem of capturing appearance and relation in video domain, by proposing a new architecture unit termed as {\em SMART} block. Our SMART block aims to Simultaneously Model Appearance and RelaTion from RGB input in a separate and explicit way with a two-branch unit, in contrast to modeling them with two-stream inputs~ _cite_ or jointly and implicitly with a ND convolution~ _cite_ . As shown in Figure~ _ref_, our SMART block is a multi-branch architecture, which is composed of appearance branch and relation branch, and fuses them with a concatenation and reduction operation. The appearance branch is based on the {\em linear combination} of pixels or filter responses in each frame to model spatial structure, while the relation branch is based on the {\em multiplicative interactions} between pixels or filter responses across multiple frames to capture temporal dynamics. Specifically, the appearance branch is implemented with a standard ND convolution and the relation branch is implemented with a square-pooling structure. The responses from two branches are further concatenated and reduced to a more compact representation. A SMART block is a basic and generic building module for video architecture design. For video classification, we present an appearance-and-relation network (ARTNet) by stacking a collection of SMART blocks. Essentially, the appearance and relation information in video domain exhibit multi-scale spatiotemporal structure. The ARTNet is able to capture this visual structure in a hierarchical manner, where SMART units in the early layers focus on describe local structure in a short term, while the ones in the later layers can capture increasingly coarser and longer-range visual structure. An ARTNet is a simple and general architecture which offers flexible implementations. In the current implementation of this paper, the ARTNet is instantiated with the network of CND-ResNetN~ _cite_ for an engineering compromise between accuracy and computation consumption. Moreover, our ARTNet is complementary to those long-term temporal models, which means any of them could be employed to enhance its modeling capacity. As an example, we use the framework of temporal segment network (TSN) ~ _cite_ to jointly train ARTNets from a set of sparsely sampled snippets and further improve the recognition accuracy. We test the ARTNet on the task of action recognition in video classification. Particularly, we first study the performance of the ARTNet on the Kinetics dataset~ _cite_ . We observe that our ARTNet obtains an evident improvement over CND, and superior performance to the exiting state-of-the-art methods on this challenging benchmark under the setting of training from scratch with only RGB input. To further demonstrate the generality of ARTNet, we also transfer its learned video representation to other action recognition benchmarks including HMDBN~ _cite_ and UCFN~ _cite_, where performance improvement is also achieved. The main contribution of this paper is three-fold: (N) A SMART block is designed to simultaneously capture appearance and relation in a separate and explicit way. (N) An ARTNet is proposed by stacking multiple SMART blocks to model appearance and relation information from different scales, which also allows for optimizing the parameters of SMART blocks in an end-to-end way. (N) ARTNets are empirically investigated on the large-scale Kinetics benchmark and state-of-the-art performance on this dataset is obtained under the setting of using only RGB input and training from scratch.