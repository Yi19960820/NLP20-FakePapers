The handwritten signature is a behavioral biometric trait that is extensively used to verify a person's identity in legal, financial and administrative areas. Automating the verification of handwritten signatures has been a subject of research since the decade of N _cite_, considering two scenarios: online (dynamic) and offline (static) . In the online case, signatures are captured using a special device, such as a pen tablet, that records the dynamic information of the writing process (e.g. position of the pen over time) . For offline signature verification, we consider signatures written on paper, that are subsequently scanned to be used as input. Most of the research effort in offline signature verification has been devoted to finding good feature representations for signatures, by proposing new feature descriptors for the problem _cite_ . Recent work, however, showed that learning features from data (signature images) can improve system performance to a large extent _cite_ . These work rely on training Deep Convolutional Neural Networks (CNNs) to learn a hierarchy of representations directly from signature pixels. Although these methods present good performance, they also introduce some issues. Signatures from different users vary significantly in size, while a feature descriptor should provide a fixed-sized representation for classification. This is not a problem in many feature descriptions used for signature verification, that by design are able to accommodate signatures of different sizes. Neural networks, on the other hand, in general require fixed-sized inputs, and thus these methods require pre-processing the signatures such that they all have the same size. Most commonly, signatures are either a) resized to a common size or b) first centered in a blank image of a ``maximum signature size", and then resized. Figure _ref_ illustrates the problems with these approaches. In alternative (a), the width of the strokes become very different depending on the size of the original image, while in alternative (b) the width of strokes is not affected, but instead we may lose detail on small signatures, that would otherwise be preserved in the first alternative. Empirically, alternative (b) presented much better results _cite_, but it also creates the problem that now a ``maximum size" is defined, and if a new signature is larger than this size, it would need to be reduced (causing similar problems to (a) regarding the width of the strokes) . Another problem in learning the representations from signature images is the selection of the resolution of the input images. The methods proposed in the literature use small images (e.g. _inline_eq_ in _cite_, _inline_eq_ in _cite_) . For the signatures used in these papers, this is equivalently of using a resolution around N dpi. However, as illustrated in figure _ref_, the distinction of genuine signatures and skilled forgeries often rely on the line quality of the strokes (in particular for slowly-traced forgeries, as noted in _cite_) . This suggests that using higher resolutions may improve performance on this task. In this paper, we propose learning a fixed-sized representation for signatures of variable size, by adapting the architecture of the neural network, using Spatial Pyramid Pooling (SPP) _cite_, _cite_ . Our contributions are as follows: we define and evaluate different training protocols for networks with SPP applied to offline handwritten signatures. After training, signatures of any size can be fed to the network in order to obtain a fixed-sized representation. We also evaluate the impact of the image resolution on the classification accuracy, and the generalization of features learned in one dataset to other operating conditions (e.g. different acquisition protocols, signatures from people of different locations), by using transfer learning to other datasets. For feature learning, we used the problem formulation presented in _cite_, where Writer-Independent features are learned using a subset of users, and subsequently used to train Writer-Dependent classifiers for another set of users. We also use the architecture defined in this work as baseline (SigNet) . We adapt this architecture to learn fixed-sized representations (proposing different training protocols) and modifying the architectures to handle images of higher resolution. We conducted experiments on four widely used signature verification datasets: GPDS, MCYT, CEDAR and the Brazilian PUC-PR dataset; and two synthetic datasets (Bengali and Devanagari scripts) . Using the proposed architecture, we obtain a similar performance compared to the state-of-the-art, while removing the constraint of having a fixed maximum signature size. We also note that using higher resolutions (Ndpi) greatly improves performance when skilled forgeries (from a subset of users) is available for training. On the other hand, if only genuine signatures are used for feature learning, higher resolutions did not improve performance. We verify that the learned features generalize to different operating conditions (by testing them on other datasets), and that fine-tuning the representation for the different conditions further improves performance. We observed that the features learned on GPDS generalize better to other western signature datasets (MCYT, CEDAR and Brazilian PUC-PR) than to other types of scripts (Bengali and Devanagari), and that fine-tuning also largely addresses this problem.