The problem of visual localization has drawn significant attention from many researchers over the past few decades. Solutions for overcoming this problem come from computer vision and robotic communities by means of Structure from Motion (SfM) and visual Simultaneous Localization and Mapping (vSLAM) ~ _cite_ . Many variants of these solutions have started to make an impact in a wide range of applications, including autonomous navigation and augmented reality. During the past few years, most of traditional visual localization techniques have been proposed and grounded on the estimate of the camera motion among a set of consecutive frames with geometric methods. For example, the feature-based method uses the projective geometry relations between ND feature points of the scene and their projection on the image plane~ _cite_, or the direct method minimizes the gradient of the pixel intensities across consecutive images~ _cite_ . However, these techniques are critical to ideal and controlled environments, e.g., with a large amount of texture, unchanged illumination and without dynamic objects. Obviously, their performance drops quickly when facing those challenging and unpredicted scenarios. Recently, a great breakthrough has been achieved in the Deep Learning (DL), through the application of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), e.g., for the object recognition and scene classification tasks. Therefore, learning-based visual odometry in the past few years has seen an increasing attention of the computer vision and robotic communities ~ _cite_ . This is due to its potentials in learning capability and the robustness to camera parameters and challenging environments. However, so far they are still unable to outperform most state-of-the-art feature-based localization methods. The drift from the true trajectory due to accumulation of errors over time is inevitable in those learning based VO system. This is due to the fact that such approaches cannot exploit high-capacity learning ND structural constraints from limited training datasets. Recent work ~ _cite_ concluded that global place recognition and camera relocalization plays a significant role in reducing these global drifts. As demonstrated in another relevant VLocNet ~ _cite_, the global and Siamese-type relative networks are designed for inferring global poses with the great help of relative motion. Nevertheless, VO drift problem still exists since its global and relative networks are separately optimized and regressed by a multitask alternating optimization strategy. To solve the drift problem completely, this paper extends VLocNet to fuse both relative and global networks, and considers more temporal sequences with LSTM incorporated in each networks for accurate pose prediction. Furthermore, we also employ a geometric consistency of the adjacent frames for regressing the relative and global networks at the same time. This proposed method brings two advantages: one is obviously that we leverage the camera re-localization to improve the accuracy of N-DoF VO. On the other hand, relative motion information from odometry can also be used to improve the global pose regression accuracy. In summary, our main contributions are as follows: