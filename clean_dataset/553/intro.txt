interaction with the environment requires robots to adapt their motor behavior according to perceived events. However, each sensorimotor cycle of the robot is affected by an inherent latency introduced by the processing time of sensors, transmission time of signals, and mechanical constraints~ _cite_ _cite_ _cite_ . Due to this latency, robots exhibit a discontinuous motor behavior which may compromise the accuracy and execution time of the assigned task. For social robots, delayed motor behavior makes human-robot interaction~ (HRI) asynchronous and less natural. Synchronization of movements during HRI may increase rapport and endow humanoid robots with the ability to collaborate with humans during daily tasks~ _cite_ . A possible solution to the sensorimotor latency is the application of predictive mechanisms which accumulate information from robot's perceptual and motor experience and learn an internal model which estimates possible future motor states~ _cite_ _cite_ . The learning of these models in an unsupervised manner and their adaptation throughout the acquisition of new sensorimotor information remains an open challenge. Latencies between perception and possible motor behavior occur in human beings~ _cite_ as well. Such discrepancies are caused by neural transmission delays and are constantly compensated by predictive mechanisms in our sensorimotor system that account for both motor prediction and anticipation of the target movement. Miall et al.~ _cite_ have proposed that the human cerebellum is capable of estimating the effects of a motor command through an internal action simulation and a prediction model. Furthermore, there are additional mechanisms for visual motion extrapolation which account for the anticipation of the future position and movement of the target~ _cite_ . Not only do we predict sensorimotor events in our everyday tasks, but we also constantly adjust our delay compensation mechanisms to the sensory feedback~ _cite_ and to the specific task~ _cite_ . Recently, there has been a considerable growth of learning-based prediction techniques, which mainly operate in a ``learn then predict" approach, i.e., typical motion patterns are extracted and learned from training data sequences and then learned patterns are used for prediction~ _cite_ _cite_ _cite_ _cite_ . The main issue with this approach is that the adaptation of the learned models is interrupted by the prediction stage. However, it is desirable for a robot operating in natural environments to be able to learn incrementally, i.e., over a lifetime of observations, and to refine the accumulated knowledge over time. Therefore, the development of learning-based predictive methods accounting for both incremental learning and predictive behavior still need to be fully investigated. In this work, we propose a novel architecture that learns sensorimotor patterns and predicts the future motor states in an on-line manner. We evaluate the architecture in the context of an imitation task in an HRI scenario. In this scenario, body motion patterns demonstrated by a human demonstrator are mapped to trajectories of robot joint angles and then learned for subsequent imitation by a humanoid robot. We approach the demonstration of the movements through motion capture with a depth sensor, which provides us with reliable estimations and tracking of a ND human body pose. Thus, the three-dimensional joint positions of the skeleton model constitute the input to the architecture. The learning module captures spatiotemporal dependencies through a hierarchy of Growing When Required~ (GWR) ~ _cite_ ~networks, which has been successfully applied for the classification of human activities _cite_ _cite_ . The learning algorithm processes incoming robot joint angles and progressively builds a dictionary of motion segments. Finally, an extended GWR algorithm, implemented at the last layer of our architecture, approximates a prediction function and utilizes the learned motion segments for predicting forthcoming motor commands. We evaluate our system on a dataset of three subjects performing N arm movement patterns. We study the prediction accuracy of our architecture while being continuously trained. Experimental results show that the proposed architecture can adapt quickly to an unseen pattern and can provide accurate predictions albeit continuously incorporating new knowledge. Moreover, we show that the system can maintain its performance even when training takes place with missing sensory information.