Enabling an autonomous robot to search and retrieve a desired object in an arbitrary indoor environment is always both fascinating and extremely challenging, as it would enable a variety of applications that could improve the quality of human life. For example, being able to navigate and localize objects is one of the basic functions that a robot elderly caregiver should be equipped with. Such technology can also be potentially used to help visually impaired people, thus significantly improving their quality of life. Moreover, self-driving cars with such an object searching capability will be able to approach and pick up their designated customers. Fundamentally, having a robot with vision that finds object is one of the major challenges that remain unsolved. With the current surge of deep reinforcement learning _cite_, a joint learning method of visual recognition and planning emerges as end-to-end learning _cite_ . Specifically, the robot learns an optimal action policy to reach the goal state by maximizing the reward it receives from the environment. Under the ``robot that finds objects'' setting, the goal state is the location of the target object with a high reward assigned. Several recent work have attempted to fulfill the challenge and achieved certain promising results. _cite_ adopted a target-driven deep reinforcement learning model to let robot find a specific visual scene. _cite_ also proposed a recognition-guided deep reinforcement learning for robot to find a user-specified target object. Although these deep reinforcement learning models can be trained to navigate a robot to find a target scene or object in an environment, a time-consuming re-training process is needed every time the target or the environment alters. In other words, these systems suffer from an unsatisfiable generalization capability to transfer the previously learned action policy to a brand new target or a novel environment. Such defect extremely limits the applications of these methods in real-world scenarios as it is impractical to conduct the inefficient training process every single time. In this paper, we argue that the limitation is deeply rooted in the task itself. While searching an object is indeed environment and object dependent, approaching an object after seen once should be a general capability. The insight could also be explained while observing human beings searching an object in a house. We first need to explore the house to locate the object once. After the object is captured with one sight, we are able to approach the target object with fairly few back and forth explorations. While the exploration policy varies a lot, the optimal approaching policy is indispensable, and provide a critical last step for a successful object search. Thus approaching policy is a much general capability of human beings, and thus in this paper, we focus on the approaching policy learning. We define an approaching task as the robot is initialized in a state where the target object can be seen, and the goal is to take the minimal number of steps to approach the target object. To tackle the challenge, we put forward a novel approach aiming at learning a generalizable approaching policy. We first treat a deep neural network as the policy approximator to map from visual signals to navigation actions, and adopt the deep reinforcement learning paradigm for model training. The trained model is expected to navigate the robot approaching a new target object in a new environment without any extra training effort. To learn an optimal action policy that can lead to a shortest path to approach the target object, previous methods typically attempts to map visual signal to an optimal action directly, no matter the signal contains clues towards reaching the goal state or not. In such a case, these methods inherently force the policy network to encode the local map information of the environment, which is specific towards a certain scene. Thus, re-training or fine-tuning is needed to update the model parameters while facing a novel target object or a new environment. Rather than learning a mapping from each visual signal directly to a navigation action, which has a much higher chance of encoding environment-dependent features, we present a method that first explicitly learns a general feature representations (scene depth and semantic segmentation map) to capture the task-relevant features solely from the visual signal. The representations serve as the input to the deep reinforcement learning model for training the action policy. To validate our proposed method's ability to generalize the approaching behavior, empirical experiments are conducted on both simulator (HouseND) and in a real-world scenario. We report the experimental results (a sharp increase of the generalization ability over baseline methods) in Section ~ _ref_ .