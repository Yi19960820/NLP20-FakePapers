Stochastic gradient descent (SGD) has been the workhorse for optimization of deep networks _cite_ . The most well-known form uses the Euclidean gradients with a varying learning rate to optimize the weights. In this regard, recent work _cite_ has brought to light scale invariance properties in weight space which commonly used deep networks possess. These symmetries or reparameterizations of the weights that even though the remains, the Euclidean gradient varies based on the chosen parameterization. Although these issues have been raised recently, the precursor to these methods is the early work of Amari _cite_, who proposed the use of natural gradients to tackle weight space symmetries in neural networks. The idea is to compute the steepest descent direction for the weight update on the manifold defined by these symmetries and use this direction to update the weights _cite_ . our analysis into a commonly used network there more complex forms of symmetries which can affect optimization, and hence there is a need to define weight updates which take into account these invariances.