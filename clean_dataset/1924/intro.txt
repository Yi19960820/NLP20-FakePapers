In the chemical sciences, designing chemicals with desired characteristics, such as a drug that interacts specifically with its intended target, or a material with specified physical performance ratings, is, despite decades of research, still largely driven by serendipity and chemical intuition. Over the decades, various machine learning (ML) algorithms have been developed to predict the activity or property of chemicals, using engineered features developed using domain knowledge. Recent work have also started using deep neural networks (DNN) ~ _cite_, that are on average, typically more accurate than traditional ML models~ _cite_ . Compared to modern deep learning research, the use of DNN models in chemistry relies heavily on engineered features. While such an approach is advantageous because it utilizes existing knowledge, using engineered features may limit the search space of potentially developable representations. This is exacerbated in situations in which engineered features are not appropriate or inadequate due to the lack of well-developed domain knowledge. With the growth of chemical data~ _cite_, it may be desirable to fully leverage representation learning, which will enable one to predict novel chemical properties for which little or no feature engineering research has been performed. In computer vision research, this is achieved by using raw data. For example, unaltered images are used as the input in various CNN models~ _cite_ . In chemistry, DNN models that leverage representation learning from raw data are starting to emerge. For example, with minimal feature engineering, molecular graphs have been used to train DNN models~ _cite_ . Other approaches use ND or ND images to train convolutional neural network (CNN) models~ _cite_, or SMILES strings to train recurrent neural network (RNN) models~ _cite_ . One factor that complicates representation learning is the limited amount of usable labeled data in chemistry, which is significantly smaller than that available in modern deep learning research. For example, having N, N labeled datapoints is considered a significant accomplishment in chemistry. In contrast, in computer vision research, datasets like ImageNet~ _cite_ that includes over a million images are typically the starting point. While sizable chemical databases like PubChem~ _cite_ and ChEMBL~ _cite_ do exist, their labels are skewed towards biomedical data, and such databases are only sparsely labelled, where each labeled data (i.e. measurements) are only available for a small subset (typically under N \%) of the entire database. Therefore, the current state of labeled chemical data is small and fragmented, which reduces the effectiveness of representation learning when using conventional supervised training approaches. Our work addresses the small and fragmented data landscape in chemistry. This is achieved by leveraging rule-based knowledge obtained from prior feature engineering research in chemistry~ _cite_ to perform weak supervised learning, and combining it with transfer learning methods used in modern deep learning research~ _cite_ . Specifically, we develop ChemNet, the first deep neural network that is pre-trained with chemistry-relevant representations, making it the analogous counterpart of a ResNet or GoogleNet for use in the chemical sciences. Our contributions are as follows. The organization for the rest of the paper is as follows. In section N, we outline the motivations in developing a chemistry-relevant rule-based weak supervised learning approach, and the design principles behind ChemNet. In section N, we examine the datasets, its broad applicability to chemical-affliated industries, as well as the training protocols used for pre-training ChemNet, and for evaluating its performance on unseen chemical tasks. Lastly, in section N, we explore different ChemNet models, and the various factors that affect model accuracy and generalization. The best ChemNet model was then evaluated against other DNN models trained using conventional supervised learning approaches. Transfer learning is an established technique in deep learning research~ _cite_ . This approach first trains a neural network on a larger database, before fine-tuning it on a smaller dataset. For example, using ResNet that has been pre-trained on ImageNet to classify various common objects, may be used with transfer learning techniques to classify specific clothing type. In addition, as long as there is sufficient overlap in the "image space" on which the network was trained on, seemingly unrelated outcomes can be achieved. For example, a model pre-trained on ImageNet can also be fine-tuned to classify medical images~ _cite_ . While medical applications are seemingly unrelated to conventional image recognition tasks, both sets of data are natural photographs (i.e. in the same "image space") and thus the lower-level basic representations can be utilized. In the chemistry literature, because ND molecular diagrams are substantially different from natural photographs, and chemistry-specific information is encoded into the image channels, existing pre-trained models in the computer vision literature on RGB images for example, would not be fully applicable. In addition, there are limited examples of using weak supervised learning to utilize large chemical databases in traing neural networks. Thus, the challenge of how to convert a sparsely labeled chemical database into a usable form that can be used in a transfer learning approach for existing chemistry DNN models is a non-trivial task. In our work, we will use molecular descriptors to generate consistent and inexpensive rule-based labels, combined with a weak supervised learning approach to train ChemNet to develop chemically-relevant internal representations, which is an approach that is conceptually unique relative to existing methods.