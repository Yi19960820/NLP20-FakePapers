Designing a system for reliable large scale localisation is a challenging problem. The discovery of the positioning system in mammalian brains, located in the hippocampus, was awarded the N Nobel prize in Physiology or Medicine _cite_ . It is an important problem for computer vision too, with localisation technology essential for many applications including autonomous vehicles, unmanned aerial vehicles and augmented reality. State of the art localisation systems perform very well within controlled environments _cite_ . However, we are yet to see their wide spread use in the wild because of their inability to cope with large viewpoint or appearance changes. Many of the visual localisation systems use point landmarks such as SIFT _cite_ or ORB _cite_ to localise. These features perform well for incremental tracking and estimating ego-motion _cite_ . However, these point features are not able to create a representation which is sufficiently robust to challenging real-world scenarios. For example, point features are often not robust enough for localising across different weather, lighting or environmental conditions. Additionally, they lack the ability to capture global context, and require robust aggregation of hundreds of points to form a consensus to predict pose _cite_ . To address this problem, we introduced PoseNet _cite_ which uses end-to-end deep learning to predict camera pose from a single input image. It was shown to be able localise more robustly using deep learning, compared with point features such as SIFT _cite_ . PoseNet learns a representation using the entire image context based on appearance and shape. These features generalise well and can localise across challenging lighting and appearances changes. It is also fast, being able to regress the camera's pose in only a few milliseconds. It is very scalable as it does not require a large database of landmarks. Rather, it learns a mapping from pixels to a high dimensional space linear with pose. The main weakness of PoseNet _cite_ was that despite its scalability and robustness it did not produce metric accuracy which is comparable to other geometric methods _cite_ . In this paper we argue that a contributing factor to this was because PoseNet naively applied a deep learning model end-to-end to learn camera pose. In this work, we reconsider this problem with a grounding in geometry. We wish to build upon the decades of research into multi-view geometry _cite_ to improve our ability to use deep learning to regress camera pose. The main contribution of this paper is improving the performance of PoseNet with geometrically formed loss functions. It is not trivial to simply regress position and rotation quantities using supervised learning. PoseNet required a weighting factor to balance these two properties, but was not tolerant to the selection of this hyperparameter. In we explore loss functions which remove this hyperparameter, or optimise it directly from the data. In we show how to train directly from the scene geometry using the reprojection error. In we demonstrate our system on an array of datasets, ranging from individual indoor rooms, to the Dubrovnik city dataset _cite_ . We show that our geometric approach can improve PoseNet's efficacy across many different datasets--narrowing the deficit to traditional SIFT feature-based algorithms. For outdoor scenes ranging from _inline_eq_ to _inline_eq_ we can achieve relocalisation accuracies of a few meters and a few degrees. In small rooms we are able to achieve accuracies of _inline_eq_ .