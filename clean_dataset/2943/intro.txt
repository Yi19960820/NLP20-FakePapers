Automated recognition of hand signals has many applications in computer science. It might facilitate the interaction between humans and computers in many situations, especially for people with disabilities. One interesting area of focus is the recognition of sign languages. This way of communication is widely used among Deaf communities. Therefore, a system capable of understanding it would be convenient for them, just like automated speech recognition is useful to people using spoken languages. Also, such a system could be the base for numerous other applications and ways of interacting with computers as an alternative to traditional input devices such as mouse and keyboard. However, sign languages usually include non-manual signs such as facial expressions in addition to thousands of manual gestures and poses. Additionnally, some of these signs can be sequential (like spoken languages) or parallel _cite_ . This large variety of signs adds to the already complex task of finding a body and hands in a dynamic environment. For this reason, research at the moment focuses mostly on specific, easier recognition tasks. One such interesting task is the recognition of American Sign Language (ASL) fingerspelling, which is a way of spelling words for which the sign is unknown or non existent in the ASL language. Although ASL does not share much with the English language syntactically, ASL fingerspelling uses the same N letter alphabet as written English to manually spell out words. Fingerspelling constitutes a significant portion of ASL exchanges, accounting for N-N~ \% of ASL communication _cite_ . Fingerspelling is performed with a single hand and is composed of N static and two dynamic hand poses, some of which appear very similar. The two dynamic poses are J and Z, and simply involve drawing the letter with either the little finger (J) or index finger (Z) . The task of classifying fingerspelling, even as a smaller subset of ASL, is still very challenging due to the similarity between poses (see Fig. _ref_) . Also, the hand pose can vary greatly between different users, due to different hand shapes and sizes, knowledge of ASL, previous signs, arm orientation and others. The data set we use (from _cite_) is challenging in that aspect, providing various orientations over five very different users (Fig. _ref_) . The system we propose is capable of on-the-fly classification of the N static signs, using inputs provided by a traditional intensity camera, in addition to a depth camera.