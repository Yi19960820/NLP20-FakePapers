Architecture design in deep convolutional neural networks has been attracting increasing interests. The basic design purpose is efficient in terms of computation and parameter with high accuracy. Various design dimensions have been considered, ranging from small kernels~ _cite_, identity mappings~ _cite_ or general multi-branch structures~ _cite_ for easing the training of very deep networks, and multi-branch structures for increasing the width~ _cite_ . Our interest is to reduce the redundancy of convolutional kernels. The redundancy comes from two extents: the spatial extent and the channel extent. In the spatial extent, small kernels are developed, such as _inline_eq_, _inline_eq_, _inline_eq_ ~ _cite_ . In the channel extent, group convolutions~ _cite_ and channel-wise convolutions or separable filters~ _cite_, have been studied. Our work belongs to the kernel design in the channel extent. In this paper, we present a novel network architecture, which is a stack of interleaved group convolution (IGC) blocks. Each block contains two group convolutions: primary group convolution and secondary group convolution, which are conducted on primary and secondary partitions, respectively. The primary partitions are obtained by simply splitting input channels, e.g., _inline_eq_ partitions with each containing _inline_eq_ channels, and there are _inline_eq_ secondary partitions, each containing _inline_eq_ channels that lie in different primary partitions. The primary group convolution performs the spatial convolution over each primary partition, and the secondary group convolution performs a _inline_eq_ convolution (point-wise convolution) over each secondary partition, the channels across partitions outputted by primary group convolution. Figure~ _ref_ illustrates the interleaved group convolution block. It is known that a group convolution is equivalent to a regular convolution with sparse kernels: there is no connections across the channels in different partitions. Accordingly, an IGC block is equivalent to a regular convolution with the kernel composed from the product of two sparse kernels, resulting in a dense kernel. We show that under the same number of parameters/computation complexity, an IGC block (except the extreme case that the number of primary partitions, _inline_eq_, is _inline_eq_) is wider than a regular convolution with the spatial kernel size same to that of primary group convolution. Empirically, we also observe that a network built by stacking IGC blocks under the same computation complexity and the same number of parameters performs better than the network with regular convolutions. We study the relations with existing related modules. (i) The regular convolution and group convolution with summation fusion~ _cite_, are both interleaved group convolutions, where the kernels are in special forms and are fixed in secondary group convolution. (ii) An IGC block in the extreme case where there is only one partition in the secondary group convolution, is very close to Xception~ _cite_ . Our main contributions are summarized as follows.