Face synthesis is essential to many applications, such as computer games, animated movies, teleconferencing, talking agents, etc. Traditional facial capture approaches have gained tremendous successes, reconstructing high level of realism. Yet, active face capture rigs utilizing motion sensors/markers are expensive and time-consuming to use. Alternatively, passive techniques capturing facial transformations from cameras, although less accurate, have achieved very impressive performance. There lies one problem with vision-based facial capture approaches, however, where part of the face is occluded, e.g. when a person is wearing a mixed reality visor, or in the extreme situation where the entire visual appearance is non-existent. In such cases, other input modalities, such as audio, may be exploited to infer facial actions. Indeed, research on speech-driven face synthesis has regained attention of the community in recent time. Latest works ~ _cite_ employ deep neural networks in order to model the highly non-linear mapping from input speech domain, either as audio or phonemes, to visual facial features. Particularly, in approaches by Karras et al.~ _cite_ and Pham et al.~ _cite_, the reconstruction of facial emotion is also taken into account to generate fully transform ND facial shapes. The method in~ _cite_ explicitly specifies the emotional state as an additional input beside waveforms, whereas~ _cite_ implicitly infers affective states from acoustic features, and represents emotions via blendshape weights~ _cite_ . In this work, we further improve the approach of~ _cite_ in several ways, in order to recreate a better ND talking avatar that can naturally rotate and perform micro facial actions to represent the time-varying contextual information and emotional intensity from speech in real-time. Firstly, we forgo using handcrafted, high-level acoustic features such as chromagram or mel-frequency cepstral coefficients (MFCC), which, as the authors conjectured, may cause the loss of important information to identify some specific emotions, e.g. happy. Instead, we directly use Fourier transformed spectrogram as input to our neural network. Secondly, we employ convolutional neural networks (CNN) to learn meaningful acoustic feature representations, and take advantage of the locality and shift invariance in the frequency domain of audio signal. Thirdly, we combine these convolutional layers with recurrent layer in an end-to-end framework, which learns both temporal transition of facial movements, as well as spontaneous actions and varying emotional states from only speech sequences. Experiments on the RAVDESS audiovisual corpus~ _cite_ demonstrate promising results of our approach in real-time speech-driven ND facial animation. The organization of the paper is as follows. Section~ _ref_ summarizes other studies related to our work. Our approach is explained in details in Section~ _ref_ . Experiments are described in Section~ _ref_, before Section~ _ref_ concludes our work.