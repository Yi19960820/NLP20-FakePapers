Astronomy has always boasted of big datasets. The data holdings are getting even larger due to surveys that observe hundreds of millions of sources hundreds of time. The observations are a time series of flux measurements called light curves. The staple for discovery has been the flux variations of individual astronomical objects as noted through such light curves-that is where the science is. The large irregular gaps in observing cadence makes classification challenging. Traditionally statistical features have been derived from the light curves in order to do follow-up classification (see, e.g., _cite_) . The features include standard statistical measures like median, skew, kurtosis as well as specialized domain knowledge based ones such as ` fading profile of a single peaked fast transient '. The standard features do not carry special powers for classifying a varied set of objects. The designer features are better for specific classes, but carry with them a bias that does not necessarily translate to the classification of a wider set. In _cite_ we introduced a two-dimensional mapping of the light curves based on the changes in magnitude (_inline_eq_) over the available time-differences (_inline_eq_) . In this work, we mold the _inline_eq_ mapping into an image format that is suitable as input for (CNNs or ConvNets) ~ _cite_ . By bringing to bear the machinery of CNNs we are able to conjure a large number of features unimagined so far. We use labeled sets to train the CNN as a classifier and following validation we classify light curves from the Catalina Real-Time Transient Survey (CRTS; _cite_) .