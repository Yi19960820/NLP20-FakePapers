In recent years, deep neural networks (DNNs) have achieved remarkable performance across a wide range of applications, including but not limited to computer vision, natural language processing, speech recognition, etc. These breakthroughs are closely related to the increased amount of training data and more powerful computing resources now available. For example, one breakthrough in the natural image recognition field was achieved by AlexNet _cite_, which was trained using multiple graphics processing units (GPUs) on about N images. Since then, the performance of DNNs has continued to improve. For many tasks, DNNs are reported to be able to outperform humans. The problem, however, is that the computational complexity as well as the storage requirements of these DNNs has also increased drastically as shown in Table _ref_ . Specifically, the widely used VGG-N model _cite_ involves more than NMB of storage and over NB FLOPs to classify a single _inline_eq_ image. Thanks to the recent crop of powerful GPUs and CPU clusters equipped with more abundant memory resources and computational units, these more powerful DNNs can be trained within a relatively reasonable time period. However, when it is time for the inference phase, such a long execution time is impractical for real-time applications. Recent years have witnessed great progress in embedded and mobile devices including unmanned drones, smart phones, intelligent glasses, etc. The demand for deployment of DNN models on these devices has become more intense. However, the resources of these devices, for example, the storage and computational units as well as the battery power remain very limited, and this poses a real challenge in accelerating modern DNNs in low-cost settings. Therefore, a critical problem currently is how to equip specific hardware with efficient deep networks without significantly lowering the performance. To deal with this issue, many great ideas and methods from the algorithm side have been investigated over the past few years. Some of these works focused on model compression while others focused on acceleration or lowering power consumption. As for the hardware side, a wide variety of FPGA/ASIC-based accelerators have been proposed for embedded and mobile applications. In this paper, we present a comprehensive survey of several advanced approaches in network compression, acceleration and accelerator design. We will present the central ideas behind each approach and explore the similarities and differences between the different methods. Finally, we will present some future directions in the field. The rest of this paper is organized as follows. In Section _ref_, we give some background on network acceleration and compression. From Section _ref_ to Section _ref_, we systematically describe a series of hardware-efficient DNN algorithms, including network pruning, low-rank approximation, network quantization, teacher-student networks and compact network design. In Section _ref_, we introduce the design and implementation of hardware accelerators based on FPGA/ASIC technologies. In Section _ref_, we discuss some future directions in the field, and Section _ref_ concludes the paper.