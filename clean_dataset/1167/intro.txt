Word spotting is the task of searching through a collection of scanned manuscripts to locate a provided search query, see Figure _ref_ . Word spotting gives an important oppertunity for digital humanities researchers, whose work is limited by the time spent on manually sifting through old manuscripts to find what they are looking for. Researchers working with old manuscripts can spend several months with a single book of a few hundred pages. There exist many digitized collections that are used for research purposes _cite_, and although there have been work automatically mining these for information _cite_, they are exclusively text-based methods. This means that the only way to search for information or do statistics in a collection is to painstakingly read and transcribe the manuscripts manually. Although there have been successful crowdsourcing projects to transcribe manuscript collections _cite_, they are limited to fairly modern transcripts and written in languages that are relatively common. Using the same approach for more esoteric work, written in multiple rarely spoken or dead languages and different alphabets and no canonical spelling would prove difficult. Similarly, OCR software have been used in research using documents, but OCR technology is even more limited than crowdsourcing, typically requiring neat machine printed text to produce legible results. In addition to these existing technologies, word spotting provides new possibilities for quantitative research in fields as diverse as demography, linguistics, paleography, genealogy, and history. Compared to letter-by-letter text recognition, word spotting is a simpler task that, as a result, often is more transferable between sources (e.g., no language model is typically used) ; less data is required, which is crucial since manual annotation of historical manuscripts is very expensive as it often requires expert knowledge, making popular crowdsourcing alternatives (like amazon turk) not applicable; and since word spotting is designed to be more like a tool to find what you are looking for, manual inspection is typically done in any case. The task of word spotting is typically defined in ways that differ in two regards. The first is whether to do segmentation-based or segmentation-free word spotting. For segmentation-based word spotting, you assume that you have access to segmented word images, which is an unrealistic assumption when it comes to a real-life practical setting. This is not the case for segmentation-free word spotting, where you only need the image of a manuscript page. The second is whether or not the query is a manually cropped images of a word, query-by-examples (QbE), or a string of characters, query-by-string (QbS) . Both work in a practical setting, though QbS is most often the preferred option since it does not require you to find an example of what you are looking for before you can search for more occurrences. Therefore, the preferred paradigm of word spotting is almost always segmentation-free QbS word spotting. This work presents an end-to-end trainable model for segmentation-free query-by-string word spotting that is designed with the intent helping professionals and enthusiasts that work with manuscripts on a daily basis. Based on a deep Convolutional Neural Network (CNN) _cite_, and using the recently introduced, fully differentiable dense localization layer for region-based training and prediction _cite_, together with the state-of-the-art word embedding approaches for handwritten word spotting _cite_, we present a model for word spotting that outperforms existing state-of-the-art results on common benchmark datasets, as well as experimental results on an early N century manuscript.