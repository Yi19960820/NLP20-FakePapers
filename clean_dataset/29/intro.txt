The research of artificial neural networks (ANN) began more than N years ago, proposed by Warren McCulloch and Walter Pitts~ _cite_, Donald Hebb~ _cite_ and Frank Rosenblatt~ _cite_ . Especially in~ _cite_, a two-layer network is introduced for pattern recognition. Figure~ _ref_ shows the basic function of ANN, that is, the neuron of higher layer is calculated by the neurons of prior layer with the connecting weights. However, there was no solution for the network training until backpropagation (gradient descent) algorithm was created by Paul Werbos~ _cite_ . After James McClelland~ _cite_ introduced the ANN as simulation of natural neural process and its usage in artificial intelligence (AI), the research of ANN became popular. Since then, ANN was successfully applied to image classification~ _cite_, character recognition ~ _cite_, face recognition~ _cite_, speech recognition~ _cite_ and so on. Moreover, a theoretical explanation of ANN's success was also given by Kurt Hornik~ _cite_ . \par Because of the limitation of computers, the ANN research stagnated until the new century came. With the development of computers, the training of large-scale neural network became possible. As a result, the research of deep neural networks (DNN) emerged and attracted more and more attention. For example, Geoff Hinton's deep belief nets~ _cite_, Yann LeCun and Dan Ciresan's study on deep convolutional neural networks~ _cite_, and Alex Graves's deep recurrent neural networks~ _cite_ . The features of DNN can be summarized as follows. \par Obviously, although the performance of DNN is promising, it still needs to be improved in different ways. On one hand, in order to pursue higher recognition rate, several optimization methods for training were proposed, such as dropout~ _cite_ and dropconnect~ _cite_ . Those methods can reduce the overfitting problem significantly. On the other hand, new understanding of the neural network emerges and may extend the ability of DNN. For example, in Ian Goodfellow's recent work~ _cite_ for digit string recognition, the output layer is trained to show both the digit number and the recognition result of each digit. By doing this, their DNN model is able to recognize the digit string directly, without any segmentation process. This work extended the DNN from single character recognition to character string recognition. Inspired by this work, we may change the basic framework of DNN to find more possibilities. \par In this paper, we focus on the basic function of ANN and try to make it more suitable for computers. Usually, the ANN is seen as simulation of natural neural process, thus its neurons and weights are all real number. Such values can represent the electric signal generated by neural cells. However, the ANN models are often realized by computers. As we all know, the computer process data based on binary value, like ``N'' and ``N''. In other words, we can say that the basic ``neural cell'' of computer generates binary signals. Inspired by this observation, we proposed a new type of neural network---the binarized deep neural network (BDNN) . In BDNN, all the neurons and weights are binary value; at the same time, the calculation of the basic function of BDNN is also Boolean. \par Actually, there were researches of binary neural network (Hopfield neural network) ~ _cite_ and corresponding training algorithms~ _cite_ . Nevertheless, this kind of neural network is quite different from BDNN. Although the input and output of the binary neural network are binary values, the weights of which are real number. Therefore, the calculation of binary neural network is not different from the conventional neural network. In contrast, the BDNN is a pure binary system, in which all the variables and operations are all binarized (Boolean) . Compared with conventional deep neural network, the BDNN is expected to have several promising merits, which are shown as follows. \par In summary, with binary variables and Boolean operations, the BDNN is able to run with reasonable computational resource and storage. Although now the performance of BDNN may not be comparable with the conventional deep neural network of the same scale, it has the potential to be improved in the future. \par This paper is organized as follows. In Section~ _ref_, the principles of the BDNN is introduced as well as the hybrid binarized deep neural network (hybrid-BDNN) for non-binary input data. In Section~ _ref_, a training method is proposed for BDNN and hybrid-BDNN. In Section~ _ref_, comparison experiments of BDNN and conventional DNN are analyzed. The last section is the conclusion part.