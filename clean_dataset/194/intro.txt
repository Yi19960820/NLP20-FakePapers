\noindent Our ability to effortlessly point out and describe all aspects of an image relies on a strong semantic understanding of a visual scene and all of its elements. However, despite numerous potential applications, this ability remains a challenge for our state of the art visual recognition systems. In the last few years there has been significant progress in image classification~ _cite_, where the task is to assign one label to an image. Further work has pushed these advances along two orthogonal directions: First, rapid progress in object detection~ _cite_ has identified models that efficiently identify and label multiple salient regions of an image. Second, recent advances in image captioning _cite_ have expanded the complexity of the label space from a fixed set of categories to sequence of words able to express significantly richer concepts. However, despite encouraging progress along the label density and label complexity axes, these two directions have remained separate. In this work we take a step towards unifying these two inter-connected tasks into one joint framework. First, we introduce the dense captioning task (see Figure _ref_), which requires a model to predict a set of descriptions across regions of an image. Object detection is hence recovered as a special case when the target labels consist of one word, and image captioning is recovered when all images consist of one region that spans the full image. Additionally, we develop a Fully Convolutional Localization Network architecture (FCLN) to address the dense captioning task. Our model is inspired by recent work in image captioning _cite_ in that it is composed of a Convolutional Neural Network followed by a Recurrent Neural Network language model. However, drawing on work in object detection _cite_, our second core contribution is to introduce a new dense localization layer. This layer is fully differentiable and can be inserted into any neural network that processes images to enable region-level training and predictions. Internally, the localization layer predicts a set of regions of interest in the image and then uses bilinear interpolation _cite_ to smoothly extract the activations inside each region. We evaluate the model on the large-scale Visual Genome dataset, which contains N, N images and N, N, N region captions. Our results show both performance and speed improvements over approaches based on previous state of the art. We make our code and data publicly available to support further progress on the dense captioning task.