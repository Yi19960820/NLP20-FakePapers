Given an image and a related textual description, phrase grounding attempts to localize objects which are mentioned by corresponding phrases in the description. It is an important building block in computer vision with natural language interaction, which can be utilized in high-level tasks, such as image retrieval~ _cite_, image captioning~ _cite_ and visual question answering~ _cite_ . Phrase Grounding is a challenging problem that involves parsing language queries and relating the knowledge to localize objects in visual domain. To address this problem, typically a proposal generation system is first applied to produce a set of proposals as grounding candidates. The main difficulties lie in how to learn the correlation between language (query) and visual (proposals) modalities, and how to localize objects based on multimodal correlation. State-of-the-art methods address the first difficulty by learning a subspace to measure the similarities between proposals and queries. With the learned subspace, they treat the second difficulty as a retrieval problem, where proposals are ranked based on their relevance to the input query. Among these, Phrase-Region CCA~ _cite_ and SCRC~ _cite_ models learn a multimodal subspace via Canonical Correlation Analysis (CCA) and a Recurrent Neural Network (RNN) respectively. Varun ~ _cite_ learn multimodal correlation aided by context objects in visual content. GroundeR~ _cite_ introduces an attention mechanism that learns to attend on related proposals given different queries through phrase reconstruction. These approaches have two important limitations. First, proposals generated by independent systems may not always cover all mentioned objects given various queries; since retrieval based methods localize objects by choosing one of these proposals, they are bounded by the performance limits from proposal generation systems. Second, even though query phrases are often selected from image descriptions, context from these descriptions is not utilized to reduce semantic ambiguity. Consider example in Fig~ _ref_ . Given the query ``a man'', phrases ``a guitar'' and ``a little girl'' can be considered to provide context that proposals overlapping with ``a guitar'' or ``a little girl'' are less likely to be the ones containing ``a man''. To address the aforementioned issues, we propose to predict mentioned object's location rather than selecting candidates from limited proposals. We adopt a regression based method guided by input query's semantics. To reduce semantic ambiguity, we assume that different phrases in one sentence refer to different visual objects. Given one query phrase, we evaluate predicted proposals and down-weight those which cover objects mentioned by other phrases (, context) . For example, we assign lower rewards for proposals containing ``a guitar'' and ``a little girl'' in Fig~ _ref_ to guide system to select more discriminative proposals containing ``a man''. Since this procedure depends on prediction results and is non-differentiable, we utilize reinforcement learning~ _cite_ to adaptively estimate these rewards conditioned on context information and jointly optimize the framework. In implementation, we propose a novel Query-guided Regression network with Context policy (QRC Net) which consists of a Proposal Generation Network (PGN), a Query-guided Regression Network (QRN) and a Context Policy Network (CPN) . PGN is a proposal generator which provides candidate proposals given an input image (red boxes in Fig.~ _ref_) . To overcome performance limit from PGN, QRN not only estimates each proposal's relevance to the input query, but also predicts its regression parameters to the mentioned object conditioned on the query's intent (yellow and green boxes in Fig.~ _ref_) . CPN samples QRN's prediction results and evaluates them by leveraging context information as a reward function. The estimated reward is then back propagated as policy gradients (Step N in Fig.~ _ref_) to assist QRC Net's optimization. In training stage, we jointly optimize PGN, QRN and CPN using an alternating method in~ _cite_ . In test stage, we fix CPN and apply trained PGN and QRN to ground objects for different queries. We evaluate QRC Net on two grounding datasets: FlickrNK Entities~ _cite_ and Referit Game~ _cite_ . FlickrNK Entities contains more than NK images and NK query phrases, while Referit Game has NK images referred by NK query phrases. Experiments show QRC Net outperforms state-of-the-art methods by a large margin on both two datasets, with more than N \% increase on FlickrNK Entities and N \% increase on Referit Game in accuracy. Our contributions are twofold: First, we propose a query-guided regression network to overcome performance limits of independent proposal generation systems. Second, we introduce reinforcement learning to leverage context information to reduce semantic ambiguity. In the following paper, we first discuss related work in Sec.~ _ref_ . More details of QRC Net are provided in Sec.~ _ref_ . Finally we analyze and compare QRC Net with other approaches in Sec.~ _ref_ .