\noindent Availability of labeled image data has helped in making great advances in Computer Vision. However, even the largest image dataset i.e Imagenet _cite_ has only N classes, with many classes having very few images. Thus collecting and training on sufficient number of images from all the classes of images that may occur in practice is a very difficult task. Moreover, new classes come into existence every day and images of certain classes may be rare and difficult to obtain. Human beings are excellent at recognizing novel objects that have not been visually encountered before. For instance given the information that an auroch is an ancient cow, has large horns, has large build, one can easily identify an image of an auroch from other animals such as a pig or sheep although one hasn't seen an auroch before. Zero shot learning tries to capture this intuition by assuming that some other information about the novel class is available although no image from that class is available in the training set _cite_ . This extra information is typically in the form of attributes or textual descriptions. More formally, let _inline_eq_ and _inline_eq_ represent the training images and their class labels respectively. Similarly, let _inline_eq_ and _inline_eq_ represent the test images and their corresponding labels. The zero shot setting states that _inline_eq_ . However for each label _inline_eq_ in _inline_eq_, we have an embedding vector, called class embedding vector _inline_eq_, that is semantically related to the class corresponding to that label. This vector could come from other modalities, such as language and may be obtained using different approaches such as manually or automatically annotated attributes (for e.g wordNvec) . Recently zero shot learning has emerged as an active area of research in the interplay between vision and language _cite_ . It is an interesting area of research since the models for this problem can help understand how well language concepts translate to visual information. If the classes are modeled accurately via the embedding vectors, the problem can be viewed as finding a relation between the embedding vector of a class and the visual features of the images in that class. Most zero shot learning approaches learn a projection from image space to the class embedding space via a transfer function. For a novel class image given at test time, the class with the closest class embedding vector to the projection in the class embedding space is assigned. Similarly it is also possible to learn a mapping function from class embedding to the image space. In the first, the mapping is a simple linear function with various kinds of regularizations. The challenges faced in learning such a mapping are well documented, the primary issue being that of domain shift first identified by _cite_ . The mapping learned from the seen class images may not correctly capture the relationship for unseen classes as the space may not be as continuous and smooth in the image domain. The image space may also be more complicated than the semantic space due to the complex image generation process. Recent advances in unsupervised learning have led to better architectures for modeling the statistical image generation process, primary among them being generative adversarial networks (GANs) _cite_ and variational autoencoders (VAEs) _cite_ . These models can also be used for condition specific image generation _cite_ . For example one can generate images conditioned on attributes. A natural question to ask is: How much can this attribute conditional image generation, generalize to unseen classes and can this be used for zero shot learning? In this work, instead of directly modeling a transfer function, we view problem as a case of missing data, and try to model the statistical image generation process via VAEs conditioned on the class embedding vector. The missing data for the unseen classes is filled using such generated image data. ZSL models are typically evaluated in two ways. In the standard setting _cite_, it is assumed that the train and test classes are disjoint _inline_eq_ i.e the training class images do not occur at test time. However, this is hardly true in the real world and can artificially boost results for the unseen classes without regard to the train class performance. Hence the Generalized zero shot setting has been proposed _cite_ where both train and test classes may occur during the test time (Note that the latter setting is much harder than the former since the classifiers are typically biased either towards the classes seen at training time or those unseen at training time) . We present our evaluation on both settings but obtained the greatest improvements in the much harder generalized setting. We follow the evaluation protocol recently proposed by _cite_ that ensures that the models are not pre-trained on any of the test classes. The main contributions of this paper are as follows: The paper is organized as follows: In section-N we present a brief overview of the various existing methods for zero shot learning. Section-N presents the motivation, and a description of the proposed approach. Section-N describes the evaluation settings. Section-N presents the experiments, results, and a comparison with existing methods. Finally in section-N, we present many directions for future work and a summary of our contributions.