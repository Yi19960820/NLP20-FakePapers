Deep learning has significantly changed the approaches for solving computer vision problems. Instead of analytic solutions with some combinations of hand chosen features, probabilistic/physical models and some optimizations, most methods now turn to deep learning which is a deeper neural network that relies on big data. Deep learning has shown superb performance in many computer vision problems including image recognition~ _cite_, face recognition~ _cite_, segmentation~ _cite_, etc. Image processing problems such as super-resolution~ _cite_ and colorization~ _cite_ are also solved with deep learning now, which provides effective ways to process input images and output images that fit the given task. In this paper, we introduce a new application of using deep learning for image processing: modelling the scene dependent image processing. We are especially interested in modelling the in-camera imaging pipeline to recover RAW images from camera processed images (usually in the form of JPEG in the sRGB color space) and vice versa. Usually called as the radiometric calibration, this process is important for many computer vision tasks that require accurate measurement of the scene radiance such as photometric stereo~ _cite_, intrinsic imaging~ _cite_, high dynamic range imaging~ _cite_, and hyperspectral imaging~ _cite_ . There are two strategies with regards to the image processing in cameras, namely, the photographic reproduction model and the photofinishing model~ _cite_ . In the photographic reproduction model, the image rendering pipeline is fixed meaning that a raw RGB value will always be mapped to an RGB value in the processed image regardless of the scene. Taking photos under the manual mode triggers this model and all previous radiometric calibration methods work only in this mode. In the photofinishing model, the image processing inside the camera varies (possibly in a spatially varying manner) in order to deliver visually optimal picture depending on the shooting environment~ _cite_ . This scene dependent mode will be activated usually when the camera is operated under the auto-mode. Figure~ _ref_ compares the photos of a scene captured under the manual mode and under the auto-mode. In (b), the scene became brighter and the colors were enhanced compared to (a) . It shows that the color rendering will be dependent on the scene in the auto-mode. Scene dependency in cameras were also verified in _cite_ . As mentioned above, none of the previous work can deal with the scene dependent color rendering. This is a problem as there are many photometry related topics in computer vision that have access to only automatically taken images (e.g. internet images) as in _cite_ . Moreover, smartphone cameras have become the major source for images and the many phone cameras only work in the auto-mode. The goal of this paper is to present a new algorithm that can model the camera imaging process under the "auto" mode. To deal with the scene dependency, we take the data-driven approach and design a deep neural network. We show that modelling the image processing using conventional CNN-based approaches is not effective for the given task, and propose a multi-scale pyramid of learnable histogram~ _cite_ to incorporate both the global and the local color histogram into pixel-wise features. The extracted multi-scale contextual features are processed with our CNN to model the scene dependent and locally varying color transformation. To the best of our knowledge, this is the first paper that can extract RAW images from processed images taken under the auto setting. Being able to radiometrically calibrate smartphone cameras is especially a significant contribution of this work. We show that we can model both the forward rendering (RAW to sRGB) and the reverse rendering (sRGB to RAW) accurately using our deep learning framework. We further apply our work to image deblurring. A blurred image is first transformed to the RAW space, in which a deblurring algorithm is executed. The deblurred image is then transformed back to the sRGB space through our deep network. We show that performing deblurring in this fashion give much better results over deblurring in the nonlinear sRGB space.