Single-task learning in computer vision has enjoyed much success in deep learning, with many single-task models now performing at or beyond human accuracies for a wide array of tasks. However, an ultimate visual system for full scene understanding must be able to perform many diverse perceptual tasks simultaneously and efficiently, especially within the limited compute environments of embedded systems such as smartphones, wearable devices, and robots/drones. Such a system can be enabled by multitask learning, where one model shares weights across multiple tasks and makes multiple inferences in one forward pass. Such networks are not only scalable, but the shared features within these networks can induce more robust regularization and boost performance as a result. In the ideal limit, we can thus have the best of both worlds with multitask networks: more efficiency and higher performance. In general, multitask networks are difficult to train; different tasks need to be properly balanced so network parameters converge to robust shared features that are useful across all tasks. Methods in multitask learning thus far have largely tried to find this balance by manipulating the forward pass of the network (e.g. through constructing explicit statistical relationships between features _cite_ or optimizing multitask network architectures _cite_, etc.), but such methods ignore a key insight: task imbalances impede proper training because they manifest as imbalances between backpropagated gradients . A task that is too dominant during training, for example, will necessarily express that dominance by inducing gradients which have relatively large magnitudes. We aim to mitigate such issues at their root by directly modifying gradient magnitudes through tuning of the multitask loss function. In practice, the multitask loss function is often assumed to be linear in the single task losses _inline_eq_, _inline_eq_, where the sum runs over all _inline_eq_ tasks. In our case, we propose an adaptive method, and so _inline_eq_ can vary at each training step _inline_eq_: _inline_eq_ . This linear form of the loss function is convenient for implementing gradient balancing, as _inline_eq_ very directly and linearly couples to the backpropagated gradient magnitudes from each task. The challenge is then to find the best value for each _inline_eq_ at each training step _inline_eq_ that balances the contribution of each task for optimal model training. To optimize the weights _inline_eq_ for gradient balancing, we propose a simple algorithm that penalizes the network when backpropagated gradients from any task are too large or too small. The correct balance is struck when tasks are training at similar rates ; if task _inline_eq_ is training relatively quickly, then its weight _inline_eq_ should decrease relative to other task weights _inline_eq_ to allow other tasks more influence on training. Our algorithm is similar to batch normalization _cite_ with two main differences: (N) we normalize across tasks instead of across data batches, and (N) we use rate balancing as a desired objective to inform our normalization. We will show that such gradient normalization (hereafter referred to as GradNorm) boosts network performance while significantly curtailing overfitting. Our main contributions to multitask learning are as follows: