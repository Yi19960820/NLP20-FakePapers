For state-of-the-art deep single-label classification models, the correct class is often in the top-_inline_eq_ predictions, leading to a top-_inline_eq_ _inline_eq_ accuracy that is significantly higher than the top-N accuracy. This is also more crucial in fine-grained classification tasks, where the differences between classes are quite subtle. For example, the Stanford Dogs fine-grained dataset on which we report results has a top-N accuracy of N \% and a top-N accuracy of N \%. Exploiting the information provided in the top _inline_eq_ predicted classes can boost the final prediction of a model. In this work, we do not completely trust the model's top-N prediction as it does not solely depend on the visual evidence in the input image, but can depend on other artifacts such as dataset bias or unbalanced training data. Instead, we exploit the discriminative visual evidence used for each of the top-_inline_eq_ predictions for decision refinement. Examples of fine-grained classes present in the literature are breeds of animals _cite_ and birds _cite_, models of aircraft _cite_ and vehicles _cite_ . Since fine-grained classification requires focusing on details, the localization of salient parts is crucial. This has been addressed using supervised approaches that utilize part bounding box annotations _cite_ or have humans in the loop to help reveal discriminative parts _cite_ . Part localization has also been addressed using weakly supervised approaches _cite_, solely relying on image labels during both training and testing. Another class of works attend to a recursively zoomed location _cite_, while other methods use multiple attention mechanisms _cite_ . Some approaches enforce correlations between parts _cite_, while others do not consider this possible source of information _cite_ . In this work, we want to answer the following question: is the evidence upon which the prediction is made reasonable? Evidence is defined to be the grounding, in pixel space, for a specific class conditional probability in the model output. The evidence proposed here is in the form of a saliency map resulting from weak supervision. It is directly obtained using grounding approaches that utilize a network's internal representation and a dataset's image-level annotation. We use evidence grounding as the signal to a module that assesses how much one can trust a Convolutional Neural Network (CNN) prediction over another. We propose {\selectfont Guided Zoom}, an approach that utilizes spatial grounding to refine model predictions in fine-grained classification scenarios. {\selectfont Guided Zoom} zooms in on the evidence used to make a preliminary decision at test time and compares it with the evidence of correct predictions made at training time. As demonstrated in Fig.~ _ref_, we propose not to solely rely on the prediction a conventional CNN produces, but to examine whether or not the evidence used to make the prediction is coherent with training evidence of correctly classified images. This is performed by the Evidence CNN module, which aids the Decision Refinement module to come up with a refined prediction. The desired goal in {\selectfont Guided Zoom} is that the evidence of the refined class prediction is more coherent with the training evidence of that class, than the evidence of any of the other candidate top classes as depicted in Fig.~ _ref_ . Our approach does not require part annotations, thus it is more scalable compared to supervised approaches. Moreover, our approach uses multiple salient regions and therefore does not propagate errors from an incorrect initial saliency localization, while implicitly enforcing part correlations enabling models to make more informed predictions. As the experiments of Wei \etal _cite_ suggest, although only part (s) of an object will be highlighted in the evidence, a more inclusive segmentation map can be extracted from the already trained model at test time. We follow their strategy of adversarial erasing to obtain a rich representation for the Evidence CNN module. We also investigate the complementarity of grounding techniques by comparing their ensemble performance to that of the adversarial erasing strategy. By questioning network evidence, we demonstrate refined accuracy on three fine-grained classification benchmark datasets. The rest of the paper is organized as follows. Section~ _ref_ reviews related works on evidence grounding and fine-grained classification. Section~ _ref_ introduces our method {\selectfont Guided Zoom}, and a variant of it, {\selectfont Ensemble Guided Zoom} . Section~ _ref_ presents our experimental setup and results on three fine-grained datasets of bird species, dog species, and aircraft models.