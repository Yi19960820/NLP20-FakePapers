Deep convolutional neural networks (CNNs) have shown outstanding results on challenging image classification and detection datasets since the seminal work of _cite_ . Off-the-shelf image representations learned by these deep networks are powerful and generic. These generic features have been used to solve numerous visual recognition problems _cite_ . Given the promising performance of these off-the-shelf CNN features, they have become the first choice for solving most computer vision problems _cite_ . Training a deep network from scratch is not a feasible option when solving a classification problem with a small number of labelled training examples. Recent evidence _cite_ suggests that off-the-shelf CNN features have outperformed previous handcrafted features for datasets with a limited amount of training data. These features are domain independent and can be transferred to any specific target task without compromising on performance _cite_ . Network width, depth and optimization parameters along with the network layer from which these features are extracted play a key role in the effectiveness of transfer learning. This paper attempts to provide an answer to the following question: What are the criteria to select an initial deep network (pre-trained on ImageNet) to extract generic features in order to maximize performance and transferability across domains? To answer this question, we hypothesise that a better optimized and a high performing deep network on ImageNet should result in more powerful and generic image representations. One such network is the deep residual network (ResNet) presented in _cite_ . ResNets are easier to train as opposed to other CNN architectures \eg VGGnet _cite_ . For example, a N-layer ResNet which is N times deeper than VGGnet, is still less complex and trains faster. Moreover, a N-layer ResNet contains N billion multiply-add operations whereas a N-layer VGGnet has N billion multiply-add operations (less than N \%) _cite_ . Very deep networks are known to cause overfitting and saturation in accuracy. However, residual learning and the identity mappings (shortcut connections) _cite_ in ResNets have been shown to overcome these problems. This enables ResNets to achieve outstanding results in image detection, localization and segmentation tasks _cite_ . In this paper, we explore the discrimination power of the image representations extracted from pre-trained ResNets. We name these off-the-shelf ResNet features as ResFeats . Fig. _ref_ depicts the evolution of traditional classification pipelines. The main contributions of this paper are listed below: The rest of the paper is organized as follows: We briefly discuss the related work in the next section. In Sec. N, we introduce our proposed approach and explain the feature extraction from ResNets. In Sec. N, we describe the dimensionality reduction and classification approaches. Sec. N reports the experimental results and Sec. N concludes the paper.