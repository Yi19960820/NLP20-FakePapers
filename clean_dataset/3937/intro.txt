With the availability of large-scale training data, the field of visual object recognition has made significant progress in the last several years~ _cite_ . However, collecting and labeling training data are laboriously difficult and costly. For example, in fine-grained classification, expert knowledge is required to discriminate between different categories. For rare categories, such as endangered species, it's an extremely difficult work to collect sufficient and statistically diverse training images. Even worse, the frequencies of observing objects follow a long-tailed distribution~ _cite_, which indicates that the number of such unfrequent objects significantly surpasses that of common objects. Given limited or zero training images, existing visual recognition models (\eg, deep CNN models) struggle to make correct predictions. Zero-Shot Learning (ZSL) ~ _cite_ has emerged as a promising paradigm to alleviate the above problem. Unlike fully supervised classification which requires sufficient labeled training images for each category, ZSL distinguishes between two types of categories: source and target, where the labeled images are only available for the source categories. To facilitate the recognition of novel target categories, ZSL assumes the source and the target categories share a common semantic space to which both the images and class names can be projected. The semantic space can be defined by attributes~ _cite_, wordNvec~ _cite_ or WordNet~ _cite_ . Under this assumption, the recognition of images from novel target categories can be achieved by the nearest neighbor search in the shared space. Depending on whether the unlabeled data of target classes are available for training, existing ZSL methods can be categorized into two schools: inductive ZSL ~ _cite_ and transductive ZSL ~ _cite_ . For the inductive ZSL, only data of the source categories are available during the training phase. For the transductive ZSL methods, both the labeled source data and the unlabeled target data are available for training. The transductive ZSL aims to utilize the information from both the labeled source data and the unlabeled target data to accomplish the ZSL task. During the test phase, most existing inductive and transductive ZSL methods~ _cite_ assume the test images come solely from the target classes. Therefore, the search space for classifying the new test images is restricted to the target classes. We call this experimental settings conventional settings . However, in a more practical situation, the test images come not only from the target but also from the source classes. Hence, both the source and the target classes should be considered. This experimental settings are usually regarded as the generalized ZSL settings~ _cite_, abbreviated to generalized settings in this paper. Existing ZSL methods perform much worse in the generalized settings than in the conventional settings~ _cite_ . One vital factor accounting for the poor performance can be explained as follows. ZSL achieves the recognition of new categories by establishing the connection between the visual embeddings and the semantic embeddings. However, during the phase of bridging the visual and the semantic embeddings, there exists a strong bias~ _cite_ (shown in Figure~ _ref_) . During the training phase of most existing ZSL methods, the visual instances are usually projected to several fixed anchor points specified by the source classes in the semantic embedding space. This leads to a strong bias when these methods are used for testing: given images of novel classes in the target dataset, they tend to categorize them as one of the source classes. To alleviate the mentioned problem above, we propose a novel transductive ZSL method in this paper. The proposed method assumes that both the labeled source and the unlabeled target data are available during the training phase. On the one hand, the labeled source data are used to learn the relationship between visual images and semantic embeddings. On the other hand, the unlabeled data of target classes are used to alleviate the strong bias towards source classes. More specifically, unlike other ZSL methods which always map input images to several fixed anchor points in the embedding space during training, our method allows the mapping from the inputs to other points, which significantly alleviates the strong bias problem. We dub the proposed ZSL method as Quasi-Fully Supervised Learning (QFSL), as it works like the conventional fully supervised classification in which a multi-layer neural network and a classifier are integrated together (shown in Figure~ _ref_) . The architecture of the multi-layer neural network is usually taken from AlexNet~ _cite_, GoogleNet~ _cite_ or other well-known deep networks. In the training phase, our model is trained in an end-to-end manner to recognize the data from both source and target classes even without labeled data for the target classes. This feature brings up a compelling advantage: when the labeled data of target classes are available in the future, it can be directly used to train our model. In the test phase, our trained model can be directly used to recognize new images from both the source and the target classes without any modifications. To sum up, we made the following contributions: N) A transductive learning (QFSL) method is proposed to learn unbiased embeddings for ZSL. To our knowledge, this is the first work to adopt transductive learning method in solving the ZSL problem in generalized settings. N) Experiments reveal that our method significantly outperforms existing ZSL methods, in both generalized and conventional settings.