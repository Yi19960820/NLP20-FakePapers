Generative Adversarial Networks are made by two neural networks competing each other. One, the gene \-rator _inline_eq_, it creates images starting from a latent space z of uniformly distributed random numbers, while the discriminator _inline_eq_ has to judge the ima \-ges x it receives as fake or real. We train _inline_eq_ with the goal to fool _inline_eq_ with fake images, minimizing _inline_eq_ . In order to do that, _inline_eq_ has to learn to produce images that are as much photorealistic as possible. This approach is a valid alternative to maximum likelihood techniques, because its conditions and constraints make feasible to run it as an unsupervised learning approach. By the contrary, training is still challenging and efforts are made to prevent both networks to fail. Several improvements have been introduced since the first GAN model. One of the first techniques was the minibatch discrimination that reduces the chance for the generator to collapse~ _cite_ . Other techniques aim to find a faster convergence, modeling the discriminator _inline_eq_ as an energy function~ _cite_ or introducing new loss definitions~ _cite_ .