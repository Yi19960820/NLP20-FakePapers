vector machine (SVM) and its extensions have been one of the most successful machine learning methods _cite_, and have been adopted in various fields, e.g., computer vision _cite_, signal processing _cite_, natural language processing _cite_ and bioinformatics _cite_ . Despite its popularity, SVM aims to seek the optimal hyperplane with the maximum margin principle, but the generalization error of SVM actually is a function of the ratio of radius and margin _cite_ . Given feature space, the radius is fixed and can be ignored, thus SVM can minimize the generalization error by maximizing the margin. However, for joint learning of feature transformation and classifier, the radius information will be valuable and cannot be ignored. By minimizing the radius-margin ratio, the generalization error of SVM can be optimized for joint learning of feature transformation and classifier. Since the radius-margin error bound is non-convex, relaxation and approximation of radius is generally adopted in the existing models _cite_ . Several approaches have been proposed from the perspective of radius-margin error _cite_, but most ones suffer from the limitations of computational burden and simplified forms of transformation. RMM _cite_ only considers the spread of the data along the direction perpendicular to the classification hyperplane. Radius-margin based SVMs, e.g., MR-SVM _cite_, R-SVM _inline_eq_ _cite_ and RSVM _inline_eq_ _cite_, are based on the constraint that the linear transformation matrix should be diagonal. Another strategy for joint feature transformation and classifier learning is to incorporate metric learning with SVM, where metric learning can be adopted to learn a better linear transformation matrix _cite_ . One simple approach to combine metric learning and SVM is to directly deploy the transformation obtained using metric learning into SVM. This approach, however, usually cannot lead to satisfactory performance improvement _cite_ . Therefore, other approaches have been proposed to integrate metric learning to SVM, e.g., support vector metric learning (SVML) _cite_ and metric learning with SVM (MSVM) _cite_ . But SVML _cite_ was designed for RBF-SVM and ignored the radius information, while MSVM _cite_ is non-convex. In this paper, we propose a novel radius-margin based SVM model for joint learning of feature transformation and SVM classifier, i.e., F-SVM. Compared with the existing radius-margin based SVM methods, we derive novel lower and upper bounds for the relaxation of the radius-margin ratio. Unlike MR-SVM _cite_, R-SVM _inline_eq_ _cite_ and RSVM _inline_eq_ _cite_ which are suggested for joint feature weighting and SVM learning, F-SVM can simultaneously learn feature transformation _inline_eq_ and classifier _inline_eq_ . Compared with the existing metric learning for SVM methods, our F-SVM model considers both the radius and the margin information, and is convex. Then, an alternating minimization algorithm is proposed to solve our F-SVM model, which iterates by updating feature transformation and classifier alternatively. Note that kernel SVM is equivalent to perform linear SVM in the kernel PCA space. We further suggest to conduct linear FSVM in the kernel PCA space for joint learning of nonlinear transformation and classifier. The contribution of this paper is of three-fold: The remainder of the paper is organized as follows: Section N reviews the related work on the radius-margin ratio based bounds and their applications. Section N describes the model and algorithm of the proposed F-SVM method. Section N extends F-SVM to the kernelized version for nonlinear classification. Section N provides the experimental results on the UCI machine learning datasets and the LFW dataset. Finally, we conclude the paper in Section N.