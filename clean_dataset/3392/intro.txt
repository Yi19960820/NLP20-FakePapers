Visual object tracking has received increasing attention over the last decades and has remained a very active research direction. It has a large range of applications in diverse fields like visual surveillance _cite_, human-computer interactions _cite_, and augmented reality _cite_ . Although much progress has been made recently, it has still been commonly recognized as a very challenging task due to numerous factors such as illumination variation, occlusion, and background clutters, to name a few _cite_ . Recently, the Siamese network based trackers _cite_ have drawn much attention in the community. These Siamese trackers formulate the visual object tracking problem as learning a general similarity map by cross-correlation between the feature representations learned for the target template and the search region. To ensure tracking efficiency, the offline learned Siamese similarity function is often fixed during the running time _cite_ . The CFNet tracker _cite_ and DSiam tracker _cite_ update the tracking model via a running average template and a fast transformation module, respectively. The SiamRNN tracker _cite_ introduces the region proposal network _cite_ after the Siamese network and performs joint classification and regression for tracking. The DaSiamRPN tracker _cite_ further introduces a distractor-aware module and improves the discrimination power of the model. Although the above Siamese trackers have obtained outstanding tracking performance, especially for the well-balanced accuracy and speed, even the best performed Siamese trackers, such as SiamPRN, the accuracy still has a notable gap with the state-of-the-arts _cite_ on tracking benchmarks like OTBN _cite_ . We observe that all these trackers have built their network upon architecture similar to AlexNet _cite_ and tried several times to train a Siamese tracker with more sophisticated architecture like ResNet _cite_ yet with no performance gain. Inspired by this observation, we perform an analysis of existing Siamese trackers and find the core reason comes from the destroy of the strict translation invariance. Since the target may appear at any position in the search region, the learned feature representation for the target template should stay spatial invariant, and we further theoretically find that, among modern deep architectures, only the zero-padding variant of AlexNet satisfies this spatial invariance restriction. To overcome this restriction and drive the Siamese tracker with more powerful deep architectures, through extensive experimental validations, we introduce a simple yet effective sampling strategy to break the spatial invariance restriction of the Siamese tracker. We successfully train a SiamRPN~ _cite_ based tracker using the ResNet as a backbone network and obtain significant performance improvements. Benefiting from the ResNet architecture, we propose a layer-wise feature aggravation structure for the cross-correlation operation, which helps the tracker to predict the similarity map from features learned at multiple levels. By analyzing the Siamese network structure for cross-correlations, we find that its two network branches are highly imbalanced in terms of parameter number; therefore we further propose a depth-wise separable correlation structure which not only greatly reduces the parameter number in the target template branch, but also stabilizes the training procedure of the whole model. In addition, an interesting phenomena is observed that objects in the same categories have high response on the same channels while responses of the rest channels are suppressed. The orthogonal property may also improve the tracking performance. To summarize, the main contributions of this work are listed below in fourfold: Based on the above theoretical analysis and technical contributions, we have developed a highly effective and efficient visual tracking model which establishs a new state-of-the-art in terms of tracking accuracy, while running efficiently at N FPS. The proposed tracker, referred as, consistently obtains the best tracking results on five of the largest tracking benchmarks, including OTBN _cite_, VOTN _cite_, UAVN _cite_, LaSOT _cite_, and TrackingNet _cite_ . Furthermore, we propose a fast variant of our tracker using MobileNet _cite_ backbone that maintains competitive performance, while running at N FPS. To facilitate further studies on the visual tracking direction, we will release the source code and trained models of the SiamRPN + + tracker.