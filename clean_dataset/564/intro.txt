Deep convolutional neural networks have been consistently showing outstanding performance in a variety of applications, however, this performance comes at a high computational cost compared to past methods. The millions of parameters of a typical neural network require immense computational power and memory for storage. Thus, model compression is required to reduce the number of parameters of the network. Our aim in this paper is to develop a method that can optimize trained neural networks for reduction in computational power and memory usage while providing competitive accuracy. Generally, filter pruning techniques have been used for network compression. The aim of such approaches is to develop pruning policies, which can satisfy the target constraints on FLOPs and memory. Layer wise search-based heuristic methods~ _cite_, reinforcement learning~ _cite_, and genetic and evolutionary algorithms~ _cite_ have been used to define the pruning policy. A greedy selection method based on a heuristic metric has been proposed in _cite_ to prune multiple filters of the network together. Another approach towards network compression is using kernel decomposition over each filter in the network. Convolutional and fully connected layers can be represented as matrix multiplications, and kernel decomposition can be applied to these matrices~ _cite_ . Kernel decomposition with singular value decomposition (SVD) automatically assigns importance (the singular values) to the decomposed kernels. This automatic sorting makes filter pruning easier, as the decomposed kernels with the lower parameters are the first to be pruned. Simply put, low-rank approximation of a layer decomposes it into two matrix multiplications for network compression as shown in Fig.~ _ref_ . With kernel decomposition schemes, the problem boils down to the choice of the optimal compression ratio for each layer of the network. We need to find the right rank configuration (i.e. compression ratios) for the whole network that satisfies constraints on speed, memory and accuracy. This is different from past methods which use a solver to minimize the approximation error at the fixed rank~ _cite_ and a training technique to better compensate for accuracy loss~ _cite_ . An iterative search-based approach was adopted in _cite_ to obtain the right rank configuration. In _cite_, the authors define an PCA energy-based accuracy feature and use it to select a layer to be compressed in every iteration. The final rank of each layer is the result of iterative layer-wise network compression. Reinforcement learning was used in _cite_ to find the rank of each layer independently. Unlike _cite_ and _cite_, which optimize each layer separately, _cite_ searched for the right rank configuration for the whole network. Although, _cite_ shows better performance compared to _cite_ and _cite_, it still takes significant amount of time due to its iterative search for the right rank configuration. In this paper, we propose Efficient Neural network Compression (ENC) to obtain the optimal rank configuration for kernel decomposition. The proposed method is non-iterative; therefore, it performs compression much faster compared to numerous recent methods. Specifically, we propose three methods: ENC-Map, ENC-Model and ENC-Inf. ENC-Map uses a mapping function to obtain the right rank configuration from the given constraint on complexity. ENC-Model uses a metric representative of the accuracy of the whole network to find the right rank configuration. ENC-Inf uses both the accuracy model and inference on a validation dataset to arrive at the right rank configuration. The code for our method is available online. The rest of the paper is structured as follows. Section N and N describe the accuracy metrics. Section N discusses ENC-Map. Search-based methods, ENC-Model and ENC-Inf, are given in Section N. Experiments are discussed in Section N and Section N concludes the paper.