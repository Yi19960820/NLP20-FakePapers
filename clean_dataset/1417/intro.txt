Neural networks are very effective models for a variety of computer vision tasks. In general, during training these networks are presented with examples from all tasks they are expected to perform. In a lifelong learning setting, however, learning is considered as a sequence of tasks to be learned~ _cite_, which is more similar to how biological systems learn in the real world. In this case networks are presented with of tasks, and at any given moment the network has access to training data from only group. The main problem which systems face in such settings is: while adapting network weights to new tasks, the network forgets the previously learned ones~ _cite_ . There are roughly two main approaches to lifelong learning (which has seen increased interest in recent years) . The first group of methods stores a small subset of training data from previously learned tasks. These stored exemplars are then used during training of new tasks to avoid forgetting the previous ones~ _cite_ . The second type of algorithm instead avoids storing any training data from previously learned tasks. A number of algorithms in this class are based on Elastic Weight Consolidation (EWC) ~ _cite_, which includes a regularization term that forces parameters of the network to remain close to the parameters of the network trained for the previous tasks. In a similar vein, Learning Without Forgetting (LWF) ~ _cite_ regularizes rather than weights. Aljundi et al.~ _cite_ learns a representation for each task and use a set of gating autoencoders to decide which expert to use at testing time. EWC is an elegant approach to selective regularization of network parameters when switching tasks. It uses the Fisher Information Matrix (FIM) to identify directions in feature space critical to performing already learned tasks (and as a consequence also those directions in which the parameters may move freely without forgetting learned tasks) . However, EWC has the drawback that it assumes the Fisher Information Matrix to be diagonal--a condition that is almost never true. In this paper we specifically address this diagonal assumption made by the EWC algorithm. If the FIM is not diagonal, EWC can fail to prevent the network from straying away from ``good parameter space'' (see Fig.~ _ref_, left) . Our method is based on the parameter space of the neural network in such a way that the output of the forward pass is unchanged, but the FIM computed from gradients during the backward pass is approximately diagonal (see Fig.~ _ref_, right) . The result is that EWC in this rotated parameter space is significantly more effective at preventing catastrophic forgetting in sequential task learning problems. An extensive experimental evaluation on a variety of sequential learning tasks shows that our approach significantly outperforms standard elastic weight consolidation. The rest of the paper is organized as follows. In the next section we review the EWC learning algorithm and in Section~ _ref_ we describe the approach to rotating parameter space in order to better satisfy the diagonal requirement of EWC. We give an extensive experimental evaluation of the proposed approach in Section~ _ref_ and conclude with a discussion of our contribution in Section~ _ref_ .