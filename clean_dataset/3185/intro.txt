Optical flow is valuable for image sequence analysis due to its ability to encode motion. Significant progress has been made on the estimation of optical flow over the past few years. Classical approaches are typically based on variational models and solved as an energy minimization process _cite_ . They remain the top performers on a number of evaluation benchmarks. Most, however, are too slow to be used in real time applications. By contrast, convolutional neural network (CNN) based methods formulate optical flow estimation as a learning task and can reduce inference time to fractions of a second. However, despite their increased accuracy, most flow estimation methods, including classical and CNN approaches, still having difficulty with multi-scale estimation, gridding artifacts, real-time computation, and/or occlusion reasoning. We focus on CNN based approaches in this work due to their efficiency. FlowNet _cite_ was the first work to directly learn optical flow given an image pair using CNNs. To deal with multi-scale, FlowNetN _cite_ proposed separate streams to encode large and small scale displacement which are then fused using a refinement network. Although FlowNetN achieves good performance, the memory footprint of the model is high due to the five separate networks similar to FlowNet. Another approach, SPyNet _cite_, instead adopts spatial pyramids to output optical flow at multiple resolutions. Its network is _inline_eq_ smaller than that of FlowNet and results in increased performance. These are all supervised methods, however, that require ground truth optical flow during training which is only available for synthetic data. The transferability from synthetic to real domains remains an open question. Unsupervised _cite_ or semi-supervised _cite_ approaches which do not require ground truth flow have thus been developed. These methods usually guide the learning using an image reconstruction loss based on a brightness constancy assumption. Although this allows for unlimited training data, the performance is limited by such a loss function. Indeed, these unsupervised approaches tend to lag far behind their supervised counterparts on standard benchmarks. One reason for this is that the loss is based on photo-consistency error which is only meaningful when there is no occlusion. Without explicit occlusion reasoning, the back-propagated gradients are incorrect and degrade the training. Two concurrent works _cite_ consider occlusion explicitly for estimating optical flow using CNNs. The intuition is based on a forward-backward consistency assumption. That is, for non-occluded pixels, the forward flow should be the inverse of the backward flow between image pairs. During training, gradients are calculated from non-occluded regions only. Our work is most similar to _cite_ in terms of unsupervised learning and occlusion reasoning, but differs in several ways: (N) We adopt dilated operations for the last few convolutional groups, which leads to high resolution feature maps throughout the network and improved multi-scale handling. (N) We incorporate dense connections in the network instead of sparse skip connections. This strategy helps to capture thin structures and small object displacements. And, (N) our network completely avoid upsampling via deconvolution and thus greatly reduces gridding artifacts. Our proposed framework outperforms state-of-the-art unsupervised approaches on standard benchmarks, and is shown to generalize well to action recognition.