\par Understanding the unspoken words from facial and body cues is a fundamental human trait, and such aptitude is vital in our daily communications and social interactions. In research communities such as human computer interaction (HCI), neuroscience and computer vision, scientists have conducted extensive research to understand human emotions. Such studies would allow creating computers that can understand human emotions as well as ourselves, and lead to seamless interactions between human and computers. \par Among many inputs that can be used to derive emotions, facial expression is by far the most popular. One of the pioneer works by Paul Ekman~ _cite_ identified N emotions that are universal across different cultures. Later, Ekman~ _cite_ developed the Facial Action Coding System (FACS), which became the standard scheme for facial expression research. Facial expression analysis can thus be conducted by analyzing facial action units for each of the facial parts (eyes, nose, mouth corners, etc.), and map them into FACS codes~ _cite_ . Unfortunately, FACS coding requires professionally trained coders to annotate, and there are very few existing data sets that are available for learning FACS based facial expressions, in particular for unconstrained real-world images. \par With the latest advances in machine learning, it is more and more popular to recognize facial expressions directly from input images. Such appearance-based approaches have the advantage that the ground truth labels may be abundantly obtained through crowd-sourcing platforms~ _cite_ . The cost of tagging a holistic facial emotion is often on the order of N-N US cents, which is orders of magnitude cheaper than FACS coding. On the other hand, crowd-sourced labels are usually much noisier than FACS codes annotated by specially trained coders. This can be attributed to two main reasons. First, emotions are very subjective, and it is very common that two people have diametrically different opinions on the same face image. Second, the workers in crowd-sourcing platforms are paid very low, and their incentive is more on getting more work done rather than ensuring the tagging quality. Consequently, crowd-sourced labels on emotions exhibit only _inline_eq_ accuracy, as reported for the original FER data set~ _cite_ . \par In this paper, we adopt the latest deep convolutional neural networks (DCNN) architecture, and evaluate the effectiveness of four different schemes to train emotion recognition on crowd-sourced labels. In order to overcome the noisy label issue, we asked N crowd taggers to re-label each image in the FER data set, resulting in a new data set named FER + _cite_ . Then, we change the cost function of the DCNN based on different schemes using the distribution of tags: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We compare the performance of the trained classifiers and found the last two schemes to be the most effective to train emotion recognition classifiers based on noisy labels. \par The rest of the paper is organized as follows. Related works are discussed in Section~ _ref_ and a description of the FER + data set is introduced in Section~ _ref_ . Then, the four schemes for DCNN training are presented in Section~ _ref_ while experimental results and conclusions are given in Section~ _ref_ and _ref_, respectively.