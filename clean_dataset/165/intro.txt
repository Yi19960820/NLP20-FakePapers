Automatically generating images according to natural language descriptions is a fundamental problem in many applications, such as art generation and computer-aided design. It also drives research progress in multimodal learning and inference across vision and language, which is one of the most active research areas in recent years~ _cite_ Most recently proposed text-to-image synthesis methods are based on Generative Adversarial Networks (GANs) ~ _cite_ . A commonly used approach is to encode the whole text description into a global sentence vector as the condition for GAN-based image generation~ _cite_ . Although impressive results have been presented, conditioning GAN only on the global sentence vector lacks important fine-grained information at the word level, and prevents the generation of high quality images. This problem becomes even more severe when generating complex scenes such as those in the COCO dataset~ _cite_ . To address this issue, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. The overall architecture of the AttnGAN is illustrated in Figure~ _ref_ . The model consists of two novel components. The first component is an attentional generative network, in which an attention mechanism is developed for the generator to draw different sub-regions of the image by focusing on words that are most relevant to the sub-region being drawn (see Figure~ _ref_) . More specifically, besides encoding the natural language description into a global sentence vector, each word in the sentence is also encoded into a word vector. The generative network utilizes the global sentence vector to generate a low-resolution image in the first stage. In the following stages, it uses the image vector in each sub-region to query word vectors by using an attention layer to form a word-context vector. It then combines the regional image vector and the corresponding word-context vector to form a multimodal context vector, based on which the model generates new image features in the surrounding sub-regions. This effectively yields a higher resolution picture with more details at each stage. The other component in the AttnGAN is a Deep Attentional Multimodal Similarity Model (DAMSM) . With an attention mechanism, the DAMSM is able to compute the similarity between the generated image and the sentence using both the global sentence level information and the fine-grained word level information. Thus, the DAMSM provides an additional fine-grained image-text matching loss for training the generator. The contribution of our method is threefold. (i) An Attentional Generative Adversarial Network is proposed for synthesizing images from text descriptions. Specifically, two novel components are proposed in the AttnGAN, including the attentional generative network and the DAMSM. (ii) Comprehensive study is carried out to empirically evaluate the proposed AttnGAN. Experimental results show that the AttnGAN significantly outperforms previous state-of-the-art GAN models. (iii) A detailed analysis is performed through visualizing the attention layers of the AttnGAN. For the first time, it is demonstrated that the layered conditional GAN is able to automatically attend to relevant words to form the condition for image generation.