Estimating the ND pose of objects is an important capability for enabling robots' interaction with real environments and objects as well as augmented reality applications. While several approaches to this problem assume RGB-D data, most mobile and wearable cameras are not paired with a depth sensor, prompting recent research focus on the RGB domain. Furthermore, even though several methods have shown promising results on ND object pose estimation with real RGB images, they either require accurate ND annotations or ND object models with realistic textures in the training stage. Currently available datasets are not large enough to capture real world diversity, limiting the potential of these methods in generalizing to a variety of applications. In addition, capturing real RGB data and manual pose annotation is an arduous procedure. The problem of object pose estimation is an inherently ND problem; it is the shape of the object which gives away its pose regardless of its appearance. Instead of attempting to learn an intrinsic decomposition of images~ _cite_, we focus on finding the association of parts of objects depicted in RGB images with their counterparts in ND depth images. Ideally, we would like to learn this association in order to establish correspondences between a query RGB image and a rendered depth image from a CAD model, without requiring any existing ND annotations. This, however, requires us to address the problem of the large appearance gap between these two modalities. In this paper, we propose a new framework for estimating the ND pose of objects in RGB images, using only ND textureless CAD models of objects instances. The easily available CAD models can generate a large number of synthetically rendered depth images from multiple viewpoints. In order to address the aforementioned problems, we define a {\em quadruplet} convolutional neural network to jointly learn keypoints and their associated descriptors for robust matching between different modalities and changes in viewpoint. The general idea is to learn the keypoint locations using a pair of rendered depth images from a CAD model from two different poses, followed by learning how to match keypoints across modalities using an aligned RGB-D image pair. Figure~ _ref_ outlines our training constraints. At test time, given a query RGB image, we extract keypoints and their representations and match them with a database of keypoints and their associated descriptors extracted from rendered depth images. These are used to establish ND-ND correspondences, followed by a RANSAC and PnP algorithm for pose estimation. To summarize, the key contributions of this work are the following: