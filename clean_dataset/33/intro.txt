Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual recognition tasks~ _cite_ . The most accurate CNNs usually have hundreds of layers and thousands of channels~ _cite_, thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. Note that many existing works~ _cite_ focus on pruning, compressing, or low-bit representing a ``basic" network architecture. Here we aim to explore a highly efficient basic architecture specially designed for our desired computing ranges. We notice that state-of-the-art basic architectures such as ~ _cite_ and ~ _cite_ become less efficient in extremely small networks because of the costly dense _inline_eq_ convolutions. We propose using to reduce computation complexity of _inline_eq_ convolutions. To overcome the side effects brought by group convolutions, we come up with a novel operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called . Compared with popular structures like ~ _cite_, for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks. We evaluate our models on the challenging ImageNet classification~ _cite_ and MS COCO object detection~ _cite_ tasks. A series of controlled experiments shows the effectiveness of our design principles and the better performance over other structures. Compared with the state-of-the-art architecture ~ _cite_, ShuffleNet achieves superior performance by a significant margin, e.g. absolute N \% lower ImageNet top-N error at level of N MFLOPs. We also examine the speedup on real hardware, i.e. an off-the-shelf ARM-based computing core. The ShuffleNet model achieves _inline_eq_ N _inline_eq_ speedup (theoretical speedup is N _inline_eq_) over AlexNet~ _cite_ while maintaining comparable accuracy.