Generative Adversarial Networks~ (GANs) ~ are a powerful framework to learn models capable of generating natural images. GANs learn these generative models by setting up an adversarial game between two learning machines. On the one hand, a {generator} plays to transform {noise vectors} into {fake samples}, which resemble {real samples} drawn from a distribution of natural images. On the other hand, a {discriminator} plays to distinguish between real and fake samples. During training, the generator and the discriminator functions are optimized in turns. First, the discriminator learns to assign high scores to real samples, and low scores to fake samples. Then, the generator learns to increase the scores of fake samples, so as to ``fool'' the discriminator. After proper training, the generator is able to produce realistic natural images from noise vectors. Recently, GANs have been used to produce high-quality images resembling handwritten digits, human faces, and house interiors~ . Furthermore, GANs exhibit three strong signs of generalization. First, the generator translates~ into~ . In other words, a linear interpolation in the noise space will generate a smooth interpolation of visually-appealing images. Second, the generator allows . Similarly to word embeddings, linear arithmetic indicates that the generator organizes the noise space to disentangle the nonlinear factors of variation of natural images into linear statistics. Third, the generator is able to to synthesize new images that resemble those of the data distribution. This allows for applications such as image in-painting and super-resolution~ . Despite their success, training and evaluating GANs is notoriously difficult. The adversarial optimization problem implemented by GANs is sensitive to random initialization, architectural choices, and hyper-parameter settings. In many cases, a fair amount of human care is necessary to find the correct configuration to train a GAN in a particular dataset. It is common to observe generators with similar architectures and hyper-parameters to exhibit dramatically different behaviors. Even when properly trained, the resulting generator may synthesize samples that resemble only a few localized regions (or modes) of the data distribution~ . While several advances have been made to stabilize the training of GANs~, this task remains more art than science. The difficulty of training GANs is aggravated by the challenges in their evaluation: since evaluating the likelihood of a GAN with respect to the data is an intractable problem, the current gold standard to evaluate the quality of GANs is to eyeball the samples produced by the generator. This qualitative evaluation gives little insight on the coverage of the generator, making the mode dropping issue hard to measure. The evaluation of discriminators is also difficult, since their visual features do not always transfer well to supervised tasks~ . Finally, the application of GANs to non-image data has been relatively limited. To model natural images with GANs, the generator and discriminator are commonly parametrized as deep Convolutional Networks~ (convnets) ~ . Therefore, it is reasonable to hypothesize that the reasons for the success of GANs in modeling natural images come from two complementary sources: This work attempts to disentangle the factors of success (AN) and (AN) in GAN models. Specifically, we propose and study one algorithm that relies on (AN) and avoids (AN), but still obtains competitive results when compared to a GAN. We investigate the importance of the inductive bias of convnets by removing the adversarial training protocol of GANs (Section~ _ref_) . Our approach, called ~ (), maps one noise vector to each of the images in our dataset by minimizing a simple reconstruction loss. Since we are predicting~, ~ borrows inspiration from recent methods to predict~ ~ . Alternatively, one can understand~ as an auto-encoder where the latent representation is not produced by a parametric encoder, but learned freely in a non-parametric manner. In contrast to GANs, we track the correspondence between each learned noise vector and the image that it represents. Hence, the goal of is to find a meaningful organization of the noise vectors, such that they can be mapped to their target images. To turn into a generative model, we observe that it suffices to learn a simple probability distribution on the learned noise vectors. We study the efficacy of to compress and decompress a dataset of images, generate new samples, perform linear interpolations and extrapolations in the noise space, and perform linear arithmetic. Our experiments provide quantitative and qualitative comparisons to Principal Component Analysis (PCA), Variational Autoencoders (VAE) and GANs. Our results show that on many image datasets, in particular CelebA, MNIST and SVHN, the celebrated properties of GAN generations can be reproduced without the GAN training protocol. On the other hand, our qualitative results on the LSUN bedrooms are worse than the results of GANs; we hypothesize (and show evidence) that this is a capacity issue. It has been observed that GANs are prone to mode collapse, completely forgetting large parts of the training dataset. In the literature this is often described as a problem with the GAN training procedure. Our experiments suggest that this is more of a feature than a bug, as it allows relatively small models to generate realistic images by intelligently choosing which part of the data to ignore. We quantitatively measure the significance of this issue with a reconstruction criterion.