Success in visual recognition mainly depends on the feature representing the input data. Scene recognition in particular has benefited from recent developments in the field. Most notably, massive image datasets (ImageNet and Places _cite_) provide the necessary amount of data to train complex convolutional neural networks (CNNs) _cite_, with millions of parameters, without falling into overfitting. The features extracted from models pretrained in those datasets have shown to be generic and powerful enough to obtain state-of-the-art performance in smaller datasets (e.g. MIT indoor N _cite_, SUNN _cite_), just using an SVM _cite_ or fine-tuning, outperforming earlier handcrafted paradigms (e.g. SIFT, HOG, bag-of-words) . Low cost depth sensors can capture depth information in addition to RGB data. Depth can provide valuable information to model object boundaries and understand the global layout of objects in the scene. Thus, RGB-D models can improve recognition over mere RGB models. However, in contrast to RGB data, which can be crowdsourced by crawling the web, RGB-D data needs to be captured with a specialized and relatively complex setup _cite_ . For this reason, RGB-D datasets are orders of magnitude smaller than the largest RGB datasets, also with much fewer categories. This prevents from training deep CNNs properly, and handcrafted features are still a better choice for this modality. However, the recent SUN RGB-D dataset _cite_ is significantly larger than previous RGB-D scene datasets (e.g. NYUN _cite_) . While still not large enough to train from scratch deep CNNs of comparable size to RGB counterparts (N RGB-D images compared with N million RGB images in Places), at least provides enough data for fine tuning deep models (e.g. AlexNet-CNN on Places) without significant overfitting. This approach typically exploit the HHA encoding for depth data _cite_, since it also a three channel representation (horizontal disparity, height above ground, and angle with the direction of gravity, see Figure~ _ref_ top) . Fine tuning is typically used when the target dataset has limited data, but there is another large dataset that covers a similar domain which can be exploited first to train a deep model. Thus, transferring RGB features and fine tuning with depth (HHA) data is the common practice to learn deep representations for depth data _cite_ . However, although HHA images resemble RGB images and shapes and objects can be identified, is it really reasonable reusing RGB features in this inter-modal scenario? In this paper we will focus on the low-level differences between RGB and HHA data, and show that a large number of low-level filters are either useless or ignored during fine tuning the network from RGB to HHA. Figure~ _ref_ (middle) shows the average activation ratio (i.e. how often the activation is non-zero) of the N filters in the layer convN of Places-CNN for different input data (sorted in descending order) . When a network is properly designed and trained, it tends to show a balanced activation rate curve (e.g. convN activations extracted from the Places validation set, where the curve is almost a constant) meaning that most of the filters are contributing almost equally to build discriminative representations. When transferred to other RGB scene datasets, the curve is still very similar, showing that the majority of the convN filters are still useful for N scenes _cite_ and MIT Indoor N. However, the curve for HHA shows a completely different behavior, where only a subset of the filters are relevant, while a large number are rarely activated, because they are not useful for HHA (see Figure~ _ref_ bottom) . Edges and smooth gradients are common, while, Gabor-like patterns and high frequency patterns are seldom found in HHA data. Thus, we observe that filters at the very bottom layers are crucial. However, conventional full fine tuning from RGB CNNs can hardly reach them (i.e. vanishing gradient problem), we explore other ways to make better use of the limited data while focusing on bottom layers. In particular, we compare the strategies of fine tuning only top and only bottom layers, and propose a weakly supervised strategy to learn filters directly from the data. In addition, we combine the pretrained RGB and depth networks into a new network, fine tuned with RGB-D image pairs. We show experimentally that these features lead to state-of-the-art performance with depth and RGB-D, and provide some insights and evidences. Code is available at _url_