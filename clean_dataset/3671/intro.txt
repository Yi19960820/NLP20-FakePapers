In digital art, the creation process often starts with a black and white outline, which is then colored and shaded in, resulting in a final product. While drawing the initial lines requires an artist's creativity and imagination, shading (and to an extent, colorization) often follows the same patterns. Shadows, highlights, and gradients can be inferred from the structure of the line art. Automating this process has a multitude of practical applications: the production time of asset-heavy creations such as computer games can be cut drastically. Furthermore, the vast amounts of comics and manga available only in black and white can be converted into colorized versions. Generative adversarial networks _cite_ has been shown to produce high quality images, even when there are multiple correct outputs for a single input, such as when coloring an arbitrary piece of clothing. However, training directly from outlines to colored images leads to misshapen results, often with similar colors placed in unrelated locations. In this paper, we present an approach to generating colored and shaded artwork from outlines, using two distinct convolutional networks in tandem. We break the problem down into two easier ones, solved separately. First, we infer a general color scheme from an outline, in the color prediction network. Next, the shading network takes this color scheme, along with an outline, and produces a final image. By structuring our networks in a tandem setup, the shading network's job is simplified to inserting already-known colors in the correct places. The color prediction network handles any ambiguities in assigning colors. To account for potential error in color prediction, we randomly remove information in the color scheme during training, forcing the shading network to generalize when given imperfect inputs.