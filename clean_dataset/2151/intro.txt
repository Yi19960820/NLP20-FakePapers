Convolutional Neural Networks (ConvNets) _cite_ have achieved excellent results on visual classification tasks like handwritten digits _cite_, toys _cite_, traffic signs _cite_, and recently N-category ImageNet classification _cite_ . ConvNets' success comes from their ability to learn complex patterns by building increasingly abstract representations layer by layer, much like other deep neural networks. However, ConvNets differ in that they exploit the two dimensional structure of images where objects and patterns appear at arbitrary locations. ConvNets apply local filters at every position in the image, allowing the network to detect and learn patterns regardless of their location. In reality, the world has a three dimensional structure, and objects at different distances will appear in an image at different scales as well as locations. ConvNets do not have a mechanism to take advantage of scale explicitly, so to detect a single pattern at multiple scales, they must learn to separate filters for each scale. Unfortunately this has several major shortcomings. Capturing patterns at multiple scales uses up resources that could be used to learn a wider variety of feature detectors. This requires an increase in the number of feature detectors, which means that the network is harder to train, takes longer to train, and is more likely to overfit. To learn multiple scales and prevent overfitting, you would need a lot of training data, and even then the network will only respond to the scales seen during training. Also, detectors that capture the same pattern but at different scales are learned independently without sharing the training samples. Finally, having multiple filters of a single pattern at different scales burdens the next layer by increasing the number of configurations that indicate the presence or absence of that pattern. In this paper, we present convolutional networks (SI-ConvNets), which applies filters at multiple scales in each layer so a single filter can detect and learn patterns at multiple scales. We max-pool responses over scales to obtain representations that are locally scale invariant yet have the same dimensionality as a traditional ConvNet layer output. The proposed architecture differs from other multi-scale approaches explored with ConvNets since scale-invariance is built in at the layer level rather than at the network level. We also achieve locally scale-invariant representation and we do not require any increase in the number of parameters to be learned. We show in our experiments that by sharing information from multiple scales, the proposed model can achieve better classification performance than ConvNets while simultaneously requiring less training data. We evaluate the proposed model on a variation of the MNIST dataset where digits appear at multiple scales, and demonstrate that the SI-ConvNets are more robust to scale variations in training data and unfamiliar scales in test data than ConvNets. Our model is complementary to other ConvNet architectures and it can easily be incorporated into existing variants, and we will make the source code available online for the research community.