The CNN models are found to be successful at various computer vision tasks such as image classification _cite_, object detection _cite_, image segmentation _cite_, and face recognition _cite_, where large-scale datasets _cite_ are available. Nevertheless, the performance of CNN models significantly reduces, when training data is limited or the domain of the training set is far from the test set. Today, the most successful and practical solution to address lack of annotated large dataset is training the networks on large-scale annotated datasets like ImageNet _cite_ and Places _cite_, then finetuning these pre-trained networks for specific problems. Thanks to community, the pre-trained models of well-known architectures like AlexNet _cite_, VGG-N _cite_, GoogLeNet _cite_, and ResNet _cite_ can be found available online. However, when some architectural changes are needed, these pre-trained networks cannot be used. For such cases, it is necessary to train models on large datasets, then, finetune for the particular problem. Unfortunately, while getting nearly human performances on a lot of applications with these models, training these networks on large datasets is still a significant problem and a very time consuming process. \par With recent advances in deep learning such as Residual Learning _cite_, successful networks have become more and more deep, and training these models become harder in terms of complexity and time. To find a solution for this problem, inspired by What makes a good detector? _cite_, we have investigated What makes a good CNN filter? Two successful CNN models, AlexNet _cite_ and VGG-N _cite_, are analyzed from a statistical perspective, and we realized that these models show similar patterns and redundancies. In addition to our findings, in _cite_, authors show that N \% of weights of neural networks could be predicted without any reduction in accuracy. This leads us to the idea that we can use these redundancies and patterns for learning better representations quickly by transfer learning. Similar to the methods used in _cite_, we introduce a regularization term for transferring the statistical information to improve the learning scheme. First, the statistical distribution of convolution filters from a well-trained network is learned with a Gaussian Mixture Model. Next, the newly trained model is encouraged to show similar statistics with source models using the regularization term. Extensive experiments on the CIFARN _cite_, PlacesN _cite_, and CMPlaces _cite_ datasets show that the proposed approach is generalizable and the networks can quickly learn a representation with statistical regularization, which could efficiently be transferred to another task and cross-domains. The overview of our proposed method can be seen in Figure _ref_ . The rest of the paper is organized as follows: in Section _ref_ related works are summarized, in Section _ref_ our detailed statistical analysis is reviewed and the regularization term is introduced, in Section _ref_ experimental results are presented and discussed, and finally in Section _ref_ the paper is concluded.