\cutsectiondown Numerous graphics algorithms have been established to synthesize photorealistic images from ND models and environmental variables (lighting and viewpoints), commonly known as rendering. At the same time, recent advances in vision algorithms enable computers to gain some form of understanding of objects contained in images, such as classification~ _cite_, detection~ _cite_, segmentation~ _cite_, and caption generation~ _cite_, to name a few. These approaches typically aim to deduce abstract representations from raw image pixels. However, it has been a long-standing problem for both graphics and vision to automatically synthesize novel images by applying intrinsic transformations (e.g., ND rotation and deformation) to the subject of an input image. From an artificial intelligence perspective, this can be viewed as answering questions about object appearance when the view angle or illumination is changed, or some action is taken. These synthesized images may then be perceived by humans in photo editing~ _cite_, or evaluated by other machine vision systems, such as the game playing agent with vision-based reinforcement learning~ _cite_ . In this paper, we consider the problem of predicting transformed appearances of an object when it is rotated in ND from a single image. In general, this is an ill-posed problem due to the loss of information inherent in projecting a ND object into the image space. Classic geometry-based approaches either recover a ND object model from multiple related images, i.e., multi-view stereo and structure-from-motion, or register a single image of a known object category to its prior ND model, e.g., faces~ _cite_ . The resulting mesh can be used to re-render the scene from novel viewpoints. However, having ND meshes as intermediate representations, these methods are N) limited to particular object categories, N) vulnerable to image alignment mistakes and N) easy to generate artifacts during unseen texture synthesis. To overcome these limitations, we propose a learning-based approach without explicit ND model recovery. Having observed rotations of similar ND objects (e.g., faces, chairs, household objects), the trained model can both N) better infer the true pose, shape and texture of the object, and N) make plausible assumptions about potentially ambiguous aspects of appearance in novel viewpoints. Thus, the learning algorithm relies on mappings between Euclidean image space and underlying nonlinear manifold. In particular, ND view synthesis can be cast as pose manifold traversal where a desired rotation can be decomposed into a sequence of small steps. A major challenge arises due to the long-term dependency among multiple rotation steps; the key identifying information (e.g., shape, texture) from the original input must be remembered along the entire trajectory. Furthermore, the local rotation at each step must generate the correct result on the data manifold, or subsequent steps will also fail. Closely related to the image generation task considered in this paper is the problem of ND invariant recognition, which involves comparing object images from different viewpoints or poses with dramatic changes of appearance. Shepard and Metzler in their mental rotation experiments~ _cite_ found that the time taken for humans to match ND objects from two different views increased proportionally with the angular rotational difference between them. It was as if the humans were rotating their mental images at a steady rate. Inspired by this mental rotation phenomenon, we propose a recurrent convolutional encoder-decoder network with action units to model the process of pose manifold traversal. The network consists of four components: a deep convolutional encoder~ _cite_, shared identity units, recurrent pose units with rotation action inputs, and a deep convolutional decoder~ _cite_ . Rather than training the network to model a specific rotation sequence, we provide control signals at each time step instructing the model how to move locally along the pose manifold. The rotation sequences can be of varying length. To improve the ease of training, we employed curriculum learning, similar to that used in other sequence prediction problems~ _cite_ . Intuitively, the model should learn how to make one-step _inline_eq_ rotation before learning how to make a series of such rotations. The main contributions of this work are summarized as follows. First, a novel recurrent convolutional encoder-decoder network is developed for learning to apply out-of-plane rotations to human faces and ND chair models. Second, the learned model can generate realistic rotation trajectories with a control signal supplied at each step by the user. Third, despite only being trained to synthesize images, our model learns discriminative view-invariant features without using class labels. This weakly-supervised disentangling is especially notable with longer-term prediction.