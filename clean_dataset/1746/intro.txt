DNNs trained on large-scale datasets have achieved impressive results on many classification problems. Generally, accurate labels are necessary to effectively train DNNs. However, many datasets are constructed by crawling images and labels from websites and often contain incorrect noisy labels (\eg, YFCCNM~ _cite_, ClothingNM~ _cite_) . This study addresses the following question: how can we effectively train DNNs on noisy labeled datasets without manually cleaning the data? The prominent issue in training DNNs on noisy labeled datasets is that DNNs can learn or memorize, any training dataset, and this implies that DNNs are subject to total overfitting on noisy data. To address this problem, commonly used regularization techniques including dropout and early stopping are helpful. However, these methods do not guarantee optimization because they prevent the networks from reducing the training loss. Another method involves using prior knowledge, such as the confusion matrix between clean and noisy labels, which typically cannot be used in real settings. Consequently, we need a new framework of optimization. In this study, we propose an optimization framework for learning on a noisy labeled dataset. We propose optimizing the labels themselves as opposed to treating the noisy labels as fixed. The joint optimization of network parameters and the noisy labels corrects inaccurate labels and simultaneously improves the performance of the classifier. shows the concept of our proposal. The main contributions are as follows.