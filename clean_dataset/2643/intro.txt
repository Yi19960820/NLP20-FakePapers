With the recent rapid development of human-centric AI, human-centric video analysis _cite_ has also begun to draw much attention from the computer vision community. Other than the conventional video content analysis that focuses on generic semantic concept analysis of video content, human-centric video analysis aims to extract, describe, and organize a wealth of information regarding the main objects of interest in most videos: humans. This topic covers a wide range of research problems such as deception detection _cite_, emotion recognition from user-generated videos _cite_, personality computing _cite_, and action recognition _cite_ . For example, it is often important to recognize the deceptive behaviors _cite_, emotions _cite_, or personality traits _cite_ of the subject of a video in real-world application scenarios. Deception detection _cite_ is a late addition to human-centric video analysis and still under-studied. Deception is defined as an intentional attempt to mislead others _cite_ . In our day-to-day life, deceptive behaviors occur in the form of intended lies, fabrications, omissions, misrepresentations, among others. Some deceptive behaviors are simply harmless, but others may have major threats to the society, e.g., those taking place in a courtroom. Detecting real-world human deceptive behaviors is a challenging task even for humans, and often requires well-trained human experts. A large-scale deployment of deception detection thus depends upon automated deception detection (ADD) _cite_ . An ADD system can find applications in many real-world scenarios including airport security screening, court trial, job interview, and personal credit risk assessment. The ADD task faces two major challenges. (N) Multi-modal fusion: As a subtle human behavioral trait, deception is hard to detect in real-life scenarios. Its reliable detection needs to resort to multiple modalities including the visual, verbal, and acoustic _cite_ . Among them, the visual modality is considered to be the most informative one. Multiple visual cues also exist visually. In particular, facial expressions _cite_ and body motions _cite_ are typically the focus of visual analysis. An important problem thus arises: How to effectively fuse these modalities/cues? Such a fusion is not straightforward because they not only have different strengths in each individual video sequence, but also are temporally asynchronized. An example of the asynchronization between the face and body cues is shown in Figure~ _ref_ . (N) Data scarcity: Unlike the conventional physiological and biological methods _cite_, an ADD model is non-contact and non-invasive. This indirectness means that collecting large quantity of high-quality data containing samples of deceptive behaviors is critical. Earlier data collection efforts focused on human contributors in a lab or in a crowdsourcing setting. In other words, they are staged; the usefulness of these datasets for real-world deployment is thus questionable. Recently, the focus of ADD has been towards detecting deceptive behaviors from real-life data. Particularly, a new multimodal benchmark dataset of real-life videos from court trials is introduced in _cite_ . However, with only N video clips and half of them containing deception, this dataset is insufficient for training a deep neural network based model that has dominated the recent ADD approaches _cite_ . We address both problems in this paper. To tackle the multimodal fusion problem, we propose a novel face-focused cross-stream network (FFCSN) . Different from the popular two-stream networks _cite_, our FFCSN model has two novel components: (a) Face detection is added into the spatial stream subnet to capture the facial expressions explicitly. (b) Correlation learning is performed across the spatial and temporal streams for joint deep feature learning from facial expressions and body motions. Importantly, our model is able to cope with the asynchronization/temporal inconsistency between facial expressions and body motions (see Figure~ _ref_) . To address the training data scarcity problem, we introduce meta learning _cite_ and adversarial learning _cite_ into the training process of our FFCSN. Meta learning, based on the principle of learning to learn, is deployed here to improve the generalization ability of the model and avoid overfitting to the insufficient training data. In the meantime, adversarial learning based feature synthesis is adopted as a data augmentation strategy. When these two are combined, our FFCSN can be trained effectively even with the very sparse data in the existing real-life deception detection benchmarks _cite_ . Our contributions are three-fold: (N) We have proposed a novel face-focused cross-stream network (FFCSN) for joint deep feature learning from facial expressions and body motions in real-life videos. Comparing to existing two-stream networks, our FFCSN model is uniquely able to cope with the asynchronization/temporal inconsistency between facial expressions and body motions. (N) To avoid model overfitting and improve generalization ability, meta learning and adversarial learning are introduced into the training process of FFCSN. (N) We demonstrate that our FFCSN model can be easily extended to other human-centric video analysis problems such as emotion recognition from user-generated videos _cite_ . Extensive experiments are carried out on benchmark datasets and the results show that our model clearly outperforms existing state-of-the-art alternatives for both ADD and multimodal emotion recognition.