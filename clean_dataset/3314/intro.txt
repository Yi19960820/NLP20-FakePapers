Nowadays, many computer vision and image analysis tasks are tackled by means of pattern recognition and machine learning techniques. This work makes some initial steps in the opposite direction. It does not reject the learning approach to computer vision, but it shows how tools form computer vision---and especially variational methods, can aid in efficiently solving some of the basic estimation tasks machine learners and pattern recognizers come across. The particular issue we consider is the problem of scale that, one way or the other, emerges in any learning tasks involving images or videos. As so often, however, it is overlooked and/or dealt with in a way that leaves much to be desired. This work is really at the very interface of computer vision and learning techniques and we may draw heavily on terminology from both fields. The main concept from machine learning and pattern recognition that we use are learning (or training) from examples (especially in relation to linear regression) and the ideas underlying artificial neural network, and convolutional neural in networks particular (see, for instance, _cite_ and _cite_) . Nonetheless, we think that researchers schooled in scale space and variational methods should be able to follow our main line of thought. Supervised classification and regression techniques have been applied to a broad range of challenging image processing and analysis tasks. Learning-based pixel classification has been around at least since the Ns. Early studies seem to have been conducted particularly within the field of remote sensing and abutting areas _cite_ . Though these approaches initially seemed to have focussed primarily on the use of the multiple spectral bands that the observations consisted of, later work also include spatial features based on derivative operator, texture measures, and the like (cf. _cite_) . An early overview of the general applicability of pixel classification can be found, for instance, in _cite_ . Training image filters on the basis of given input-output pairs by means of regression techniques seems to have been considered less often. The problem, as opposed to pixel classification, may be obtaining proper input and output image pairs. Also for this reason, possibly the most often studied application is the prediction of (supposedly) noiseless images from images corrupted with a known noise model, as in this case the input-output pairs are easily generated. Perhaps the first paper to consider such option is _cite_---but the approach may only turn popular now it has been presented at a more fashionable venue _cite_ . A few years later, more advanced applications found their way into medical image analysis, in particular for filtering complex image structures out of chest radiographs _cite_ . Where not so many years ago, pixel-based methods relied on features extracted by means of more or less complex, linear or nonlinear filter banks, the past decade has seen a trend of so-called representation learning _cite_ . The idea is to avoid any initial (explicit) bias in the learning that entails from prespecifying the particular image features that are going to be used. Rather, one relies on raw input data (images in our case) and a complex learner that is capable of simulating the necessary filtering based on the raw input. Particular architectures that are used as learners are so-called deep networks, which are simply a specific type of artificial neural networks. Two examples of the use of such networks in image denoising can be found in _cite_ and the earlier mentioned work _cite_ . A first approach to supervised segmentation using these methods can be found in _cite_ . We see the current work in the light of these developments in representation learning, although our results are not ``deep''. In fact, here we will deal with shallow networks with a single linear convolutional neuron _cite_ ; a basic element in the more complex deep structures referred to above. The problem we focus on is inferring an image-to-image mapping, which is not necessarily limited to image denoising. The core of the issue we study is how to control the complexity of that single neuron. In our case, this is achieved by controlling the aperture scale at which the neuronal mapping operates. In current applications of deep networks, notably convolutional neural networks _cite_, the spatial extent from which the different layers draw their information is coarsely modeled by a rectangle with preset dimensions. We propose to not prefix the spatial range explicitly---and to basically have every pixel intensity in every location have potential influence on any other pixel. We decide to integrate the influence of scale by a regularization term into the overall objective function that is used to determine the fit of the neuron to the training data. In this way, we can trade off the influence of the training data and the scale of the aperture in a gradual and controlled way. Section _ref_ formulates the initial problem setting in mathematical terms. The loss on the data term considered is the regular squared error and so we are basically dealing with standard least squares linear regression. The section shows that our nonregularized prediction problem can be seen as a convolution in which the convolution kernel is to be determined. As it turns out, the formulations allows us to solve regression problems in features spaces with very high dimensionality and with even larger numbers of observations. Section _ref_, covering the main part of our theory, argues that some form of regularization would typically be necessary, after which it introduces and explains our scale-regularized objective function. It also shows how to reformulate the optimization problem so that its minimization can be performed by means of a variational method and finally sketches a basic scheme to come to an actual solution. Section _ref_ provides some limited and artificial, yet illustrative examples and Section _ref_ discusses and concludes.