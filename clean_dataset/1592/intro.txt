In the recent years, artificial neural networks (ANN) have become very popular amongst researchers due to their great success on image classification, feature extraction, segmentation, object recognition and detection~ _cite_ . Deep learning is a more sophisticated and particular form of machine learning, which enables the user to form complex models which are composed of multiple hidden layers. Deep learning methods have enhanced the state-of-the-art performance in object recognition \& detection and computer vision tasks. Deep learning is also advantageous for processing raw data such that it can automatically find a suitable representation for detection or classification~ _cite_ . Convolutional neural network (CNN) is a specific deep learning architecture for processing data which is composed of multiple arrays. Images can be a good example of input to CNN with its ND grid of pixels. Convolutional Neural Networks have become popular with the introduction of its modern version LeNet-N for the recognition of handwritten numbers~ _cite_ . Besides, AlexNet, the winner of ILSVRC object recognition challenge in N, aroused both commercial and scientific interest in CNN and it is the main reason of the intense popularity of CNN architectures for deep learning applications~ _cite_ . The usage of CNN in AlexNet obtained remarkable results such that the network halved the error rate of its previous competitors. Thanks to this great achievement, CNN is the most preferred approach for most detection and recognition problems and computer vision tasks. Although CNNs are suitable for efficient hardware implementations such as in GPUs or FPGAs, the training is computationally expensive due to the high number of parameters. As a result, excessive amount of energy consumption and memory usage make the implementation of neural networks ineffective. According to ~ _cite_, especially matrix multiplications at the layers of a neural network consume too much energy compared to addition or activation function and becomes a major problem for mobile devices with limited batteries. As a result, replacing the multiplication operation becomes the main concern in order to achieve energy efficiency. Many solutions are proposed in order to handle the energy efficiency problem. An energy efficient _inline_eq_-norm based operator is introduced in~ _cite_ . This multiplier-less operator is first used in image processing tasks such as cancer cell detection and licence plate recognition in~ _cite_ . Multiplication-free neural networks (MFNN) based on this operator are studied in~ _cite_ . This operator achieved promising performance especially at image classification on MNIST dataset with multi-layer perceptron (MLP) models~ _cite_ . Han et al. reduces both the computation and storage in three steps: First, the network is trained to learn the important connections. Then, the redundant connections are discarded for a sparser network. Finally, the remaining network is retrained~ _cite_ . Using neuromorphic processors with its special chip architecture is another solution for energy efficiency~ _cite_ . In order to improve energy consumption, Sarwar et al. exploits the error resiliency of artificial neurons and approximates the multiplication operation and defines a Multiplier-less Artificial Neuron (MAN) by using Alphabet Set Multiplier (ASM) . In ASM, the multiplication is approximated as shifting and adding in bitwise manner with some previously defined alphabets~ _cite_ . Binary Weight Networks are energy efficient neural networks whose filters at the convolutional layers are approximated as binary weights. With these binary weights, convolution operation can be computed only with addition and subtraction~ _cite_ . There is also a computationally inexpensive method called distillation~ _cite_ . A very large network or an emsemble model is first trained and transfers its knowledge to a much smaller, distilled network. Using this small and compact model is much more advantageous in mobile devices in terms of speed and memory size. This method shows promising results at image processing tasks such as facial expression recognition~ _cite_ . In this study, an energy-efficient neural network framework based on Binary Weight Network (BWN) ~ _cite_ and Hadamard Transform is developed. The weights at the convolutional layers of the BWN are approximated to binary values, _inline_eq_ or _inline_eq_ ~ _cite_ . Instead of utilizing the original images as network inputs, the network is modified to use compressed images. This network is called Hadamard-transformed Image Network (HIN) . Since Hadamard transform is implemented by Fast Walsh-Hadamard Transform algorithm which requires only addition or subtraction~ _cite_, the HIN network is energy efficient. Our main contribution is the combination of BWN and HIN models: Binary Weight and Hadamard-transformed Image Network (BWHIN) . The combination is carried out after the energy efficient layers, i.e. convolutional layers with two different averaging techniques. All of the energy efficient models are also examined with different CNN architectures. One of them (ConvoPool-CNN) contains pooling layers along with convolutional layers, while the other (All-CNN~ _cite_) uses strided convolution instead of pooling layer~ _cite_ . We analyze the performance of the models on two famous image datasets MNIST and CIFAR-N. While working on MNIST, we also study the effects of certain hyperparameters on the classification accuracy of energy efficient neural networks.