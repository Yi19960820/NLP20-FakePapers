Booted by the development of Deep Learning, letting the computer understand an image seems to be increasingly closer. With the research on object detection gradually becoming mature~ _cite_, increasingly more researchers put their attention on higher-level understanding of the scene~ _cite_ . As an intermediate level task connecting the image caption and object detection, visual relationship/phrase detection is gaining more attention in scene understanding~ _cite_ . detection is the task of localizing a _inline_eq_-_inline_eq_-_inline_eq_ phrase, where predicate describes the relationship between the subject and object. detection involves detecting and localizing pairs of interacting objects in an image and also classifying the predicate or interaction between them. For the example of _inline_eq_-_inline_eq_-_inline_eq_ in Figure~ _ref_, visual relationship detection aims at locating the person, the kite and classifying the pair-wise relation `hold', while visual phrase concentrates on describing the region as an integrated whole. Compared with localized objects, visual relationships have more expressive power. It can help to build up the connection between objects within the image, which opens a new dimension for image understanding. In addition, unlike captions, the fixed structure of visual relationships makes it possible to explicitly design the architecture of the model for better use of the domain knowledge. Therefore, we can get a richer semantic understanding of the image through detecting visual relationships. For visual relationship detection, there are two commonly used pipelines, the bottom-up and the top-down. The bottom-up design first detects objects and then recognizes the possible interactions among them, which is adopted by the state-of-art method~ _cite_ . The top-down design detects the _inline_eq_-_inline_eq_-_inline_eq_ phrase simultaneously by regarding the relationship as an integrated whole. In comparison, the sequential order of the bottom-up implementation cuts off the feature-level connection between the two steps, which is important for correctly recognizing the relationship. Thus, we follow the top-down design by viewing the visual relationship as a phrase and solve it as three closely-connected recognition problems. Because of the joint training for the subject, predicate and object, our model is capable of learning specific visual patterns for the interaction and taking the visual interdependency into consideration. Another motivation in our model is that the predictions of different phrase components are dependent on each other at the visual feature level. This corresponds to learning special features for the pair-wise relationships. For example, visual connection of the subject (person) appearing sitting on something and an object (sofa) with the appearance of human's legs on it help to enhance the evidence of the predicate ``sit on''. In return, the specific visual features for ``sit on'' also help to inference the subject~ (person) and object~ (sofa) as well. They collectively help to detect the relationship, _inline_eq_-_inline_eq_-_inline_eq_ . Therefore, how to use utilize such dependencies would be the core problem of visual relationship detections. Based on the analysis above, we propose Phrase-guided Message Passing Strucvture (PMPS) to model the connection among relationship components. It can be implemented with convolutional layers or fully-connected layers. Deep networks typically pass information feedforward. In contrast, PMPS extracts useful information from other components in the phrase to refine the features before going to the next level. With the sense of other components introduced by PMPS, our model can predict the subject, object and predicate simultaneously as an integrated whole. Besides, unlike simply widening the network, PMPS makes good use of the domain knowledge of the problem and the message passing strategies are designed according to the specific structure of phrases. Such explicit design reduces the parameter number and makes the training easier. Our main contributions of our work are summarized as three-fold: First, we propose a phrase-guided visual relationship detection framework, which can detect the relationship in one step. Corresponding non-maximum suppression method and training strategy are also proposed to improve the speed and accuracy of our model. Second, we propose Phrase-guided Message Passing Structure (PMPS) and corresponding training strategy to leverage the interdependency of three models for more accurage recognition. In PMPS, a new gather-broadcast message passing flow mechanism is proposed, which can be applied to various layers across deep models. Third, we investigate two ways of utilizing Visual Genome Relationship dataset~ _cite_ to pretrain ViP-CNN, both of which improve the performance of our model when compared with the pretraining on ImageNet. On the benchmark dataset~ _cite_, our approach is, respectively, N \% and N \% higher for visual phrase detection and visual relationship detection task compared with the state-of-art method.