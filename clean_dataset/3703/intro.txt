neural networks (CNNs) have demonstrated their exceptional superiority in visual recognition tasks, such as traffic sign recognition~ _cite_, biological image segmentation~ _cite_, and image classification~ _cite_ . CNNs are originally motivated by the computational model of the cat visual cortex specializing in processing vision and signal related tasks~ _cite_ . Since LetNet-N was proposed in N~ _cite_, which is an implementation of CNNs and whose connection weights are optimized by the Back-Propagation (BP) algorithm~ _cite_, various variants of CNNs have been developed, such as VGGNet~ _cite_ and ResNet~ _cite_ . These variants significantly improve the classification accuracies of the best rivals in image classification tasks. Diverse variants of CNNs differ from their architectures and weight connections. Mathematically, a CNN can be formulated by~ (_ref_) in the context of an image classification task with the input _inline_eq_, where _inline_eq_ and _inline_eq_ are the input data and corresponding label, respectively, _inline_eq_ denotes the architecture choosing function with the given data, _inline_eq_ refers to the initialization method of the connection weights _inline_eq_ based on the chosen architecture, and _inline_eq_ measures the differences between the true label and the label predicted by the CNN using _inline_eq_ and _inline_eq_ . Typically, the Gradient Descend (GD)-based approaches, e.g., Stochastic GD (SGD), are utilized to minimize _inline_eq_ within the given number of epochs, where the connection weight values are iteratively updated. Although _inline_eq_ is not differentiable in all occasions, GD-based methods are preferred due to their effectiveness and good scalability as the number of connection weights increases. A CNN commonly has a huge number of connection weights. However, _inline_eq_ and _inline_eq_ are countable functions that are discrete and neither convex or concave, and they are not well addressed in practice. Furthermore, because the gradient-based optimizers are heavily dependent on the initial values of the weights (including biases), it is essential to choose a suitable _inline_eq_ that can help the consecutive GD-based approaches to escape from local minima. Furthermore, the performance of assigned architectures cannot be evaluated until the minimization of _inline_eq_ is finished, while the minimization is a progress of multiple iterations, which in turn increases the difficulty of choosing the potential optimal _inline_eq_ . Therefore, the architecture design and connection weight initialization strategy should be carefully treated in CNNs. Typically, most of the architecture design approaches were initially developed for the deep learning algorithms in the early date (e.g., the Stacked Auto-Encoders (SAEs) ~ _cite_ and the Deep Belief Networks (DBNs) ~ _cite_), such as Grid Search (GS), Random Search (RS) ~ _cite_, Bayesian-based Gaussion Process (BGP) ~ _cite_, Tree-structured Parzen Estimators (TPE) ~ _cite_, and Sequential Model-Based Global Optimization (SMBO) ~ _cite_ . Theoretically, GS exhaustively tests all combinations of the parameters to expectedly seize the best one. However, GS cannot evaluate all combinations within an acceptable time in reality. Moreover, GS is difficult to optimize the parameters of continuous values~ _cite_ . RS could reduce the exhaustive adverse of GS, but the absolute ``random'' severely challenges the sampling behavior in the search space~ _cite_ . In addition, BGP incurs extra parameters (i.e., the kernels) that are arduous to tune. TPE treats each parameter independently, while the most key parameters in CNNs are dependent (e.g., the convolutional layer size and its stride, more details can be seen in Subsection~ _ref_) . The methods mentioned above have shown their good performance in most SAEs and DBNs, but are not suitable to CNNs. Their success in SAEs and DBNs is due largely to the architecture construction approaches, which are greedy layer-wised by stacking a group of building blocks with the same structures (i.e., the three-layer neural networks) . In each building block, these architecture-search methods are utilized for only optimizing the parameters, such as the number of neurons in the corresponding layer. However in CNNs, the layer-wised method cannot be applied due to their architecture characteristics of non-stacking routine, and we must confirm the entire architectures at a time. Furthermore, multiple different building blocks exist in CNNs, and different orders of them would result in significantly different performance. Therefore, the architecture design in CNNs should be carefully treated. Recently, Baker et al. ~ _cite_ proposed an architecture design approach for CNNs based on reinforcement learning, named MetaQNN, which employed N Graphic Processing Unit (GPU) cards with N-N days for the experiments on the CIFAR-N dataset. Due to the drawbacks of existing methods and limited computational resources available to interested researchers, most of these works in CNNs are typically performed by experts with rich domain knowledge~ _cite_ . Genetic Algorithms (GAs), which are a paradigm of the evolutionary algorithms that do not require rich domain knowledge~ _cite_, adapt the meta-heuristic pattern motivated by the process of natural selection~ _cite_ for optimization problems. GAs are preferred in various fields due to their characteristics of gradient-free and insensitivity to local minima~ _cite_ . These promising properties are collectively achieved by a repeated series of the selection, mutation, and crossover operators. Therefore, it can be naturally utilized for the optimization of architecture design and the connection weight initialization for CNNs. Indeed, GAs for evolving neural networks can be traced back to N~ _cite_ . In N, Yao~ _cite_ presented a survey about these different approaches, which are largely for the optimization of connection weights in the fixed architecture topologies. In N, Stanley and Miikkulainen proposed the Neuron-Evolution Augmenting Topology (NEAT) ~ _cite_ algorithm to evolve the architecture and connection weights of a small scale neural network. Afterwards, the HyperNEAT~ _cite_, i.e., NEAT combined with the compositional pattern producing networks~ _cite_, was proposed to evolve a larger scale neural network with an indirect encoding strategy. Motivated by the HyperNEAT, multiple variants~ _cite_ were proposed to evolve even larger scale neural networks. However, the major deficiencies of the HyperNEAT-based approaches are: N) they are only suitable for evolving deep neural networks with global connection and single building blocks, such as SAEs and DBNs, but not CNNs where local connection exists and multiple different building blocks need to be evolved simultaneously, and N) hybrid weigh connections (e.g., connections between two layers that are non-adjacent) may be evolved, which are contrast to the architectures of CNNs. Indeed, the views have been many years that evolutionary algorithms are incapable of evolving the architecture and connection weights in CNNs due to the tremendous number of related parameters~ _cite_ . Until very recently in N, Google showed their Large Evolution for Image Classification (LEIC) method specializing at the architecture optimization of CNNs~ _cite_ . LETC is materialized by GAs without the crossover operator, implemented on N high-end computers, and archives competitive performance against the state-of-the-art on the CIFAR-N dataset by training for about N days. Actually, by directly using GAs for the architecture design of CNNs, several issues would raise in nature: N) the best architecture is unknown until the performance is received based on it. However, evaluating the performance of one individual takes a long time, and appears more severely for the entire population. This would require much more computational resources for speeding up the evolution; N) the optimal depth of CNNs for one particular problem is unknown, therefore it is hard to constrain the search space for the architecture optimization. In this regard, a variable-length gene encoding strategy may be the best choice for both N) and N), but how to assign the crossover operation for different building blocks is a newly resulted problem; and N) the weight initialization values heavily affect the performance of the confirmed architecture, but addressing this problem involves a good gene encoding strategy and the optimization of hundreds and thousands decision variables. The aim of this paper is to design and develop an effective and efficient GA method to automatically discover good architectures and corresponding connection weight initialization values of CNNs (i.e., the first two formulae in~ (_ref_)) without manual intervention. To achieve this goal, the objectives below have been specified: The reminder of this paper is organized as follows: the background of the CNNs, the related works on the architecture design and weight initialization approaches of CNNs are reviewed in Section~ _ref_ . The framework and the details of each step in the proposed algorithm are elaborated in Section~ _ref_ . The experiment design and experimental results of the proposed algorithm are shown in Sections~ _ref_ and~ _ref_, respectively. Next, further discussions are made in Section~ _ref_ . Finally, the conclusions and future work are detailed in Section~ _ref_ .