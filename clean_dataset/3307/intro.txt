Deep residual networks (ResNets) _cite_ consist of many stacked ``Residual Units". Each unit (Fig.~ _ref_ (a)) can be expressed in a general form: where _inline_eq_ and _inline_eq_ are input and output of the _inline_eq_-th unit, and _inline_eq_ is a residual function. In _cite_, _inline_eq_ is an identity mapping and _inline_eq_ is a ReLU _cite_ function. ResNets that are over N-layer deep have shown state-of-the-art accuracy for several challenging recognition tasks on ImageNet _cite_ and MS COCO _cite_ competitions. The central idea of ResNets is to learn the additive residual function _inline_eq_ with respect to _inline_eq_, with a key choice of using an identity mapping _inline_eq_ . This is realized by attaching an identity skip connection (``shortcut'') . In this paper, we analyze deep residual networks by focusing on creating a ``direct" path for propagating information---not only within a residual unit, but through the entire network. Our derivations reveal that, the signal could be propagated from one unit to any other units, in both forward and backward passes. Our experiments empirically show that training in general becomes easier when the architecture is closer to the above two conditions. To understand the role of skip connections, we analyze and compare various types of _inline_eq_ . We find that the identity mapping _inline_eq_ chosen in _cite_ achieves the fastest error reduction and lowest training loss among all variants we investigated, whereas skip connections of scaling, gating _cite_, and N _inline_eq_ N convolutions all lead to higher training loss and error. These experiments suggest that keeping a ``clean'' information path (indicated by the grey arrows in Fig.~ _ref_, ~ _ref_, and~ _ref_) is helpful for easing optimization. To construct an identity mapping _inline_eq_, we view the activation functions (ReLU and BN~ _cite_) as `` '' of the weight layers, in contrast to conventional wisdom of ``post-activation''. This point of view leads to a new residual unit design, shown in (Fig.~ _ref_ (b)) . Based on this unit, we present competitive results on CIFAR-N/N with a N-layer ResNet, which is much easier to train and generalizes better than the original ResNet in _cite_ . We further report improved results on ImageNet using a N-layer ResNet, for which the counterpart of _cite_ starts to overfit. These results suggest that there is much room to exploit the dimension of, a key to the success of modern deep learning.