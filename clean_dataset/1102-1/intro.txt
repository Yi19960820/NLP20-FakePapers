Facial computing, mainly face recognition, is the process of automatically identifying or verifying a person from an image or video frame and has been an active research area for a long time. There are mainly two types of approaches to do face recognition. The first one is on the basis of designing image-specific features to represent the images and then applying common classifiers. The second group of methods, which have recently become very common, utilize deep learning methods. Deep learning models such as Convolutional Neural Networks (CNNs) ~ _cite_ have shown high accuracy in face recognition. Although deep learning performs much better than most of the common machine learning and feature extraction approaches, researchers are still trying to improve these models by optimizing the training or fine-tuning these methods. A different approach to improving the performance of deep learning models is by using multiple channels or modalities when available in the training data. In most feature fusion methods, the feature fusion aims to convert a single channel to multiple channels either by data augmentation or network structure modification. For most multi-channel learning approaches, an extra source of features could provide additional information. However, using only single-frame face images makes finding extra sources of information a big challenge. Face recognition is the main task in facial computing. In addition to face recognition and verification, there have also been efforts for using the face images for facial expression, race or gender detection. These classification tasks work by training models and extracting features or descriptors that are suitable for particular tasks. In this paper, we present a feature fusion approach to achieve higher accuracy through deep learning models. Instead of focusing on challenging the state of the art and improving existing face recognition algorithms, we try to prove the power of fused features automatically extracted by deep learning models for multiple tasks, and then used for each of the tasks. The rationale is that it is possible that the feature extracted for one classification task may have useful information that another type of features extracted for a different classification task misses. This leads to the assumption that we may have better classification results if we combine the features from different classifiers, even though the classifiers classify the data based on different criteria. Our approach is different with multi-task learning in the sense that we do not change the generation of original primary task feature. To verify this hypothesis, we train multiple classification models using CNNs, all on the same data, but labeled differently. The classification criteria, which determine the labels, include person identification (ID), gender, race, and age recognition. Feeding an input image to the four models, we can extract four types of features, which can be fused to generate a more powerful feature for any of the four tasks (ID, gender, race and age recognition) . Since our goal is not proposing a new approach to outperform the state of the art work of face recognition, we are not focusing on designing and trying new structures for the deep learning models. The deep learning models we use in this work are CNNs. We perform multiple comparison experiments, which all show that the fusion features improve face recognition and all other recognition accuracy compared to the original features. The rest of the paper is organized as follows. Section N discusses related work. In Section N we explain our feature fusion approach and propose our hypothesis of multi-task features, after a brief introduction of the feature learning models we use. In Section N we describe the methods and experiments in multimodal feature learning for multiple tasks. Then in Section N, we present the feature fusion approach and the experiments. We conclude and explain our future work in section N.