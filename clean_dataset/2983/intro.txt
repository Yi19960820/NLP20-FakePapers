Generative models have attracted considerable attention recently. The underlying idea behind such models is to attempt to capture the distribution of high-dimensional data such as images and texts. Though these models are highly useful in various applications, it is computationally expensive to train them as they require intractable integration in a very high-dimensional space. This drastically limits their applicability. However, recently there has been considerable progress in {\em deep generative models}--conglomerate of deep neural networks and generative models--as they do not explicitly require the intractable integration, and can be efficiently trained using back-propagation algorithm. Two such famous examples are Generative Adversarial Networks (GANs) ~ _cite_ and Variational Autoencoders~ _cite_ . In this paper we focus on GANs as they are known to produce sharp and plausible images. Briefly, GANs employ a generator and a discriminator where both are involved in a minimax game. The task of the discriminator is to learn the difference between {\em real} samples (from true data distribution _inline_eq_) and {\em fake} samples (from generator distribution _inline_eq_) . Whereas, the task of the generator is to maximize the mistakes of the discriminator. At convergence, the generator learns to produce real looking images. A few successful applications of GANs are video generation~ _cite_, image inpainting~ _cite_, image manipulation~ _cite_, ND object generation~ _cite_, interactive image generation using few brush strokes~ _cite_, image super-resolution~ _cite_, diagrammatic abstract reasoning~ _cite_ and conditional GANs~ _cite_ . Despite the remarkable success of GAN, it suffers from the major problem of {\em mode collapse} ~ _cite_ . Though, theoretically, convergence guarantees the generator learning the true data distribution. However, practically, reaching the true equilibrium is difficult and not guaranteed, which potentially leads to the aforementioned problem of mode collapse. Broadly speaking, there are two schools of thought to address the issue: (N) improving the learning of GANs to reach better optima~ _cite_ ; and (N) explicitly enforcing GANs to capture diverse modes~ _cite_ . Here we focus on the latter. Borrowing from the multi-agent algorithm~ _cite_ and coupled GAN~ _cite_, we propose to use multiple generators with one discriminator. We call this framework the Multi-Agent GAN architecture, as shown in Fig.~ _ref_ . In detail, similar to the standard GAN, the objective of each generator here is to maximize the mistakes of the {\em common} discriminator. Depending on the task, it might be useful for different generators to share information. This is done using the initial layer parameters of generators. Another reason behind sharing these parameters is the fact that initial layers capture low-frequency structures which are almost the same for a particular type of dataset (for example, faces), therefore, sharing them reduces redundant computations. However, when the dataset contains images from completely different modalities, one can avoid sharing these parameters. Naively using multiple generators may lead to the {\em trivial solution} where all the generators learn to generate {\em similar} samples. To resolve this issue and generate different visually plausible samples capturing diverse high probability modes, we propose to modify the objective function of the discriminator. In the modified objective, along with finding the real and the fake samples, the discriminator also has to correctly identify the generator that generated the given fake sample. Intuitively, in order to succeed in this task, the discriminator must learn to push generations corresponding to different generators towards different identifiable modes. Combining the Multi-Agent GAN architecture with the diversity enforcing term allows us to generate diverse plausible samples, thus the name Multi-Agent Diverse GAN (MAD-GAN) . As an example, an intuitive setting where mode collapse occurs is when a GAN is trained on a dataset containing images from different modalities/classes. For example, a diverse-class dataset containing images such as forests, iceberg, and bedrooms. This is of particular interest as it not only requires the model to disentangle intra-class variations, it also requires inter-class disentanglement. Fig.~ _ref_ demonstrates the surprising effectiveness of MAD-GAN in this challenging setting. Generators among themselves are able to disentangle inter-class variations, and each generator is also able to capture intra-class variations. In addition, we analyze MAD-GAN through extensive experiments and compare it with several variants of GAN. First, for the proof of concept, we perform experiments in controlled settings using synthetic dataset (mixture of Gaussians), and complicated Stacked/Compositional MNIST datasets with hand engineered modes. In these settings, we empirically show that our approach outperforms all other GAN variants we compare with, and is able to generate high quality samples while capturing large number of modes. In a more realistic setting, we show high quality diverse sample generations for the challenging tasks of {\em image-to-image translation} ~ _cite_ (conditional GAN) and {\em face generation} ~ _cite_ . Using the SVHN dataset~ _cite_, we also show the efficacy of our framework for learning the feature representation in an unsupervised setting. We also provide theoretical analysis of this approach and show that the proposed modification in the objective of discriminator allows generators to learn together as a mixture model where each generator represents a mixture component. We show that at convergence, the global optimum value of _inline_eq_ is achieved, where _inline_eq_ is the number of generators.