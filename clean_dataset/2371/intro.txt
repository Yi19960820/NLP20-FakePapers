have applications in image classification, object detection, machine translation, and many others . In such applications, even a marginal improvement in model performance can have significant business value. Ensemble methods are commonly used in computer vision competitions and achieve better performance comparing compared to single models . However, in the case of, training even a single model is computationally intensive, making ensemble approaches less tractable. The distribution of parameters has been studied extensively as part of Bayesian Neural Networks. The state-of-the-art variational inference provides robustness to overfitting leading to better model performance . However, the information from training updates is not fully utilized. Recently, _cite_ proposed a procedure to ensemble a model at different training stages. The method enables a fast ensemble by reducing the number of models that need to be trained from scratch. Furthermore, the same team improved the method by directly averaging the weights instead of using an ensemble thereby reducing the computation cost . The above-mentioned methods all require retraining the model. We propose a new method to use the uncertainty residing in the updates for the model ensembling and parameter averaging to improve the model prediction performance. The key contributions of the paper include: In this paper, we first introduce our approach in Sec.~ _ref_ . Then we carry out an extensive analysis using LeNet model on MNIST dataset and evaluate the result on a variety models on ImageNet dataset in Sec.~ _ref_ . Finally, we discuss the proposed methods and compare with other related works in Sec.~ _ref_ and conclude the paper.