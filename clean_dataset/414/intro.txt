Deep learning has facilitated breakthroughs for a variety of AI tasks and, in my cases, has achieved performance equal or superior to human performance [N] . Despite considerable success, most learning models do not satisfactorily explain why they reach a decision; thus, model deployment is delayed or even abandoned. Recognizing that this deficiency could cause potential harm, the European Union have adopted a regulation for algorithmic decision making, that addresses the “right to explanation” [N] . This regulation will restrict the deployment of AI systems that do not satisfy this constraint, which will have a significant impact on AI industry. To address this situation, a variety of explanation techniques have been devised and evaluated [N] . The evaluation of some techniques involved user studies to ensure that the explanations increased user trust and helped users choose better models [N, N] . In the early days of development, interpreting the learning model itself dominated [N] . Currently, interpreting predictions for individual instances, which is the focus of this study, is receiving greater attention. An interpretation model should satisfy several requirements. The most critical requirement is that the interpretation be class-discriminative, i.e., it should identify features that make the greatest contribution to determining the given class. As shown in Figure _ref_, sensitivity analysis (SA), gradient-weighted class activation map (Grad-CAM), and the proposed method are class-discriminative, whereas the other methods are not. This paper suggests another requirement, i.e., a visually pleasing saliency map. The proposed method demonstrates better performance for both requirements. In the first row of Figure _ref_ where the target class is “eggnog, ” note that the proposed method correctly indicates the eggnog regions inside the glass. In addition, our saliency map retains the object shapes clearly and thus visually pleasing. We believe that visually pleasing saliency maps can help machine learning experts select appropriate models and optimize hyper-parameters and are essential to help laypersons, such as doctors and public safety agents, choose and use a reasonable AI system. Our primary contribution is a region-based approach that estimates feature importance in terms of appropriately segmented regions. By fusing saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained. Avoiding a tradeoff between accuracy and interpretability is another important issue. For example, the class activation map (CAM) method working on only the convolutional neural networks (CNN) with global average pooling sacrifices _inline_eq_ accuracy to obtain high class-discriminability [N] . In our implementation, we incorporate the regional multi-scale idea into a prediction difference method that is model-agnostic, i.e., applicable to any learning model without modifying internal operations. The proposed method was evaluated qualitatively and quantitatively, and compared to state-of-the-art methods. The results demonstrate that the proposed method produces much more class-discriminative and visually pleasing saliency maps (Figure _ref_) . In addition, the proposed method is two orders of magnitude faster than the conventional prediction difference algorithm.