Generating descriptions for images has long been regarded as a challenging perception task integrating vision, learning and language understanding. One not only needs to correctly recognize what appears in images but also incorporate knowledge of spatial relationships and interactions between objects. Even with this information, one then needs to generate a description that is relevant and grammatically correct. With the recent advances made in deep neural networks, tasks such as object recognition and detection have made significant breakthroughs in only a short time. The task of describing images is one that now appears tractable and ripe for advancement. Being able to append large image databases with accurate descriptions for each image would significantly improve the capabilities of content-based image retrieval systems. Moreover, systems that can describe images well, could in principle, be fine-tuned to answer questions about images also. This paper describes a new approach to the problem of image caption generation, casted into the framework of encoder-decoder models. For the encoder, we learn a joint image-sentence embedding where sentences are encoded using long short-term memory (LSTM) recurrent neural networks _cite_ . Image features from a deep convolutional network are projected into the embedding space of the LSTM hidden states. A pairwise ranking loss is minimized in order to learn to rank images and their descriptions. For decoding, we introduce a new neural language model called the structure-content neural language model (SC-NLM) . The SC-NLM differs from existing models in that it disentangles the structure of a sentence to its content, conditioned on distributed representations produced by the encoder. We show that sampling from an SC-NLM allows us to generate realistic image captions, significantly improving over the generated captions produced by _cite_ . Furthermore, we argue that this combination of approaches naturally fits into the experimentation framework of _cite_, that is, a good encoder can be used to rank images and captions while a good decoder can be used to generate new captions from scratch. Our approach effectively unifies image-text embedding models (encoder phase) _cite_ with multimodal neural language models (decoder phase) _cite_ _cite_ . Furthermore, our method builds on analogous approaches being used in machine translation _cite_ . While the application focus of our work is on image description generation and ranking, we also qualitatively analyse properties of multimodal vector spaces learned using images and sentences. We show that using a linear sentence encoder, linguistic regularities _cite_ also carry over to multimodal vector spaces. For example,* image of a blue car*-"blue" + "red" results in a vector that is near images of red cars. We qualitatively examine several types of analogies and structures with PCA projections. Consequently, even with a global image-sentence training objective the encoder can still be used to retrieve locally (e.g. individual words) . This is analogous to pairwise ranking methods used in machine translation _cite_ . A large body of work has been done on learning multimodal representations of images and text. Popular approaches include learning joint image-word embeddings _cite_ as well as embedding images and sentences into a common space _cite_ . Our proposed pipeline makes direct use of these ideas. Other approaches to multimodal learning include the use of deep Boltzmann machines _cite_, log-bilinear neural language models _cite_, autoencoders _cite_, recurrent neural networks _cite_ and topic-models _cite_ . Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA _cite_, normalized CCA _cite_ and dependency tree recursive networks _cite_ . From an architectural standpoint, our encoder-decoder model is most similar to _cite_, who proposed a two-step embedding and generation procedure for semantic parsing. We group together approaches to generation into three types of methods, each described here in more detail: Template-based methods. Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships _cite_ . While these approaches can produce accurate descriptions, they are often more `robotic' in nature and do not generalize to the fluidity and naturalness of captions written by humans. Composition-based methods. These approaches aim to harness existing image-caption databases by extracting components of related captions and composing them together to generate novel descriptions _cite_ . The advantage of these approaches are that they allow for a much broader and more expressive class of captions that are more fluent and human-like then template-based approaches. Neural network methods. These approaches aim to generate descriptions by sampling from conditional neural language models. The initial work in this area, based off of multimodal neural language models _cite_, generated captions by conditioning on feature vectors from the output of a deep convolutional network. These ideas were recently extended to multimodal recurrent networks with significant improvements _cite_ . The methods described in this paper produce descriptions that at least qualitatively on par with current state-of-the-art composition-based methods _cite_ . Description generation systems have been plagued with issues of evaluation. While Bleu and Rouge have been used in the past, _cite_ has argued that such automated evaluation methods are unreliable and do not match human judgements. These authors instead proposed that the problem of ranking images and captions can be used as a proxy for generation. Since any generation system requires a scoring function to access how well a caption and image match, optimizing this task should naturally carry over to an improvement in generation. Many recent methods have since used this approach for evaluation. None the less, the question on how to transfer improvements on ranking to generating new descriptions remained. We argue that encoder-decoder methods naturally fit into this experimentation framework. That is, the encoder gives us a way to rank images and captions and develop good scoring functions, while the decoder can use the representations learned to optimize the scoring functions as a way of generating and scoring new descriptions. Our proposed pipeline, while new to caption generation, has already experienced several successes in Neural Machine Translation (NMT) . The goal of NMT is to develop an end-to-end translation system with a large neural network, as opposed to using a neural network as an additional feature function to an existing phrase-based system. NMT methods are based on the encoder-decoder principle. That is, an encoder is used to map an English sentence to a distributed vector. A decoder is then conditioned on this vector to generate a French translation from the source text. Current methods include using a convolutional encoder and RNN decoder _cite_, RNN encoder and RNN decoder _cite_ and LSTM encoder with LSTM decoder _cite_ . While still a young research area, these methods have already achieved performance on par with strong phrase-based systems and have improved on the start-of-the-art when used for rescoring. We argue that it is natural to think of image caption generation as a translation problem. That is, our goal is to translate an image into a description. This point of view has also been used by _cite_ and allows us to make use of existing ideas in the machine translation literature. Furthermore, there is a natural correspondence between the concept of scoring functions (how well does a caption and image match) and alignments (which parts of a description correspond to which parts of an image) that can naturally be exploited for generating descriptions.