In recent years, sensor-based human action recognition (HAR) plays a key role in the area of ubiquitous computing due to its wide application in daily life _cite_ . In most cases, utilization of raw data without special process is impractical since raw recordings confound noise and meaningless components. Therefore, the process of feature extraction and feature learning is supposed to be conducted _cite_, which is commonly based on the sliding window technique through sampling overlapping frames from signal streams _cite_ . In general, studies on sensor-based HAR have mainly focused on low-level _cite_ and mid-level _cite_ features. Low-level features include statistical features _cite_, frequency-domain features _cite_ and some hand-crafted methods _cite_ . Low-level features are popular owing to their simplicity as well as their acceptable performances across a variety of action recognition problems. However, their simplicity is often accompanied by less discrimination in representation, considering that actions are often highly complex and diverse in daily-life cases. Compared with low-level features, mid-level features which are mainly obtained through dictionary learning methods _cite_ have proven to be more robust and discriminative _cite_ . The representations include the sparse coding method _cite_ and the bag-of-words (BOW) algorithm _cite_ . These methods analyze components (motion primitives) and explore inherent structures of signals. However, when the dictionary size is relatively large, mid-level features would suffer high computation, redundancy in representation and further burden the following classification. In this paper, we reduce the redundancy of the mid-level representation by introducing high-level features, which can achieve better overall performances and be robust to dictionary size. Many studies have applied probabilistic graphical models _cite_ and pattern-mining methods _cite_ in semantic (high-level) recognition tasks. However, few works focus on feature learning methods in sensor-based HAR. In this paper, a semantic feature learning algorithm is proposed on two motivations. First, it can remove the redundancy in mid-level representation. A compact and discriminative description can be achieved by applying a latent pattern learning method on mid-level features. Second, implementing semantic analysis on sensor signals is an efficient and intuitive way to discover and deal with the variety of action patterns, and thus it can remove ambiguities in descriptions of the action. In particular, the ambiguities mainly come from two aspects, physical level and annotation level, both of which make it difficult to generate generalized good features in HAR tasks _cite_ . At the physical level, one challenge is that different or even the same people who conduct the same action may produce completely different signals due to changes in environment, which is called the intra-class variability _cite_ . For example, walking style in morning after a good sleep commonly differs from the one after day's hard work. The other challenge is the inter-class similarity _cite_, which refers to the similarity among different actions such as `drinking milk' and `drinking coffee'. As the inter-class similarity is generally resolved through obtaining additional data from different sensors _cite_ or analyzing co-occurring actions _cite_, our work focuses on dealing with the intra-class variability. At the annotation level, a specific action performed in many ways can be cognitively identified as the same one _cite_ . Take `cleaning the table' as an example. It is rational to consider cleaning table from right to left and from up to down as the same action, though they behave absolutely differently in signal records. For those reasons, generalized features that are directly learned from action perspectives would compromise to the common characteristic shared by different action patterns, resulting in ambiguous feature representations _cite_ . Inspired by Multiple-Instance Learning (MIL) _cite_, our solution is to mine discriminative latent patterns from actions and construct features based on descriptions of those patterns, which can eliminate ambiguities from both physical and annotation levels. We name this method the Max-margin Latent Pattern Learning (MLPL) method. Instead of being constrained by generic property, the MIL method is a natural solution given that the diversity inside the class can be learnt. Although MIL methods are widely performed in computer vision _cite_, as for sensor-based HAR problems, relevant works are mainly designed to cope with sparse annotation _cite_ . Instead of dealing with sparsely labeled cases, MLPL proposed in this paper implements MIL to learn discriminative latent patterns of actions, by which high-level features would be acquired. In this paper, we integrate the advantages of low-, mid-and high-level features and propose the framework known as Multi-Level Complementary Feature Learning (MLCFL) . To avoid being confused with the high-level feature learned by the latent pattern learning process, the output feature of our framework is denoted as Multi-Level Complementary Feature (MLCF) . In particular, this framework learns multi-level features through three phases, which are respectively designed to analyze signal-based (low-level), components (mid-level) and semantic (high-level) information. In the first phase, the low-level feature (statistical values, FFT coefficients, etc.) is extracted from raw signals. In the second phase, from the component perspective, the mid-level representation can be attained through hard coding processes and occurrence statistics. In the third phase, the MLPL method, from the semantic perspective, is implemented on the Compl feature (the concatenation of low-and mid-level features) to obtain MLCF as the output of this framework. Various experiments on Opp _cite_, Skoda _cite_ and WISDM _cite_ datasets show that MLCF possesses higher feature representation ability than low-and mid-level features. Moreover, compared with existing methods, the method we proposed achieves state-of-the-art performances. Our contributions in this paper are as follows: The rest of this paper is organized as follows: Section N presents related work; Section N describes the MLCFL framework for action recognition; Section N presents and analyzes experimental results; finally, we conclude the study in Section N.