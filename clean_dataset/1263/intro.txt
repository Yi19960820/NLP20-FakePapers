Multicolor fluorescence in situ hybridization (mFISH) is a cytogenetic methodology that allows the simultaneous visualization of each chromosome pair in a different color, providing a genome-wide picture of cytogenetic abnormalities in a single experiment . It was introduced in N as spectral karyotyping (SKY) and multiplex-FISH (M-FISH), similar methodologies in terms of labeling but differing in terms of imaging system requirements and image acquisition and analysis process. After the mFISH spectral information has been acquired, different features can be analyzed to assign a chromosome label to each pixel. Manual interpretation of mFISH images is a time-consuming task where not only the intensity of each pixel is compared across channels but also the shape, size and centromere position. Many attempts were made to automate the task, being the most notable approaches pixel and region based classifiers. These classifiers usually build a feature vector using pixel or patch based intensity information and use that information to train a classifier, which is later used to classify pixels from the same image, or from a different one . Multiple pixel based classifiers have been developed for the analysis of mFISH images, showing that the spectral information present in a pixel can be successfully used to train machine learning classifiers. . In the other hand, region based classification has also been studied, showing that it generally outperforms pixel based classification approaches., underlining the importance of using spatial information to improve the performance of mFISH analysis algorithms. Despite the relative success of the above mentioned approaches, none of them take into account spatial information about the shape, size, or texture of the objects being analyzed. This limits the performance of the algorithms in challenging scenarios where the identification of the chromosome is not clear based only on the spectral information. Some important features typically used in manual analysis, but not incorporated into classification algorithms, are the relative length of a chromosome, the arm ratio, or the centromeric index . Such features can be automatically learned running the input images through a network of convolutions and resampling operations, comparing the resulting image to the expected segmentation map, and backpropagating the error to learn the network parameter. This approach is usually called “end to end semantic segmentation”. End to end semantic segmentation using convolutional networks has been shown to achieve state of the art results by automatically learning features based on spatial and intensity information . The convolutional network approach shifts the focus from feature engineering to network architecture engineering, searching for the best network layout for a given problem. In the field of biomedical image processing, network architectures such as U-Net have been widely used to perform end to end semantic segmentation. This architecture consists of two paths: the first one builds an abstract representation of the image by iteratively convolving and subsampling the image, while the second creates the target segmentation map by iterative upsampling and convolving the abstract feature maps. These two paths are symmetrical and connected by connecting each subsampling step with the analogous upsampling step by concatenating the corresponding layers. Different architectures of end-to-end convolutional networks for semantic segmentation have been developed since the creation of U-Net, being Deep-Lab architecture one of the best performing ones, with an average precision of N \% in the Pascal VOC challenge . The core of this architecture is the use of atrous convolution for probing convolutional features at different scales which has proven to be a powerful way of incorporating context information. The good results in the Pascal VOC N semantic segmentation challenge led us to incorporate some of the main ideas into our work with mFISH images. The main challenge of applying end to end convolutional networks is the limited number of samples found in the commonly used benchmarks, mainly the ADIR dataset . This dataset contains mFISH samples prepared using Vysis, ASI, and PSI/Cytocell probes, were each cell is captured in N images, N of them representing the observed intensity of each fluorophore and the remaining one containing manual expert labeling of every pixel. The three different probes used to prepare the samples do not share a common labeling scheme, which means that the features used to segment a sample hybridized with a Vysis probe set may not work in samples where the ASI probes were used. To reduce the impact of using different probe sets for training and testing, this work focuses on the Vysis subset, since it is the largest one. In this work, we present a fully convolutional network for semantic segmentation of mFISH images that uses both spectral and spatial information to classify every pixel in an image in and end-to-end fashion and provide evidence that our approach performs well even in challenging scenarios.