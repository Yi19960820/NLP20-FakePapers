Images depicting natural objects are ND representations of an underlying ND structure from a specific viewpoint in specific lighting conditions. This work demonstrates a method for recovering the underlying ND geometry of an object depicted in a single ND image or video. To accomplish this we first encode the image as a separate description of the shape and transformation properties of the input such as lighting and pose. The shape description is used to generate a volumetric representation that is interpretable by modern rendering software. State of the art computer vision models perform recognition by learning hierarchical layers of feature detectors across overlapping sub-regions of the input space. Invariance to small transformations to the input is created by sub-sampling the image at various stages in the hierarchy. In contrast, computer graphics models represent visual entities in a canonical form that is disentangled with respect to various realistic transformations in ND, such as pose, scale and lighting conditions. ND images can be rendered from the graphics code with the desired transformation properties. A long standing hypothesis in computer vision is that vision is better accomplished by inferring such a disentangled graphical representation from ND images. This process is known as `de-rendering' and the field is known as `vision as inverse graphics'~ _cite_ . One obstacle to realising this aim is that the de-rendering problem is ill-posed. The same ND image can be rendered from a variety of ND objects. This uncertainty means that there is normally no analytical solution to de-rendering. There are however, solutions that are more or less likely, given an object class or the class of all natural objects. Recent work in the field of vision as inverse graphics has produced a number of convolutional neural network models that accomplish de-rendering~ _cite_ . Typically these models follow an encoding / decoding architecture. The encoder predicts a compact ND graphical representation of the input. A control signal is applied corresponding with a known transformation to the input and a decoder renders the transformed image. We use a similar architecture. However, rather than rendering an image from the graphics code, we generate a full volumetric representation. Unlike the disentangled graphics code generated by existing models, which is only renderable using a custom trained decoder, the volumetric representation generated by our model is easily converted to a polygon mesh or other professional quality ND graphical format. This allows the object to be rendered at any scale and with other rendering techniques available in modern rendering software.