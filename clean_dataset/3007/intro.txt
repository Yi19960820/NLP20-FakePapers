Gait has received significant attention as a biometric, particularly with the increasing quality of security cameras, the growth in city populations and the subsequent interest in tracking the movement of people in such environments. There have been several studies _cite_ that support the usefulness of gait for identity recognition within a sample population. However, there are many confounding factors that can reduce usefulness of gait as an identifying biometric; these range from the difficulties in extracting particular signatures to the distinctiveness and robustness of those features to confounding factors. The confounding factors can be divided into three different categories, according to whether they are related to: Emerging gait recognition systems depend on supervised machine/deep learning. Machine learning algorithms (especially deep networks) require vast amounts of application-specific, high-quality labelled training data, which is either very expensive or not feasible to acquire. In order to collect sufficient amounts of labelled data for training deep networks for gait recognition from scratch, participants would need to walk for hours. In addition, to collect data under different conditions, the experiment would have to be repeated multiple times, to cover all the possible combinations of conditions that are likely to occur. The only alternative strategy would be to employ transfer learning, but it is hard to find a problem for transfer learning which is sufficiently close to the complexity of gait recognition. Taking several confounding variables together poses significant challenges. How does one gather sufficient variations across all confounding factors, and with many different subjects, within a finite time in order to train, let alone test, a system to provide accurate recognition? One strategy would be to synthesize gait motion within virtual environments in which clothing, lighting, camera pose and environments can be systematically changed. This paper takes a first step towards showing that this is feasible, conditional on sufficient quality of real motion capture (mocap) data, rendering and imaging models. We suggest a method to generate augmented labelled training data for machine learning, including quantities of data that could be used for deep supervised learning algorithms. A new multi-modal dataset is introduced, consisting of more than N million frames of real motion capture data, video data and ND mesh models. Based on the motion capture data collected from N subjects, we also provide simulation files with several confounding factors being controllable (see Figure _ref_) . The motion data itself is captured from real people walking and running on a treadmill at different speeds. Generating synthetic data for data augmentation is not new in the field of Computer Vision. Studies like _cite_ use synthetic generated data to augment training and/or testing sets, while others use it to learn features invariant to certain conditions _cite_ . While such methods focus on a very limited number of covariate factors, to our knowledge, this is the first attempt to provide the ability to generate synthetic video data with so many controllable conditions from real human motion data. This opens the possibility of bootstrapping or pre-training gait recognition methods that can identify people by their gait while being invariant to several different confounding factors simultaneously. We discuss the process by which this dataset is generated and demonstrate that characteristics of identity are preserved within the motion of the synthetically generated data.