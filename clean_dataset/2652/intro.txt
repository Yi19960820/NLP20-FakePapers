actions consist of simultaneous flow of different body parts. Based on this complex articulated essence of human movements, the analysis of these signals could be highly complicated. To ease the task of classification, actions could be broken down into their components. This is done by a body part detection on depth sequences of human body movements _cite_ . Having the ND locations of body joints in the scene, we can separate the complicated motion of body into a concurrent set of behaviors on major skeleton joints; therefore human action sequences could be considered as multipart signals. Throughout this paper, we use the term ``part'' to denote each body joint as defined in _cite_ . Limiting the learning into skeleton based features cannot deliver high levels of performance in action recognition, because: (N) most of the usual human actions are defined based on the interaction of body with other objects, and (N) depth based skeleton data is not always accurate due to the noise and occlusion of body parts. To alleviate these issues, different depth based appearance features can be leveraged. The work in _cite_ proposed LOP (local occupancy patterns) around each of the body joints in order to represent ND appearance of the interacting objects. Another solution is HONND (histogram of oriented ND normals) _cite_, which gives more descriptive and robust models of the local depth based appearance and motion, around the joints. Based on the complementary properties of mentioned features, it is beneficial to utilize all of them as different descriptors for each joint. Combining heterogeneous features of each part of the skeleton, leads into a multimodal-multipart combination, which demands sophisticated fusion algorithms. An interesting approach to handle the articulation of actions was recently proposed by _cite_ . As the key intuition, they have shown each individual action class can be represented by the behavior and appearance of few informative joints in the body. They utilized a data mining technique to find these discriminative sets of joints for each class of the available actions and tied up the features of those parts as ``actionlets''. They employed a multi-kernel learning method to build up ensembles of actionlets as kernels for action classification. This method is highly robust against the noise in depth maps, and the results show its strength to characterize the human body motion and also human-object interactions. However the downside of this approach is the inconsistency of their heuristic selection process (mining actionlets) with the following learning step. Moreover, it simply concatenates different types of features for multimodal fusion, which is another drawback of this work. In this fashion, achieving the optimal combination of features regarding the classification task cannot be guaranteed. To overcome the limitations mentioned above, we propose a joint structured sparsity regression based learning method which integrates part selection into the learning process considering the heterogeneity of features for each joint. We associate all the features for each part as a bundle and apply a group sparsity regularization to select a small number of active parts for each action class. To model the precise hierarchy of the multimodal-multipart features in an integrated learning and selection framework, we propose a hierarchical mixed norm which includes three levels of regularization over learning weights. To apply the modality based coupling over heterogeneous features of each part, it applies a mixed norm with two degrees of ``diversity'' induction _cite_, followed by a group sparsity among the feature groups of different parts to apply part selection. The main contributions of this paper are two \--fold: First, we integrated the part selection process into our learning in order to select discriminative body parts for different action classes latently, and utilize them to learn classifiers. Second, a hierarchical mixed norm is proposed to apply the desired simultaneous sparsity and regularization over different levels of learning weights corresponding to our special multimodal-multipart features in a joint group sparsity regression framework. We evaluate our method on three challenging depth based action recognition datasets: MSR \--DailyActivity dataset~ _cite_, MSR \--ActionND dataset _cite_, and ND \--ActionPairs dataset _cite_ . Our experimental results show that the proposed method is superior to other available methods for action recognition on depth sequences. The rest of this paper is organized as follows: Section N reviews the related works on depth based action recognition, joint sparse regression, mixed norms, and multitask learning. Section N presents the proposed integrated feature selection and learning scheme. It also introduces the new multimodal-multi part mixed norm which applies regularization and group sparsity into the proposed learning model. Experimental results on three above-mentioned benchmarks are covered in section N and we conclude the paper in section N.