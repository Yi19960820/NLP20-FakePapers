The recovery of a image or video from its counter part is topic of great interest in digital image processing. This task, referred to as, finds direct applications in many areas such as HDTV _cite_, medical imaging _cite_, satellite imaging _cite_, face recognition _cite_ and surveillance _cite_ . The global problem assumes data to be a low-pass filtered (blurred), downsampled and noisy version of data. It is a highly ill-posed problem, due to the loss of high-frequency information that occurs during the non-invertible low-pass filtering and subsampling operations. Furthermore, the SR operation is effectively a one-to-many mapping from to space which can have multiple solutions, of which determining the correct solution is non-trivial. A key assumption that underlies many techniques is that much of the high-frequency data is redundant and thus can be accurately reconstructed from low frequency components. is therefore an inference problem, and thus relies on our model of the statistics of images in question. Many methods assume multiple images are available as instances of the same scene with different perspectives, i.e. with unique prior affine transformations. These can be categorised as multi-image methods _cite_ and exploit by constraining the ill-posed problem with additional information and attempting to invert the downsampling process. However, these methods usually require computationally complex image registration and fusion stages, the accuracy of which directly impacts the quality of the result. An alternative family of methods are techniques _cite_ . These techniques seek to learn that is present in natural data to recover missing information from a single instance. This usually arises in the form of local spatial correlations for images and additional temporal correlations in videos. In this case, prior information in the form of reconstruction constraints is needed to restrict the solution space of the reconstruction. The goal of methods is to recover a image from a single input image _cite_ . Recent popular methods can be classified into edge-based _cite_, image statistics-based _cite_ and patch-based _cite_ methods. A detailed review of more generic methods can be found in _cite_ . One family of approaches that has recently thrived in tackling the problem is sparsity-based techniques. Sparse coding is an effective mechanism that assumes any natural image can be sparsely represented in a transform domain. This transform domain is usually a dictionary of image atoms _cite_, which can be learnt through a training process that tries to discover the correspondence between and patches. This dictionary is able to embed the prior knowledge necessary to constrain the ill-posed problem of super-resolving unseen data. This approach is proposed in the methods of _cite_ . A drawback of sparsity-based techniques is that introducing the sparsity constraint through a nonlinear reconstruction is generally computationally expensive. Image representations derived via neural networks _cite_ have recently also shown promise for . These methods, employ the back-propagation algorithm _cite_ to train on large image databases such as ImageNet _cite_ in order to learn nonlinear mappings of and image patches. Stacked collaborative local auto-encoders are used in _cite_ to super-resolve the image layer by layer. Osendorfer et al. _cite_ suggested a method for based on an extension of the predictive convolutional sparse coding framework _cite_ . A multiple layer inspired by sparse-coding methods is proposed in _cite_ . Chen et. al. _cite_ proposed to use multi-stage as an alternative to where the weights and the nonlinearity is trainable. Wang et. al _cite_ trained a cascaded sparse coding network from end to end inspired by LISTA (Learning iterative shrinkage and thresholding algorithm) _cite_ to fully exploit the natural sparsity of images. The network structure is not limited to neural networks, for example, a random forest _cite_ has also been successfully used for . With the development of, the efficiency of the algorithms, especially their computational and memory cost, gains importance _cite_ . The flexibility of deep network models to learn nonlinear relationships has been shown to attain superior reconstruction accuracy compared to previously hand-crafted models _cite_ . To super-resolve a image into space, it is necessary to increase the resolution of the image to match that of the image at some point. In Osendorfer et al. _cite_, the image resolution is increased in the middle of the network gradually. Another popular approach is to increase the resolution before or at the first layer of the network _cite_ . However, this approach has a number of drawbacks. Firstly, increasing the resolution of the images before the image enhancement step increases the computational complexity. This is especially problematic for convolutional networks, where the processing speed directly depends on the input image resolution. Secondly, interpolation methods typically used to accomplish the task, such as bicubic interpolation _cite_, do not bring additional information to solve the ill-posed reconstruction problem. Learning upscaling filters was briefly suggested in the footnote of Dong et.al. _cite_ . However, the importance of integrating it into the CNN as part of the SR operation was not fully recognised and the option not explored. Additionally, as noted by Dong et al. _cite_, there are no efficient implementations of a convolution layer whose output size is larger than the input size and well-optimized implementations such as convnet _cite_ do not trivially allow such behaviour. In this paper, contrary to previous works, we propose to increase the resolution from to only at the very end of the network and super-resolve data from feature maps. This eliminates the need to perform most of the operation in the far larger resolution. For this purpose, we propose an efficient sub-pixel convolution layer to learn the upscaling operation for image and video super-resolution. The advantages of these contributions are two fold: We validate the proposed approach using images and videos from publicly available benchmarks datasets and compared our performance against previous works including _cite_ . We show that the proposed model achieves state-of-art performance and is nearly an order of magnitude faster than previously published methods on images and videos.