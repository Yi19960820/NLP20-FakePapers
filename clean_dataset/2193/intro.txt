With the rapid progress of human civilization and technology, multimodal data as image, text, video, and audio has been rapidly increasing on the Internet, and gradually becomes the main form of information. In this situation, cross-modal retrieval has become an important application of artificial intelligence, which is a novel paradigm where the retrieval results and user query are relevant in semantics but of different modalities. For example, user can submit an image to retrieve relevant text documents, and vice versa. Different from single-modal analysis as _cite_, cross-modal retrieval faces the challenge that different modalities have inconsistent representations, and the existing mainstream is to learn common representation for them. Recent years, cross-modal retrieval based on Deep Neural Network (DNN) has become an active research topic _cite_, which aims to fulfill the DNN's strong ability of abstraction for dealing with complex cross-modal correlation. Training data is important for the performance of DNN-based methods, but there is often insufficient training data for a specific task. In single-modal scenario, the problem of insufficient training data is usually relieved by the idea of transfer learning _cite_, which can transfer the knowledge of large-scale training data in source domain to target domain. For example, CNN model pre-trained on a subset of ImageNet _cite_ with over N, N, N labeled images usually acts as the basic model for many problems in computer vision _cite_, which is from ImageNet large-scale visual recognition challenge (ILSVRC) N. However, the knowledge transfer is usually performed within the same modality. Knowledge contained in such large-scale single-modal datasets is also very valuable for cross-modal retrieval because it can provide rich general semantic information, which can be shared by different modalities to facilitate cross-modal semantic learning. But it is a challenging problem to effectively transfer useful knowledge from such to . For example, we aim to train a cross-modal retrieval model on a small image/text dataset as target domain, and have a large-scale image dataset ImageNet as source domain. On the one hand, because there are only labeled images in ImageNet, the knowledge contained in source domain cannot be directly transferred to both image and text modalities in the target domain. On the other hand, the inherent cross-modal semantic correlation contained in the image/text dataset provides key hints for cross-modal retrieval and should be preserved in the transfer process. For addressing the above problems, this paper proposes Cross-modal Hybrid Transfer Network (CHTN), which is a unified architecture consisting of two subnetworks: modal-sharing transfer subnetwork and layer-sharing correlation subnetwork. utilizes the modality contained in both source and target domains as a bridge, for propagating knowledge to both the two modalities in target domain simultaneously by single-modal transfer and cross-modal transfer. focuses on preserving the inherent cross-modal semantic correlation to further adapt to the cross-modal retrieval task in the target domain. CHTN performs knowledge transfer and correlation learning at the same time, and can generate common representation for different modalities to perform cross-modal retrieval. For verifying the effectiveness of CHTN, we conduct cross-modal retrieval experiments on N widely-used datasets: Wikipedia, NUS-WIDE-Nk, and Pascal Sentences. The experimental results show CHTN achieves the best accuracy among N state-of-the-art methods.