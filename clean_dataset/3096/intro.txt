Batch Normalization (BN) _cite_ is considered one of the breakthrough enabling techniques in training deep neural networks. Without it, the gradient in each layer is tightly coupled to all other layers. Should the gradient on any layer be close to zero, then this chokes off the gradient to all subsequent layers during back-propagation of the gradient. This problem is known as vanishing gradients. BN works as follows. The activations from a layer, for a full mini-batch, are passed to the BN function. It calculates the sample mean and standard deviation for this mini-batch. It subtracts this mean and divides by this standard deviation to leave the activations for the mini-batch with a mean of zero and a standard deviation of one. Next, it does the reverse of this step by multiplying by a new standard deviation called _inline_eq_ and adds a new mean called _inline_eq_ . Importantly _inline_eq_ are trainable parameters that exist for each channel of activations in a layer. The net effect is that the distribution of activations of one layer is shifted and expanded/contracted to match the input to the next layer. Generative networks take various forms. _cite_ shows an example of image-in image-out for super-resolution and style transfer. Generative Adversarial Networks (GANs) introduced by _cite_ and Variational Autoencoders (VAE) use a latent vector _inline_eq_ as input with the output being an image. All of these forms of generative networks must at some point constrain the activations to pixel values. For the case of colour images, the network must reduce down to three channels and the values would normally be constrained to be integers in the range _inline_eq_ . To constrain the pixel values to be in an appropriate range the activation of choice is the _inline_eq_ although a _inline_eq_ could also be used. The _inline_eq_ function takes an unbounded real number and constrains it to the real number range _inline_eq_ . However as _inline_eq_ is a non-linear function and we see from Figure _ref_ that inputs outside the range _inline_eq_ will be saturated. When converted to an N-bit image these saturated values will convert to colour values of _inline_eq_ and _inline_eq_ . For most real images the pixel values will be well spread between _inline_eq_ . If the generator network is to produce realistic looking images, it should naturally produce images that have pixel values well spread between _inline_eq_ on the output of the _inline_eq_ . The network previous to the _inline_eq_ should be aiming to produce activations with a mean close to zero and standard deviation close to one, to ensure a good spread of values entering the _inline_eq_ . It is still reasonable for some activations of the _inline_eq_ to saturate. Many real-world colour values will be _inline_eq_ or _inline_eq_ . Placing a BN layer between the final activations and the _inline_eq_ allows the activations earlier in the network to be less constrained. The BN will shift and spread/condense the values to a range that suits the _inline_eq_ function. Indeed we show that the _inline_eq_ may not be optimal and that BN with appropriate _inline_eq_ values may suffice with simple clipping to _inline_eq_, keeping in mind that clipping is a non-linear operation.