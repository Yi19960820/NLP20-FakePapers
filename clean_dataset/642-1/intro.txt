Deep neural networks have established themselves as an important tool in the machine learning arsenal. They have shown spectacular success in a variety of tasks in a broad range of fields such computer vision, computational and medical imaging, signal, image, speech and language processing _cite_ . However, while deep learning models' performance is impressive, the computational and storage requirements of both training and inference are harsh. For example, ResNet-N _cite_, a popular choice for image detection, has N MB parameters and requires N GFLOPs of computations for a single inference. In many cases, the devices do not have such a big amount of resources, which makes deep learning infeasible in smart phones and the Internet of Things (IoT) . In attempt to solve these problems, many researchers have recently came up with less demanding models, often at the expense of more complicated training procedure. Since the training is usually performed on servers with much larger resources, this is usually an acceptable trade-off. One prominent approach is to quantize the networks. The default choice for the data type of the neural networks' weights and feature maps (activations) is N-bit (single precision) floating point. have shown that quantizing the pre-trained weights to _inline_eq_-bit fixed point have almost no effect on the accuracy of the networks. Moreover, minor modifications allow performing an integer-only N-bit inference with reasonable performance degradation _cite_, which is utilized in DL frameworks, such as TensorFlow. One of the current challenges in network quantization is reducing the precision even further, up to N-N bits per value. In this case, straightforward techniques result in unacceptable quality degradation. Contribution. This paper introduces a novel simple approach denoted \uniqname (noise injection and clamping estimation) for neural network quantization that relies on the following two easy to implement components: (i) Noise injection during training that emulates the quantization noise introduced at inference time; and (ii) Statistics-based initialization of parameter and activation clamping, for faster model convergence. In addition, activation clamp is learned during train time. We also propose integer-only scheme for an FPGA on regression task _cite_ . Our proposed strategy for network training lead to an improvement over the state-of-the-art quantization techniques in the performance vs. \complexity tradeoff. Unlike several leading methods, our approach can be applied directly to existing architectures without the need to modify them at training (as opposed, for example, to the teacher-student approaches _cite_ that require to train a bigger network, or the XNOR networks _cite_ that typically increase the number of parameters by a significant factor in order to meet accuracy goals) . Moreover, our new technique allows quantizing all the parameters in the network to fixed point (integer) values. This include the batch-norm component that is usually not quantized in other works. Thus, our proposed solution makes the integration of neural networks in dedicated hardware devices such as FPGA and ASIC easier. As a proof-of-concept, we present also a case study of such an implementation on hardware.