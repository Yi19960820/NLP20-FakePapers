Convolutional neural networks (CNNs) ~ _cite_ have achieved superior performance in various tasks. Besides the discrimination power of neural networks, the interpretability of networks has received an increasing attention in recent years. Motivation, trustiness of CNNs: The network interpretability is directly related to the trustiness of a CNN, which is crucial in critical applications. As mentioned in _cite_, a high testing accuracy cannot fully ensure correct logics in neural networks, owing to the potential bias in datasets and feature representations. Instead, it is common for a CNN to use unreliable reasons for prediction. Traditional studies usually interpreted neural networks at the pixel level, such as the visualization of network features~ _cite_, the extraction of pixel-level correlations between network inputs and outputs~ _cite_ . In contrast to above qualitative analysis of CNNs, our semantically and quantitatively clarifying the logic of each network prediction is a more trustworthy way to diagnose feature representations of neural networks. Fig.~ _ref_ compares our explanation with previous studies. \noindent _inline_eq_ Semantic explanations: We hope to explain the logic of each network prediction using clear visual concepts, instead of using middle-layer features without clear meanings or simply extracting pixel-level correlations between network inputs and outputs. Semantic explanations satisfy specific demands in real applications. \noindent _inline_eq_ Quantitative explanations: In contrast to traditional qualitative explanations for neural networks~ _cite_, quantitative explanations enable people to diagnose feature representations inside neural networks and help neural networks earn trust from people. We expect the neural network to provide the quantitative rationale of the prediction, clarifying which visual concepts activate the neural network and how much they contribute to the prediction score. Figs.~ _ref_ and _ref_ show our explanations for CNN predictions. Predictions whose explanations conflict with people's common sense reflect problematic feature representations inside the CNN. However, above two requirements of ``semantic explanations'' and ``quantitative explanations'' present core challenges of understanding neural networks. To the best of our knowledge, no previous studies simultaneously explained network predictions using clear visual concepts and quantitatively decomposed the prediction score into value components of these visual concepts. Task: In order to explain the specific rationale of each network prediction semantically and quantitatively, in this study, we propose to learn another neural network, namely an explainer network. Accordingly, the target CNN is termed a performer network. Besides the performer, we also require a set of models that are pre-trained to detect different visual concepts. These visual concepts will be used to explain the logic of the performer's prediction. We are also given input images of the performer without any additional annotations on the images. The explainer is learned to uses the pre-trained visual concepts to mimic the logic inside the performer, the explainer uses features of the visual concepts to generate similar prediction scores. As shown in Fig.~ _ref_, the explainer is designed as an additive model, which decomposes the prediction score into the sum of multiple value components. Each value component is computed based on a specific visual concept. In this way, we can roughly consider these value components as quantitative contributions of the visual concepts to the final prediction score. More specifically, we learn the explainer via knowledge distillation. We do not use any ground-truth annotations on input images to supervise the explainer. It is because the task of the explainer is not to achieve a high prediction accuracy, but to mimic the performer's logic in prediction, even when the performer's prediction is incorrect. Thus, the explainer can be regarded as a semantic paraphrase of feature representations inside the performer, and we can use the explainer to understand the logic of the performer's prediction. Theoretically, the explainer cannot precisely recover the exact prediction score of the performer, owing to the limit of the representation capacity of visual concepts. The difference of the prediction score between the performer and the explainer corresponds to the information that cannot be explained by the visual concepts. Explaining black-box networks or learning interpretable networks: Some recent studies~ _cite_ learned neural networks with interpretable middle-layer features. Interpretable neural networks usually have specific requirements for structures~ _cite_ or losses~ _cite_, which limit the model flexibility and applicability. Meanwhile, the interpretability of features is not equivalent to, and usually even conflicts with the discrimination power of features~~ _cite_ . In comparisons, the explainer explains the performer without affecting the original discrimination power of the performer. Compared to forcing the performer to learn interpretable features, our strategy of explaining the performer solves the dilemma between the interpretability and the discriminability. Core challenges: Distilling knowledge from a pre-trained neural network into an additive model usually suffers from the problem of bias-interpreting. When we use a large number of visual concepts to explain the logic inside the performer, the explainer may biasedly select very few visual concepts, instead of all visual concepts, as the rationale of the prediction (see Fig.~ _ref_) . Just like the typical over-fitting problem, theoretically, the bias interpreting is an ill-defined problem. Therefore, we propose new losses for prior weights of visual concepts to overcome the bias-interpreting problem. The prior weights push the explainer to compute a similar Jacobian of the prediction score visual concepts as the performer in early epochs, in order to avoid bias-interpreting. Contributions of this study are summarized as follows. (i) In this study, we focus on a new explanation strategy, semantically and quantitatively explaining CNN predictions. (ii) We propose a new method to explain neural networks, distilling knowledge from a pre-trained performer into an interpretable additive explainer. Our strategy of using the explainer to explain the performer avoids hurting the discrimination power of the performer. (iii) We develop novel losses to overcome the typical bias-interpreting problem. Preliminary experimental results have demonstrated the effectiveness of the proposed method. (iv) Theoretically, the proposed method is a generic solution to the problem of interpreting neural networks. We have applied our method to different benchmark CNNs for different applications, which has proved the broad applicability of our method.