Semantic image search or content based image retrieval is one of the well studied problems in computer vision. Variations in appearance, scale and orientation changes and change in the view point pose a major challenge for image representation. In addition, the huge volume of image data over the Internet adds to the constraint that the representation should also be compact for efficient retrieval. Typically, an image is represented as a set of local features for various computer vision applications. Bag of Words (BOW) model~ _cite_ is a well studied approach for content based image retrieval (CBIR) . The robustness of the SIFT like local features~ _cite_ ~ _cite_ ~ _cite_ to various geometric transformations and applicability of different distance measures for similarity computation have led to a widespread adoption of this framework. The BOW model is inspired from the familiar Bag of Words approach for document retrieval. In this model, an image is represented as a histogram over a set of learned codewords after quantizing each of the image features to the nearest codeword. However, computing the BOW representation for an image is tedious and the computation time grows linearly with the size of the codebook. Although BOW model provides good accuracy for image search, it is not scalable for large image databases, as it is intensive both in memory and computations. In order to have a more informative image representation, Peronnin et al. proposed a generative model based approach called Fisher vector _cite_ . They estimate a parametric probability distribution over the feature space from a huge representative set of features, generally a gaussian mixture model (GMM) . The features extracted from an image are assumed to be sampled independently from this probability distribution. Each feature in the sample is represented by the gradient of the probability distribution at that feature with respect to its parameters. Gradients corresponding to all the features with respect to a particular parameter are summed. The final image representation is the concatenation of the accumulated gradients. They achieve a fixed length vector from a varying set of features that can be used in various discriminative learning activities. This approach considers the _inline_eq_ and _inline_eq_ order statistics of the local image features as well, whereas the BOW captures only the _inline_eq_ order statistics, which is just their count. J gou et al. proposed Vector of Locally Aggregated Descriptors (VLAD) ~ _cite_ as a non-probabilistic equivalent to Fisher vectors. VLAD is a simplified and special case of Fisher vector, in which the residuals belonging to each of the codewords are accumulated. The concatenated residuals represent the image, using which the search is carried out via simple distance measures like _inline_eq_ and _inline_eq_ . A number of improvements have been proposed to make VLAD a better representation by considering vocabulary adaptation and intra-normalization~ _cite_, residual normalization and local coordinate system~ _cite_, geometry information ~ _cite_ and multiple vocabularies~ _cite_ . All these approaches are built on top of the invariant SIFT-like local features. On the other hand, features like GIST~ _cite_ are proposed towards an image representation via a global image feature. Douze et al. ~ _cite_ attempted to identify the cases in which a global description can reasonably be used in the context of object recognition and copy detection. Recently, the deep features extracted from the Convolutional Neural networks (CNN) have been observed _cite_ to show a better performance over the the state-of-the-art for important vision applications such as object detection, recognition and image classification _cite_ . This has inspired many researchers to explore deep CNNs in order to solve a variety of problems~ _cite_ . For obtaining an image representation using CNNs, the mean-subtracted image is forward propagated through a set of convolution layers. Each of these layers contains filters that convolve the outputs from previous layer followed by max-pooling of the resulting feature maps within a local neighborhood. After a series of filtering and sub-sampling layers, another series of fully connected layers process the feature maps and leads to a fixed size representation in the end. Similar to the multi layer perceptron (MLP), the output at each of the hidden units is passed through a non-linear activation function to induce the nonlinearity into the model. Because of the local convolution operations, the representation preserves the spatial information to some extent. For example, Zeiler et al. _cite_ have shown that the activations after the max-pooling of the fifth layer can be reconstructed to a representation that looks similar to the input image. This makes the representation to be sensitive to the arrangement of the objects in the scene. Though max-pooling within each feature map contributes towards the invariance to small-scale deformations~ _cite_, transformations that are more global are not well-handled due to the preserved spatial information in the activations. Intuitively, the fully connected layers, because of their complexity, are supposed to provide the highest level of visual abstraction. For the same reason, almost all the works represent the image content with the activations obtained from the fully connected layers. There is no concrete reasoning provided to comment on the invariance properties of the representations out of the fully connected layers. However, Gong et al. _cite_ have empirically shown that the final CNN representation is affected by the transformations such as scaling, rotation and translation. In their experiments, they report that this inability of the activations has been translated into direct a loss of classification accuracy and emphasize that the activations preserve the spatial information. As another evidence, Babenko et al. ~ _cite_ show that the retrieval performance of the global CNN activations (called `neural codes') increases considerably, when the rotation in the reference (database) images is nullified manually. We have considered two images of the same scene from the Holidays database. We have conducted an experiment to demonstrate the sensitivity of the CNN features to global transformation that results in a different arrangement of the underlying objects. We extracted the _inline_eq_ dimensional CNN feature for both the images after the first fully connected layer (fcN) in the Alexnet, as described in~ _cite_ . Figure _ref_ shows the two images and the corresponding deep features reshaped to a _inline_eq_ matrix. We can clearly observe that the corresponding activations to be very different in both the representations, leading to an inferior similarity between the two images. Gong et al. _cite_ proposed a simple framework called Multi-scale Orderless Pooling (MOP-CNN) towards an orderless representation via pooling the activations from a set of patches extracted at different scales. They resized the image to a fixed size and considered patches at three fixed scales in an exhaustive manner to extract CNN features from each of the patches. The resulting CNN features are VLAD pooled (aggregated) in order to achieve an image representation. Motivated by this approach, we construct an image representation on top of the CNN activations without binding any spatial information. The proposed approach, constructs a novel object based invariant representation on top of the CNN descriptions extracted from an image. We describe the objects present in an image through the deep features and pool them into a representation, as to keep the effective visual content of the image. Generating object proposals is one of the effective solutions in the recent times to achieve computational efficiency for object detection. Our proposed system employs the selective search object proposals scheme developed by Uijlings et al. _cite_ to extract regions from the given image. Selective search relies on a bottom-up grouping approach for segmentation that results in a hierarchical grouping. Hence, it naturally enables to generate object proposals at all scales. After obtaining the object level deep features, constructing an image representation on top of them is another challenge. In order to have a holistic image representation that summarizes the entire visual content, we propose to keep the largest of the activations at each of the output units in the CNN. That means the representation is the max pooled output vector of all the object level CNN activations extracted from an image. We have experimentally observed that the proposed representation consistently outperforms other representations based on deep features (refer section _ref_) . Figure _ref_ shows the overview of the proposed system. The reminder of the paper is organized as follows: In section _ref_, we discuss each of the modules of the proposed retrieval system in detail. In section _ref_, we present the experimental setup using which we evaluate the invariance of the proposed image representation derived from the object level CNN activations and discuss the results. Section _ref_ summarizes our approach with a conclusion and possible expansions.