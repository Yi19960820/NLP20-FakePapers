Predicting depth from a single image is a challenging task and has many applications in ND vision and robotics, such as autonomous driving, planning, obstacle avoidance and Simultaneous localization and mapping (SLAM) . This task is different from the traditional multiple view reconstruction, which uses a set of images of a scene to recover the ND information, mainly considering the appearance matching and geometric constraints among these images. Due to its importance, much effort has been made in this task. As in other computer vision tasks, deep learning approaches have achieved great success. Early works _cite_ formulated single image depth prediction as a supervised learning problem. The difficulty of the supervised method lies in the lack of ground truth depth information. Recent works _cite_ show that view synthesis can be an effective supervisory signal to train the neural network. This removes the requirement of ground truth depth supervisory labels, and makes unsupervised learning for depth estimation possible. In the literature, stereo and monocular videos are explored to train the network _cite_ . Compared to stereo, the monocular video is a broader training source and is easier to capture. However, training on monocular video is more challenging, due to the unknown camera motion, moving objects, and varying lighting conditions. Although current works _cite_ using monocular videos have showed promising results, there still exists significant gap between the results obtained by stereo and monocular training strategies. This paper focuses on unsupervised learning using monocular videos and seeks to reduce this gap. The contributions of the paper are as follows: Bundle Adjustment Framework \quad We introduce a bundle adjustment framework to train the network. It is well known that a large baseline is essential for accurate depth estimation. In SLAM or VO systems _cite_, bundle adjustment is used to jointly optimize a set of camera poses and landmarks, which increases the baseline of a moving camera. Our bundle adjustment framework jointly optimize depths and camera poses within a sequence. Compared to previous works _cite_ that use consecutive frames to generate constraints, our method increases the baseline and introduces more constraints. Super-resolution Network \quad Motivated by super-resolution of image _cite_, we introduce a super-resolution network to generate a high resolution depth map from a low resolution input. In previous works, the image is downsampled and fed into the network. The network produces a low resolution depth map which has the same resolution as the input image. This low resolution depth map is then upsampled to recover the depth of the original image. The interpolation combines nearby depths, which may result in large errors, especially at the boundaries of objects. We solve this problem by introducing a super-resolution network which learns to generate a high resolution depth map. Clip Loss Function \quad We introduce a clip loss to deal with the moving object and occlusion. The supervisory signal of the unsupervised learning framework comes from view synthesis. The resulting loss function is under the assumption of static scene and photometric consistency. One challenge of learning depth from monocular videos lies in the moving object and occlusion, which will violate the static assumption. The large errors coming from these regions will degrade the performance. This paper introduces a clip loss function to deal with this problem. During training, errors higher than a certain percentile will be capped. They will generate zero gradients and will not impact on training. Estimating depth from a single image is a challenging task. This differs from the traditional structure-from-motion (SfM) _cite_ or multi-view stereo (MVS) _cite_, where multiple images are used to recover the depth. A large number of learning based approaches, including both supervised and unsupervised approaches, have been proposed to address the single image depth estimation problem, and great progress has been made in this task. \noindent Supervised Depth Estimation Most supervised approaches formulate the depth estimation problem as a supervised regression problem. In early works _cite_, Markov random field (MRF) with hand-crafted features was trained to estimate the depth. Liu \etal _cite_ introduced semantic labels into the MRF learning. Ladick \'y \etal _cite_ showed that combining semantic labeling and depth estimation can benefit each other. Karsch \etal _cite_ adopted non-parametric sampling for pose estimation. To avoid feature engineering, supervised learning using deep neural networks has been explored. Eigen \etal _cite_ presented a multi-scale deep convolutional neural network (CNN) to predict the depth, which is later extended for depth prediction, surface normal estimation, and semantic labeling _cite_ . Due to the promising results demonstrated by this approach, various deep network structures have been explored to further improve performance, such as combining CNNs with conditional random field (CRF) _cite_, using fully convolutional residual networks with reverse Huber loss _cite_, formulating the depth estimation problem as a pixel-wise classification task _cite_, or jointly learning depth and camera motion from two unconstrained images _cite_ . Recently, Cheng \etal _cite_ proposed a convolutional spatial propagation network to learn the affinity matrix for depth prediction. Besides CNNs, recurrent neural networks (RNN) is also explored to yield spatio-temporally accurate monocular depth prediction _cite_ . In these works, depth sensors are used to produce supervisory signs. The depth information from RGB-D sensors is noisy, and has limited range (generally for indoor scenarios) . On the other hand, the depth measurements from LiDARs are sparse and need an accurate GPS/IMU device to register the LiDAR scan _cite_ . To reduce the requirement of supervisory depth, some works _cite_ showed that relative depth can be used to learn the metric depth. Kuznietsov \etal _cite_ presented a semi-supervised algorithm which combines sparse ground-truth depth and photometric consistency as supervisory signs. Li \etal _cite_ adopted structure-from-motion (SfM) and multi-view stereo (MVS) technology to generate the supervisory ND information. This method is not applicable to scenarios where SfM or MVS fails to work. Ground truth depth is still required to adapt the pretrained model to a specific application. In recent work _cite_, synthetic data with perfect depth were used to train a depth estimation network, and an image style transfer network was trained to convert a real image into the synthetic domain, so that depth could be estimated from real images. \noindent Unsupervised Depth Estimation The Photometric consistency assumption for nearby frames gives a way to avoid the requirement of ground truth depth at training time. Although various costs are proposed for unsuperivsed learning, view synthesis _cite_ is critical in generating self-supervisory signals for unsupervised learning of depth estimation. Specifically, a source and a target image pair are considered at training time. The network yields the depth of the source image, which together with the target image and the pose between the image pair is used to synthesize the source image. The training is conducted by minimizing the error between the real source image and the synthesized one. According to the type of training images, unsupervised approaches can be divided into two categories. The first category considers learning depth from stereo sequences . The left and right images and the known pose of the stereo camera rig form a self-supervisory loop to train the network. Garg \etal _cite_ first applied this self-supervised methodology on stereo image pairs. They used the Taylor expansion to approximate the cost function for gradient computation, which may result in a sub-optimal objective. To solve this problem, Godard \etal _cite_ applied the spatial transformer network _cite_ to yield a differentiable reconstruction cost function. They also enforced geometric constraints during the training by introducing the left-right disparity consistency loss. Poggi \etal _cite_ extended Godard's work _cite_ to trinocular camera system. They introduced an interleaved training procedure to adapt the trinocular network to binocular input. Except for the above photometric reconstruction error of the left and right image pair, the temporal photometric and deep feature reconstruction errors were also exploited to improve the performance in _cite_ . Recent works _cite_ showed that generative adversarial network (GAN) _cite_ paradigm, and supervision from stereo matching network trained by the depth from synthetic data _cite_ can benefit unsupervised depth learning. The second category focuses on learning depth from monocular sequences . Compared to the above case, this is a more challenging problem, as the camera pose is unknown. Zhou \etal _cite_ and Vijayanarasimhan \etal _cite_ showed that it is capable of learning depth prediction and pose estimation at the same time using the supervisory signs from view reconstruction error and spatial smoothness cost. The pose estimation network removes the requirement of stereo training samples. Several recent works explored introducing different constraints during training. The consistency between normal and depth was utilized in _cite_ . The ND point cloud alignment loss was introduced in _cite_ . Depth and optical flow prediction are related tasks. Recent works _cite_ show that jointly learning depth and optical flow can benefit each other. Motivated by current direct visual odometry (DVO) technologies _cite_, Wang \etal _cite_ introduced a differentiable DVO (DDVO) module to replace the previous pose estimation network. They also presented a simple depth normalization strategy to address the scale sensitivity problem cased by the generally used depth regularization term. Godard \etal _cite_ presented several ways to improve the depth estimation, including using a pretrained encoder, sharing lower layers between pose and depth estimator, and training each lower resolution depth map by upsampling them to the input image resolution. Moving objects are another problem for training using monocular sequences. The stereo camera pair capture objects at the same, so the scene satisfies the rigid transformation and the moving object is not problematic. Vijayanarasimhan \etal _cite_ dealt with this problem by introducing an object motion model and an object mask. This method requires knowledge of the number of moving objects in the scene, which is difficult to be estimated. Zhou \etal _cite_ proposed to an explanation mask to get rid of regions undergoing motion and occlusion. However, they later found that this reduced performance. Yin \etal _cite_ introduced a ResFlowNet to learn the residual non-rigid flow which is caused by the moving object. The forward-backward consistency is used in _cite_ to deal with moving objects and occlusions.