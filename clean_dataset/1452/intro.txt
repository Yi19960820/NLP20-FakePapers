For our human beings, vision, and how the brain uses visual information, are learned skills. Meanwhile, the ultimate goal of computer vision research is to teach machines to understand the visual world. But obviously we cannot do it all in the manner of hand over hand, i.e. via empirically man-devised models. It would be more ideal and practicable if we can teach them to learn vision by themselves. This work focuses on the fundamental problem of establishing ND-ND correspondences across a pair of consecutive frames, and notably proves that a solution to this low-level vision problem could be achieved in an unsupervised way by relying only on natural video sequences. Our key insight lies in the understanding that frame interpolation implicitly solves for dense correspondences between the input image pair. It is well known that dense matching can be regarded as a sub-problem of frame-interpolation, as the interpolation could be immediately generated by correspondence-based image warping once dense inter-frame matches are available. It then comes as no surprise that if we were able to train a deep neural network for frame interpolation, its application would implicitly also generate knowledge about dense image correspondences. Retrieving this knowledge is known as analysis by synthesis _cite_, a paradigm in which learning is described as the acquisition of a measurement synthesising model, and inference of generating parameters as model inversion once correct synthesis is achieved. In our context, synthesis simply refers to frame interpolation. We then, for the analysis part, show that the correspondences can be recovered from the network through gradient back-propagation, which produces sensitivity maps for each interpolated pixel. The procedure is summarised in Figure _ref_, explaining how the reciprocal mapping between frame-interpolation and dense correspondences is encoded in the forward and backward propagation through one and the same network architecture. We call our approach MIND, which stands for Matching by INverting a Deep neural network. The key benefit of MIND lies in the fact that the deep convolutional network for frame-interpolation can be trained from ordinary video sequences without any man-made ground truth signals. The training data in our case is given by triplets of images, each one consisting of two input images and one output image that represents the ground-truth interpolated frame. A correct example of a ground truth output image is an image that---when inserted in between the input pair of images---forms a temporally coherent sequence of frames. Such temporal coherency is naturally contained in regular video sequences, which allows us to simply use triplets of sequential images from almost arbitrary video streams for training our network. The first and the third frame of each triplet are used as inputs to the network, and the second frame as the ground truth interpolated frame. Most importantly, since the inversion of our network returns frame-to-frame correspondences, it therefore learns how to do image matching without any requirement for manually designed models or expensive ground truth correspondences. In other words, the presented approach learns image matching by simply ``watching videos''. The paper is organized as follows. Section _ref_ reviews relevant prior work. Section _ref_ explains the present analysis-by-synthesis approach, including both the analysis part of how MIND works and the synthesis part of the deep convolutional architecture for frame interpolation. Section _ref_ demonstrates the surprising performance for the present purely unsupervised learning approach, which is comparable to several traditional empirically designed methods. Section _ref_ finally discusses our contribution and provides an outlook onto future works.