Pedestrian attribute recognition is a high-demanding problem due to its wide applications in person re-identification, person retrieval, and social behavioral analysis. Boosted by the increasing demand, three pedestrian attribute benchmarks ~ _cite_ are released to facilitate the revolution of pedestrian attribute recognition approaches. Inspired by the increasing amount of annotated data, a sequence of deep learning based methods~ _cite_ are proposed with remarkable advantages over previous SVM-based approaches. Most recently-proposed methods formulate pedestrian attribute recognition as a multi-label classification problem, and address it by optimizing a parameter-sharing convolutional network to exploit semantic relations among attributes in an adaptive manner, which achieves a huge success. The performances of these deep learning methods are constrained by several factors. Firstly, the resolution of pedestrian images is not admirable. Specifically, the lowest resolutions of images are _inline_eq_ for RAP and _inline_eq_ for PA-NK. Moreover, the scales of attributes vary from the whole image (e.g. "fat", "thin") to small regions (e.g. "glasses", "shoes") . It could be especially challenging to classify some small attributes due to the information loss caused by down-sampling. Secondly, due to the divergence of the angles and distances between the pedestrians and the surveillance cameras, as well as the imperfection of pedestrian detection algorithms, the pedestrian images have great variation in shapes and viewpoints. The recognition difficulty increases due to the variation in viewpoints, as introduced by Sarfraz \etal~ _cite_ . For example, Figure N (a) shows some examples of backpack recognition. The original images in column (a) show images from three viewpoints, and the backpack appears in hand in the last image. The large variance of relative positions between the pedestrians and these backpacks suggests that it is hard to localize objects only by its relative position regardless of the input image. One intuitive solution to address the problems mentioned above is to apply local feature mining in order to fully explore the information from specific regions. Most existing CNN-based methods use either human part or pose information to extract local features for multiple pedestrian recognition tasks. For part information, one typical solution as introduced in ~ _cite_ is to split the image vertically into upper, middle and lower part, and feed each part into a distinct network for feature extraction. The features are merged by concatenating with a global feature vector learned from the whole image for end-to-end optimization. One problem with these methods is that the splitting of the body part is defined by a fixed, human-specific strategy that can be imprecise. It may lead to truncated parts for some images. To better exploit the human part in the images, pose information is introduced in~ _cite_ that utilize a landmark model for body part localization. While due to the imperfection of human detector quality and pose variations, the missing body part in the image could confuse such models. Another solution is to design a network based on attention models. Liu \etal~ _cite_ introduced a soft attention model that extract and concatenate multi-level global and local feature patterns for attribute classification. The common constraint of the methods mentioned above is that they do not specifically locate attributes in the images, but use human-defined local feature extraction strategies. These methods surpassed ordinary results that only extracts global information, indicating that the exploited local information can boost the recognition of attributes to some extent. To overcome the drawbacks of previous methods and fulfill our expectations, we propose a novel network architecture, referred as Localization Guided Network (LG-Net) as shown in Figure N to fully exploit the attribute-specific local features for attribute predictions. Specifically, we modify the class activation map~ _cite_ to dynamically generate localization result for the accurate localization of attributes. The localization of a certain attribute is further projected to the local feature maps to achieve attribute-wise local feature extraction. The extracted local features are merged with global features in an adaptive manner. Compared with previous methods, our architecture could accurately address the locations of certain attributes, especially object-type attributes, and match the surrounding local features for location-specific attribute recognition. We evaluate our result on the two largest pedestrian datasets by five matrices, showing that our goal is achieved with a significant improvement in attribute prediction result.