Acoustic scene classification (ASC) aims to distinguish between the various acoustical scenes, and effective identification of scenes through the analysis of unstructured patterns has many potential applications, such as, intelligent monitoring systems, context aware devices design and so on. Multiple overlapping sound sources are contained in the acoustic mark of a certain scene, as a result, ASC is of a great challenge despite the sustainable efforts have been made. From the perspective of scene classification, different methods have been tested in the computer vision field, and dramatic progress has been made during last two decades, especially with the improvements of local invariant feature, such as (SIFT _cite_, SURF _cite_) and convolutional neural network _cite_ . Nevertheless, compared to the image-based scene classification, audio-based approach is still under-explored. The state-of-the-art scene-classification methods using audio are not able to provide comparable accuracy with comparison to the image-based methods _cite_ . However, the audio is more descriptive and salient than the images in some practical situations. In the past several years, an increasing interest has been observed, which aims to find more robust and efficient approaches for acoustic scene classification and sound event detection, by using both supervised learning and unsupervised-learning methods. Specifically, the first Detection and Classification of Acoustic Scenes and Events (DCASE) N challenge _cite_ was organized by the IEEE Audio and Acoustic Signal Processing (AASP) Technical Committee, aims to solve the problem of lacking common benchmarking datasets, and has stimulated the research community to explore more efficient methods. Since the release of the relatively larger labeled data, there has been a plethora of efforts made for the audio scene classification task _cite_, _cite_, _cite_ . Recently, deep learning have achieved convincing performance in different fields, ranging from computer vision _cite_ ., speech recognition _cite_ to natural language processing _cite_ . Extensive deep learning architectures have been explored for the audio signal processing, for example, auto-encoder, convolutional neural network, recurrent neural network, and different regularization methods are also tested for the task. Most of the previous attempts aimed to apply the deep learning by modifying the CNN architectures. In this paper, we aim to improve the ASC performance by using the multi-scale DenseNet and culling sample-based regularization method. In more detail, multi-scale DenseNet is employed to extract multi-scale information embedded in the time-frequency of the audio signal. Moreover, unlike previous attempts to dropout hidden layers in the neural network training, we explore the low-variance sample dropout approaches, with the goal to culling the “outliers” in the training samples. After removing the specified samples in the training data set, the neural network classifier is trained with the remaining examples to obtain robust models. Using the DCASE N audio scene classification dataset _cite_, our experimental evaluation shows that the proposed method can improve the robustness of the classifier. The paper is organized as follows: Section N gives a short summary for the related work. Section N presents the data used and the experimental setup, while section N describes the multi-scale DenseNet. The approach to cull samples in the training set is given in Section N. The experimental results are presented in section N, while Section N concludes this paper.