Automatically generating coherent captions for images has attracted remarkable attention for its strong applicability, such as picture auto-commenting _cite_ and helping blind people to see _cite_ . This task is often referred to as image captioning, which combines computer vision, natural language processing and artificial intelligence. Most recent image captioning systems focus on generating an objective, neutral and indicative caption without any style characteristics, which is defined as a factual caption. However, the art of language motivates researchers to generate captions with different styles, which can give people different feelings when focusing on a specific image. The ``style'' can refer to multiple meanings. For example, as shown in Figure~ _ref_, in terms of the fashion of the caption, caption style can be either ``romantic'' or ``humorous''. In addition, in terms of the sentiment it brings to people, caption style can be either ``positive'' or ``negative''. Without doubt, generating such kinds of captions with different styles will greatly enrich the expressibility of the captions and make them more attractive. Ideally, a high-performing stylized image captioning model should satisfy two requirements: N) it generates appropriate stylized words/phrases in appropriate positions of the caption, N) it still describes the image content accurately. Focused on stylized caption generation, existing state-of-the-art work _cite_ _cite_ train their captioning models based on two datasets separately, a large dataset with paired images and ground truth factual captions, and a small dataset with paired images and stylized ground truth captions. From the large factual dataset, the model is learned to generate factual captions that can correctly describe the images; from the small stylized dataset, the model is learned to transform factual captions to stylized captions by incorporating suitable non-factual words/phrases at correct positions of the caption. In the training and predicting process, how to effectively take these two aspects into consideration is paramount for the model to generate high quality stylized captions. To combine and preserve the knowledges learned from both factual and stylized dataset, Gan et al. _cite_ propose a factored LSTM, which factorizes matrix _inline_eq_ into three matrices _inline_eq_, _inline_eq_, _inline_eq_ . _inline_eq_ and _inline_eq_ are updated by the ground truth factual captions while _inline_eq_ is updated by ground truth captions with a specific style. In the predicting process, _inline_eq_, _inline_eq_ and _inline_eq_ are combined to generate the stylized caption. Since _inline_eq_ and _inline_eq_ preserve the factual information and _inline_eq_ preserves the stylized information, the model can thus generate stylized captions that correspond to input images. However, for both the training and predicting processes, factored LSTM cannot differentiate whether paying more attention to the fact-related part (i.e. _inline_eq_ and _inline_eq_) or the style-related part (i.e. _inline_eq_) . It is natural that when the model focuses on predicting a stylized word, it should pay more attention to the style-related part, and vice versa. Mathews et al. _cite_ consider this problem and propose Senticap, which consists of two parallel LSTMs--one updated by the factual captions and one updated by the sentimental captions. When predicting a word, Senticap obtains the result by weighting the predicted word probability distributions of the two LSTMs. However, directly weighting the high level probability distributions can be too ``coarse'' in that it doesn't consider the low level attention effect on stylized and factual elements. In addition, Senticap obtains the weights of the two distributions by predicting the sentiment strength of the current word. In this step, it uses the extra ground truth word sentiment strength label, which is unavailable for other datasets. In this paper, we propose a novel stylized image captioning model. In particular, we first design a new style-factual LSTM as a core building block of our model. Compared with factored LSTM, it combines fact-related and style-related parts of LSTM in a different way and incorporates self-attention for this two parts. More concretely, for both input word embedding feature and input hidden state of LSTM, we assign two independent groups of matrices to capture the factual and stylized knowledges, respectively. At each time step, it feeds an effective attention mechanism to weight the importance of the two groups of parameters based on previous context information, and combines the two groups of parameters by weighted-sum operation. In addition, to help the model preserve factual information while learning from stylized captions, we develop an adaptive learning approach that feeds a reference factual model as a guidance. At each time step, the model can adaptively learn whether to focus more on the ground truth stylized label or on the factual guidance, based on the similarity between the outputs of the real stylized captioning model and the reference factual model. Overall, both improvements help the model capture and combine the factual and stylized knowledge in a better way. In summary, the main contributions of this paper are: