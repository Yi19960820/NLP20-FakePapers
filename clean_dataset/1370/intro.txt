The goal of object segmentation is to produce a pixel level segmentation of different object categories. It is challenging as the objects may appear in various backgrounds and in different visual conditions. \crfs _cite_ model the conditional distribution of labels given observations, and represents the state-of-the-art in image/object segmentation _cite_ . The max-margin principle has also been applied to predict structured outputs, including \ssvm _cite_, and max-margin Markov networks _cite_ . These three methods share similarities when viewed as optimization problems using different loss functions. Szummer \etal cite {SzummerKHN} proposed to learn linear coefficients of \crf potentials using \ssvm and graph cuts. To date, most of these methods assume a pre-defined parametric model for the potential functions, and typically only the linear coefficients of the parametric model are learned. This can greatly limit the flexibility of the model capability of \crf, and thus calls for effective methods to incorporate nonlinear nonparametric models for learning the potential functions in \crf. As similar in standard support vector machines (\svm), nonlinearity can be achieved by introducing nonlinear kernels for \ssvm. However, the time complexity of nonlinear \svm is roughly _inline_eq_ with _inline_eq_ being the number of training data examples. This time complexity is problematic for \ssvm, where the number of constraints grows exponentially in the description length of the label _inline_eq_ . Moreover, nonlinear functions can significantly slow down the test time in most cases. Because of these reasons, currently most \ssvm applications use linear kernels (or linear parametric potential functions in \crf), despite the fact that nonlinear functions usually deliver more promising prediction accuracy. In this work, we address this issue by combining \crf with nonparametric decision trees. Both \crf and decision trees have gained tremendous success in computer vision. Decision trees are capable of modelling complex relations and generalize well on test data. Unlike kernel methods, decision trees are fast to evaluate and can be used to select informative features. In this work, we propose to use ensembles of decision trees to map the image content to both the unary terms and the pairwise interaction values in \crfs. The proposed method is termed as \structens. Specifically, we formulate both the unary and pairwise potentials as forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a single optimization framework. In this way, the nonlinearity is easily introduced into \crf learning without confronting the kernel dilemma. Furthermore, we learn class-wise decision trees for each object. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging in the sense that it can involve exponentially or even infinitely many variables and constraints. We summarize our main contributions as follows. {\bf Related work} We briefly review the recent works that are relevant to ours. A few attempts have been made to apply nonlinear kernels in \ssvm. Yu \etal _cite_ and Severyn \etal _cite_ developed sampled cuts based methods for training \ssvm with kernels. Sampled cuts methods were originally proposed for standard kernel \svm. When applied to \ssvm, the performance is compromised _cite_ . In _cite_, the image-mask pair kernels are designed to exploit image-level structural information for object segmentation. However, these kernels are restricted to the unary term. Although not in the large margin framework, the kernel \crf proposed in _cite_ incorporates kernels into the \crf learning. The authors only demonstrated the efficacy of their method on a synthetic and a small scale protein dataset. To sum up, these approaches are hampered by the heavy computation complexity. Furthermore, it is not a trivial task to design appropriate kernels for structured problems. Recently, Lucchi et al. cite {LucchiN} proposed a two-step solution to tackle this problem. Specifically, they train linear \structsvm by using kernelized feature vectors that are obtained from training a standard non-linear kernel \svm model. They experimentally demonstrate that the kernel transferred linear \svm model achieves similar performance as the Gaussian \svm. However, this approach is heuristic and it cannot be shown theoretically that their formulation approximates a nonlinear \ssvm model. Besides, their method consumes extra usage of memory and training time since the dimension of the transformed features equals to the number of support vectors, while the latter is linearly proportional to the size of the training data _cite_ . Moreover, compared to the above mentioned works of _cite_ and _cite_, we achieve nonlinear learning on both the unary and the pairwise terms while theirs are limited to nonlinear unary potential learning. The recent work of Shen \etal _cite_ generalizes standard boosting methods to structured learning, which shares similarities to our work here. However, our method bears critical differences from theirs: N) We design a column generation method for non-linear tree potentials learning in \crf directly from the \ssvm formulation. Different from the case in _cite_, which can directly derive column generation method analogous to LPBoost _cite_, our derivation here is more challenging. This is because we can not obtain the most violated constraint from the constraints of the dual problem, on which the column generation technique relies. We instead inspect the KKT condition to seek for the most violated constraint. This is an important difference compared to existing column generation techniques. N) We develop a \crf learning method for multi-class semantic segmentation, while _cite_ only shows \crf learning for binary foreground/background segmentation. Our experiments on the MSRC-N dataset shows that our method achieves state-of-the-art results. N) We learn class-wise decision trees (potentials) for each object that appears in the image. This is different from _cite_ . The work of decision tree fields _cite_ is close to ours in that they also use decision trees to model the pairwise potentials. The major difference is that in _cite_ potential functions are constructed by directly summing the energy tables associated with the set of nodes taken during evaluating the decision trees. Their trees are generally deep, with depth N for the unary potential and N for the pairwise potential in their experiment. By contrast, we model the potential functions as an ensemble of decision trees and learn them in the large margin framework. In our method, the decision trees are shallow and simple with binary outputs.