Recent successful advances in object categorization, detection and segmentation have been fueled by high capacity deep learning models (\eg, CNNs) learned from massive labeled corpora of data (\eg, ImageNet _cite_, COCO _cite_) . However, the large-scale human supervision that makes these methods effective at the same time, limits their use; especially for fine-grained object-level tasks such as detection or segmentation, where annotation efforts become costly and unwieldily at scale. One popular solution is to use a pre-trained model (\eg, VGG \_N trained on ImageNet) for other, potentially unrelated, image tasks. Such pre-trained models produce effective and highly generic feature representations _cite_ . However, it has also been shown that fine-tuning with task-specific labeled samples is often necessary _cite_ . Unsupervised learning is one way to potentially address some of these challenges. Unfortunately, despite significant research efforts unsupervised models such as auto-encoders _cite_ and, more recently, context encoders _cite_ have not produced representations that can rival pre-trained models (let alone beat them) . Among the biggest challenges is how to encourage a representation that captures semantic-level (\eg, object-level) information without having access to explicit annotations for object extent or class labels. In the text domain, the idea of local spatial context within a sentence, proved to be an effective supervisory signal for learning distributed word vector representations (\eg, continuous bag-of-words (CBOW) _cite_ and skip-gram models _cite_) . The idea is conceptually simple; given a word tokenized corpus of text, learn a representation for a target word that allows it to predict representations of contextual words around it; or vice versa, given contextual words to predict a representation of the target word. Generalizing this idea to images, while appealing, is also challenging as it is not clear how to N) {\em tokenize} the image (\ie, what is an elementary entity between which context supervision should be applied) and N) apply the notion of context effectively in a N-D real-valued domain. Recent attempts to use spatial context as supervision in vision, resulted in models that used (regularly sampled) image patches as {\em tokens} and either learned a representation that is useful for classifying contextual relationships between them _cite_ or attempted to learn representations that fill in an image patch based on the larger surrounding pixels _cite_ . In both cases, the resulting feature representations fail to perform at the level of the pre-trained ImageNet models. This could be attributed to a number of reasons: N) spatial context may indeed not be a good supervisory signal; N) generic and neighboring image patches may not be an effective {\em tokenization} scheme; and/or N) it may be difficult to train a model with a contextual loss from scratch. Our motivation is similar to _cite_ ; however, we posit that image {\em tokenization} is important and should be done at the level of objects. By working with patches at object scale, our network can focus on more object-centric features and potentially ignore some of the texture and color detail that are likely less important for semantic tasks. Further, instead of looking at immediate regions around the patch for context _cite_ and encoding the relationship between the contextual and target regions implicitly, we look at potentially non-overlapping patches with longer spatial contextual dependencies and explicitly condition the predicted representation on the relative spatial offset between the two regions. In addition, when training our network, we make use of a pre-trained model to extract intermediate representations. Since lower levels of CNNs have been shown to be task independent, this allows us to learn a better representation. Specifically, we propose a novel architecture--Spatial Context Network (SCN)--which is built on top of existing CNN networks and is designed to predict a representation of one (object-like) image patch from another (object-like) image patch, conditioned on their relative spatial offset. As a result, the network learns a spatially conditioned contextual representation of image patches. In other words, given the same input patch and different spatial offsets it learns to predict different contextual representations (\eg, given a patch depicting a side-view of a car and a horizontal offset, the network may output a patch representation of another car; however, the same input patch with a vertical offset may result in a patch representation of a plane) . We also make use of ImageNet pre-trained model as both an initialization and to define intermediate representations. {Once an SCN model is trained (on pairs of patches), we can use one of its two streams as an image representation that can be used for a variety of tasks, including object categorization or localization (\eg, as part of Faster R-CNN _cite_) .} This setting allows us to definitively answer the question of whether spatial context can be an effective supervisory signal--it can, improving on the original ImageNet pre-trained models. Our main contribution is the spatial context network (SCN), which differs from other models in that it uses two offset patches as a form of contextual supervision. Further, we explore a variety of tokenization schemes for mining training patch pairs, and show that an object proposal mechanism is the most effective. This observation validates the intuition that for semantic tasks, context is most useful at the object scale. Finally, we conduct extensive experiments to investigate the capacity of the proposed SCN for capturing context information in images, and demonstrate its ability to improve, in an unsupervised manner, on ImageNet pre-trained CNN models for both categorization (on VOCN and VOCN) and detection (on VOCN), where the bottom stream of the trained SCN is used as a generic feature extractor (see Fig.~ _ref_ (bottom)) .