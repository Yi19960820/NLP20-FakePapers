With the proliferation of mobile devices, it is becoming possible to use mobile perception functionalities (e.g., cameras, GPS, and Wi-Fi) to perceive the surrounding environment _cite_ . Among such techniques, mobile visual search plays a key role in mobile localization, mobile media search, and mobile social networking. However, rather than simply porting traditional visual search methods to mobile platforms, for mobile visual search, one must face the challenges of a large aural-visual variance of queries, stringent memory and computation constraints, network bandwidth limitations, and the desire for an instantaneous search experience. Most research on mobile visual search has predominantly focused on achieving high recognition bitrates _cite_ . Recently, an increasing number of researchers are attempting to exploit feature signatures produced through hashing in mobile visual search because of the good balance that can be achieved among computation and memory requirements, training efficiency, quantization complexity, and search performance. However, most of the existing hashing-based mobile visual search methods attempt to compress existing classical handcrafted features into binary code. These methods all focus on how to decrease the loss suffered during compression. Only a few of them attempt to automatically learn effective binary code features from a large-scale image dataset using a deep neural network. There are two main reasons for this: N) the lack of effective deep learning hashing methods for mobile visual search and N) the high computational complexity of existing deep neural networks. For traditional visual search, convolutional neural networks have become ubiquitous _cite_ _cite_ . Studies have shown that the deep features learned using such networks capture rich image representations and enable better performance than handcrafted features in visual classification _cite_ _cite_ _cite_ _cite_, object detection _cite_ _cite_, semantic segmentation _cite_, and image retrieval _cite_ _cite_ . The general trend in research on deep learning methods is to construct deeper and more complicated networks to achieve higher accuracy _cite_ _cite_ _cite_ . In the field of mobile visual search, however, a deeper neural network model consumes more memory and time, which cannot be easily supplied by mobile devices. To adapt deeper neural networks for mobile devices, a new network architecture called MobileNet has recently been proposed by Google _cite_ . In the MobileNet model, a standard convolutional layer is decomposed into a depthwise convolutional layer and a pointwise convolutional layer, thereby greatly reducing the amount of calculation necessary and the model size. This model works very well on image classification problems. Inspired by the works presented in _cite_ and _cite_, to develop a more effective and efficient mobile visual search system, this paper proposes to combine a few-parameter, low-latency, and high-accuracy network architecture with a hash function. We incorporate the hash function as a latent layer between the image representations and the classification outputs in MobileNet, which allows us not only to maintain the accuracy of the visual search but also to adapt the model to the mobile environment. An overview of the system is illustrated in Figure N. The final binary hash codes can be learned by minimizing an objective function defined over the classification error. Experimental results on a mobile location recognition dataset show that our method achieves superior performance compared with other hashing approaches.