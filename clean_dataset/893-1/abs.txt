Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems. One of the directions the research is heading is the use of neural networks which are adept at estimating complex functions that depend on a large number and diverse source of input data. In this paper we attempt to exploit this effectiveness of neural networks to enable us to perform multimodal emotion recognition on IEMOCAP dataset using data from speech, text, and motions captured from face expressions, rotation and hand movements. Our approach first identifies best individual architectures for classification on each modality and performs fusion only at the final layer which allows for a more robust and accurate emotion detection.