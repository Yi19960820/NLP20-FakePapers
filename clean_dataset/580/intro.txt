An interesting extension of unimodal learning consists of deep models which ``fuse'' several modalities (for example, sound, image or text) and thereby learn a shared representation, outperforming previous architectures on discriminative tasks. However, the cross-modality in existing models using restricted Boltzmann machines~ _cite_, deep Boltzmann machines~ _cite_ and similarity-based loss functions in deep convolutional networks~ _cite_ only occurs after the unimodal features are learned. This prevents the unimodal feature extractors from exploiting any information contained within the other modalities. The work presented in this paper has focused on direct information exchange between the unimodal feature extractors, while deriving more interpretable features, therefore making it possible to directly exploit the correlations between modalities. This information exchange may occur between data of varying dimensionality (for example, ND/ND for audiovisual data) and thus poses a highly nontrivial problem. Cross-connections were previously introduced by {Veli {\v c} kovi {\'c}} et al.~ _cite_ to obtain better performance on sparse datasets (when limited numbers of samples are available) through directly exploiting the heterogeneity of the available features. The cross-connections achieve this by allowing information to be exchanged between hidden layers of neural networks that are each processing a subset of the input data. Each constituent network will consequently learn the target function from exactly one of these subsets. Partitioning the data helps each superlayer achieve better predictive performance by reducing the dimensionality of the input. However, each of the networks is processing data that is compatible with the other networks. To the best of our knowledge, there are no multimodal learning algorithms capable of transforming and exchanging features between learning streams in a modular and flexible manner. As discussed in the previous paragraph, cross-connections have been effective in improving the classification performance on sparse datasets by passing feature maps between constituent networks. We hypothesise that predictive tasks involving multimodal data can benefit from a generalised cross-connection approach, primarily in domains where the different modalities are aligned and highly correlated---for example, in the audiovisual data domain. Our proposed method is motivated by the plentiful existence of correlations in audio and visual streams from speech recordings, which can lead to a stronger joint representation of the corresponding signals. These alignments should be exploited before the feature extraction phase has ended, so devising a generalised method of feature passing between learning streams seems like a natural approach. In this study, we present cross-connections that are capable of feature exchange between N-and N-dimensional signals and can be, in principle, generalised to data types of any dimensionality. We validate their effectiveness in significantly improving model performance on audiovisual classification tasks, showing that cross-modal feature exchanges are beneficial for the learning streams of a multimodal architecture. Our contributions are as follows: