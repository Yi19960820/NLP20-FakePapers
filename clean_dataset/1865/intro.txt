adversarial networks (GANs) are an emerging technique for both semi-supervised and unsupervised learning. They achieve this through implicitly modelling high-dimensional distributions of data. Proposed in N _cite_, they can be characterized by training a pair of networks in competition with each other. A common analogy, apt for visual data, is to think of one network as an art forger, and the other as an art expert. The forger, known in the GAN literature as the generator, _inline_eq_, creates forgeries, with the aim of making realistic images. The expert, known as the discriminator, _inline_eq_, receives both forgeries and real (authentic) images, and aims to tell them apart (see Fig.~ _ref_) . Both are trained simultaneously, and in competition with each other. Crucially, the generator has no direct access to real images-the only way it learns is through its interaction with the discriminator. The discriminator has access to both the synthetic samples and samples drawn from the stack of real images. The error signal to the discriminator is provided through the simple ground truth of knowing whether the image came from the real stack or from the generator. The same error signal, via the discriminator, can be used to train the generator, leading it towards being able to produce forgeries of better quality. The networks that represent the generator and discriminator are typically implemented by multi-layer networks consisting of convolutional and/or fully-connected layers. The generator and discriminator networks must be differentiable, though it is not necessary for them to be directly invertible. If one considers the generator network as mapping from some representation space, called a latent space, to the space of the data (we shall focus on images), then we may express this more formally as _inline_eq_, where _inline_eq_ is a sample from the latent space, _inline_eq_ is an image and _inline_eq_ denotes the number of dimensions. In a basic GAN, the discriminator network, _inline_eq_, may be similarly characterized as a function that maps from image data to a probability that the image is from the real data distribution, rather than the generator distribution: _inline_eq_ . For a fixed generator, _inline_eq_, the discriminator, _inline_eq_, may be trained to classify images as either being from the training data (real, close to N) or from a fixed generator (fake, close to N) . When the discriminator is optimal, it may be frozen, and the generator, _inline_eq_, may continue to be trained so as to lower the accuracy of the discriminator. If the generator distribution is able to match the real data distribution perfectly then the discriminator will be maximally confused, predicting _inline_eq_ for all inputs. In practice, the discriminator might not be trained until it is optimal; we explore the training process in more depth in Section _ref_ . On top of the interesting academic problems related to training and constructing GANs, the motivations behind training GANs may not necessarily be the generator or the discriminator {\em per se}: the representations embodied by either of the pair of networks can be used in a variety of subsequent tasks. We explore the applications of these representations in Section~ _ref_ .