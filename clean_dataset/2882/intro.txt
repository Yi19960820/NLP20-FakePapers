One of the most important steps in the traditional machine learning pipeline is feature extraction. A number of hand-crafted features have been proposed throughout the years that have shown to be robust. Examples can be found in audio analysis where the Mel-Frequency Cepstral Coefficients (MFCC) (Logan (N)), or the Perceptual Linear Prediction (PLP) coefficients (Hermansky (N)) are widely used, but also in computer vision with several popular features such as the Scale Invariant Feature Transform (SIFT) (Ng and Henikoff (N)) and the Histogram of Oriented Gradients (HOG) (Dalal and Triggs (N)) . However, in recent years, Deep Neural Networks (DNN) have gained much popularity due to repeatedly better performing, and often more robust features that can be extracted automatically, as compared to the hand-engineered ones. Additionally, suited networks can provide the capability to be trained in an end-to-end manner, i. \, e., using raw information (Trigeorgis et al. (N)), and the least possible human a-priori knowledge (Graves and Jaitly (N)) . To this end, several deep architectures have been proposed such as Convolutional Neural Networks (CNNs) (Lecun (N)) or Deep Belief Networks (DBNs) (Hinton et al. (N)) . To exploit the potential temporal information in the data, which is often of time series nature, Recurrent Neural Networks (RNN) with memory have been proposed such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber (N)) and the Gated Recurrent Unit (GRU) models (Chung et al. (N)) . Several areas of pattern recognition have benefited from these networks, such as the affective computing and computational paralinguistic domains--e. \, g., for emotion recognition (Kim et al. (N)) . To further increase the performance of such systems, multiple inputs from different modalities, such as audio and video, are often considered (Tzirakis et al. (N)) . In this paper and context, we propose the first of its kind EndNYou tool--the Imperial College London toolkit for multimodal profiling. The toolkit provides models for training and evaluating with either unimodal or multimodal input. The input can be audio, video, audiovisual, or any other kind of modality or sensor data such as physiological data. In addition, the models can be easily combined in any desirable way. This elegantly includes fusion aspects of these heterogeneous information streams.