It has been shown that state-of-the-art deep neural networks (DNNs) often achieve better performance compared with human subjects on large scale classification tasks _cite_ . Given this, we might conclude that the DNNs do a better job of representing and organizing visual data. But does the success of DNNs carry over to other more difficult tasks? Recently it has been discovered that deep neural networks perform poorly in the presence of distortions such as blur and noise _cite_ . Blur removes high frequency information and likewise, noise injects high frequency information. Current deep networks seem to experience difficulty reasoning in the presence of high levels of such distortions. Do humans have a similar trouble with distorted images? By studying the human visual system (HVS), we can perhaps gain insight into how to build DNN models that are more robust to distortions. If human performance on distorted images is better than DNNs, then this exposes a vulnerability in the DNN's representation of visual data. But if human performance is also poor, then recognition under distortions may be inherently difficult. Previous studies have tested human capability for recognition under noise and blur _cite_, finding that humans have some robustness with respect to these distortions. In this work, we wish to compare human ability and DNN ability on a common task. Recent works perform similar experiments with distorted images _cite_ . However, in this work we wish to test the by limiting the stimuli display time to Nms. Within Nms, there is no time for eye movements _cite_, thus the human visual system is limited to more global ``gist'' representations _cite_ . Can the human visual system still recognize distorted images only by the gist? By contrast, the experiments in _cite_ allow the subject to view the image for unlimited duration. This allows the subject to analyze more local information that can help classify the stimuli. Human performance on distorted stimuli has been extensively studied. Torralba \etal _cite_ showed that humans are able to recognize very low resolution images. Similarly, studies on face images show that the human visual system can perform well in the presence of blur _cite_ and noise _cite_ . There are also several works that study deep neural network performance on distorted data. Dodge and Karam~ _cite_ studied several different types of common image distortions and found that noise and blur have the largest effect on the performance of DNNs on the ImageNet dataset. Rodner \etal show similar findings on smaller fine grained datasets~ _cite_ . Comparing human and machine vision performance has also been studied in the past. Borji and Itti _cite_ compare N different computer vision models on several datasets and compare with human performance. However the study does not consider modern neural networks, which greatly outperform older vision models. Fleuret \etal compare human and machine vision performance on synthetic visual reasoning tasks _cite_ . These tasks are designed such that there must be some sort of reasoning, instead of pure pattern recognition. For many of these tasks humans outperform artificial vision systems. A followup study showed that, for some of these synthetic problems, state-of-the-art neural networks surprisingly achieve accuracy equivalent with random chance _cite_ . Parikh _cite_ studies human and artificial vision system performance on jumbled images. These jumbled images are formed by randomly permuting blocks of an image. On these jumbled images human classification performance is degraded to near the performance of a bag-of-words based classifier. While the jumbled images may give some insight into the human visual system, jumbled images are not typical visual stimuli. Kheradpisheh \etal _cite_ compare human and DNN performance on images of objects with arbitrary backgrounds and rotations. The highest performing DNNs match human performance. This is consistent with other studies (e.g., ~ _cite_) that show that DNN classification performance is at-par with or superior to human performance. To further evaluate human vs. DNNs for classification we design our experiments to test highly distorted images. We extend on the work in _cite_, where Amazon Mechanical Turk testing is used to compare human and machine performance on a subset of the ImageNet dataset with added distortions. In this work there are two primary differences. First we use human subjects in a controlled lab setting. This is in contrast to Amazon Mechanical Turk studies where there is no mechanism to control viewing distance, screen brightness, etc. Secondly, we use a fixed display time instead of free viewing, which lets us analyze the accuracy of the HVS without allowing for higher level processes like eye movements. A concurrent independent study _cite_ compares human and deep learning performance on distorted images. However, the study does not fine tune networks on distorted images. This gives an unfair advantage to the human subjects, which may have previously seen distorted images. Additionally, we chose a Nms display time which is less than the Nms display time in _cite_ . This ensures that we are testing early ``gist''-based processes of the visual system.