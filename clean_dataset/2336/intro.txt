Multimedia retrieval has attracted increasing attention in the presence of multimedia big data emerging in search engines and social networks. Cross-modal retrieval is an important paradigm of multimedia retrieval, which supports similarity retrieval across different modalities, e.g. retrieval of relevant images with text queries. A promising solution to cross-modal retrieval is hashing methods, which compress high-dimensional data into compact binary codes and generate similar codes for similar objects _cite_ . To date, effective and efficient cross-modal hashing remains a challenge, due to the heterogeneity across modalities _cite_, and the semantic gap between features and semantics _cite_ . An overview of cross-modal retrieval problems is shown in Figure~ _ref_ . Traditional cross-modal hashing methods _cite_ have achieved promising performance for multimedia retrieval. However, they all require that the heterogeneous relationship between query and database is available for hash function learning. This is a very strong requirement for many practical applications, where such heterogeneous relationship is not available. For example, a user of YahooQA (Yahoo Questions and Answers) may hope to search images relevant to his QAs from an online social media such as ImageNet. Unfortunately, since there are no link connections between YahooQA and ImageNet, it is not easy to satisfy the user's information need. Therefore, how to support cross-modal retrieval without direct relationship between query and database is an interesting problem worth investigation. This paper proposes a novel transitive hashing network (THN) approach to address the above problem, which generates compact hash codes of images and texts in an end-to-end deep learning architecture to construct the transitivity between query and database of different modalities. As learning cross-modal correlation is impossible without any heterogeneous relationship information, we leverage an auxiliary dataset readily available from a different but related domain (such as Flickr.com), which contains the heterogeneous relationship (e.g. images and their associated texts) . We craft a hybrid deep network to enable heterogeneous relationship learning on this auxiliary dataset. Note that, the auxiliary dataset and the query/database sets are collected from different domains and follow different data distributions, hence there is substantial dataset shift which poses a major difficulty to bridge them. To this end, we further integrate a homogeneous distribution alignment module to the hybrid deep network, which closes the gap between the auxiliary dataset and the query/database sets. Based on heterogeneous relationship learning and homogeneous distribution alignment, we can construct the transitivity between query and database in an end-to-end deep architecture to enable efficient heterogeneous multimedia retrieval. Extensive experiments show that our THN model yields state of the art multimedia retrieval performance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.