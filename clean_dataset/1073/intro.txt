Object segmentation is a computer vision tasks which consists in assigning an object class to sets of pixels in an image. This task is extremely challenging, as each object in the world generates an infinite number of images with variations in position, pose, lightning, texture, geometrical form and background. Natural image segmentation systems have to cope with these variations, while being limited in the amount of available training data. Increasing computing power, and recent releases of reasonably large segmentation datasets such as Pascal VOC~ _cite_ have nevertheless made the segmentation task a reality. We rely on Convolutional Neural Networks (CNNs) ~ _cite_, an important class of algorithms which have been shown to be state-of-the-art on large object recognition tasks~ _cite_, as well as on fully supervised segmentation task~ _cite_ . One advantage of CNNs is that they learn sufficiently general features, and therefore they can excel in transfer learning: \eg CNN models trained on the Imagenet classification database~ _cite_ could be exploited for different vision tasks~ _cite_ . Their main disadvantage, however, is the need of a large number of fully-labeled dataset for training. Given that classification labels are much more abundant than segmentation labels, it is natural to find a bridge between classification and segmentation, which would transfer efficiently learned features from one task to the other one. Our CNN-based model is not trained with segmentation labels, nor bounding box annotations. Instead, we only consider a single object class label for a given image, and the model is constrained to put more weight on important pixels for classification. This approach can be seen as an instance of Multiple Instance Learning (MIL) ~ _cite_ . In this context, every image is known to have (or not)--through the image class label--one or several pixels matching the class label. However, the positions of these pixels are unknown, and have to be inferred. Because of computing power limitations, we built our model over the \verb + Overfeat + feature extractor, developed by Sermanet \etal~ _cite_ . This feature extractor correspond to the first layers of a CNN, well-trained over ImageNet. Features are fed into few extra convolutional layers, which forms our ``segmentation network''. Training is achieved by maximizing the classification likelihood over the classification training set (subset of Imagenet), by adding an extra layer to our network, which constrains the model to put more weight on pixels which are important for the classification decision. At test time, the constraining layer is removed, and the label of each image pixel is efficiently inferred. Figure~ _ref_ shows a general illustration of our approach. The paper is organized as follows. Section~ _ref_ presents related work. Section~ _ref_ describes our architecture choices. Section~ _ref_ compares our model with both weakly and fully supervised state-of-the-art approaches. We conclude in Section~ _ref_ .