Over the past few years, deep convolutional neural networks (CNNs) have been very successful in a wide range of computer vision tasks such as image classification _cite_, object detection _cite_ and image segmentation _cite_ . In general, with each passing year, these networks are becoming deeper and deeper with a corresponding increase in the performance _cite_ . However, this increase in performance is accompanied by an increase in the number of parameters and computations. This makes it difficult to port these models on embedded and mobile devices where storage, computation and power are limited. In such cases, it is crucial to have small, computationally efficient models which can achieve performance at par or close to large networks. This practical requirement has led to an increasing interest in model compression where the aim is to either (i) design efficient small networks _cite_ or (ii) efficiently prune weights from existing deep networks _cite_ or (iii) efficiently prune filters from deep convolutional networks _cite_ or (iv) replace expensive floating point weights by binary or quantized weights _cite_ or (v) guide the training of a smaller network using a larger (teacher) network _cite_ . In this work, we focus on pruning filters from deep convolutional neural networks. The filters in the convolution layers typically account for fewer parameters than the fully connected layers (the ratio is N: N for VGG-N _cite_), but they account for most of the floating point operations done by the model (N \% for VGG-N _cite_) . Hence reducing the number of filters effectively reduces the computation (and thus power) requirements of the model. All existing works on filter pruning _cite_ follow a very similar recipe. The filters are first ranked based on a specific criterion such as, _inline_eq_-norm _cite_ or percentage of zeros in the filter _cite_ . The scoring criterion essentially determines the importance of the filter for the end task, typically image classification _cite_ . Only the top-_inline_eq_ ranked filters are retained and the resulting pruned network is then fine tuned. It is observed that when pruning up to N \% of the filters using different proposed criteria, the pruned network almost recovers the original performance after fine-tuning. The claim is that this recovery is due to soundness of the criterion chosen for pruning. However, in this work we argue that this recovery is not due the specific pruning criterion but due to the inherent plasticity of deep CNNs. Specifically, we show that even if we prune filters randomly we can match the performance of state-of-the-art pruning methods. To effectively prove our point, it is crucial that we look at factors/measures other than the final performance of the pruned model. To do so we draw an analogy with the human brain and observe that the process of pruning filters from a deep CNN is akin to causing damage to certain portions of the brain. It is known that the human brain has a high plasticity and over the time can recover from such damages with appropriate treatment _cite_ . In our case, the process of fine-tuning would be akin to such post-damage (post-pruning) treatment. If the injury damages only redundant or unimportant portions of the brain then the recovery should be completed quickly and with minimal treatment. Similarly, we could argue that if the pruning criteria is indeed good and prunes away only unimportant filters then (i) the performance of the model should not drop much (ii) the model should be able to regain its full performance after fine-tuning (iii) this recovery should be fast (i.e, with fewer iterations of fine tuning) and (iv) the quantum of data used for fine-tuning should be less. None of the existing works on filter pruning do a thorough comparison w.r.t. these factors. We not only consider these factors but also present counter-intuitive results which show that a random pruning criteria is comparable to state-of-the-art pruning methods on all these factors. Note that we are not claiming that we can always recover the full performance of the unpruned network. For example, it should be obvious that in the degenerate case if N \% of the filters are pruned then it would be almost impossible to recover. The claim being made is that, at different pruning levels (N \%, N \% or N \%) a random pruning strategy is not much worse than of state-of-the-art pruning strategies. To further prove our point, we wanted to check if such recovery from pruning is task agnostic. In other words, in addition to showing that a network trained for image classification (taskN) can be pruned efficiently, we also show that same can be done with a network trained for object detection (taskN) . Here again, we show that a random pruning strategy works at par with state-of-the-art pruning methods. Stretching this idea further and continuing the above analogy, we note that once the brain recovers from such damages, it is desirable that in addition to recovering its performance on the tasks that it was good at before the injury, it should also be able to do well on newer tasks. In our case, the corresponding situation would be to take a network pruned and fine-tuned for image classification (old task) and plug it into a model for object detection (new task) . Specifically, we show that when we plug a randomly pruned and fine tuned VGG-N network into a Faster RCNN model we can get the same performance on object detection as obtained by plugging (i) the original unpruned network or (ii) a network pruned using a state-of-the-art pruning method. This once again hints at the inherent plasticity of deep CNNs which allows them to recover (up to a certain level) irrespective of the pruning strategy. Finally, we consider the case of class specific pruning which has not been studied in the literature. We note that in many real world scenarios, it is possible that while we have trained an image classification network on a large dataset containing many classes, at test time we may be interested in only a few classes. A case in point, is the task of object detection using the Pascal VOC dataset _cite_ . RCNN and its variants _cite_ use as a sub-component an image classification model trained on all the N ImageNet classes. We hypothesize that this is an overkill and instead create a class specific benchmark dataset from ImageNet which contains only those N classes which correspond to the N classes in Pascal VOC. Ideally, one would expect that a network trained, pruned and fine-tuned only for these N classes when plugged into faster RCNN should do better than a network trained, pruned and fine-tuned on a random set of N classes (which are very different from the classes in Pascal VOC) . However, we observe that irrespective of which of these networks is plugged into Faster RCNN the final performance after fine-tuning is the same, once again showing the ability to recover from unfavorable situations. To the best of our knowledge, this is a first of its kind work on pruning filters which: