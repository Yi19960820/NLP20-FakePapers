Most recent research in biomedical modelling involves qualitative and quantitative classification of a single pixel (voxel), a region of interest ROI and or an image (volume) . These classification tasks mostly involve three main steps-feature extraction, feature selection and classification _cite_ . Out of these three steps, the feature extraction step is the most crucial since it goes a long way to determine how successful the classification task would be. Feature extraction is the process of generating features to be used in the selection and classification tasks _cite_ . In whole image or volume classification, feature extraction and selection can serve as a dimensionality reduction task where a subset of the extracted features is selected to eliminate redundant features while maintaining the underlying discriminatory information _cite_ . The newly extracted features normally are of lower dimension than the original feature space. However, most pixelwise feature extraction task leads to dimensionality extension. That is, a new set of features of high dimension is extracted from a given pixel and its neighbourhood. Feature extraction techniques come mainly in three main flavours-hand crafted texture features, supervised learned features and unsupervised feature extraction. Textures are complex visual patterns composed of entities, or subpatterns, that have characteristic brightness, colour, slope, size, etc _cite_ . The local subpattern properties give rise to the perceived lightness, uniformity, density, roughness, regularity, linearity, frequency, phase, directionality, coarseness, randomness, fineness, smoothness, granulation, etc., of the texture as a whole _cite_ . For a review of texture features, categorization and various uses one can refer to _cite_ . Other groups of hand crafted features are based on differential geometry and the analysis of gradient and Hessian of pixel intensity. These are mostly used as image enhancement to objects of specific shape of interest in a given image. For example in _cite_ The multiscale second order local structure of an image (Hessian) is examined with the purpose of developing a vessel enhancement filter and ultimately a vesselness measure is obtained on the basis of the eigenvalues of the Hessian. This vesselness measure serves as a measure of the likelihood of the presence of geometrical structures which can be regarded as tubular. Also a curvilinear structure detector, called Optimally Oriented Flux (OOF) is proposed by _cite_ for vesselness enhancement. OOF finds an optimal axis on which image gradients are projected in order to compute the image gradient flux _cite_ . The second class of feature extraction techniques are in the form of unsupervised learning and transfer learning. These are mainly autoencoders and its variations like Restricted Boltzmann's Machine. Autoencoders are simple learning circuits which aim to transform inputs into outputs with the least possible amount of distortion _cite_ . For detailed discussion of Autoencoders, Unsupervised learning and deep architectures one can refer to _cite_ . These architectures though very simple are very important in the field of machine learning and form the basic component of deep learning architectures. Architectures like CNN and our deep networks also extract hierarchical features in a supervised manner through the use of ground truth annotations. Szegedy, C et al. _cite_ proposed the inceptions model as a way of building deeper networks capable of learning and extracting dense feature while maintaining acceptable speed and memory usage. This idea has been used in building the GoogLeNET which achieves the state of the art results on image classification task. In this paper we discuss briefly inception models in general and extend the idea to build feature extraction layers based on these inception models in an autoencoder fashion. We will also look at how to stack these pixelwise features extraction layers to form a deep architecture which is then fine tuned for the purpose of supervised learning.