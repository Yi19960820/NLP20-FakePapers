Tremendous progress has been made in the development of reinforcement learning (RL) algorithms for tasks where the input consists of images. For instance, model-free RL techniques~ _cite_ now outperform humans on the majority of the Atari games in the arcade learning environment~ _cite_ . Despite this outstanding performance, RL techniques require a lot more interactions with the environment than human players to perform well on Atari games. This is largely due to the fact that RL techniques do not have any prior knowledge of these games and therefore require a lot more interactions to learn to extract relevant information from images. In contrast, humans immediately recognize salient objects (e.g., spaceships, monsters, balls) and exploit texture cues to guess a strategy~ _cite_ . Within a few interactions with the environment, humans quickly learn the effects of their actions and can easily predict future frames. At that point, humans can focus on learning a good policy. Existing RL techniques for image inputs typically use convolutional neural networks as part of a policy network or Q-network. The beauty of deep RL is that there is no need for practitioners to handcraft features such as object detectors, but this obviously requires more interactions with the environment since the convolutional neural network needs to learn what features to extract for a good policy and/or value function. We propose a new approach that leverages existing work in computer vision to estimate the segmentation and motion of objects in video sequences~ _cite_ . In many games, the position and velocity of moving objects constitute important features that should be taken into account by an optimal policy. More precisely, we describe how to obtain an object mask with flow information for each moving object in an unsupervised fashion based on a modified version of SfM-Net~ _cite_ that is trained on the first N \% of the frames seen by the agent while playing. The encoding part of the resulting network is then used to initialize the image encoder of actor-critic algorithms. This has two benefits: N) the number of interactions with the environment needed to find a good policy can be reduced and N) the object masks can be inspected to interpret and verify the features extracted by the policy. Since moving objects are not the only salient features in games (e.g., fixed objects such as treasures, keys and bombs are also important), we combine the encoder for moving objects with a standard convolutional neural network that can learn to extract complementary features. We showcase the performance of MOREL on all N Atari games where we observe a notable improvement in comparison to ANC and PPO for N and N games respectively, and a worse performance for N and N games respectively. The paper is organized as follows. Sec.~ _ref_ provides some background about reinforcement learning. Sec.~ _ref_ discusses related work in reducing the number of interactions with the environment and improving the interpretability of RL techniques. Sec.~ _ref_ describes our new approach, Motion-Oriented REinforcement Learning (MOREL), that learns to segment moving objects and infer their motion in an unsupervised way as part of a model free RL technique. Sec.~ _ref_ evaluates the approach empirically on N Atari games. Finally, Sec.~ _ref_ concludes the paper and discusses possible future extensions.