Current state-of-the-art object detection performance has been achieved with a fully supervised paradigm. However, it requires a large quantity of high-quality object-level annotations (, object bounding boxes) at training stages _cite_, _cite_, _cite_, which are very costly to collect. Fortunately, the prevalence of image tags allows search engines to quickly provide a set of images related to the target category _cite_, _cite_, making image-level annotations much easier to acquire. Hence it is more appealing to learn detection models from such weakly labeled images. In this paper, we focus on object detection under a weakly supervised paradigm, where indicating the presence of an object are available during training. The main challenge in weakly supervised object detection is how to disentangle object instances from the complex backgrounds. Most previous methods model the missing object locations as latent variables, and optimize them via different heuristic methods _cite_, _cite_, _cite_ . Among them, a typical solution is alternating between model re-training and object re-localization, which shares a similar spirit with Multiple Instance Learning (MIL) ~ _cite_, _cite_, _cite_ . Nevertheless, such optimization is non-convex and easy to get stuck in local minimums if the latent variables are not properly initialized. Then mining object instances with only image-level labels becomes a classical chicken-and-egg problem: without an accurate detection model, object instances cannot be discovered, while an accurate detection model cannot be learned without appropriate object examples. To solve this problem, this paper proposes a learning strategy for weakly supervised object detection, which aims at mining reliable object instances for model training, and meanwhile avoiding getting trapped in local minimums. As our first contribution, different from previous works which perform model training and object re-localization over the entire images all at once _cite_, _cite_, _cite_, we progressively feed the images into the learning model in an easy-to-difficult order _cite_ . To this end, we propose an effective criterion named mean Energy Accumulated Scores (mEAS) to automatically measure the difficulty of an image containing the target object, and progressively add samples during model training. As shown in Fig. _ref_, car and dog are simpler to localize while horse and sheep are more difficult. Intuitively, ignoring this discrepancy of object difficulty in localization would inevitably include many poorly localized samples, which deteriorates the trained model. On the other hand, processing easier images in the initial stages leads to better detection models, which in turn increases the probability of successfully localizing objects in difficult images. Due to lack of object annotations, the mined object instances inevitably include false positive samples. Current approaches _cite_, _cite_ simply treat these pseudo annotations as ground truth, which is suboptimal and easy to overfit the initial seeds. This is especially true for a deep network due to its high fitting capacity. As our second contribution, we design a novel masking strategy over the last convolutional feature maps, which randomly erases the discriminative regions during training. It prevents the model from concentrating on part details at earlier training, and induces the network to focus more on those less discriminative parts at current training. In this way, the model is able to discover more integrated objects as desired. Another advantage is that the proposed masking operation introduces many random occluded samples, which can be treated as data augmentation and enhances the generalization ability of the model. Integrating the progressive learning and masking regularization formulates a zigzag learning process. The progressive learning endeavours to discover reliable object instances in an easy-to-difficult order, while the masking strategy increases the difficulty in a way favorable of object mining via introducing many random occluded samples. These two adversarial modules boost each other, and benefit both object instance mining and reducing model overfitting risks. The effectiveness of zigzag learning has been validated experimentally. On benchmark dataset PASCAL VOC N, we achieve an accuracy of _inline_eq_ under weakly supervised paradigm, which surpasses the-state-of-the-arts by a large margin. To sum up, we make following contributions. _inline_eq_ We propose a new and effective criterion named mean Energy Accumulated Scores (mEAS) to automatically measure the difficulty of an image w.r.t. localizing a specific object. Based on mEAS, we train detection models via an easy-to-hard strategy. This kind of progressive learning is beneficial to finding reliable object instances especially for the difficult images. _inline_eq_ We introduce a feature masking strategy during an model learning, which not only forces the network to focus on less discriminative details during training, but also avoids model overfitting via introducing random occluded positive instances. Integrating these two components gives a novel zigzag learning method and achieves state-of-the-art performance for weakly supervised object detection.