Human activity recognition (HAR) is a classification task for recognizing human movements. It is the core of smart assistive technologies, e.g., in smart-homes, in rehabilitation and health support, and in the industry _cite_ . HAR uses as inputs signals from videos or a set of on-body sensors. This paper covers HAR tasks using multichannel time series signals acquired from a set of on-body sensors. The recognition of human activities is a complicated task due to the large intra-and inter-class variability of human actions. Humans carry out the same tasks in different ways; even, a single person realizes a task differently. Furthermore, HAR is difficult due to the unbalance problem, where there are more samples of certain actions than others. Based on the assumption that body movements present certain patterns, HAR's idea is to classify them with different techniques. Multichannel time-series based HAR uses a combination of signals recorded from different types of sensors, e.g., accelerometers, gyroscopes, magnetometers, and heart rate monitors _cite_ . Usually, segmentation by means of a sliding window, extraction of engineered-features followed by a classification constitute the standard pipeline for recognizing human actions _cite_ . Nevertheless, relevant features are hard to compute, not accurate, and not scalable. Convolutional neural networks (CNNs) have been successfully applied to HAR tasks, unifying conveniently the feature extraction and classification _cite_ . These architectures extract hierarchically the basic and complex features of the human body movements and learn their temporal dependencies. Motivated by the success of attribute representations for image classification _cite_, human actions can be likewise represented by a collection of attributes. These attributes describe semantically and coarsely human actions, i.e., attributes like moving left foot and right foot, forward, and sequential can be taken as the "walking" action _cite_ . Common attributes represent a set of similar human actions. For example, "walking" and "running" could have the movement of the feet as common attributes. The usage of this representation is suitable for recognition tasks where the data is unbalanced or training and testing sets are disjoint, e.g., zero shot learning. As such collection of attributes in the context of multichannel time series-based HAR is not available, we propose learning an attribute representation for HAR using deep architectures. The paper is structured as follows: _ref_ will discuss the related work in the field of multichannel time series based HAR. In _ref_, attribute representations in images and sequences will be introduced. In _ref_ and _ref_, deep architectures and learning of attribute representations are described. Experiments on two datasets, the Opportunity-gestures and locomotion datasets, and the PamapN datasets, will be presented in _ref_ and _ref_ . In the last section, conclusions will be drawn