content based image analysis has obtained popularity in recent years. The most complex part of content based image analysis is scene text recognition which is categorized as a special problem in the field of Optical Character Recognition (OCR) . In OCR, the techniques and methods that have applied on cleaned machine rendered and synthetic images produced desired results. It is considered as a solved problem for most of the scripts. However, due to infancy of scene text recognition, it is struggling towards accuracy particularly in cursive scripts~ _cite_ . The scene image having a text captured from camera has built-in complex noise associated to it. The detection and recognition of text from scene text images is considered as subtle task because there may be non-text elements in an image which should be detected and removed before applying any classification technique on such intrinsic images. We can not process scene text data or printed data in a same way. The techniques and methods that have already been applied on printed and clean scanned data drastically failed on recognition of scene text data. Because captured images do not have only textual information, instead we need to tackle non-text objects and the issue of text appearance in various colors, formats, and sizes that make it harder to apply automated tools to detect and eliminate such irrelevant data. The probable applications of scene text recognition is to assist the visually impaired, number plate recognition, intelligent vehicle driving systems, machine language translation and may provide help in machine reading for robotics systems. The image content depicts the intuitive information, each with a different challenge. Among various challenges the most prominent is orientation and size of a text in a scene image. The scene text recognition has been divided into three phases by most of the authors ~ _cite_ . These stages included text segmentation or localization, text extraction and text recognition. In every phase, intense preprocessing is required to accomplish the task~ _cite_ . In text segmentation or localization, we detect text area in presence of other objects in an image, while extraction means to segment the text carefully so that it may recognized in last stage by OCR technique. It is obvious that OCR will not directly process the video image because as mentioned before the nature of OCR is more towards to process clean document images taken in standard resolution and in specific settings. The video images often has color blending, blur, low resolution and complicated background in presence of different objects. It is hereby assumed that scene text and video text shares same sort of problems and difficulties in the recognition. Most of scene text recognition techniques have been witnessed on Latin or English text. The database plays a vital role in evaluation of state-of-the-art techniques. Some scene text datasets are available for Latin script~ _cite_ . The cursive script is not thoroughly investigated by researchers yet. The availability of benchmark or large size dataset is a fundamental requirement for training and testing the state-of-the-art classifiers in scene text recognition. Therefore, the acquisition of scene text images, development of scene text based database, and its distribution to the researchers for comparison of different techniques and methods is one main focus of attention. We have prepared and compiled Arabic scene text data and consider its subset for evaluation on Convolutional Neural Network (ConvNet) . In this paper, we evaluate the potential of ConvNets on Arabic scene text recognition. The Arabic scene was segmented from captured images. The preprocessing was performed for uniform representation of segmented data before passed them to classifier. We performed experiments on different parameters variations that reveals satisfactory results. The rest of this paper includes related work as presented in Section II. The proposed methodology including feature extraction technique and description about learning classifier and dataset is elaborated in Section III whereas in Section IV, we managed to explain about our experimental parameters and their settings. This section further discuss about learning accuracy and influential parameters. Section V summarized our work under conclusion.