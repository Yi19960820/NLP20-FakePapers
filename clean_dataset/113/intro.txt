Due to the rapid development of computer vision along with the increasing amount of videos, many breakthroughs have been observed on video content analysis in recent years. Videos from realistic scenarios are often complex, which may contain multiple action instances of different categories with varied lengths. This problem leads to a challenging task: temporal action localization, which requires to not only handle the category classification of untrimmed videos but also determine the temporal boundaries of action instances. Nevertheless, it implies the huge amounts of temporal annotations for training an action localization model, which are more labor-intensive to obtain than video-level class labels. Contrary to the fully supervised counterparts, Weakly Supervised Temporal Action Localization (WSTAL) task learns TAL using only video-level class labels, which can be regarded as a temporal version of Weakly Supervised Object Detection (WSOD) in image. A popular series of models in WSOD generate Class Activation Maps (CAMs) _cite_ to highlight the discriminative object regions contributing to the classification results most. Inspired by _cite_, recently many WSTAL works generate the Class Activation Sequence (CAS) to locate the action instances in temporal domain. However, many drawbacks have been observed in this `` '' mechanism: (N) the CAS fails to generate dense detections of target actions, causing many missing detections; (N) the classification network usually leverages features of discriminative rather than entire regions for recognition, failing to handle the action instances with varied lengths; (N) some true negative regions are falsely activated, which is mainly due to the action classifier realizes the recognition task based on a global knowledge of the video, resulting in inevitably neglecting the local details. To address these issues and generate high quality detections, we propose the Cascaded Pyramid Mining Network (CPMN), which adopts two effective modules to mine entire regions of target actions and remove the false positive regions respectively. Specifically, CPMN generates detections in three steps. First, CPMN adapts two classifiers with different input feature maps to discover discriminative regions separately, and the input feature maps of the second classifier are erased with the guidance of the CAS from the first one. Second, CPMN combines the discriminative regions discovered by the two classifiers to form the entire detections. Final, taking full advantage of hierarchical contextual representations, CPMN generates a scale-invariant attention map to correct the false positive regions and reduce the missing detections. These pyramidal feature representations offer `` " context information for better evaluation. The overview of our algorithm is illustrated in Fig. N. To sum up, the main contributions of our work are three-fold: (N) We propose a new architecture (CPMN) for weakly supervised temporal action localization in untrimmed videos, where entire temporal regions of action instances are located with less missing detections. (N) We introduce an Online Adversarial Erasing (OAE) method to discover entire regions of target actions using two cascaded classifiers with different input feature representations, and explicitly handle the action instances with varied lengths by exploiting hierarchical contextual information. (N) Extensive experiments demonstrate that our method achieves the state-of-the-art performance on both THUMOSN and ActivityNet-N datasets.