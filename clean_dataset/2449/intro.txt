Pedestrian attributes, e.g. age, gender, and hair style are humanly searchable semantic descriptions and can be used as soft-biometrics in visual surveillance, with applications in person re-identification _cite_, face verification _cite_, and human identification _cite_ . An advantage of attributes as semantic descriptions over low-level visual features is their robustness against viewpoint changes and viewing condition diversity. However, it is inherently challenging to automatically recognise pedestrian attributes from real-world surveillance images because: (N) The imaging quality is poor, in low resolution and subject to motion blur (Fig.~ _ref_) ; (N) Attributes may undergo significant appearance changes and situate at different spatial locations in an image; (N) Labelled attribute data from surveillance images are difficult to collect and only available in small numbers. These factors render learning a pedestrian attribute model very difficult. Early attribute recognition methods mainly rely on hand-crafted features like colour and texture _cite_ . Recently, deep learning based attribute models have started to gain attraction _cite_, due to deep model's capacity for learning more expressive representations {\em when large scale data is available} _cite_ . However, large scale training data is not available for pedestrian attributes. The two largest pedestrian attribute benchmark datasets PETA _cite_ and RAP _cite_ contain only _inline_eq_ and _inline_eq_ training images, much smaller than the popular ILSVRC (_inline_eq_ million) _cite_ and MS COCO (_inline_eq_) datasets _cite_ . Deep learning of pedestrian attributes is further compounded by degraded fine-grained details due to poor image quality, low resolution and complex appearance variations in surveillance scenes. To address these difficulties, one idea is to discover the interdependency and correlation among attributes _cite_, e.g. two attributes ``female'' and ``skirt'' are likely to co-occur in a person image. This correlation provides an inference constraint complementary to visual appearance recognition. Another idea is to explore visual context as an extra information source to assist attribute recognition _cite_ . For instance, different people may share similar attributes in the same scene, e.g. most skiers wear sun-glasses. However, these two schemes are mostly studied independently by existing methods. In this work, we explore {\em both} modelling inter-person image context and learning intra-person attribute correlation in a unified framework. To this end, we formulate a Joint Recurrent Learning ({\bf JRL}) of attribute correlation and context. We introduce a novel Recurrent Neural Network (RNN) encoder-decoder architecture specifically designed for {\em sequential} pedestrian attribute prediction jointly guided by both {\em intra-person attribute} and {\em inter-person similarity} context awareness. This RNN based model explores explicitly a {\em sequential} prediction constraint that differs significantly from the existing CNN based {\em concurrent} prediction policy _cite_ . We argue that this approach enables us to exploit more latent and richer higher-order dependency among attributes, therefore better mitigating the small training data challenge. Our approach is motivated by natural language sentence prediction which models inter-word relations _cite_ . Importantly, two information sources (intra-person attribute correlations and inter-person image similarities) are {\em simultaneously} modelled to learn person-centric inter-region correlation to compensate poor (or missing) visual details. This provides the model with a more robust embedding given poor surveillance images and learns more accurate intra-person attribute correlations. Crucially, we do not assume people in the same scene share common attributes _cite_, nor assuming person body-part detection _cite_ . Because people appearances in surveillance scenes are without a common theme, and person body-part detection in low resolution images under poor lighting is inconsistent, resulting in many poor detections. More specifically, our approach considers the {\em sequence-to-sequence mapping} framework (with a paired encoder and decoder) _cite_ . To explore a sequence prediction model, we convert any given person image into a region sequence (Fig.~ _ref_ ({b})) and a set of attributes into an ordered list (Fig.~ _ref_ ({c})) . An {\em encoder} maps a {\em fixed-length} image region sequence into a continuous feature vector. The recurrent step is to encode sequentially localised person spatial context and to propagate inter-region contextual information. We call this {\em intra-person attribute context} modelling. Moreover, we also incorporate {\em inter-person similarity context} (Fig.~ _ref_ ({a})) . That is, we identify visually similar exemplar images in the training set, encode them so to be combined with the encoded image by similarity max-pooling. This fused encoding feature representation is used to initialise a decoder. The {\em decoder} transforms the image feature vector from the encoder to a {\em variable-length} attribute sequence as output. This joint sequence-to-sequence encoding and decoding process enables a low-to high-order attribute correlation learning in a unified model. As attributes are weakly-labelled at the image level without fine-grained localisation, we further exploit a data-driven attention mechanism _cite_ to identify attribute sensitive image regions and to guide the decoder to those image regions for feature extraction. The {\bf contributions} of this work are: {\bf (N)} We propose a Joint Recurrent Learning (JRL) approach to pedestrian attribute correlation and context learning in a unified model. This is in contrast to existing methods that separate the two learning problems thus suboptimal _cite_ . {\bf (N)} We formulate a novel end-to-end encoder-decoder architecture capable of jointly learning image level context and attribute level sequential correlation. To our best knowledge, this is the first attempt of formulating {\em pedestrian attribute recognition as a sequential prediction problem} designed to cope with poor imagery data with missing details. {\bf (N)} We provide extensive comparative evaluations on the two largest pedestrian attribute benchmarks (PETA _cite_ and RAP _cite_) against N contemporary models including N pedestrian attribute models (SVM _cite_, MRFrN _cite_, ACN _cite_, DeepSAR and DeepMAR _cite_), a multi-label image classification model (Semantically Regularised CNN-RNN _cite_), and a multi-person image annotation model (Contextual CNN-RNN _cite_) . The proposed JRL model yields superior performance compared to these methods.