During the past decades, taking personal photos in daily life has become easier and more common with the advent of smartphones and digital cameras. Massive amounts of these personal images are uploaded to the internet, mostly through social media. Given that most of the times these images contain people, smart platforms are interested in the organization of identities in these photos. To perform the person recognition task, the question of "what is the identity of this person?" should be answered _cite_ . The first person recognition models were developed based on hand-crafted features and were tested on constrained tiny datasets _cite_ . But, these models cannot be easily applied to the problem of person recognition in photo album settings due to various challenges like occlusion, viewpoint changes, pose variance and low resolution represented by People in Photo Album (PIPA) _cite_ . Sample images of PIPA are shown in Figure _ref_ . There have been numerous studies like _cite_ on person recognition in photo albums. The main ideas are to extract more sophisticated features from or about the input image and to employ more advanced classification methods for learning the relations between the features and identities. Regarding the information sources used in the literature of person recognition task, some studies focus on relational nature of photos in an album belonging to an identity. Perhaps the most obvious way to capture this relation is to extract additional information from the photo. Contextual cues such as clothes, glasses, and surrounding objects, or even metadata like photo location and social relationship of identities, can drastically help the inference about an identity present in a photo album. Extraction, exploitation, and fusion of such information are extensively studied in previous works _cite_ . Moreover, current person recognition methods, like many other image processing techniques, enjoy the informative representations of the input images provided by the convolutional neural networks (CNNs) . Human body parts, other than the face, are another source of information beneficial for identifying a person. As discussed in studies like _cite_, we have observed that relying on facial features in person recognition have shortcomings, specifically in dealing with non-frontal views or cluttered faces, which frequently happens in personal photos. head, upper body and whole body are the main body regions used in many person recognition models. However, the models differ in the way they aggregate the information extracted from these regions. In the early fusion approach _cite_, the feature vectors extracted from different parts are combined to form the final descriptor used for the classification while in the late fusion _cite_, each feature vector is separately classified to form a probability vector on different identities and these initial decision vectors are then aggregated. In this paper, we propose a novel fusion method, called zooming RNN, for combining the evidence extracted from main human body parts; head, upper body, and whole body. The proposed model incorporates both approaches of early-stage decision making based on the evidence obtained from each part and the late identification based on the final aggregated feature vector. To do so, a two-part recurrent neural network is applied to the feature and probability vectors extracted by convolutional neural networks from different regions of the human body. Experimental results on PIPA dataset show the superiority of the proposed model over other fusion mechanisms. The proposed model can be easily generalized to include more contextual information in recognition. The rest of this paper proceeds as follows. After an overview of related works in Section _ref_, we describe and formulate our approach in Section _ref_ . In Section _ref_, the evaluation benchmark, implementation details, experimental procedures and results are presented and compared to other methods. We provide visualizations of our predictions in Section _ref_, and conclude our work in Section _ref_ .