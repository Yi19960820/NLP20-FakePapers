An important component in visual reasoning is the ability to understand the interaction between forces and objects; and the ability to predict the movements caused by those forces. We humans have an amazing understanding of how applied and action-reaction forces work. In fact, even with a static image _cite_, humans can perform a mental simulation of the future states and reliably predict the dynamics of the interactions. For example, a person can easily predict that the couch in Figure~ _ref_ (a) will not move if it is pushed against the wall and the mouse in Figure~ _ref_ (b) will eventually drop if it is pushed towards the edge of a desk. In this paper, we address the problem of predicting the effects of external forces applied to an object in an image. Figure~ _ref_ shows a long-term prediction of the sequence of movements of a cup when it is pushed toward the edge of the table. Solving this problem requires reliable estimates of the scene geometry, the underlying physics, and the semantic and geometric properties of objects. Additionally, it requires reasoning about interactions between forces and objects where subtle changes in how the force is applied might cause significant differences in how objects move. For example, depending on the magnitude of the force, the cup remains on the table or falls. What makes this problem more challenging is the sequential nature of the output where predictions about movements of objects depend on the estimates from the previous time steps. Finally, a data-driven approach to this problem requires a large-scale training dataset that includes movements of objects as their reaction to external forces. Active interaction with different types of scenes and objects to obtain such data is non-trivial. Most visual scene understanding methods (e.g., _cite_) are in that they are focused on predicting the scene structure, the objects, and their attributes and relations. These methods cannot estimate what happens if some parts of the scene are changed actively. For example, they can predict the location or ND pose of a sofa, but they cannot predict how the sofa will move if it is pushed from behind. In this paper, we focus on an setting, where the goal is to predict ``What happens if Force X is applied to Point Y in the scene?'' We design a deep neural network model that learns long-term sequential dependencies of object movements while taking into account the geometry and appearance of the scene by combining Convolutional and Recurrent Neural Networks. The RNN learns the underlying physical rules of movements while the CNN implicitly encodes the appearance and geometry of the object and the scene. To obtain a large number of observations of forces and objects to train this model, we collect a new dataset using physics engines; current datasets in the literature represent static scenes and are not suitable for active settings. Instead of training our model on synthetic images we do the inverse: we replicate all the scenes of SUN RGB-D dataset _cite_ in a physics engine. The physics engine can then simulate forward the effect of applying forces to different objects in each image. We use the original RGB images, the forces, and their associated movements to form our dataset for training and evaluation. Our experimental evaluations show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image. Our model obtains promising results in predicting the direction of the velocity of objects in ND as the result of applying forces to them. We provide results for different variations of our method and show that our model outperforms baseline methods that perform regression and nearest neighbor search using CNN features. Furthermore, we show that our method generalizes to object categories that it has not seen during training.