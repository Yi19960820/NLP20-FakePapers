Accurate ND information of the environment is essential to several tasks in the field of robotics such as navigation and mapping. Current state-of-the-art technologies for robust depth estimation rely on powerful active sensors like Light Detection And Ranging (LIDAR) . Despite the fact that smaller scale solutions as the Microsoft Kinect exist, they are still too heavy when the available payload and power consumption are limited, such as on-board of Micro Air Vehicles (MAVs) . RGB cameras provide a good alternative, as they can be light, small, and consume little power. The traditional setup for depth estimation from images consists of a stereo system. Stereo vision has been vastly studied and is considered a reliable method. For instance, NASA's rover Curiosity was equipped with stereo vision _cite_ to help detecting potential obstacles in the desired trajectory. However, stereo vision exhibits limited performance in regions with low-texture or with repetitive patterns and when objects appear differently to both views or are partly occluded. Moreover, the resolution of the cameras and the distance between them-baseline-also affect the effective range of accurate depth estimation. Monocular depth estimation is also possible. Multi-view monocular _cite_ methods work in a way similar to stereo vision: single images are captured at different time steps and structures are matched across views. However, opposite to stereo, the baseline is not known, which hampers the process of absolute depth retrieval. This is a main challenge in this area and typically relies on additional sensors. \newpage Depth estimation from single still images _cite_-"still-mono"-provides an alternative to multi-view methods in general. In this case, depth estimation relies on the appearance of the scene and the relationships between its components by means of features, such as texture gradients and color _cite_ . The main advantage of still-mono compared to stereo vision is that since only one view is considered, a priori there are no limitations in performance imposed by the way objects appear in the field of view or their disposition in the scene. Thus, single mono estimators should not have problems related with very close or very far objects nor when these are partly occluded. As single still-mono depth estimation is less amenable to mathematical analysis than stereo vision, still-mono estimators often rely on learning strategies to infer depths from images _cite_ . Thus, feature extraction for depth prediction is done by minimizing the error on a training set. Consequently, there are no warranties that the model will be able to generalize well to the operational environment, especially if there is a big gap between the operational and training environments. A solution to this problem is to have the robot learn depth estimation directly in its own environment. In _cite_ a very elegant method was proposed, making use of the known geometry of the two cameras. In essence, this method trains a deep neural network to predict a disparity map that is then used together with the provided geometrical transformations to reconstruct (or predict) the right image. Follow-up studies have obtained highly accurate depth estimation results in this manner _cite_ . In this article, we explore an alternative path to self-supervised learning of depth estimation in which we assume a robot to be equipped already with a functional stereo vision algorithm. The disparities of this stereo vision algorithm serve as supervised targets for training a deep neural network to estimate disparities from a single still image. Specifically, only sparse disparities in high-confidence image regions are used for the training process. The main contribution of this article is that we show that the of the resulting monocular and stereo vision depth estimates gives more accurate results than the stereo vision disparities alone. Fig. _ref_ shows an overview of the proposed self-supervised learning setup.