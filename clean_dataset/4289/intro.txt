Over the past few years, the landscape of computer vision has been noticeably changed from the engineered feature architecture to an end-to-end feature learning architecture, deep neural networks, by which many state-of-the-art work advanced the development of classical tasks such as object detection _cite_, semantic segmentation _cite_, and image retrieval _cite_ . Such a revolutionary change mainly results from several crucial elements, such as big datasets, high-performance hardware, new effective models, and regularization techniques. In this work, we focus on two notable elements, activation function and the corresponding initialization of network. One of known activation functions is Rectified Linear Unit (ReLU) _cite_ which produced profound effect on the development of deep neural networks. ReLU is a piecewise-linear function that keeps positive inputs and outputs zero for negative inputs. Owing to this form, it can alleviate the problem of vanishing gradient, allowing the supervised training of much deeper neural networks. However, it experiences a potential disadvantage that units will never activate once gradients reach zero. Seeing this, Maas _cite_ presented Leaky ReLU (LReLU) where the negative part of activation function is replaced with a linear function. He _cite_ further extended LReLU to a Parametric Rectified Linear Unit (PReLU) which can learn the parameters of the rectifiers, leading to higher classification accuracy with little overfitting risk. In addition, Clevert _cite_ presented the Exponential Linear Unit (ELU), leading to faster learning and better generalization performance than the rectified unit family on deep networks. The above rectified and exponential linear units are commonly adopted by the recent deep learning architectures _cite_ to achieve good performance. However, there exists a gap of representation space between the two types of activation functions. For the negative part, ReLU or PReLU are able to represent the linear function family but not the non-linear one, while ELU is able to represent the non-linear function family but not the linear one. The representation gap to some extent undermines the representational power of those architectures using a particular activation function. In addition, ELU is at a potential disadvantage when used with Batch Normalization _cite_ . Clevert _cite_ showed that using Batch Normalization with ELU could harm the classification accuracy, which is also verified in our experiments. This work is mainly motivated by PReLU and ELU. Firstly, we present a new Multiple Parametric Exponential Linear Unit (MPELU), a generalization of ELU, to bridge the gap. In particular, an extra learnable parameter, _inline_eq_, is introduced into the inputs of ELU to control the shape of negative part. By optimizing _inline_eq_ through stochastic gradient descent (SGD), MPELU is able to adaptively switch between the rectified and exponential linear units. Secondly, motivated by PReLU, we make the hyper-parameter _inline_eq_ of ELU learnable to further improve its representational ability and tune the function shape. This design makes MPELU more flexible than its antecedents, ReLU, PReLU, and ELU that can be seen as special cases of MPELU. Therefore, through learning _inline_eq_ and _inline_eq_, the linear and non-linear space of the negative part can be covered in a single activation function module, whereas its special existing cases do not have this property. The introduction of learnable parameters into ELU may likely bring an additional benefit. This is inspired by the observation that Batch Normalization does not improve ELU networks but can improve ReLU and PReLU networks. To see this, MPELU can be inherently decomposed into a composition of PReLU and learnable ELU: where x is the inputs of activation function, and _inline_eq_ denotes the ELU _cite_ with a learnable parameter _inline_eq_ . Applying Batch Normalization to the inputs gives As we can see, the outputs of Batch Normalization flow into PReLU before ELU, which can result in not only the improvement of the classification performance, but the alleviation of the potential problem of working with ELU. Eqn.~ (_ref_) suggests that MPELU is also able to share the advantages of PReLU and ELU simultaneously, for example, the superior learning behavior of ELU compared to ReLU and PReLU, as described in _cite_ . Our experimental results on CIFAR-N and ImageNet N demonstrate that by introducing the learnable parameters, MPELU networks provide better classification performance and convergence property than its counterparts. Because of the introduction of extra parameters, overfitting could be a concern. To address this, we adopt the same strategy as PReLU to reduce the overfitting risk. For each MPELU layer, _inline_eq_ and _inline_eq_ are initialized as the channel-share version or the channel-wise version. Therefore, the increment of parameters of the entire network is at most twice the total number of channels, which is negligible compared to the number of weights. Although lots of activation functions, e.g., ELU _cite_, were proposed recently, few works determine a weight initialization for networks using them. Improper initialization often hampers the learning of very deep networks _cite_ . Glorot _cite_ proposed an initialization scheme but only considered the linear activation functions. He _cite_ derived an initialization method that considers the rectifier linear units (e.g., ReLU) but not makes allowance for the exponential linear units (e.g., ELU) . Even though Clevert _cite_ applied it to the networks using ELU, this lacks theoretical analysis. Furthermore, none of these works is suitable for non-convex activation functions. Observing this, this paper presents a strategy of weight initialization, enabling the training of networks using exponential linear units including ELU and MPELU, and thus extends the current theory to the wider range. In particular, since MPELU is non-convex, the proposed initialization also applies to non-convex activation functions. The main contributions of this work are: The remainder of this paper is organized as follows. Sec.~ _ref_ reviews the related work. In Sec.~ _ref_, we propose our activation function and initialization method. The experiments and analysis are given in Sec.~ _ref_ to show their effectiveness. Utilizing the proposed methods, Sec.~ _ref_ presents a deep MPELU residual architecture to provide state-of-the-art performance on CIFAR-N/N. Finally, Sec.~ _ref_ concludes. To keep the paper at a reasonable length, the implementation details of our experiments are given in appendix.