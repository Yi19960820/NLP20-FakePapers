With the advancement of technologies, more and more novel devices provide people various visual experiences. Among them, a device providing an immersive visual experience is one of the most popular, including virtual reality devices~ _cite_, augmented reality devices~ _cite_, ND movie systems~ _cite_, and ND televisions~ _cite_ . A common component shared by these devices is the stereo imaging technique, which creates the illusion of depth in a stereo pair by means of stereopsis for binocular vision. To provide more appealing visual experiences, lots of studies strive to apply engrossing visual effects to stereoscopic images~ _cite_ . Neural style transfer is one of the emerging techniques that can be used to achieve this goal. Style transfer is a longstanding problem aiming to combine the content of one image with the style of another. Recently, Gatys \etal~ _cite_ revisited this problem and proposed an optimization-based solution utilizing features extracted by a pre-trained convolutional neural network, dubbed Neural Style Transfer, which generates the most fascinating results ever. Following this pioneering work, lots of efforts have been devoted to boosting speed~ _cite_, improving quality~ _cite_, extending to videos~ _cite_, and modeling multiple styles simultaneously~ _cite_ . However, the possibility of applying neural style transfer to stereoscopic images has not yet been sufficiently explored. For stereoscopic images, one straightforward solution is to apply single-image style transfer~ _cite_ to the left view and right view separately. However, this method will introduce severe view inconsistency which disturbs the original depth information incorporated in the stereo pair and thus brings observers an uncomfortable visual experience~ _cite_ . Here view inconsistency means that the stylized stereo pair has different stereo mappings from the input. This is because single image style transfer is highly unstable. A slight difference between the input stereo pair may be enormously amplified in the stylized results. An example is shown in the second row of Fig. _ref_, where stylized patterns of the same part in the two views are obviously inconsistent. In the literature of stereoscopic image editing, a number of methods have been proposed to satisfy the need of maintaining view consistency. However, they introduce visible artifacts~ _cite_ and require precise stereo matchings~ _cite_, while being computationally expensive~ _cite_ . An intuitive approach is to run single-image style transfer on the left view, and then warp the result according to the estimated disparity to generate the style transfer of the right view. However, this will introduce extremely annoying black regions due to the occluded regions in a stereo pair. Even if filling the black regions with the right-view stylized result, severe edge artifacts are still inevitable. In this paper, we propose a novel dual path convolutional neural network for the stereoscopic style transfer, which can generate view-consistent high-quality stylized stereo image pairs. Our model takes a pair of stereoscopic images as input simultaneously and stylizes each view of the stereo pair through an individual path. The intermediate features of one path are aggregated with the features from the other path via a trainable feature aggregation block. Specifically, a gating operation is directly learned by the network to guide the feature aggregation process. Various feature aggregation strategies are explored to demonstrate the superiority of our proposed feature aggregation block. Besides the traditional perceptual loss used in the style transfer for monocular images~ _cite_, a multi-layer view loss is leveraged to constrain the stylized outputs of both views to be consistent in multiple scales. Employing the proposed view loss, our network is able to coordinate the training of both the paths and guide the feature aggregation block to learn the optimal feature fusion strategy for generating view-consistent stylized stereo image pairs. Compared against previous methods, our method can produce view-consistent stylized results, while achieving competitive quality. In general, the main contributions of our paper are as follows: