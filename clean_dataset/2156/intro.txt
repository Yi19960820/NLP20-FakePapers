Mobile and Internet-of-Things (IoT) ~ _cite_ devices are increasingly relying on Artificial Intelligence (AI) engines to enable sophisticated applications such as personal digital assistants~ _cite_, self-driving vehicles, autonomous drones, smart cities, and so on. The AI engines themselves are generally built on deep learning models. The most common way of deploying such models is to place them in the cloud and have the sensor data (images, speech, etc.) uploaded from the mobile to the cloud for processing. This is referred to as the approach. More recently, with smaller graphical processing units (GPUs) making their way into mobile/IoT devices, some deep models might be able to run on the mobile device, an approach referred to as . A recent study~ _cite_ has examined a spectrum of possibilities in between the cloud-only and mobile-only extremes. Specifically, they considered splitting a deep network into two parts: the front end (consisting of an input layer and a number of subsequent layers), which runs on the mobile, and the back end (consisting of the remaining layers), which runs on the cloud. In this approach, termed, the front end computes features up to some layer in the network, then these features are uploaded to the cloud for the remainder of the computation. The authors examined the energy consumption and latency associated with performing computation in this way, for various split points in typical deep models. Their findings indicate that significant savings can be achieved in both energy and latency if the network is split appropriately. They also proposed an algorithm called to find the optimal split point, depending on whether energy or latency is to be minimized. The reason why collaborative intelligence can be more efficient than cloud-only and mobile-only approaches is that the feature data volume in deep convolutional neural networks (CNNs) typically decreases as we move from the input to the output. Executing initial layers on the mobile will cost some energy and time, but if the network is split appropriately, we will end up with far less data to be uploaded to the cloud, which will save both transmission latency on the uplink and the energy used for radio transmission. Hence, on the balance, there may be a net benefit in energy and/or latency. Based on~ _cite_, depending on the resources available (GPU or CPU on the mobile, speed and energy for wireless transmission, etc.), optimal split points for CNNs tend to be deep in the network. A recently released study~ _cite_ has extended the approach of ~ _cite_ to include model training and additional network architectures. While the network is again split between the mobile and the cloud, in the framework proposed in~ _cite_ the data can move both ways between the mobile and the cloud in order to optimize efficiency of both training and inference. While~ _cite_ have established the potential benefits of collaborative intelligence, the issue of efficient transfer of feature data between the mobile and the cloud is largely unexplored. Specifically, ~ _cite_ does not consider feature compression at all, while~ _cite_ uses N-bit quantization of feature data followed by lossless compression, but does not examine the impact of such processing on the application. Feature compression can further improve the efficiency of collaborative intelligence by minimizing the latency and energy of feature data transfer. The impact of compressing the input has been studied in several CNN applications~ _cite_ and the effects vary from case to case. However, the impact of feature compression has not been studied yet, to our knowledge. In this work, we focus on a deep model for object detection and study the impact of feature compression on its accuracy. Section~ _ref_ presents preliminaries, while Section~ _ref_ describes the proposed methods. Experimental results and conclusions are presented in Sections~ _ref_ and~ _ref_, respectively.