As neural networks have become more powerful, an increasing number of studies have sought to decipher their internal representations _cite_ . Most of these have focused on the role of individual units in the computations performed by individual networks. Comparing population representations across networks has proven especially difficult, largely because networks converge to apparently distinct solutions in which it is difficult to find one-to-one mappings of units _cite_ . Recently, _cite_ applied Canonical Correlation Analysis (CCA) as a tool to compare representations across networks. CCA had previously been used for tasks such as computing the similarity between modeled and measured brain activity _cite_, and training multi-lingual word embeddings in language models _cite_ . Because CCA is invariant to linear transforms, it is capable of finding shared structure across representations which are superficially dissimilar, making CCA an ideal tool for comparing the representations across groups of networks and for comparing representations across time in RNNs. Using CCA to investigate the representations of neural networks, we make three main contributions: