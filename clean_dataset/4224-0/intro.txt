Convolutional Neural Networks (ConvNets) has shown phenomenal results _cite_ for many computer vision applications. With ConvNets, the object detection tasks have been practiced in a more accurate model better than ever. Existing detection algorithms aim to detect a predefined object category from the given image. As a result, detection based retrieval systems usually take the target object name as the query, which largely ignore the context information within an image. In the real world, however, the rich information that a user is searching could be more than what a single object name can describe. Compared to object names, language description contains context information such as the relative location of the object, \eg, ``the book on your left-hand side '', or a specific part of an object, \eg, `` his face ''. With language descriptions, one can even specify detailed attributes of the object of interest, \eg ``the man in middle jeans and T-shirt ''. Therefore, natural language provides users with more powerful tools than the scheme of adopting object name as the query. In this work, we propose a new method of natural language object retrieval. The goal is to localize a referred object in an image or a set of images according to a language description, which can be interpreted as a new type of cross-media retrieval~ _cite_ . A typical and straightforward way is to divide the task into two non-overlapping phases. In the first phase, a set of object region proposals are generated as which has been done in~ _cite_ . If the algorithm uses handcrafted features, \eg, EdgeBoxes~ _cite_, the quality of proposals may not be good. _cite_ and _cite_ use the Fast R-CNN~ _cite_ and SSD~ _cite_ to generate object detection results as object proposals. However, this type of approaches rely heavily on the training data of object proposals and are restricted to the predefined object categories. As a result, these algorithms~ _cite_ can only deal with the predefined objects and are not extendable to natural language queries containing new objects and complex reasoning of relative location. In the second phase, these methods adopt a ranking function to locate the region which best matches the description. The limitation of this kind of method is that the two critical phases are conducted independently. In this case, the training process is not well aligned, leading to suboptimal solution for the retrieval task. Furthermore, those approaches usually rely on a large number of proposals to guarantee a satisfactory recall for the target object, which drastically increases redundancies and degrades the discriminative performance of the ranking function. Inspired by the recent successes of deep reinforcement learning _cite_, we propose to train a neural network for natural language object retrieval in an end-to-end manner. As illustrated in Figure~ _ref_, our method adopts a top-down approach to localize the referred object. Specifically, we define different actions for an agent to change the shape and location of a bounding box. The ``agent'' takes one of those predefined actions according to the spatial and temporal context, the local image feature as well as the natural language a priori at each time step, until an optimal result is reached, \ie, the agent takes a special action (denoted to as a ``trigger'') and stops the process. It is worthwhile highlighting the following aspects of the proposed method. First, our approach performs natural language object retrieval in an end-to-end manner without the pre-computation of proposals, which could be very noisy and redundant. Different from our approach, existing natural object retrieval methods _cite_ either use handcrafted features or ConvNet features to generate proposals in the first phase. The performance of handcrafted features is comparatively poor. On the other hand, the ConvNet based detectors can only deal with a limited number of predefined object categories. Our end-to-end approach exploits language information and visual information in a joint framework, thereby being able to leverage the mutual benefits of the two inputs for training. Moreover, our approach also avoids the non-trivial task of tuning the number of proposals. Instead, the network decides to stop searching the object by selecting the ``trigger'' action, thus it constructs a dynamic length search procedure per query. Second, our approach generates a series of ``experiences'' to better use the training information under the deep reinforcement learning paradigm~ _cite_ . Image-level context information is complementary to local information within a bounding box~ _cite_ . This context is presumably important in natural language object retrieval, especially when the language description contains relative locations. Therefore, we propose to use image-level ConvNet representation as spatial context, and explicitly encode such information into the ``experience''. Further, a Recurrent Neural Network (RNN) is added into the policy and value networks to track the temporal context, \ie, the history states that the agent has encountered. This temporal context would help the agent avoid entering similar mistakes in the previous time steps. The existing approaches~ _cite_ merely use the labeled images for training. The ``context-aware experience" in our algorithm is generated at each time step after the ``agent'' takes action. The number of ``context-aware experience" is greater than the number of labeled images, and have more diversified information. Meanwhile, as shown in Figure~ _ref_, the difference between the bounding boxes at state _inline_eq_ and state _inline_eq_ is subtle but has different IoU values with the ground truth. These subtle differences are also encoded in the ``experience''. In that way, our method is able to exploit subtle changes of bounding boxes for a better result. Third, environment state, agent action and reward function are three key factors for reinforcement learning~ _cite_ . Different from a typical deep reinforcement learning scenario, \eg, game playing, computer vision tasks have no well-defined reward function provided by the environment. To address this issue, we define a simple yet effective reward function for the agent. In addition, a potential based reward strategy is also adopted to improve the training speed. Besides, the visual content of an environment are quite similar in game playing scenario. For example, An Atari game~ _cite_ has less diversified visual information. Our task is very different because the environment states presented to the agent keep changing dramatically, \ie, natural language queries and images can be very different from one to another. We take advantage of this diversity nature by paralleling a series of agents and environments when collecting experiences in training as practiced in _cite_ .