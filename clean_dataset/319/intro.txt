Deep learning has achieved significant recent successes. However, large amounts of training samples, which sufficiently cover the population diversity, are often necessary to produce high quality results. Unfortunately, data availability in the medical image domain, especially when pathologies are involved, is quite limited due to several reasons: significant image acquisition costs, protections on sensitive patient information, limited numbers of disease cases, difficulties in data labeling, and large variations in locations, scales, and appearances. Although efforts have been made towards constructing large medical image datasets, options are limited beyond using simple automatic methods~, huge amounts of radiologist labor~, or mining from radiologist reports~ . Thus, it is still an open question on how to generate effective and sufficient medical data samples with limited or no expert-intervention. One enticing alternative is to generate synthetic training data. However, historically synthetic data is less desirable due to shortcomings in realistically simulating true cases. Yet, the advent of generative adversarial networks (GANs) ~ has made game-changing strides in simulating real images and data. This ability has been further expanded with developments on fully convolutional~ and conditional~ GANs. In particular, Isola extend the conditional GAN (CGAN) concept to predict pixels from known pixels~ . Within medical imaging, Nie use a GAN to simulate CT slices from MRI data~, whereas Wolterink introduce a bi-directional CT/MRI generator~ . For lung nodules, Chuquicusma train a simple GAN to generate simulated images from random noise vectors, but do not condition based on surrounding context~ . In this work, we explore using CGAN to augment training data for specific tasks. For this work, we focus on pathological lung segmentation, where the recent progressive holistically nested network (P-HNN) has demonstrated state of the art results~ . However, P-HNN can struggle when there are relatively large (_inline_eq_) peripheral nodules touching the lung boundary. This is mainly because these types of nodule are not common in Harrison 's~ training set. To improve P-HNN's robustness, we generate synthetic ND lung nodules of different sizes and appearances, at multiple locations, that naturally blend with surrounding tissues (see \Fig~ _ref_ for an illustration) . We develop a ND CGAN model that learns nodule shape and appearance distributions directly in ND space. For the generator, we use a U-Net-like~ structure, where the input to our CGAN is a volume of interest (VOI) cropped from the original CT image with the central part, containing the nodule, erased (\Fig~ _ref_ (c)) . We note that filling in this region with a realistic nodule faces challenges different than generating a random ND nodule image from scratch~ . Our CGAN must generate realistic and natural ND nodules conditioned upon and consistent with the surrounding tissue information. To produce high quality nodule images and ensure their natural blending with surrounding lung tissues, we propose a specific multi-mask reconstruction loss that complements the adversarial loss. The main contributions of this work are: (N) we formulate lung nodule generation using a ND GAN conditioned on surrounding lung tissues; (N) we design a new multi-mask reconstruction loss to generate high quality realistic nodules alleviating boundary discontinuity artifacts; (N) we provide a feasible way to help overcome difficulties in obtaining data for "edge cases" in medical images; and (N) we demonstrate that GAN-synthetized data can improve training of a discriminative model, in this case for segmenting pathological lungs using P-HNN~ .