Image annotation is a task to associate multiple semantic tags regarding to the contents of images. With the rapid development of Internet and social web applications, the amount of online images created by users is continuously increasing. The large amount of images brings a heavy burden for image management and retrieval. Since the major approaches for people to search or to index images are through referring to the associated tags, it is a necessary step to annotate these images with proper tags. However, manually annotating images is an expensive and labor intensive work for human beings. Hence it is better if we can learn a model from available image-tag samples and use the model to automatically label new images with keywords (tags) from the annotation vocabulary. In fact, this kind of technique is called automatic image annotation (AIA) ~ _cite_, which has been an important research topic in computer vision for decades. Previous researches focus on learning the image-to-tag correlation as well as tag-to-tag correlation to improve the annotation performance. Although much progress has been made in the research community, most of the existing methods overlooked a fundamental philosophy of recognition: recognizing the right things. A common conventional evaluation setting has a fixed annotation length _inline_eq_, and a typical _inline_eq_ value _inline_eq_ has been used in many previous methods~ _cite_ for the ease of comparison. However, we argue that this convention can be insufficiency in previous work, since it is not the normal way that we humans annotate images, and the assumption of fixed annotation length is not the fact of realistic images either, as shown in Figure _ref_ . Therefore arbitrary length annotation is required for more reasonable annotation results. For top-_inline_eq_ predictions, traditional methods simply select the _inline_eq_ tags with highest prediction scores. For arbitrary length annotation, it is possible to easily imagine a naive extension that is to threshold the prediction scores. However, finding a good threshold is more difficult than merely setting a hyper-parameter as we might expect, because the optimal threshold can actually be dependent on each different image. Instead of struggling to find the appropriate threshold, we want to import an explicit mechanism to model the annotation length, for which we originally form the image annotation task as a sequence generation problem. Therefore we propose a novel model called Recurrent Image Annotator (RIA) that jointly uses Convolutional Neural Networks and Recurrent Neural Networks (RNN) for predicting tag sequences. In the annotation phase, we just use an image as the initial input of RIA and then it will automatically generate annotation tags one by one, as shown in Figure _ref_ . The idea is inspired by recent success of RNN in machine translation~ _cite_, and especially in image captioning~ _cite_, where the task is to generate natural language sentences from images. The advantages of using RNN do not only include its nature to generate varied length outputs, but also its ability to refer to previous inputs when predicting the current time step output. Such ability allows RNN to exploit the correlations of both image-to-tag and tag-to-tag. Now we have a CNN to extract image visual features, and an RNN to generate the tag sequence from the visual features, what do we need next? The answer is: an order. Both machine translation and image captioning aim to generate sentences, which have a natural order available for the RNN model to learn from. Unfortunately, in our image annotation task, there is no natural order available. Instead, we have to choose or learn an order to make our proposed model actually work. Just like sentences obey the language rules to form the order, we believe that there exist intrinsic ``language rules" for tags to form an order to describe an image. There are two points for an order to be good in our task. First, the order ``rule" should be based on semantic image and tag information. Second, tag sequences in each training example should follow the same rule to be sorted, since only in this way can the model learn the ``rule" from the training examples, and further generalize the prediction on the test images. To facilitate the training of our model as well as testing the importance of tag orders, we propose several strategies to provide tag orders. And we compare the performance of our model with different tag orders in the experiments. The main contributions of our work are as follows: