Deep convolutional neural networks has recently achieved significant progress and have become the gold standard for object recognition, image classification, and retrieval. Almost all of the recent successful recognition systems are built on top of this architecture. Importing CNN onto embedded platforms, the recent trend toward mobile computing, has a wide range of application impacts. It is especially useful when the bandwidth is limited or photos are not allowed to be sent to servers. However, the size of the CNN models are typically very large (e.g. more than NM bytes), which limits the applicability of such models on the embedded platform. For example, it will be almost impossible for users to download an iPhone application with more than NM. Thus, in order to apply neural network methods to embedded platforms, one important research problem is how to compress parameters to reduce storage requirements. In this article we mainly consider compressing CNNs for computer vision tasks. For example, a typical CNN that works well for object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters (e.g. _inline_eq_) in order to produce state-of-the-art results. Because as is widely known, the parameters are heavily over-parameterized, it is very interesting to investigate whether we can compress these parameters by exploring their structure. Here, we are mainly interested in compressing the parameters to reduce storage instead of speeding up the testing time . For a typical network described in, about N \% of the storage is taken up by the dense connected layers; more than N \% of the running time is taken by the convolutional layers. Therefore, we shall focus upon how to compress the dense connected layers to reduce storage of neural networks. A few early works on compressing CNNs have been published; however, their focus is different from ours. The most closely related one, _cite_, explored matrix factorization methods for speeding up CNN testing time. These researchers showed that by exploring the linear structure of CNN parameters (in particular, convolutional layers), CNN testing time can be sped up by as much as N \% while keeping the accuracy within N \% of the original model. Another similar work on speeding up CNN is, in which the authors described several reconstruction methods for approximating the filers in convolutional layers. Their goal of both works is complimentary to ours, in that they focus on compressing convolutional layers for speeding up CNN. Our focus, however, is on compressing dense connected layers in order to reduce the size of the model. In this work, instead of the traditional matrix factorization methods considered in, we mainly consider a series of information theoretical vector quantization methods for compressing dense connected layers. For example, we consider binarizing the parameters, scalar quantization using _inline_eq_ means, and structured quantization using product quantization or residual quantization. Surprisingly, we have found that simply applying _inline_eq_ means-based scalar quantization achieves very impressive results, and that this approach is better than matrix factorization methods. Structured quantization methods can give additional gain by exploring redundancy in the parameters. To our knowledge, this is the first work to systematically study different vector quantization methods for compressing CNN parameters. This paper makes the following contributions: N) We are among the first to systematically explore vector quantization methods for compressing the dense connected layers of deep CNNs to reduce storage; N) We have performed a comprehensive evaluation of different vector quantization methods, which has shown in particular that structured quantization such as product quantization works significantly better than other methods; N) We have performed experiments on other tasks such as image retrieval, to verify the generalization ability of the compressed model.