During the last decade, the Internet has become an increasingly important distribution channel for videos. Video hosting services like YouTube, Flickr, and Vimeo have millions of users uploading and watching content every day. At the same time, powerful search methods have become essential to make good use of such vast databases. By analogy, without textual search tools like Google or Bing, it would be nearly hopeless to find information from the websites. Our objective is to study the problem of retrieving video clips from a database using natural language queries. In addition, we consider the analogous problem of retrieving sentences or generating descriptions based on a given video clip. We are particularly interested in learning appropriate representations for both visual and textual inputs. Moreover, we intend to leverage the supporting information provided by the current image search approaches. This topic has recently received plenty of attention in the community, and papers have presented various approaches to associate visual and textual data. One direction to address this problem is to utilize metadata that can be directly compared with queries. For instance, many web image search engines evaluate the relevance of an image based on the similarity of the query sentence with the user tags or the surrounding HTML text _cite_ . For sentence retrieval, Ordonez \etal~ _cite_ proposed to compare an image query and visual metadata with sentences. While these methods using comparable metadata have demonstrated impressive results, they do not perform well in cases where appropriate metadata is limited or not available. Moreover, they rely strongly on the assumption that the associated visual and textual data in the database is relevant to each other. These problems are more apparent in the video retrieval task since video distribution portals like YouTube often provide less textual descriptions compared to other web pages. Furthermore, available descriptions (e.g. title) often cover only a small portion of the entire visual content in a video. An alternative approach would be to compare textual and visual inputs directly. In many approaches, this is enabled by embedding the corresponding representations into a common vector space in such a way that the semantic similarity of the original inputs would be directly reflected in their distance in the embedding space (Fig. _ref_) . Recent work _cite_ has proposed deep neural network models for performing such embeddings. The results are promising, but developing powerful joint representations still remains a challenge. In this paper, we propose a new embedding approach for sentence and video inputs that combines the advantages of the metadata-based web image search and deep neural network-based representation learning. More precisely, we use a standard search engine to obtain a set of supplementary images for each query sentence. Then, we pass the sentence and the retrieved images to a two-branch neural network that produces the sentence embedding. The video inputs are embedded into the same space using another neural network. The network parameters are trained jointly so that videos and sentences with similar semantic content are mapped to close points. Figure _ref_ illustrates the overall architecture of our approach. The experiments indicate a clear improvement over the current state-of-the-art baseline methods. Our main contributions are as follows: