The need for manually labelled data continues to be one of the most important limitations in large scale object/scene recognition. Recently, use of visual attributes have become attractive as being helpful in describing properties shared by multiple categories and resulting in novel category recognition. owever, most of the methods require learning of visual attributes from labelled data, and cannot eliminate human effort. Yet, it may be more difficult to describe an attribute than an object, and localisation may not be trivial. Alternatively, images tagged with attribute names are available on the web in large amounts. However, data collected from web inherits all type of challenges due to illumination, reflection, scale, and pose variations as well as camera and compression effects _cite_ . Most importantly, the collection is very noisy with several irrelevant images as well as variety of images corresponding to different characteristic properties of the attribute (Figure _ref_) . Localisation of attributes inside the images arises as another important issue. The region corresponding to the attribute may cover only a fraction of the image, or the same attribute may be in different forms in different parts of an image. For the data collected through querying to be beneficial for automatic learning of attributes, we propose a novel method to obtain an organised collection with irrelevant images removed. Our intuition is that, given an attribute category defined by the query word, although the list of images returned is likely to include irrelevant ones, there will be some common characteristics shared among subset of images. Our main idea is to obtain visually coherent subsets, that are possibly corresponding to semantic sub-categories, through clustering and to build models for each sub-category (see Figure _ref_) . The model for each attribute category is then a collection of multiple models, each representing a different property of the attribute. We aim to answer not only "which attribute is in the image?", but also "where the attribute is?". For this purpose, we consider image patches as the basic units for providing localisation. To retain only the relevant patches that describe the attribute category correctly, during clustering we need to remove outliers, i.e. irrelevant ones. The outliers may resemble to each other while not being similar to the correct category patches resulting in a separate {\bf outlier cluster} . Alternatively, some outlier patches could be mixed with correct category patches inside {\bf salient clusters} corresponding to relevant ones. These patches, that we refer to as {\bf outlier elements}, should also be removed for the data to be sufficiently clean for learning. We propose a novel method {\bf Rectifying Self Organizing Maps (RSOM)} which improves the well-known Self Organizing Maps (SOM) _cite_ through detection and elimination of outliers. The purpose of RSOM is to "rectify" the data by purifying it not only from outlier clusters but also from outlier elements in salient clusters. It is a generic method for capturing category specific characteristics through organising the set of given instances into sub-categories pruned from irrelevant instances. Going beyond low-level attributes, RSOM is capable of learning higher level concept, as we show through learning scene concepts. In this case, we treat each image as a single instance, and aim to find groups of images representing a different property of scene category, at the same time by eliminating the ones that are either irrelevant, or poor to sufficiently describe any characteristics.