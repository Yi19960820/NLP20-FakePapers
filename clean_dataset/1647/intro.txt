einforcement learning (RL) is a family of methods aimed at training an agent to collect rewards from an environment through trial-and-error approaches. Since the deep Q-network (DQN) algorithm was introduced in N, there has been a surge of interest in using convolutional neural networks (CNN) in vision-based RL algorithms _cite_ . In the context of cyber-physical systems, vision-based RL has exciting potential to provide high levels of autonomy in applications like robotics, self-driving cars, and infrastructure inspection. However, CNNs are known to be opaque to debugging and RL's emphasis on trial-and-error demands rigorous behavioral verification before they may be allowed control over safety-critical physical systems. This work adapts CNN visualization techniques to the domain of RL. Existing CNN visualization techniques attempt to visualize classes, provide decision attribution, or cluster inputs according to their resulting label. Techniques considered here include: These techniques are useful for understanding what would cause a CNN to make a particular decision (through class visualization) and why a CNN made a particular decision (through t-SNE and attribution visualization) . CNN visualizations have proven to be valuable for identifying strengths and weaknesses in a trained network. For example, the feature visualization for GoogLeNet's saxophone class indeed extracts a saxophone shaped object from the network. That is, the method generates an image, which, when input into the trained GoogLeNet CNN, will maximize the output probability of the saxophone class. However, when looking at the generated image, one can clearly see that the outline of a man has also been extracted from the network! The cause of this is the fact that CNNs, unlike humans, look at every pixel in the training set and will therefore extract all biases with which it was presented during training. Existing visualization methods are powerful tools for gaining understanding and trust during CNN image classification development, but they are not adequate by themselves for use with RL. For example CNN-based RL often uses a stochastic policy which means that an agent's action is chosen randomly under a distribution defined by the CNN's output (i.e. ``class'') probabilities. Visual diagnostics for stochastic policies should capture such uncertainty. Finally, existing CNN visualizations are designed for static images, not for the types of time-series data collected when an agent interacts with an environment. We have adapted CNN visualization techniques to the domain of CNN-based RL. We show that these tools are valuable for providing explanations regarding an agent's decision-making process and can help an engineer understand policy deficiencies. In the following section, we provide: N) a formal introduction to the goals of RL, our methodology, and related work; N) visualization results for simulated drone experiments; N) conclusions and opportunities for further research.