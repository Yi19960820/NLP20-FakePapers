classification is one of the important tasks in the field of image processing, computer vision, and pattern recognition. To implement this task, let _inline_eq_ be a collection of _inline_eq_ training samples drawn from _inline_eq_ classes with labels _inline_eq_ . Supervised classification method will predict the label vector _inline_eq_ for a query test sample _inline_eq_ based on the training pairs _inline_eq_, which can be formulated as the following maximum a posterior (MAP) problem from the viewpoint of the probabilistic model _cite_: In general, this problem will be addressed with the help of two sequential procedures, i.e., feature extraction and classifier construction. Instead of the conventional feature engine fashion where a large amount of features are collected from some user-specific non-parametric transformations according to the domain knowledge _cite_, the learning based parametric models are much appealing in recent year. These methods aim at computing a parametric feature extraction operator _inline_eq_ associated with some more discriminative and representative features _inline_eq_ as well as a classifier _inline_eq_ in a joint framework, yielding the following approximation classification schemes _cite_ . Here and after, we use the matrix notation to indicate a set of vectors for clarity,, _inline_eq_ . where _inline_eq_ is the feature vector for the query sample, _inline_eq_ stand for the parameters for _inline_eq_ and _inline_eq_, respectively and _inline_eq_ will be the MAP estimations of the problem in the constraint corresponding to the training phase. It can be observed from that these optimal MAP solutions will be obtained by maximizing either the posterior distribution or joint distribution following the Bayes' law. As a consequence, the core issue will become to characterize the joint or posterior distribution with a suitable parametric model. \par In the last decade, the regularized linear synthesis model attracts so many attentions that it has achieved remarkable progresses in many image processing applications such as image de-noising and super-resolution _cite_ . As a parametric model, it characterizes an input sample _inline_eq_ via the linear combinations _inline_eq_, where _inline_eq_ is referred to as the dictionary formed of _inline_eq_ column vectors termed atoms and the coefficient vector _inline_eq_ is a regularized hidden feature vector of _inline_eq_ . Specially, this model in statistical machine learning is also known as linear regression with a distinct motivations and explanations _cite_ . In above image processing applications, one always focuses on optimizing the column vectors, namely atoms in _inline_eq_ to achieve a minimized representation error, which is generally called synthesis dictionary learning (DL) _cite_ . On the contrary, we will pay more attention to the properties of _inline_eq_ in some machine learning tasks _cite_ . When this model meets the task of image classification, roughly speaking, it appears as a generative model to characterize the joint distribution in with the following two types of classification implementations: \par Considering the first type, the parametric classifier model _inline_eq_ will learn a set of _inline_eq_ in the spirit of regression such that each _inline_eq_ will mostly fit the samples from class _inline_eq_ _cite_ . Accordingly, classification can be realized by measuring the regression residuals. Alternatively, the second type of classifier will focus on _inline_eq_ to measure the classification loss in a straightforward way, where some off-the-shelf loss functions in the typical classifiers will be exploited to model this term,, softmax, square _inline_eq_ norm in ridge regression _cite_ . The rest _inline_eq_ will correspond to a regularization on the classifier parameters _inline_eq_ to prevent from over-fitting. More detailed information for these two types of classifiers will be discussed in Sec. _ref_ and this paper will focus on the latter two shared terms corresponding to the feature model. \par In the previous researches, the parametric feature model _inline_eq_ will be characterized by the above regularized linear synthesis model with _inline_eq_ and a regularization term on _inline_eq_, sparsity inducing _inline_eq_ norm _cite_, square _inline_eq_ norm _cite_, group sparsity inducing _inline_eq_ norm _cite_, low rank inducing nuclear norm _cite_, their combinations _cite_, hierarchical prior _cite_, elastic net and Fisher term _cite_ . Although these different regularizations can all compute the MAP solutions of a discriminative dictionary and features to promote the classification performance, this feature model will suffer from the following intrinsic problems. \par Let us firstly consider the term _inline_eq_ . When _inline_eq_ for either an over-complete or an under-complete compact dictionary with many coherent atoms, _inline_eq_ will exist a non-trivial null space. In this case, there will be generally a large amount of candidate feature vectors fitting this term while only a few of them will be of benefit to the classification task. As a consequence, once an incorrect atom is selected during feature extraction in training or testing phases, it will subsequently result in a domino effect to yield an incorrect classification result or training performance _cite_ . Although many strategies have been proposed to relieve this problem _cite_, it will be always an intrinsic deficiency caused by the synthesis formulation,, _inline_eq_ . \par Secondly, in order to describe the term _inline_eq_, various user-specific regularizations have been exploited to encode the domain prior knowledge and our preferences in features to make a selection, resulting in numerous supervised or semi-supervised DL frameworks _cite_ . Since no explicit discriminative strategy is encoded in the most regularizers, whether these regularizations or selections will be benefit to the task at hand and fit for the input samples is still an open question _cite_ whereas tuning their adjunctive regularization parameters will bring us another tedious problem to achieve a satisfactory generalizable performance. What's worth, most of these regularizers are non-smoothness so that the feature extractor _inline_eq_ will be an inexplicit complex nonlinear function without the closed form. It follows that it is rather time consuming for feature extraction in both training and testing phases by means of some iterative optimization algorithms _cite_, which will heavily limit its applications to some large scale problems. Some efforts have been made for acceleration, including fast recovery algorithms _cite_, training a parametric function with the explicit form to approximate _inline_eq_ _cite_, introducing the online strategy _cite_,, but this intrinsic inefficiency caused by the regularized linear synthesis model cannot be eliminated. Can we directly learn a task specific parametric prior term to develop a task-driven prior learning framework following the way of dictionary learning _cite_ ? \par Finally, since feature vectors are coupled in the classifier and feature model in, both types of implementations will suffer from the inconsonant feature extractors in training and testing phases _cite_ . More concretely, since the label is not available in the testing phase, it can be concluded that the inexplicit feature extractor will be _inline_eq_ and _inline_eq_ in the training and testing phases, respectively. To our best knowledge, this problem is only concerned and addressed in _cite_ at the cost of solving a relatively complicated bi-level optimization. \par According to the above motivations, the mentioned deficiencies are mostly caused by modeling the joint distribution with the regularized linear synthesis model. As a consequence, the objective of this paper is to solve the problems raised above by developing a parametric discriminative model to characterize the alternative counterpart in, namely the posterior distribution. Our main contributions can be summarized as follows: \par Our proposed framework is validated on four image benchmarks for classification. By evaluating the classification accuracy and time costs in the training and testing phases, our framework can achieve higher accuracy than their counterpart dictionary learning algorithms with the same discriminative strategies. Compared with other state-of-the-art DL algorithms containing more discriminative strategies, our framework can still achieve better or competitive classification accuracies while the required time costs will be dramatically reduced, which evidently demonstrates the effectiveness and superiorities of our proposed framework. The rest paper is organized as follows. In Sec. _ref_, we present a preliminary on linear analysis cosparse model and the related researches on analysis operator learning. We propose NACM and discuss its superiority in Sec. _ref_ . In Sec. _ref_, we develop a DPNOL framework with two classification schemes and we derive a detailed optimization algorithm for parameters learning. We evaluate the proposed framework on several popular image databases in Sec. _ref_ and concluding remarks are given in Sec. _ref_ . For clarity, the important notations are summarized in the Table _ref_ .