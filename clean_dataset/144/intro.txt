n the community of computer-assisted interventions (CAI), recognition of the surgical workflow is an important topic because it offers solutions to numerous demands of the modern operating room (OR) _cite_ . For instance, such recognition is an essential component to develop context-aware systems that can monitor the surgical processes, optimize OR and staff scheduling, and provide automated assistance to the clinical staff. With the ability to segment surgical workflows, it would also be possible to automate the indexing of surgical video databases, which is currently a time-consuming manual process. In the long run, through finer analysis of the video content, such context-aware systems could also be used to alert the clinicians to probable upcoming complications. Various types of features have been used in the literature to carry out the phase recognition task. For instance, in _cite_, binary tool usage signals were used to perform phase recognition on cholecystectomy procedures. In more recent studies _cite_, surgical triplets (consisting of the utilized tool, the anatomical structure, and the surgical action) were used to represent the frame at each time step in a surgery. However, these features are typically obtained through a manual annotation process, which is virtually impossible to perform at test time. Despite existing efforts _cite_, it is still an open question whether such information can be obtained reliably in an automatic manner. Another feature type that is typically used to perform the phase recognition task is visual features, such as pixel values and intensity gradients _cite_, spatio-temporal features _cite_, and a combination of features (color, texture, and shape) _cite_ . However, these features are handcrafted, i.e., they are empirically designed to capture certain information from the images, leading to the loss of other possibly significant characteristics during the feature extraction process. In this paper, we present a novel method for phase recognition that overcomes the afore-mentioned limitations. First, instead of using handcrafted features, we propose to learn inherent visual features from surgical (specifically cholecystectomy) videos to perform phase recognition. We focus on visual features because videos are typically the only source of information that is readily available in the OR. In particular, we propose to learn the features using a convolutional neural network (CNN), because CNNs have dramatically improved the results for various image recognition tasks in recent years, such as image classification _cite_ and object detection _cite_ . In addition, it is advantageous to automatically learn the features from laparoscopic videos because of the visual challenges inherent in them, which make it difficult to design suitable features. For example, the camera in laparoscopic procedures is not static, resulting in motion blur and high variability of the observed scenes along the surgery. The lens is also often stained by blood which can blur or completely occlude the scene captured by the laparoscopic camera. Second, based on our and others' promising results of using tool usage signals to perform phase recognition _cite_, we hypothesize that tool information can be additionally utilized to generate more discriminative features for the phase recognition task. This has also been shown in _cite_, where the tool usage signals are used to reduce the dimension of the handcrafted visual features through canonical correlation analysis (CCA) in order to obtain more semantically meaningful and discriminative features. To incorporate the tool information, we propose to implement a multi-task framework in the feature learning process. The resulting CNN architecture, that we call EndoNet, is designed to jointly perform the phase recognition and tool presence detection tasks. The latter is the task of automatically determining all types of tools present in an image. In addition to helping EndoNet learn more discriminative features, the tool presence detection task itself is also interesting to perform because it could be exploited for many applications, for instance to automatically index a surgical video database by labeling the tool presence in the videos. Combined with other signals, it could also be used to identify a potential upcoming complication by detecting tools that should not appear in a certain phase. It is important to note that this task differs from the usual tool detection task _cite_, because it does not require tool localization. In addition, the tool presence is solely determined by the visual information from the laparoscopic videos. Thus, it does not result in the same tool information as the one used in _cite_, which cannot always be obtained from the laparoscopic videos alone. For example, the presence of trocars used in _cite_ is not always apparent in the laparoscopic videos. Automatic presence detection for such tools would require another source of information, e.g., an external video. Training CNN architectures requires a substantial capacity of parallel computing and a large amount of labeled data. In the domain of medicine, labeled data is particularly difficult to obtain due to regulatory restrictions and the cost of manual annotation. Girshick et al. _cite_ recently showed that transfer learning can be used to train a network when labeled data is scarce. Inspired by _cite_, we perform transfer learning to train the proposed EndoNet architecture. To validate our method, we build a large dataset of cholecystectomy videos containing N videos recorded at the University Hospital of Strasbourg. In addition, to demonstrate that our proposed (i.e., EndoNet) features are generalizable, we carry out additional experiments on the EndoVis workflow challenge dataset containing seven cholecystectomy videos recorded at the Hospital Klinikum Rechts der Isar in Munich. Through extensive comparisons, we also show that EndoNet outperforms other state-of-the-art methods. Moreover, we also demonstrate that training the network in a multi-task manner results in a better network than training in a single-task manner. In summary, the contributions of this paper are five-fold: (N) for the first time, CNNs are utilized to extract visual features for recognition tasks on laparoscopic videos, (N) we design a CNN architecture that jointly performs the phase recognition and tool presence detection tasks, (N) we present a wide range of comparisons between our method and other approaches, (N) we show state-of-the-art results for both tasks on cholecystectomy videos using solely visual features, and (N) we demonstrate the feasibility of using EndoNet in addressing several practical CAI applications.