Images are generated under factors of variation, including pose, illumination etc. Recently, deep ConvNet architectures learn rich and high-performance features by leveraging millions of labelled images. They have achieved state-of-the-art object recognition performance. Contemporary CNNs, such as AlexNet _cite_, VGG _cite_, GoogLeNet _cite_ and ResNet _cite_, pose object recognition as a single task learning problem, and learn features that are sensitive to object categories but invariant to other nuisance information (e.g., pose and illumination) _cite_ as much as possible. To achieve this, current CNNs usually stack several stages of subsampling/pooling _cite_ and apply normalization operations _cite_ to make representations invariant to small pose variations and illumination changes. However, as argued by Hinton et al _cite_, to recognize objects, neural networks should use ``capsules'' to encode both identity and other instantiation parameters (including pose, lighting and shape deformations) . In _cite_, authors argue as well that image understanding is to tease apart these factors, instead of emphasizing one and disregarding the others. In this work, we formulate object recognition as a multi-task learning (MTL) problem by taking images as inputs and learning both object categories and other image generating factors (pose in our case) simultaneously. Thanks to the availability of both identity and ND pose labels in the iLab-NM dataset of N million images of objects shot on a turntable, we use object identity and pose during training, and then investigate further generalization to other datasets which lack pose labels (Washington RGB-D and ImageNet) . Contrary to the usual way to learn representations invariant to pose changes, we take the opposite approach by retaining the pose information and learning it jointly with object identities during the training process. We leverage the power of ConvNets for high performance representation learning, and build our MTL framework on it. Concretely, our architecture is a two-streams ConvNet which takes a pair of images as inputs and predicts both the object category and the pose transformation between the two images. Both streams share the same CNN architecture (e.g., AlexNet) with the same weights and the same operations on each layer. Each stream independently extracts features from one image. In the top layer, we explicitly partition the representation units into two groups, with one group representing object identity and the other its pose. Object identity representations are passed down to predict object categories, while two pose representations are concatenated to predict the pose transformation between images (Fig. _ref_) . By explicitly partitioning the top CNN layer units into groups, we learn the ConvNet in a way such that each group extracts features useful for its own task and explains one factor of variation in the image. We refer our architecture as disentangling CNN (disCNN), with disentangled representations for identity and pose. During training, disCNN takes a pair of images as inputs, and learns features by using both object categories and pose-transformations as supervision. The goal of disCNN is to recognize objects, therefore, in test, we take only one stream of the trained disCNN, use it to compute features for the test image, and only the identity representations in top layer are used and fed into the object category layer for categorization. In other words, pose representations are not used in test, and the pose-transformation prediction task in the training is auxiliary to the object recognition task, but essential for better feature learning.