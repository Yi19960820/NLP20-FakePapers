Deep convolutional neural network~ (CNN) ~ _cite_ models have been achieved the great performance on various image recognition tasks~ _cite_ . However, despite CNN models performing well on such tasks, it is difficult to interpret the decision making of CNN in the inference process. To understand the decision making of CNN, methods of interpreting CNN have been proposed~ _cite_ . ``Visual explanation" has been used to interpret the decision making of CNN by highlighting the attention location in a top-down manner during the inference process. Visual explanation can be categorized into gradient-based or response-based. Gradient-based visual explanation typically use gradients with auxiliary data, such as noise~ _cite_ and class index~ _cite_ . Although these methods can interpret the decision making of CNN without re-training and modifying the architecture, they require the backpropagation process to obtain gradients. In contrast, response-based visual explanation can interpret the decision making of CNN during the inference process. Class activation mapping~ (CAM) ~ _cite_, which is a representative response-based visual explanation, can obtain an attention map in each category using the response of the convolution layer. CAM replaces the convolution and global average pooling~ (GAP) ~ _cite_ and obtains an attention map that include high response value positions representing the class, as shown in Fig.~ _ref_ (a) . However, CAM requires replacing the fully-connected layer with a convolution layer and GAP, thus, decreasing the performance of CNN. To avoid this problem, gradient-based methods are often used for interpreting the CNN. The highlight location in an attention map for visual explanation is considered an attention location in image recognition. To use response-based visual explanation that can visualize an attention map during a forward pass, we extended a response-based visual explanation model to an attention mechanism. By using the attention map for visual explanation as an attention mechanism, our network is trained while focusing on the important location in image recognition. The attention mechanism with a response-based visual explanation model can simultaneously interpret the decision making of CNN and improve their performance. Inspired by response-based visual explanation and attention mechanisms, we propose _inline_eq_ _inline_eq_ _inline_eq_ ~ (ABN), which extends a response-based visual explanation model by introducing a branch structure with an attention mechanism, as shown in Fig~ _ref_ (b) . ABN consists of three components: feature extractor, attention branch, and perception branch. The feature extractor contains multiple convolution layers for extracting feature maps. The attention branch is designed to apply an attention mechanism by introducing a response-based visual explanation model. This component is important in ABN because it generates an attention map for the attention mechanism and visual explanation. The perception branch outputs the probabilities of class by using the feature and attention maps to the convolution layers. ABN has a simple structure and is trainable in an end-to-end manner using training losses at both branches. Moreover, by introducing the attention branch to various baseline model such as ResNet~ _cite_, ResNeXt~ _cite_, and multi-task learning~ _cite_, ABN can be applied to several networks and image recognition tasks. Our contributions are as follows: