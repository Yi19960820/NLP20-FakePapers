Activity detection is an important computer vision problem with many societal applications, including smart surveillance, monitoring of patients or elderly (e.g., for quality-of-life systems), online video retrieval, and robot perception. Given a continuous video, the task is to find the frames corresponding to every event occurring in the video. This is more challenging compared to the activity classification problem of categorizing a pre-segmented video or localizing a single activity in a trimmed video. Although activity detection is an important area to study as almost all real-world videos contain multiple activities and are rarely segmented (e.g., surveillance systems), it has been investigated much less, particularly for multi-event videos. In the past years, end-to-end learning methods using convolutional neural networks (CNNs) obtained a great amount of success in video analysis. These approaches successfully modeled per-frame (or per-local-segment) information in activity videos, such as a single RGB frame or optical flows over a small number of frames _cite_ . Recently, models such as IND~ _cite_ have been developed to capture longer-term dynamics (e.g., N frames) . However, because such end-to-end models are optimized for capturing per-segment information, the primary focus of existing works has been mainly on activity classification and not detection. There are recent works on activity detection using end-to-end models (e.g., the detection task in _cite_), but many of these works also focus on making better per-segment decisions (and their post-processing) rather than learning details of temporal structure/context over the entire (variable length) video. A video contains multiple activities (in a sequence or in parallel) and they are correlated. This means that detecting the frames of one activity in the video should benefit from information in the frames corresponding to another activity, which are often temporally very separated. Existing approaches of representing/classifying video segments without regard to contextual information is thus limited; continuous videos contain rich temporal structure which can be exploited to improve activity detection. For example, in a video of a basketball game, shooting and blocking events must occur near-by, as shown in Figure~ _ref_ . A block or rebound event cannot occur without a shot event. In this paper, we introduce the concept of and present how its representation learning can benefit activity detection. We define a super-event as a set of multiple events occurring together in videos with a particular temporal pattern (i.e., structure) . More specifically, if the event we are interested in is a part of a longer-term event, we call the longer-term event its super-event. This is the opposite concept of `sub-events'. For example, the events of shooting and blocking mentioned above forms a super-event, which may be named as a blocked-shoot. Learning such latent super-events allows the model to capture how the events are temporally related in their videos. Once learned, when making a prediction (i.e., testing), the super-events can serve as temporal context to better detect the events. This enables the detection decision at each frame to be made while considering longer-term temporal structure. Note that such super-events are `latent', meaning that no super-event annotations are provided. We newly design, and convolve it with the video representation to obtain a super-event representation. Temporal structure filters allow the model to focus on particular sub-intervals. Such temporal structure filters are learned for each event, optimized based on the training data for the best super-event representation construction. For each frame, we combine the super-event representation with the per-frame or per-segment CNN representation for its binary classification per event. Our method is fully-differentiable and can be learned end-to-end using back propagation, making it suitable for any length video. Our experimental results with three different datasets confirm the benefits of our latent super-event learning, obtaining the state-of-the-art results on MultiTHUMOS and Charades.