Deep learning for robotics vision demands highly efficient real-time solutions. A promising approach to deliver one such extremely efficient solution is through the use of low numeric precision deep learning algorithms. Operating in lower precision mode reduces computation as well as data movement and storage requirements. Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to N-bit ternary mode~ _cite_ and N-bit mode~ _cite_ . However, the majority of existing works in low-precision DNNs sacrifice accuracy over the baseline full-precision networks. Further, most prior works target reducing the precision of the model parameters (network weights) . This primarily benefits the inference step only when batch sizes are small. We observe that activation maps (neuron outputs) occupy more memory compared to the model parameters for batch sizes typical during training. This observation holds even during inference when batch size is N or more for modern networks. Based on this observation, we study schemes for training and inference using low-precision DNNs where we reduce the precision of activation maps as well as the model parameters without sacrificing the network accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer. We call networks using this scheme wide reduced-precision networks (WRPN) and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. Although the number of raw compute operations increase as we increase the number of filter maps in a layer, the compute bits required per operation is now a fraction of what is required when using full-precision operations. We present results using our scheme on AlexNet and ResNet on ILSVRC-N dataset. Our results show that the proposed scheme offer better accuracies on ILSVRC-N dataset, while being computationally less expensive compared to previously reported reduced-precision networks. Further, our reduced-precision quantization scheme is hardware friendly allowing for efficient hardware implementations of such networks for servers, deeply-embedded and real-time deployments.