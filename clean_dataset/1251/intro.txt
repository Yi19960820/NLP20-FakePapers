Structured prediction is an important machine learning problem that occurs in many different fields, e.g., natural language processing, protein structure prediction and semantic image annotation. The goal is to learn a function that maps an input vector _inline_eq_ to an output _inline_eq_, where _inline_eq_ is a vector representing all the labels whose components take on the value _inline_eq_ or _inline_eq_ (presence or absence of the corresponding label) . The traditional approach to such multi-label classification problems is to train a set of binary classifiers independently. Structured prediction on the other hand also considers the relationships among the output variables _inline_eq_ . For example, in the image annotation problem, an entire image or parts of an image are annotated with labels representing an object, a scene or an event involving multiple objects . These labels are usually dependent on each other, e.g., buildings and beaches occur under the sky, a truck is a type of automotive, and sunsets are more likely to co-occur with beaches, sky, and trees (Figure~ _ref_) . Such relations capture the semantics among the labels and play an important role in human cognition. A major advantage of structured prediction is that the structured representation of the output can be much more compact than an unstructured classifier, resulting in smaller sample complexity and greater generalization . Extending traditional classification techniques to structured prediction is difficult because of the potentially complicated inter-dependencies that may exist among the output variables. If the problem is modeled as a, it is well-known that exact inference over a general graph is NP-hard. Therefore, practical approaches make simplifying assumptions about the dependencies among the output variables in order to simplify the graph structure and maintain tractability. Examples include (MEMMs), (CRFs), (MNNs) and (SSVMs) . These approaches typically restrict the tree-width of the graph so that the algorithm or the algorithm can still be efficient. On the other hand, there has been much research on fast approximate inference for complicated graphs based on, e.g., (MCMC),, or combinations of these methods. In general, MCMC is slow, particularly for graphs with strongly coupled variables. Good heuristics have been developed to speed up MCMC, but they are highly dependent on graph structure and associated parameters . is another popular approach where a complicated distribution over _inline_eq_ is approximated with a simpler distribution so as to trade accuracy for speed. For example, if the variables are assumed to be independent, one obtains the algorithm. A Bethe energy formulation yields the (LBP) algorithm . If a combination of trees is considered, one obtains the algorithm . One can also relax the higher-order marginal constraints to obtain a algorithm . The lesser the dependency constraints, the less accurate these inference algorithms become, and the faster their speed. However, the sacrificed accuracy in inference could be detrimental to learning. For example, can produce highly biased estimates, and might even cause the learning algorithm to diverge ~ . Long-range dependencies and complicated graphs are necessary to accurately and precisely represent semantic knowledge. Unfortunately, the approaches discussed above all operate under the assumption that one cannot avoid the trade-off between the representational power and computational efficiency. In this paper, we propose (LMSBNs) and (LMBMs), two new models for structured prediction. We provide a theoretical analysis tool to derive the generalization bounds for both of them. Most importantly, LMSBNs allow fast inference for arbitrarily complicated graph structures. Inference is based on a (BB) technique that does not depend on the dependency structure of the graph and exhibits the interesting property that the better the fit of the model to the data, the faster the inference procedure. Section~ _ref_ describes both LMSBNs and LMBMs. We present learning algorithms for both and the fast BB inference algorithm for LMSBNs. LMBMs, being undirected, rely on traditional inference algorithms. Section~ _ref_ applies both LMSBNs and LMBMs to the semantic image annotation problem using a fully-connected graph structure. We empirically study the performance of the BB inference algorithm and illustrate its efficiency and effectiveness. We present results from experiments on a benchmark dataset which demonstrate that LMSBNs outperform current state-of-the-art methods for image annotation based on kernels and threshold-tuning.