Segmentation of the internal structures, like body organs, in medical images is an essential task for many clinical applications such as computer-aided diagnosis (CAD), computer-aided surgery (CAS) and radiation therapy (RT) . However, despite intensive studies of automatic or semi-automatic segmentation methods, there remain challenges which need to be overcome before these methods can be applied to clinical environments. In particular, detailed abdominal organ segmentation on CT is a challenging task both for manual human annotation and for automatic segmentation algorithms for various reasons including the morphological complexity of the structures, the large variations between inter-and intra-subjects, and image characteristics such as low contrast of soft tissues. Early studies of abdominal organ segmentation focused on specific single organs, for example relatively large isolated structures such as the liver _cite_ or critical structures such as blood vessels _cite_ . However, most of the algorithms were based on specific features of the target organ, and so extensibility to the simultaneous segmentation of multiple organs was limited. For multi-organ segmentation, atlas-based approaches were adopted for many applications _cite_ . The general framework of atlas-based segmentations is to deformably register selected atlas images with segmented structures to the target image. Critical issues for this approach, which affect performance accuracy, include proper atlas selection, accurate deformable image registration, and label fusion. In particular, for the abdominal region, inter-subject variations are relatively large compared with other parts of the body (e.g., the brain) so the segmentation results are dependent on deformable registration between inter-subjects from the limited set of atlases, which is a challenging problem that critically affects the final accuracies. In addition, computational time is strongly dependent on the number of atlases. Therefore, selection of the proper number and types of atlases is a critical factor for both of the accuracy and efficiency. Recently, learning-based approaches exploiting large datasets have been applied to the segmentation of medical images _cite_ . In particular, deep convolutional neural networks (CNN) have been very successful _cite_ . Targets include regions in the brain _cite_, chest _cite_, and abdomen _cite_ . The performance results of CNNs for organs (and even tumors) reach, or outperform, alternative state-of-the-art methods. Unlike multi-atlas-based approaches, deep networks do not require selecting a specific atlas or require deformable registration from training sets to a target image. In this study, we apply deep network approaches to abdominal organ segmentation. Most studies based on deep networks, however, focused on a single structure segmentation, particularly for abdominal regions, and there are few studies of multi-organ segmentation partly due to technical challenges discussed later. We note that fully convolutional networks (FCNs) _cite_ have been generally accepted for organ segmentations on CT scans _cite_ partly because they give state-of-the-art performance for semantic segmentation of natural images _cite_ . But there are three major characteristics of abdominal CT which we must address in order to obtain strong performance on multi-organ segmentation. Firstly, many abdominal organs have weak boundaries between spatially adjacent structures on CT, e.g. between the head of the pancreas and the duodenum. In addition, the entire CT volume includes a large variety of different complex structures. Morphological and topological complexity includes anatomically connected structures such as the gastrointestinal (GI) track (stomach, duodenum, small bowel and colon) and vascular structures. The correct anatomical borders between connected structures may not be always visible in CT, especially in sectional images (i.e., ND slices), and may be indicated only by subtle texture and shape change, which causes uncertainty even for human experts. This makes it hard for deep networks to distinguish the target organs from the complex background. Secondly, there are large variations in the relative sizes of different target organs, e.g. the liver compared to the gallbladder. This causes problems when applying deep networks to multi-organ segmentation because lower layers typically lack semantic information when segmenting small structures. The same problem has been observed in semantic segmentation of natural images where the segmentation performance on small regions is typically much worse than on large regions, motivating the need to introduce mechanisms which attend to the scale _cite_ . Thirdly, although CT scans are high-resolution three-dimensional volumes, most current deep network methods were designed for ND images. To overcome the limitations of using ND CNNs for ND images, Setio _cite_ used multiple ND patches reconstructed from _inline_eq_ different directions around the target region for the task of pulmonary nodule detection. Zhuang _cite_ used ND axial, coronal, and sagittal slices for pancreas detection at the coarse level and also for segmentation at the finer level. More recently, there are studies which use ND deep networks _cite_ . These, however, are not networks that act on the entire ND CT volume but instead are local patch-based approaches (due to complex challenges of ND deep networks discussed later in this paragraph) . To address the problems caused by restricting to image patches, _cite_ used a hierarchical approach with multi-resolutions, which reduces the dimension of the whole volume for initial detection and focuses on smaller regions at the finer resolution. But this strategy is best suited to a single target structure. Roth _cite_ applied a bigger patch size to deal with the whole dense pancreatic volume, but this was also for single pancreas segmentation and hard to extend to the whole abdominal region. In general, ND deep networks face far greater complex challenges than ND deep networks. Both approaches rely heavily on graphics processing units (GPUs) but these GPUs have limited memory size which makes it difficult when dealing with full ND CT volumes compared to ND CT slices (which require much less memory) . In addition, ND deep networks typically require many more parameters than ND deep networks and hence require much more training data, unless they are restricted to patches. But there is limited training data for abdominal CT images, because annotating them is challenging and requires expert human radiologists, which makes it particularly difficult to apply ND deep networks to abdominal multi-organ segmentation. We have, however, implemented a ND patch based approach for comparison. To deal with the technical difficulties for abdominal multi-organ segmentation on CT, we introduce a novel framework of an organ-attention ND deep networks with reverse connections (OAN-RC) followed by statistical fusion to combine the information from the three different views exploiting structural similarity using local isotropic ND patches. OAN is a two-stage deep network, which computes an organ-attention map (OAM) from typical probability map of labels for input images in the first stage and combines OAM to the original input image for the second stage. This two-stage strategy effectively reduces the complexity of the background while enhancing the discriminative information of target structures (by concentrating attention close to the target structures) . By training OAM with additional deep network, uncertainties and errors from the first stage are adjusted and the fidelity of the final probability map is improved. In this procedure, we apply reverse connections _cite_ to the first stage so that we can localize organ information at different scales by assisting the lower layers with semantic information. More specifically, we apply OAN-RC to each sectional slice, which is an extreme form of anisotropic local patches but include the whole semantic (i.e. volume) information from one viewing direction. This yields segmentation information from separate sets of multi-sectional images (axial, coronal, and sagittal planes in this study similarly to most of medical image platforms for ND visualization) . We statistically fuse the three sources of information using local isotropic ND patches based on direction-dependent local structural similarity. The basic fusion framework uses expectation-maximization (EM) similar to _cite_ . But, unlike typical statistical fusion methods used for atlas-based segmentation, the input volumes and the target volumes for segmentation in our problem are the same. But different structures and texture patterns, from different viewing directions, will often generate nonidentical segmentations in ND. Our strategy is to exploit structural similarity by computing a direction-dependent local property at each voxel. This models the structural similarity from the ND images to the original ND structure (in the ND volume) by local weights. This structural statistical fusion improves our overall performance by combining the information from the three different views in a principled manner and also imposing local structure. Figure _ref_ describes the graphical concept of our framework. Our proposed algorithm was tested on _inline_eq_ abdominal CT scans of normal cases collected as a part of FELIX project for pancreatic cancer research _cite_ . By experiments, our method showed robust and high fidelities to the ground-truth for all target structures with smooth boundaries. It outperformed ND patch-based algorithms as well as ND-based in terms of DICE-similarity coefficient and average surface distance with memory and computational efficiency.