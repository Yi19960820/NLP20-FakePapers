Sketches are different to photos. They exhibit a high-level of abstraction yet are surprisingly illustrative. With just a few strokes, they are able to encode an appropriate level of semanticness that depicts objects and communicate stories (\eg, ancient cave drawings) . Such unique characteristics of sketches, together with the prevalence of touchscreen devices, to a large extent drove the recent surge of sketch research. Problems studied so far range from sketch recognition~ _cite_, sketch-based image retrieval (SBIR) ~ _cite_, to sketch synthesis~ _cite_ . Despite great strides made, a major obstacle facing all sketch research is the lack of freely available sketch data. Compared with photos where million-scale datasets had been readily accessible for almost a decade (\eg, ImageNet _cite_), all aforementioned research worked with sub-million level crowd-sourced sketch datasets (Nk for TU-Berlin _cite_ and Nk for Sketchy~ _cite_) . These datasets served as key enablers for the community, though have very recently started to bottleneck the progress of sketch research--sketch recognition performance had already gone far beyond human-level _cite_ on TU-Berlin _cite_, and steadily approaching human performance~ _cite_ for the problem of SBIR on Sketchy ~ _cite_ . In particular, two unique traits of human sketches had been mostly overlooked: (i) sketches are highly abstract and iconic, whereas photos are pixel perfect depictions, (ii) sketching is a dynamic process other than a mere collection of static pixels. Such oversights can be partially attributed to the lack of a large and diverse dataset of stroke-level human sketches, since more data samples are required to broadly capture (i) the substantial variances on visual abstraction, and (ii) the highly complex temporal stroke configurations--an apple might look like an apple once drawn (though more abstract than photos), there is more than one way of drawing it. The seminal work of _cite_ on sketch recognition tackled these problems to some extent yet were limited in that (i) sketches are treated as static pixelmaps, where deep architecture for feature learning is limited to variants of photo CNNs, and (ii) temporal ordering information is modeled coarsely by temporally segmenting one sketch into three separate pixelmaps, which are then encoded using a multi-branch CNN. The very recent work of _cite_ was the first to fully acknowledge the temporal nature of sketches, and proposed a RNN-based generative model to synthesize novel sketches from scratch. In this paper, we combine RNN stroke modeling with conventional CNN under a dual-branch setting to learn better sketch feature representations. However, the problem of visual abstraction, especially how it can be accommodated under a large-scale retrieval setting remains unsolved. In this paper, for the first time, we leverage on a newly released multi-million human sketch dataset _cite_, and introduce the novel problem of sketch hashing retrieval (SHR) . Different to the conventional task of sketch recognition where classification is usually performed by computing feature distances in Euclidean space _cite_, given a query sketch, SHR aims to compute an exhaustive ranking of all sketches in a very large test gallery. It is thus a more difficult problem than sketch recognition, since (i) more discriminative feature representations are needed to accommodate the much larger variations on style and abstraction, and meanwhile (ii) a compact binary code needs to be learned to facilitate efficient large-scale retrieval. Importantly, the availability of such a large dataset enables us to better explore the aforementioned sketch-specific traits of being highly abstract and sequential in nature. In particular, we fully examine the temporal ordering of strokes through a two-branch CNN-RNN network, and address the abstraction problem by proposing a novel hashing loss that enforces more compact feature clusters for each sketch category in Hamming space. More specifically, we first construct a dataset of N, N, N human sketches, by randomly sampling from every category of the Google QuickDraw dataset _cite_, which we term as ``QuickDraw-N This dataset is highly noisy when compared with TU-Berlin, for that (i) users had only N seconds to draw, and (ii) no specific post-processing was performed. Figure _ref_ offers a visual comparison between the two datasets. We then analyze the intrinsic data traits of sketch and design a novel end-to-end deep hashing model to conduct fast retrieval. The main contributions of this paper can be summarized as: (i) For the first time, we introduce the problem of sketch hashing retrieval on a multi-million scale human sketch dataset, and propose a deep hashing network that directly accommodates the key characteristics of human sketch. We show that our network is able to outperform state-of-the-art alternatives specifically designed for photo-photo and sketch-photo retrieval, highlighting the advantage of our sketch-specific design. Moreover, our network also achieves state-of-the-art performance when re-purposed for the task of sketch recognition, . (ii) We propose a novel multi-branch CNN-RNN architecture that specifically encode the temporal ordering information of sketches to learn a more fine-grained feature representation. We find that stroke-level temporal information is indeed helpful in sketch feature learning in that it alone can outperform CNN features for the sketch recognition task, and offers the best performance when combined with CNN features. (iii) We design a novel hashing loss to accommodate the abstract nature of sketches, especially on such a large dataset where noise is also present. More specifically, we propose a sketch center loss to learn more compact feature clusters for each object category and in turn improve retrieval performance. The rest of the paper is organized as follows: Section~ _ref_ briefly summarizes related work. Section~ _ref_ describes our proposed deep hashing model for large-scale sketch retrieval. Experimental results and discussion are presented in Section~ _ref_ . Finally, we draw some conclusions in Section~ _ref_ .