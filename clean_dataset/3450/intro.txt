In recent years, Convolutional Neural Networks (CNN) have become the {\em de facto} standard in many computer vision tasks, such as image classification and object detection _cite_ . Top performing image classification architectures usually involve {\em very} deep CNN trained in a supervised fashion on a large datasets _cite_ and have been shown to produce generic hierarchical visual representations that perform well on a wide variety of vision tasks. However, these deep CNNs heavily reduce the input resolution through successive applications of pooling or subsampling layers. While these layers seem to contribute significantly to the desirable invariance properties of deep CNNs, they also make it challenging to use these pre-trained CNNs for tasks such as semantic segmentation, where a per pixel prediction is required. Recent advances in semantic segmentation tend to convert the standard deep CNN classifier into Fully Convolutional Networks (FCN) ~ _cite_ to obtain coarse image representations, which are subsequently upsampled to recover the lost resolution. However, these methods are not designed to take into account and preserve both and contextual dependencies, which has shown to be useful for semantic segmentation tasks~ _cite_ . These models often employ Conditional Random Fields (CRFs) as a post-processing step to locally smooth the model predictions, however the long-range contextual dependencies remain relatively unexploited. Recurrent Neural Networks (RNN) have been introduced in the literature to retrieve global spatial dependencies and further improve semantic segmentation~ _cite_ . However, training spatially recurrent neural networks tends to be computationally intensive. In this paper, we aim at the {\em efficient} application of Recurrent Neural Networks RNN to retrieve contextual information from images. We propose to extend the ReNet architecture~ _cite_, originally designed for image classification, to deal with the more ambitious task of semantic segmentation. ReNet layers can efficiently capture contextual dependencies from images by first sweeping the image horizontally, and then sweeping the output of hidden states vertically. The output of a ReNet layer is therefore implicitly encoding the local features at each pixel position with respect to the whole input image, providing relevant global information. Moreover, in order to {\em fully} exploit local and global pixel dependencies, we stack the ReNet layers on top of the output of a FCN, i.e. the intermediate convolutional output of VGG-N~ _cite_, to benefit from generic local features. We validate our method on Weizmann~Horse and Oxford~Flower foreground/background segmentation datasets as a proof of concept for the proposed architecture. Then, we evaluate the performance in the standard benchmark of urban scenes CamVid; achieving state-of-the-art in all three datasets.~