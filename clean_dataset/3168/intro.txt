Many computer vision tasks rely on semantic analysis of data in sequential forms. Typical examples include video-based human action recognition _cite_, image/video captioning _cite_, speech recognition _cite_ etc. In other cases, tasks of interest might be recast as sequential learning problems, so that their learning objectives can be easily and iteratively achieved. For example, in human pose estimation, the joint locations can be predicted using multi-stage CNNs, the hidden features produced by one stage are used as input for the next stage. This multi-stage scheme for pose estimation can be cast using the recurrent scheme that the hidden output from one time step will be fed into in the next time step for refinement. RNNs in which each neuron or unit can use its internal memory to maintain information of the previous input have become the de facto learning models for many computer vision tasks whose observed input data are or can be recast in a sequential form. In these tasks, we might have access to multiple and heterogenous input sources. In general, having multiple input sources working together towards the same goal, can contribute to a more robust algorithm. The input sources each may be more effective at expressing one form or aspect of information than others. This is the reason why the two-stream architectures is widely accepted in human action recognition. RGB frames and corresponding motion inputs (e.g. optical flow or RGB difference) is used as multiple input sources which, separately, will pass through the independent network. The results of the individual models will be combined together to predict the actions happened in the video. The independent use of networks on different input sources and fusing the probabilities at the end does not fully exploit the reciprocal information between each other. Moreover, there are few researches on how to exploit reciprocal information between complementary inputs. Therefore, in this paper, we propose an end-to-end architecture, Coupled Recurrent Network (CRN) to investigate how to aggregate the features to better fuse spatial and temporal information and achieve more effective sequential learning from multiple inputs. CRN has two recurrent branches, i.e. branch A and branch B, each of which takes one input source. Different from two-stream architecture which is trained independently, during the learning of CRN, branch A and branch B can ` communicate ' with each other. In order to well interpret the reciprocal information between them, within CRN, we propose two modules, recurrent interpretation block (RIB) and recurrent adaptation block (RAB) . The hidden output from branch B will pass through RIB to extract reciprocal representations for branch A . The extracted reciprocal information will be concatenated with current input sources of branch A and then they will pass through the RAB to obtain the re-mapped input for branch A . The distilling module RIB and remapping module RAB is shared inside the recurrent networks. An illustration of a CRN and its corresponding computational flow is shown in Fig. _ref_ . The proposed CRN can be generlized to many computer vision tasks which can be understood by two or several input sources. In this paper, we apply the proposed CRN to two human centric problems, i.e. human action recognition and multi-person pose estimation. In human action recognition, a sequence of RGB frames and corresponding motion signals (e.g. optical flows or RGB difference) are the two heterogenous input sources for two branches in CRN. Two cross entropy losses for the same recognition target with identical form are applied at the end of each branch. While in multi-person pose estimation, besides the commmonly used individual body joints, a field of part/joint affinities that characterizes pair-wise relations between body joints _cite_ is used as the additional supervision information. Two different regression losses, one for joint estimation and the other one for vector prediction, are applied at the end of each network. The standard procedure for training a RNN is to apply appropriate loss for each input at each time step or only apply at the last time step. However, in the experiments, we find neither of two training strategies work well for CRN. Having supervision at each time step seems to make supervision signals assertive and arbitrary and the whole training becomes numerically unstable, leading to a poor performance; while having supervision only at the last time step makes supervision signal too weak to reach the end and leads to the performance drop on the considered tasks. Therefore, we propose a new training scheme which has a good balance of the supervision strength along time steps. Apart from having the loss at the last time step, we randomly select some previous time steps for supervision. Only the losses at selected time step will contribute to the back-propagation. Comparative experiments show that our proposed CRN outperforms the baselines by a large margin. CRN sets a new state-of-the-art on benchmark datasets of human action recognition (e.g., HMDB-N _cite_, UCF-N _cite_ and larger dataset, Moments in Time _cite_) and multi-person pose estimation (e.g., MPII _cite_) . Moreover, since better reciprocal information can be exploited within CRN, using RGB and RGB differences as input sources, CRN can achieve more than _inline_eq_ accuracy on the UCF-N. Compared to optical flow, RGB difference can be calculated online without burden. Therefore, more than N FPS can be achieved for real-time action recognition. We summarize the contributions as below: