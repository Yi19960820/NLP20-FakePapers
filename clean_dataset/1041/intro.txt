A Real-time ND semantic mapping is desired in a lot of robotics applications, such as autonomous navigation and robot arm manipulation. The inclusion of semantic information with a ND dense map is much useful than geometric information alone in robot-human or robot-environment interaction. It enables robots to perform advantage tasks like "nuclear wastes classification and sorting" or "autonomous warehouse package delivery" more intelligently. A variety of well-known methods such as RGB-D SLAM~ _cite_, Kinect Fusion~ _cite_ and ElasticFusion~ _cite_ can generate dense or semi-dense ND map from RGB-D videos. But those ND maps contain no semantic-level understanding of the observed scenes. Meanwhile, the semantic segmentation achieved a significant progress with advantage of convolution neural network. Thus far, FCN~ _cite_, SegNet~ _cite_ and Deeplab~ _cite_ are the most popular methods for RGB level semantic segmentation. FuseNet~ _cite_ and LSTM-CF~ _cite_ take advantage of both RGB and depth images to improve semantic segmentation. PointNet~ _cite_ is the forerunner for ND semantic segmentation that consumes an unordered point cloud. During RGB-D mapping, both RGB image with rich contextual information and point cloud with rich ND geometric information can be obtained directly. To date, there are no existing methods that make use of both RGB and point cloud for the semantic segmentation and mapping. In this paper, we proposed a dense RGB-D semantic mapping system with a Pixel-Voxel neural network which can perform dense ND mapping while simultaneously recognizing and semantically labelling each point in the ND map. The main contributions of this paper can be summarized as follows: The rest of this paper is organized as follows. The related work is reviewed in Section _ref_ firstly. Then the details of the proposed methods are introduced in Section _ref_ . The experimental results and analyses are given in Section _ref_ . Finally, we conclude the paper in Section _ref_ .