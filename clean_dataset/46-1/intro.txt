In situ sequencing _cite_ is a very powerful tool to quantify gene expression directly in biological tissue samples without losing spatial information on tissue morphology. Investigated genes are targeted with controlled design of barcoded padlock probes, locally amplified and sequenced by repeated fluorescent staining and imaging cycles. An in situ sequencing dataset consists of five fluorescent channels for each sequencing cycle: one nuclei channel and four colour channels where the fluorescent signals belonging to the four bases of the genetic code (T, G, C, A) are imaged. An additional general stain is used in the first cycle to detect all four bases in a single image used as reference. Fluorescent signals appear as bright spots of N-Npx size in a noisy background caused by scattering light and autofluorescence. Moreover, fluorescent spots have a blurry appearance without any clear border due to the low signal-to-noise ratio caused by the diffraction limit of the microscope. Therefore, it is often difficult to distinguish real fluorescent signals from noise and background structures focusing the analysis on criteria based only on intensity values. We previously published an image analysis pipeline for signal decoding of the barcoded padlock probes mapping targeted mRNAs with morphological and spatial information in cells and tissue _cite_ . This previous approach detected fluorescent signals by applying a global threshold on intensities of the general stain image, followed by a watershed segmentation to resolve clusters. The resulting segmentation mask was then used to extract fluorescence intensities in the four colour channels for each sequencing cycle. Thereafter, barcodes were decoded selecting the base from the channel with the highest intensity for each sequencing cycle and finally filtered using a quality threshold based on intensity values. As compared to visual assessment, many signals were missed using this approach. In order to improve recall of the decoded barcode sequences we here present a pipeline that aims to be as inclusive as possible in the first processing steps. Thus, we delay the decision of which fluorescent signal candidates contribute to an expected sequence. We use a Convolutional Neural Network (CNN) to extract self-learned features from signal candidates and use them as a probability prediction to describe how similar a signal candidate is compared to a true signal judged by visual examination. We thereafter feed signal candidates to a graphical model that resolves the sequences and provides the final barcodes. Finally, a quality measurement of the decoded sequences is assessed, making it possible for the user to set a threshold to achieve high recall or high accuracy.