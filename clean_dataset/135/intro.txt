Aspiring writers are often given the following advice: produce a first draft, then remove unnecessary words and shorten phrases whenever possible. Can a similar recipe be followed while building deep networks? For large-scale tasks like object classification, the general practice has been to use large networks with powerful regularizers . This implies that the overall model complexity is much smaller than the number of model parameters. A smaller model has the advantage of being faster to evaluate and easier to store-both of which are crucial for real-time and embedded applications. Given such a large network, how do we make it smaller? A naive approach would be to remove weights which are close to zero. However, this intuitive idea does not seem to be theoretically well-founded. LeCunn \etal proposed Optimal Brain Damage ~ (OBD), a theoretically sound technique which they showed to work better than the naive approach. A few years later, Hassibi \etal came up with Optimal Brain Surgeon ~ (OBS), which was shown to perform much better than OBD, but was much more computationally intensive. This line of work focusses on pruning unnecessary weights in a trained model. There has been another line of work in which a smaller network is trained to mimic a much larger network. Bucila \etal proposed a way to achieve the same-and trained smaller models which had accuracies similar to larger networks. Ba and Caruna used the approach to show that shallower (but much wider) models can be trained to perform as well as deep models. Knowledge Distillation (KD) is a more general approach, of which Bucila \etal 's is a special case. FitNets use KD at several layers to learn networks which are deeper but thinner (in contrast to Ba and Caruna's shallow and wide), and achieve high levels of compression on trained models. Many methods have been proposed to train models that are deep, yet have a lower parameterisation than conventional networks. Collins and Kohli propose a sparsity inducing regulariser for backpropogation which promotes many weights to have zero magnitude. They achieve reduction in memory consumption when compared to traditionally trained models. Denil \etal demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Ciresan \etal train networks with random connectivity, and show that they are more computationally efficient than densely connected networks. Some recent works have focussed on using approximations of weight matrices to perform compression. Jenderberg \etal and Denton \etal use SVD-based low rank approximations of the weight matrix. Gong \etal, on the other hand, use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk. Unlike the methods discussed previously, these do not need any training data to perform compression. However, they change the network structure in a way that prevents operations like fine-tuning to be done easily after compression. One would need to `uncompress' the network, fine-tune and then compress it again. Similar to the methods discussed in the paragraph above, our pruning method doesn't need any training/validation data to perform compression. Unlike these methods, our method merely prunes parameters, which ensures that the network's overall structure remains same-enabling operations like fine-tuning on the fly. The following section explains this in more detail.