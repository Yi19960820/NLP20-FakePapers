Robot-assisted clinical systems have been increasingly adopted due to their advantages during surgical procedures. However, obtaining haptic feedback of a teleoperated surgical system still constitutes a hard problem due to practical challenges such as control loop stability. In the current version of the da Vinci Surgical System~ _cite_ (Intuitive Surgical, Inc., Sunnyvale, CA, USA), there is no haptic technology and no feedback on the grip forces. Surgeons depend on visual cues to infer the forces to avoid damage to tools and anatomy since excessive mechanical force can lead to the breakage of an end-effector string, serious artery or nerve injury, and even post-operation trauma~ _cite_ . As a result, there is a critical need to design force sensing systems in the field of surgical robotics. Recently, many researchers have focused their efforts on solutions to this problem. For instance, numerous tactile sensing devices have been developed to estimate tactile information during static (point based) measurements, including indentation-based contact devices, aspiration devices, optical fiber devices, and non-contact devices~ _cite_ . Such devices are capable of providing accurate tactile information during static measurements of a single point, but they cannot scan soft tissue in a dynamic way, which is not a real-time solution~ _cite_ . Another area of investigation is directed towards using a torque sensor to model and compensate for grip force. This may provide a consistent internal force compensation based on the quantitative model, but it largely relies on the surgeon's skills and experience~ _cite_ . In addition, most of these hardware-based solutions have delicate and expensive components, which often cannot withstand sterilization. Vision based approaches are one way to overcome above limitations of hardware solutions. Starting from~ _cite_, computer vision has been used to measure the deformed object and recover the applied force from linear elasticity equations. Recent advances in deep learning bring opportunities to such vision based force prediction in real surgical scenarios~ _cite_ . For example, researchers in [N] extract the ND deformable structure of the heart and use a neural network with the architecture of LSTM-RNN to predict the applied force. In this paper, we propose a vision-based surgical force prediction model called GB-oint loud emporal onvolutional etwork () . The model is based on a spatial block that encodes information at individual time-steps from a video and a temporal block to reason over sequences of observations. The spatial block combines ND features (e.g., from an RGB image) and ND features (e.g., from a ND point cloud) for a given time, while the temporal block makes use of multiple static features via the Temporal Convolutional Network~ _cite_ (TCN) to model force change over time. To better abstract the core feature, we apply a pre-trained VGGN image model~ _cite_ along with a pre-trained ND point cloud-based architecture called PointNet~ _cite_ to extract features from raw visual data and then concatenate these two features to train a TCN time-series model. We evaluate our approach on internally-collected da Vinci surgical video, and show that our model produces highly accurate results. Figure~ _ref_ shows representative test result on an ex vivo liver. Much work has focused on modeling tissue deformation during force prediction, since for reasonably soft material the applied force is positively correlated to the deformation of the tissue surface~ _cite_ . Therefore, accurately measuring surface deformation in ND is vitally important for vision-based force estimation. Furthermore, depth data can then be converted to ND point cloud. The recently proposed PointNet~ _cite_ directly works on ND unordered point cloud data, which essentially breaks the pixel order limit of the ND depth image. The unordered point cloud is robust to camera view points and invariant to transformations, which brings the potential ability to generalize to different objects. In prior work, Temporal Convolutional Networks (TCN) have been proposed to improve video-based analysis models~ _cite_ . The input feature vector is the latent encoding of a spatial CNN which corresponds to each frame of the video sequence. Here, we define an observation window which has _inline_eq_ frames backward and forward, centered at the current time-step _inline_eq_ . The label for each window is the force at time _inline_eq_, which corresponds to the middle vector in this window. The intuition behind utilizing a time-series model lies with the observation that anatomical surfaces are often deforming continuously. It is then reasonable to introduce time-varying features to determine these forces. In this paper, our RPC-TCN coalesces the above mentioned features to fully grasp the vision-based properties and then make the force prediction.