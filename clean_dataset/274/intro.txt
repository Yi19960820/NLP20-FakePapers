Recent progress on OCR methods using recurrent neural networks with LSTM architecture enabled effective training of recognition models for both modern (Nth century and later) and historical (Nth century and earlier) manuscripts and printings . Individually trained models regularly reached character recognition rates of about N \% for even the earliest printed books. The need to train individual models in order to reach this level of recognition accuracy for early printings sets the field of historical OCR apart from readily available (commercial and open-source) general (also called polyfont or omnifont) models trained on thousands of modern fonts which yield better than N \% recognition rates on Nth century printings and better than N \% on modern documents. Training historical recognition models on a variety of typesets results in mixed models which may be seen as a first approximation to modern polyfont models, but their predictive power is considerably lower than that of individual models. In view of the mass of available scans of historical printings we clearly need automatic methods of OCR which in turn require good historical polyfont models. As long as these models are not available and at present cannot be easily constructed (we lack the necessary historical fonts to be able to synthesize large amounts of training material automatically), our next best approach is to maximize the recognition rate of a small amount of manually prepared ground truth (GT) . This is the subject of the present paper which applies the methods of pretraining, voting, and active learning (AL) to the field of historical OCR. Using an already trained model as a starting point for subsequent training with additional individual material requires the capability to add specific characters not previously included in the symbol set (the codec) and the dynamic expansion (and reduction) of the output layer of the neural network. In the context of recurrent neural networks this was recently made possible by Christoph Wick as reported in _cite_ . Voting is a well known method of classifier combination resulting in less errors than the best single classifier output . Active learning ensures that lines showing maximal disagreement among classifiers are included in the training set to enable the maximal learning effect. While more training data is always better, combining these three methods results in a level of recognition accuracy that could otherwise only be reached by a much larger amount of GT and therefore a much larger manual effort to generate it. Chapter~ _ref_ summarizes the extensive corpus of related work for each of the three methods. In Chapter~ _ref_ we describe the printing material which the experiments of Chapter~ _ref_ are based on. Chapter~ _ref_ contains the discussion of our results and we conclude the paper with Chapter~ _ref_ .