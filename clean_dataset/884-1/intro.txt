Robot-assisted surgery has led to new opportunities to study human performance of surgery by enabling scalable, transparent capture of high-quality surgical-motion data in the form of surgeon hand movement and stereo surgical video. This data can be collected in simulation, benchtop training, and during live surgery, from novices in training and from experts in the operating room. This has in turn spurred new research areas such as automated skill assessment and automated feedback for trainees _cite_ . Although the ability to capture data is practically unlimited, a key barrier to progress has been the focus on supervised learning, which requires extensive manual annotations. Unlike the surgical-motion data itself, annotations are difficult to acquire, are often subjective, and may be of variable quality. In addition, many questions surrounding annotations remain open. For example, should they be collected at the low level of gestures _cite_, at the higher level of maneuvers _cite_, or at some other granularity? Do annotations transfer between surgical tasks? And how consistent are annotations among experts? We show that it is possible to learn meaningful representations of surgery from the data itself,, by searching for representations that can reliably predict future actions, and we demonstrate the usefulness of these representations in an information-retrieval setting. The most relevant prior work is _cite_, which encodes short windows of kinematics using denoising autoencoders, and which uses these representations to search a database using motion-based queries. Other unsupervised approaches include activity alignment under the assumption of identical structure _cite_ and activity segmentation using hand-crafted pipelines _cite_, structured probablistic models _cite_, and clustering _cite_ . Contrary to these approaches, we hypothesize that if a model is capable of predicting the future then it must encode contextually relevant information. Our approach is similar to prior work for learning video representations _cite_, however unlike _cite_ we leverage mixture density networks and show that they are crucial to good performance. Our contributions are N) introducing a recurrent-neural-network (RNN) encoder-decoder architecture with MDNs for predicting future motion and N) showing that this architecture learns encodings that perform well both qualitatively (Figs. _ref_ and _ref_) and quantitatively (Table _ref_) .