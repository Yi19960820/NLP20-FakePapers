Future video prediction is a challenging task that recently received much attention due to its capabilities for learning in an unsupervised manner, making it possible to leverage large volumes of unlabelled data for video-related tasks such as action and gesture recognition _cite_, task planning _cite_, weather prediction _cite_, optical flow estimation _cite_ and new view synthesis _cite_ . One of the main problems in this task is the need of expensive models both in terms of memory and computational power in order to capture the variability present in video data. Another problem is the propagation of errors in recurrent models, which is tied to the inherent uncertainty of video prediction: given a series of previous frames, there are multiple feasible futures. This, left unchecked, results in a blurry prediction averaging the space of possible futures that propagates back into the network when predicting subsequent frames. In this work we propose a new approach to recurrent auto-encoders (AE) with state sharing between encoder and decoder. We show how the exposed state in Gated Recurrent Units (GRU) can be used to create a bijective mapping between the input and output of each layer. To do so, the input is treated as a recurrent state, adding another set of logic gates to update it based on the output. Creating a stack of these layers allows for a bidirectional flow of information. Using the forward gates to encode inputs and the backward ones to generate predictions, we obtain a structure similar to an AE, but with many inherent advantages. It reduces memory and computational costs during both training and testing: only the encoder or decoder is executed for input encoding or prediction, respectively. Furthermore, the representation is stratified, encoding only part of the input at each layer: low level information not necessary to capture higher level dynamics is not passed to the next layer. Also, it naturally provides a noisy identity mapping of the input, facilitating the initial stages of training: the input to the first bGRU holds the last encoded frame or, if preceded by convolutional layers, an over-complete representation of the same. During generation, that first untrained bGRU randomly modifies the last input, introducing a noise signal. The approach also mitigates the propagation of errors: while it does not solve the problem of blur, it prevents its magnification in subsequent predictions. Moreover, a trained network can be deconstructed in order to analyse the role of each layer in the final predictions, making the model more explainable. Since the encoder and decoder states are shared, the architecture can be thought of as a recurrent AE folded in half, with encoder and decoder layers overlapping. We call our method Folded Recurrent Neural Network (fRNN) . Our main contributions are: N) A new shared-state recurrent AE with lower memory and computational costs. N) Mitigation of error propagation through time. N) It naturally provides an identity function during training. N) Model explainability and optimisation through layer removal. N) Demonstration of representation stratification.