Representing human emotions has been a basic topic of research in psychology. The most frequently used emotion representation is the categorical one, including the seven basic categories, i.e., Anger, Disgust, Fear, Happiness, Sadness, Surprise and Neutral _cite_ _cite_ . Discrete emotion representation can also be described in terms of the Facial Action Coding System (FACS) model, in which all possible facial actions are described in terms of Action Units (AUs) _cite_ . Finally, the dimensional model of affect _cite_ _cite_ has been proposed as a means to distinguish between subtly different displays of affect and encode small changes in the intensity of each emotion on a continuous scale. The N-D Valence and Arousal Space (VA-Space) is the most usual dimensional emotion representation. Figure _ref_ shows: i) on the left hand side, the N-D Emotion Wheel _cite_, with valence ranging from very positive to very negative and arousal from very active to very passive; ii) on the right hand side, some of the most common AUs along with definitions of the represented actions. Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past N years, in many application areas, such as medicine _cite_, health _cite_ and entertainment. Current research in automatic analysis of facial affect aims at developing systems, such as robots and virtual humans, that will interact with humans in a naturalistic way under real-world settings _cite_ . To this end, such systems should automatically sense and interpret facial signals relevant to emotions, appraisals and intentions. Basic research in face perception and emotion theory cannot be completed without large annotated databases of images and video sequences of facial expressions and underlying emotions. Some datasets that have been developed in the labs and are still used in many recent works include the Cohn-Kanade database _cite_ _cite_, MMI database _cite_ _cite_, Multi-PIE database _cite_ and BU-ND/BU-ND ones _cite_ _cite_ . Previous studies have reported good results in the automatic analysis of facial expressions and related emotions _cite_ . However, these results were obtained with analysis of images and videos captured in laboratory environments. That is, even when the expressions were spontaneous, the filming was done in controlled conditions, with full awareness of the participants. Hence, efforts have been made in order to collect videos of subjects displaying behaviors in-the-wild. To this end, Aff-Wild was created _cite_, constituting the first large-scale "in-the-wild" database, with over N hours of video data, annotated in terms of valence-arousal dimensions. However, even in this case, annotation is limited only to a single emotion representation, i.e., the VA one. Generating databases which are annotated in terms of more than a single emotion representation could assist in developing domain adaptation _cite_ and image generation techniques _cite_, as well as multi-task learning. Deep generative models have become widely popular for generative modeling of data. Generative adversarial networks (GANs) _cite_, in particular, have shown remarkable success in generating very realistic images in several cases _cite_ _cite_ . A typical GAN consists of the discriminator-which tries to tell apart real from fake examples by minimizing an appropriate loss function-and the generator-which tries to generate samples that maximize that loss _cite_ . One of the primary motivations for studying deep generative models has been semi-supervised learning. Indeed, several recent works have shown promising empirical results on semi-supervised learning with GANs. Most state-of-the-art semi-supervised learning methods based on GANs _cite_ use the GAN discriminator as a classifier which outputs _inline_eq_ probabilities (_inline_eq_ probabilities for the _inline_eq_ real classes and one for the fake class) . The generator is mainly used as a source of additional data (fake samples) which the discriminator tries to classify under the _inline_eq_ th label. Multi-task learning (MTL) _cite_ is a machine learning paradigm for learning a number of supervised learning tasks simultaneously, exploiting commonalities between them. MTL proved to successfully boost the performance of an individual task with the inclusion of other correlated tasks in the training process _cite_ _cite_ . MTL was first studied in _cite_, where the authors proposed to jointly learn parallel tasks sharing a common representation, and transferring part of the knowledge learned to solve one task to improve the learning of the other related tasks. One of the main difficulties with multitask approaches using different databases is the fact that not all the samples are labeled for all the tasks. In this work we make the following contributions: N) We annotate a part of the Aff-Wild database in terms of eight AUs and another part in terms of the seven basic expressions, exploiting its in-the-wild nature with: i) great variability of behaviors, ii) wide range of emotions, iii) rapid emotional changes and iv) different head poses, illumination conditions and occlusions. This work is a preliminary of _cite_ . N) By adapting current state-of-the-art GAN architectures to semi-supervised settings and by using the above annotations, we generate: i) realistic and vivid images of the persons appearing in the newly annotated parts of Aff-Wild and ii) new images of either unseen people, or new expressions and features of people already appearing in them. N) By designing and testing new appropriate loss functions for our data, we perform MTL experiments: i) using CNN-RNN networks with shared hidden layers, that jointly learn emotional attributes by exploiting their inter-dependencies and ii) using the GAN described in (N) above, so that it can generate realistic images, whilst serving as a good classifier and regressor.