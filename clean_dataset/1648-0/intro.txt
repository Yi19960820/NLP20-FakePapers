Naming or describing real life objects is only meaningful with respect to a relevant scale _cite_ . For example, a view can be described as a leaf, a branch, or a tree depending on the distance of the observer. Natural and casual scenes are generally composed of many different entities/objects at different scales. During image acquisition, the true physical scale is usually ignored. However, the relative scale of the objects is somehow implicitly captured and stored in the image grid and pixels. An automated method to identify or describe objects in images can be analyzed in two parts: representation + classification. Basic classification algorithms without add-ons can not successfully handle variation and complexity of raw pixel-level representation of objects, instead they rely on functions that map image pixels into different constructs,-named features-, which are sought to represent the image content more briefly and invariant to various geometric and intensity changes. Traditionally, computer vision researchers relied on manually designed feature extractors for representation. Recently, we are witnessing the success of the algorithms which can self-learn appropriate feature extractors. In either case, the size of an operator or a probe usually determines and fixes scale of the entities that can be represented. However, even in the self-learn case, size of the probes or operators is often manually selected. On the other hand, last two decades has seen many automated object detection/recognition algorithms that were superior to their counterparts because they have comprised multi-scale processing of images _cite_, _cite_ . Multi-scale feature extractors gather and present the inherent scale information of image pixels to a subsequent classifier. In SIFT _cite_ and wavelets _cite_, this is done by creating a multi-scale pyramid from the input image and then applying a fixed size probe-kernel to each scale. In an application of Gabor filters for object recognition, Serre et al. _cite_ used a hierarchy of stacked Gabor filtering layers, where the filters have predetermined scales and orientations. However, Chan et al. _cite_ showed that the adaptation of handcrafted filters to low-level representations is difficult. On the other hand, convolution neural networks (CNN) rely on stacked and hierarchical convolutions of the image to extract multi-scale information. Convention of CNNs for filter size selection is to use small fixed size weight kernels in the lower levels. However, thanks to the stacked operation of convolutional layers, sandwiched by pooling layers which down-sample intermediate outputs, the deeper levels of a network are able to learn representations of larger scales. Though the optimality of fixed size kernels has not proven, the convention is to use filters small as NxN in the first layer, which can be larger NxN or NxN in the later stages _cite_ . During back propagation training, filters are evolved to imitate lower level receptive fields in biological vision which are sensitive to certain shapes and orientations. Another justification for avoiding large filter sizes is that, while certainly increasing computation time, they may also increase over-fitting. Though the number of filters and their sizes in convolution layers are usually selected intuitively, researchers are seeking alternatives to improve representation capacity of the network in deeper architectures. For example, Szegedy et al. _cite_ handcrafted their ``inception'' architecture to include mixing of parallel and wide convolution layers which use different sized filter kernels. In a deep architecture, this approach allows multi-scale, parallel and sparse representations. _cite_ In summary, existing CNN based methods use fixed size convolution kernels and then rely on the fact that shape and orientation of the filters can be inferred from the training data. Additionally, CNNs employ stacked convolution layers to successfully create multi-scale representations. On the other hand, Hubel and Wiesel _cite_ discovered three type of cells in visual cortex: simple, complex, hyper-complex (i.e. \end-stopped cells) . The simple cells are sensitive to the orientation of the excitatory input, whereas the hypercomplex cells are activated with a certain orientation, motion, and of the stimuli. Therefore it is biologically plausible to assume that filters of different scales next to different orientations and direction may also work better in CNNs. In this study, we create a new and adaptive model of convolution layers where the filter size (actually scale) and orientation are learned during training. Therefore, a single convolution layer can have distinct filter scales and orientations. Broadly speaking, this corresponds to extracting multi-scale information using a single convolution layer. However, our aim is not to fully replace the stacked architectures and deep networks for multi-scale information. Instead, our approach improves the information that can be extracted from an input (may be an image), in a single layer. Additionally, the model removes the necessity of fixing convolution kernel sizes, so that the filter size can be removed from the list of hyper-parameters of deep learning networks. Our experiments use MNIST, MNIST-cluttered and CIFAR-N datasets to show that the proposed model can learn and produce different scaled filters in a single convolution layer whilst improving classification performance compared to conventional convolution layers. In experimental side, our work is concentrated on developing and proving an effective methodology for learnable adaptive filter scales and orientations, rather than improving highly optimized state-of-the-art results in these datasets. Organization of the paper is as follows _ref_ .