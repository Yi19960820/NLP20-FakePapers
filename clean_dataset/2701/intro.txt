One of the most explored problems in remote sensing is the pixelwise labeling of satellite imagery. Such a labeling is used in a wide range of practical applications, such as precision agriculture and urban planning. Recent technological developments have substantially increased the availability and resolution of satellite data. Besides the computational complexity issues that arise, these advances are posing new challenges in the processing of the images. Notably, the fact that large surfaces are covered introduces a significant variability in the appearance of the objects. In addition, the fine details in high-resolution images make it difficult to classify the pixels from elementary cues. For example, the different parts of an object often contrast more with each other than with other objects _cite_ . Using high-level contextual features thus plays a crucial role at distinguishing object classes. Convolutional neural networks (CNNs) _cite_ are receiving an increasing attention, due to their ability to automatically discover relevant contextual features in image categorization problems. CNNs have already been used in the context of remote sensing _cite_, featuring powerful recognition capabilities. However, when the goal is to label images at the pixel level, the output classification maps are too coarse. For example, buildings are successfully detected but their boundaries in the classification map rarely coincide with the real object boundaries. We can identify two main reasons for this coarseness in the classification: a) There is a structural limitation of CNNs to carry out fine-grained classification. If we wish to keep a low number of learnable parameters, the ability to learn long-range contextual features comes at the cost of losing spatial accuracy, i.e., a trade-off between detection and localization. This is a well-known issue and still a scientific challenge _cite_ . b) In the specific context of remote sensing imagery, there is a significant lack of spatially accurate reference data for training. For example, the OpenStreetMap collaborative database provides large amounts of free-access maps over the earth, but irregular misregistrations and omissions are frequent all over the dataset. In such circumstances, CNNs cannot do better than learning rough estimates of the objects' locations, given that the boundaries are hardly located on real edges in the training set. Let us remark that in the particular context of high-resolution satellite imagery, the spatial precision of the classification maps is of paramount importance. Objects are small and a boundary misplaced by a few pixels significantly hampers the overall classification quality. In other application domains, such as semantic segmentation of natural scenes, while there have been recent efforts to better shape the output objects, a high resolution output seems to be less of a priority. For example, in the popular Pascal VOC semantic segmentation dataset, there is a band of several unlabeled pixels around the objects, where accuracy is not computed to assess the performance of the methods. There are two recent approaches to overcome the structural issues that lead to coarse classification maps. One of them is to use new types of CNN architectures, specifically designed for pixel labeling, that seek to address the detection/localization trade-off. For example, Noh et al.~ _cite_ duplicate a base classification CNN by attaching a reflected ``deconvolution'' network, which learns to upsample the coarse classification maps. Another tendency is to use first the base CNN as a rough classifier of the objects' locations, and then process this classification using the original image as guidance, so that the output objects better align to real image edges. For example, Zheng et al.~ _cite_ use a fully connected CRF in this manner, and Chen et al.~ _cite_ diffuse the classification probabilities with an edge-stopping function based on image features. Both approaches have also been adopted by the remote sensing community, mostly in the context of the ISPRS Semantic Labeling Contest, to produce fine-grained labelings of high-resolution aerial images~ _cite_ . While all these works have certainly pushed the boundaries of CNN capabilities for pixel labeling, they assume the availability of large amounts of precisely labeled training data. This paper targets the task of dealing with more realistic datasets, seeking to provide a means to refine classification maps that are too coarse due to poor reference data. The first scheme, i.e., the use of novel CNN architectures, seems unfeasible in the context of large-scale satellite imagery, due to the nature of the available training data. Even if an advanced architecture could eventually learn to conduct a more precise labeling, this is not useful when the training data itself is inaccurate. We thus here adopt the second strategy, reinjecting image information to an enhancement module that sharpens the coarse classification maps around the objects. To train or set the parameters of this enhancement module, as well as to validate the algorithms, we assume we can afford to manually label small amounts of data. In Fig.~ _ref_ (a) we show an example of imprecise data to which we have access in large quantities, and in Fig.~ _ref_ (b) we show a portion of manually labeled data. In our approach, the first type of data is used to train a large CNN to learn the generalities of the object classes, and the second to tune and validate the algorithm that enhances the coarse classification maps outputted by the CNN. An algorithm to enhance coarse classification maps would require, on the one hand, to define the image features to which the objects must be attached. This is data-dependent, not every image edge being necessarily an object boundary. On the other hand, we must also decide which enhancement algorithm to use, and tune it. Besides the efforts that this requires, we could also imagine that the optimal approach would go beyond the algorithms presented in the literature. For example we could perform different types of corrections on the different classes, based on the type of errors that are often present in each of them. Our goal is to create a system that learns the appropriate enhancement algorithm itself, instead of designing it by hand. This involves learning not only the relevant features but also the rationale behind the enhancement technique, thus intensively leveraging the power of machine learning. To achieve this, we first formulate a generic partial differential equation governing a broad family of iterative enhancement algorithms. This generic equation conveys the idea of progressively refining a classification map based on local cues, yet it does not provide the specifics of the algorithm. We then observe that such an equation can be expressed as a combination of common neural network layers, whose learnable parameters define the specific behavior of the algorithm. We then see the whole iterative enhancement process as a recurrent neural network (RNN) . The RNN is provided with a small piece of manually labeled image, and trained end to end to improve coarse classification maps. It automatically discovers relevant data-dependent features to enhance the classification as well as the equations that govern every enhancement iteration. A common way to tackle the aerial image labeling problem is to use classifiers such as support vector machines~ _cite_ or neural networks _cite_ on the individual pixel spectral signatures (i.e., a pixel's ``color'' but not limited to RGB bands) . In some cases, a few neighboring pixels are analyzed jointly to enhance the prediction and enforce the spatial smoothness of the output classification maps _cite_ . Hand-designed features such as textural features have also been used _cite_ . The use of an iterative classification enhancement process on top of hand-designed features has also been explored in the context of image labeling _cite_ . Following the recent advent of deep learning and to address the new challenges posed by large-scale aerial imagery, Penatti et al.~ _cite_ used CNNs to assign aerial image patches to categories (e.g., `residential', `harbor') and Vakalopoulou et al.~ _cite_ addressed building detection using CNNs. Mnih _cite_ and Maggiori et al.~ _cite_ used CNNs to learn long-range contextual features to produce classification maps. These networks require some degree of downsampling in order to consider large contexts with a reduced number of parameters. They perform well at detecting the presence of objects but do not outline them accurately. Our work can also be related to the area of natural image semantic segmentation. Notably, fully convolutional networks (FCN) _cite_ are becoming increasingly popular to conduct pixelwise image labeling. FCN networks are made up of a stack of convolutional and pooling layers followed by so-called deconvolutional layers that upsample the resolution of the classification maps, possibly combining features at different scales. The output classification maps being too coarse, the authors of the Deeplab network _cite_ added a fully connected conditional random field (CRF) on top of both the FCN and the input color image, in order to enhance the classification maps. Most of the strategies developed for natural images segmentation have been adapted to high-resolution aerial image labeling and tested on the ISPRS benchmark, including advanced FCNs~ _cite_ and CNNs coupled with CRFs~ _cite_ . Zheng et al.~ _cite_ recently reformulated the fully connected CRF of Deeplab as an RNN, and Chen et al.~ _cite_ designed an RNN that emulates the domain transform filter _cite_ . Such a filter is used to sharpen the classification maps around image edges, which are themselves detected with a CNN. In these methods the refinement algorithm is designed beforehand and only few parameters that rule the algorithm are learned as part of the network's parameters. The innovating aspect of these approaches is that both steps (coarse classification and enhancement) can be seen as a single end-to-end network and optimized simultaneously. Instead of predefining the algorithmic details as in previous works, we formulate a general iterative refinement algorithm through an RNN and let the network learn the specific algorithm. To our knowledge, little work has explored the idea of learning an iterative algorithm. In the context of image restoration, the preliminary work by Liu et al.~ _cite_ proposed to optimize the coefficients of a linear combination of predefined terms. Chen et al.~ _cite_ later modeled this problem as a diffusion process and used an RNN to learn the linear filters involved as well as the coefficients of a parametrized nonlinear function. Our problem is however different, in that we use the image as guidance to update a classification map, and not to restore the image itself. Besides, while we drew inspiration on diffusion processes, we are also interested in imitating other iterative processes like active contours, thus we do not restrict our system to diffusions but consider all PDEs.