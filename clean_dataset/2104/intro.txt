Deep learning has enabled us to learn various sophisticated models using large amounts of labeled data. Computer vision tasks such as image recognition, segmentation, face recognition, etc. require large volumes of labeled data to build reliable models. However, when the training data is not sufficient, in order to avoid over-fitting, it is a common practice to use pre-trained models rather than training from scratch. This enables us to utilize the large volumes of data (eg: ~ _cite_) on which the pre-trained models are learned and transfer that knowledge to the new target task. Hierarchical nature of the learned representations and task specific optimization makes it easy to reuse them. This process of reusing pre-training and learning new task specific representations is referred to as transfer learning or fine-tuning the pre-trained models. There exist many successful instances of transfer learning~ (e.g.~ _cite_) in computer vision using Convolution Neural Networks (CNNs) . Large body of these adaptations are fine-tuned architectures of the well-known recognition models~ _cite_ trained on the IMAGENET~ _cite_ and Places~ _cite_ datasets. However, these models perform object or scene classification and have very limited information about the image. All that these models are provided with during training is the category label. No other information about the scene is provided. For example, the image shown in Figure _ref_ has as labels. Useful information such as is missing. Tasks such as image search (similar image retrieval) suffer from the features learned by this weak supervision. Image retrieval requires the models to understand the image contents in a better manner (eg: ~ _cite_) to be able to retrieve similar images. Specifically, when the images have multiple objects and graded relevance scores (multiple similarity levels, eg: on a scale from _inline_eq_ to _inline_eq_) instead of binary relevance (similar or dissimilar), the problem becomes more severe. On the other hand, automatic caption generation models~ (e.g.~ _cite_) are trained with human given descriptions of the images. These models are trained with stronger supervision compared to the recognition models. For example, Figure~ _ref_ shows pair of images form MSCOCO~ _cite_ dataset along with their captions. Richer information is available to these models about the scene than mere labels. In this paper, we exploit the features learned via strong supervision by these models and learn task specific image representations for retrieval via pairwise constraints. In case of CNNs, the learning acquired from training for a specific task (e.g. recognition on IMAGENET) is transferred to other vision tasks. Transfer learning followed by task specific fine-tuning has proven to be efficient to tackle less data scenarios. However, similar transfer learning is left unexplored in the case of caption generators. For the best of our knowledge, this is the first attempt to explore that knowledge via fine-tuning the representations learned by them to a retrieval task. The major contributions of our work can be listed as: The paper is organised as follows: Section~ _ref_ provides a short summary of _cite_ and _cite_ before presenting details about the proposed approach to perform transfer learning. This section also discusses the proposed fusion architecture. Section~ _ref_ details the experiments performed on benchmark datasets and discusses various aspects along with the results. Finally, Section~ _ref_ concludes the paper.