We focus on a problem that we call ``local light field synthesis", which we define as the promotion of a single photograph to a plenoptic camera light field. One can think of this as expansion from a single view to a dense ND patch of views. We argue that local light field synthesis is a core visual computing problem with high potential impact. First, it would bring light field benefits such as synthetic apertures and refocusing to everyday photography. Furthermore, local light field synthesis would systematically lower the sampling rate of photographs needed to capture large baseline light fields, by ``filling the gap" between discrete viewpoints. This is a path towards making light field capture for virtual and augmented reality (VR and AR) practical. In this work, we hope to convince the community that local light field synthesis is actually a tractable problem. From an alternative perspective, the light field synthesis task can be used as an unsupervised learning framework for estimating scene geometry from a single image. Without any ground-truth geometry for training, we can learn to estimate the geometry that minimizes the difference between the light field rendered with that geometry and the ground-truth light field. Light field synthesis is a severely ill-posed problem, since the goal is to reconstruct a ND light field given just a single image, which can be interpreted as a ND slice of the ND light field. To alleviate this, we use a machine learning approach that is able to utilize prior knowledge of natural light fields. In this paper, we focus on scenes of flowers and plants, because they contain interesting and complex occlusions as well as a wide range of relative depths. Our specific contributions are the introduction of the largest available light field dataset, the prediction of ND ray depths with a novel depth consistency regularization to improve unsupervised depth estimation, and a learning framework to synthesize a light field from a single image. We collect the largest available light field dataset (Sec.~ _ref_), contaning N light fields of flowers and plants, taken with the Lytro Illum camera. Our dataset limits us to synthesizing light fields with camera-scale baselines, but we note that our model can generalize to light fields of any scene and baseline given the appropriate datasets. Current view synthesis methods generate each view separately. Instead, we propose to concurrently predict the entire ND light field by estimating a separate depth map for each viewpoint, which is equivalent to estimating a depth for each ray in the ND light field (Sec.~ _ref_) . We introduce a novel physically-based regularization that encourages the predicted depth maps to be consistent across viewpoints, alleviating typical problems that arise in depths created by view synthesis (Fig.~ _ref_) . We demonstrate that our algorithm can predict depths from a single image that are comparable or better than depths estimated by a state-of-the-art physically-based non-learning method that uses the entire light field~ _cite_ (Fig.~ _ref_) . We create and study an end-to-end convolutional neural network (CNN) framework, visualized in Fig.~ _ref_, that factorizes the light field synthesis problem into the subproblems of estimating scene depths for every ray (Fig.~ _ref_, Sec.~ _ref_) (we use depth and disparity interchangeably, since they are closely related in structured light fields), rendering a Lambertian light field (Sec.~ _ref_), and predicting occluded rays and non-Lambertian effects (Sec.~ _ref_) . This makes the learning process more tractable and allows us to estimate scene depths, even though our network is trained without any access to the ground truth depths. Finally, we demonstrate that it is possible to synthesize high-quality ray depths and light fields of flowers and plants from a single image (Fig.~ _ref_, Fig.~ _ref_, Fig.~ _ref_, Fig.~ _ref_, Sec.~ _ref_) .