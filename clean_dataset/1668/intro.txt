To achieve robust, autonomous operation in unstructured environments, robots must be able to identify relevant objects and features in their surroundings, recognize the context of the situation, and plan their motions and interactions accordingly. Recent efforts in autonomous manipulation challenges such as the DARPA Robotics Challenge~ _cite_ and the Amazon Picking Challenge~ _cite_ resulted in state-of-the-art perception capabilities enabling systems to perceive, reason about, and manipulate their surroundings. However, existing object identification and pose estimation solutions for closed-loop manipulation tasks are generally (N) not robust in cluttered environments with partial occlusions, (N) not able to operate in real-time (_inline_eq_), (N) not sufficiently accurate~ _cite_, or (N) incapable of high accuracy without good initial seeds _cite_ . We present a novel perception pipeline that tightly integrates deep semantic segmentation and model-based object pose estimation, achieving real-time pose estimates with a median pose error of _inline_eq_ and _inline_eq_ . Our solution (referred to as) uses RGB-D sensors (and proprioceptive information when available) to provide semantic segmentation of all relevant objects in the scene along with their respective poses (see Figure~ _ref_) in a architecture. The main contributions of this manuscript are as follows: