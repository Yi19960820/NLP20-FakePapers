In recent years, deep learning models have attracted significant attention thanks to their powerful regression capacity in a spectrum of applications from speech recognition to natural language processing and computer vision. It is recognized that training of deep neural networks requires a large corpus of labeled data to attain generality of the learned models and robustness of the feature maps. Such networks seem less useful for one-shot learning tasks where the objective is to learn a model, often in an online fashion, from a single exemplar (or a few) . One exemption is the embedding with Siamese deep networks _cite_, since it is not necessary to retrain the deep model for a newly given object or class. Siamese architectures can identify other instances of the target class from its original exemplar using a fixed model. Conventional Siamese networks use tuples of two labeled instances for training _cite_ . They are sensitive to calibration and the adopted notion of similarity vs. dissimilarity depending on the given context _cite_ . The requirement of calibration can be removed by applying triplets for training with a contrastive loss, which favors a small distance between pairs of exemplars labeled as similar, and a large distance for pairs labeled dissimilar _cite_ . At the same time, triplets enable utilization of the underlying connections among more than two instances. Our intuition is that more instances (larger tuples) lead to better performance in the learning process. Therefore, we design a new network structure by adding as many instances into a tuple as possible (including a triplet and multiple pairs) and connect them with a novel loss combining a pair-loss and a triplet based contractive-loss. Existing Siamese _cite_ or triplet networks _cite_ do not use the full potential of the training instances since they take randomly sampled pairs or triplets to construct the training batches. In contrast, our framework aims to select the triplets that would lead to a powerful representation for a stronger deep network. In this paper, we introduce a novel quadruplet network for one-shot learning. Our quadruplet network is a discriminative model to one-shot learning. Given a single exemplar of a new object class, its learned model can recognize objects of the same class. To capture the underlying relationships of data samples, we design four branch networks with shared weights for different inputs such as instances, exemplar, positive and negative branches. We randomly sample a set of positive and negative instances as inputs of instances branch. A pair of loss function is designed to connect the exemplar and instances branch to utilize the underlying relationships of pairs. To achieve the triplet of representation, we select the powerful positive instance (which is most similar to the exemplar) and the negative instance (which is most dissimilar one) as the inputs of positive and negative branches, respectively. Then, we use a contractive loss function to measure the similarity of this triplet. Finally, the weighted average of the pair loss and the contractive loss is assigned as the final loss. The triplet loss is the key to utilize the underlying connections among instances to achieve improved performance. To combine it and pair loss, a simple solution is to apply a weighted average with prior weights between these two losses. However, directly applying prior weights maybe not improve even reduce performance. For example, we test our approach with prior weights for visual object tracking on OTB-N benchmark _cite_ while the distance precision is reduced from N to N Thus, we propose a weight layer to automatically choose suitable combination weights during training to solve this problem. We evaluate the quadruplet network with one-shot learning for visual object tracking. SiamFc-Ns _cite_ is our baseline tracker. We apply our quadruplet network instead of its Siamese network to train the shared net and adapt the same mechanism for online tracking. As shown in Fig. _ref_, our training method achieves better tracking accuracy, which demonstrates the more powerful representation of our framework. In several popular tracking benchmarks, our experimental results show that the tracker runs at high real-time speed (N frames-per-second on OTB-N) and achieves excellent results compared with recent state-of-the-art real-time trackers. The main contributions of this work are summarized as: