This paper discusses the problem of learning representation coefficients between two spaces/subspaces. This is one typical and general research topic that can be used in various tasks, such as learning feature embedding in Few-shot learning (FSL) and capturing relational structures in Zero-Shot Learning (ZSL) () . In particular, FSL () aims to learn new concepts with only few training samples, while ZSL tends to learn new concepts without any training samples. The semantic spaces such as attributes, textual descriptions and word vectors are served as the auxiliary knowledge to assist the ZSL. This paper concerns the FSL and ZSL in transfer learning scenario. The data in the source domain is abundant to train the feature extractors (e.g., deep Convolutional Neural Networks (CNNs)) ; and the data in the target are very limited to learn/fine-tune a deep model. The natural solutions of FSL and ZSL are to learn the linear embedding models, which can map the image features to the label space (FSL) (or semantic space (ZSL)) . To efficiently learn such a linear model, _inline_eq_ or _inline_eq_ penalty terms are frequently applied to regularize the weights of embedding models. In particular, the _inline_eq_ regularization can capture the strong and sparse signals in the embedding weights, which is also a process of feature selection. Nevertheless, the feature selection property of _inline_eq_ penalty suffers from two problems. N) the inaccurate estimation of strong signals if irrepresentable condition does not hold ; N) the underfitting of training data due to the ignorance of weak signals from the embedding / relational weights. In contrast, _inline_eq_ penalty yet does a proportional shrinkage of feature dimension, and thus it may introduce the bias in learning the embedding model. However, in real-world applications, it is of equal importance to do both the feature selection and well data-fitting. For example, in Text Classification (), Bioinformatics () and Neuroimaging Analysis (), researchers need to fit the training data well; and meantime, select a few strong signals (features) which are comprehensible for human beings. In this paper, we propose that the embedding features consist of random noise, sparse strong signals and dense weak signals. In Sec. _ref_, the LBI is for the first time proposed to facilitate the decomposition of features. Particularly, in our linear embedding models, the {MSplit} LBI will decompose the embedding weights into three orthogonal parts (in Sec. _ref_),, random noise, sparse strong signals and dense weak signals as illustrated in Fig. _ref_ . The sparse strong signals can serve the purpose of feature selection, and the dense estimation can be done by integrating both sparse strong signals and dense weak signals. Furthermore, we theoretically analyze the property of {MSplit} LBI estimator in Sec. _ref_ which is less biased than _inline_eq_ regularization and can facilitate the feature selection at the same time. We further show the way of using proposed {MSplit} LBI in FSL and ZSL tasks in Sec. _ref_ . Extensive experiments had been done to validate the proposed {MSplit} LBI can learn better embedding models. \noindent Contribution . The main contributions are several folds: (N) We for the first time propose the idea of decomposing the feature representation into three orthogonal elements,, strong signals, weak signals and random noise. (N) The algorithm is for the first time proposed to facilitate such orthogonal decomposition and thus learn two estimators: the sparse estimator learning strong signals and the dense estimator additionally capturing the weak signals that also contribute to the estimation. (N) The theoretical analysis is given in terms of the advantages over commonly applied regularization penalties (_inline_eq_, _inline_eq_ or elastic net) ; (N) The benefits and effectiveness of proposed methodology are demonstrated on simulation experiments and tasks of feature embedding learning in FSL and relational structure learning in ZSL.