Do machine learning algorithms need to be explainable? This is an important question in today's world where machine learning algorithms, especially those based on deep learning are being used at a wide range of tasks and have shown tremendous efficacy in performing these tasks. Deep learning has touted as being very disruptive to many sectors, particularly the finance sector. However, deep learning, to large extent, have essentially been unexplainable "black boxes", with no clear explanation as to how they reach particular decisions~ _cite_ . This is a major hindrance to the widespread adoption of deep learning in industries like finance, where regulations are very tight. In such industries with strict regulatory processes, the AI models used are required to be transparent, interpretable, and explainable. Many experts in these sectors believe that relying on such 'black box' methods is a growing problem that is already very relevant due to regulatory processes in these sectors, and it is going to be increasingly more relevant in the future. For example, in finance, law requires companies to explain the reason behind every decision to its perspective customer~ _cite_ . As such, current approaches for leveraging deep learning are not feasible such in these scenarios. The limitation of deep learning in terms of transparency and interpretability have forced industries dealing with regulatory scenarios to use comparatively simple machine learning algorithms such as linear or logistic regression, decision trees, or ensemble methods such as random forests which are significantly more explainable and quite effective in simple cases. However, as the complexity of the problem increases, which is very true in finance, deep learning algorithms have been shown to outperform such traditional algorithms by a wide margin across a wide range of problem domains~ _cite_ . As such, strategies for explaining the decisions made by deep learning algorithms are highly desired to enable their widespread use in sectors that have strong regulatory processes. More recently, a number of methods were proposed to mitigate this issue of interpretability and transparency in deep learning. For example, Zeiler \& Fergus~ _cite_ proposed the formation of a parallel deconvolutional network to peer into different units of the network. Ribeiro~ _cite_ introduced a method to build trust in models that are locally accurate, i.e., it is correct near the input data sample. Selvaraju et.al.~ _cite_ proposed a method called Grad-CAM that enables users to discern "strong" networks from the weaker ones. While promising, all of the aforementioned approaches are restricted to identifying regions of interest and their influence in the decision made by the deep neural network only, thus restricting their utility for gaining a more detailed understanding of the decision process. To address this issue, Kumar et. al.~ _cite_ recently proposed a CL ass E nhanced A ttentive R esponse (CLEAR) approach that not only identifies attentive regions of interest and their influence on the decision made, but more important provides the dominant classes associated with the attentive regions of interest. This additional information about the dominant classes and their influence on the decision making progress leads to a higher degree of human interpretability, which makes it very well suited for scenarios that necessitate regulatory processes such as in finance. Motivated by this, in this paper, we propose CLEAR-Trade, a CL ass E nhanced A ttentive R esponse approach to explaining and visualizing deep learning-driven stock market prediction. In particular, CLEAR-Trade is designed in this paper to provide detailed explanations for the prediction decisions made by a deep learning-driven binary stock market prediction network, as shown in Fig.~ _ref_ . Our aim is to create a powerful tool for peering into the minds of these otherwise uninterpretable 'black box' financial AI models to better visualize and understand why they are making the decisions the way they do. Doing this will have a tremendous impact on day-to-day work of financial analysts in helping them better understand these deep learning-driven financial AI models, thus potentially enabling the widespread adoption of transparent financial AI.