Road detection is an important task that needs to be solved accurately and robustly in order to achieve higher automation levels. Knowing what regions of the road surface are available for driving is in fact a crucial prerequisite for carrying out safe trajectory planning and decision-making. Although some automated driving vehicles are already available on the market, the recent crash of a Tesla car controlled by its autopilot system highlighted that further research and testing are very much necessary. In that case, it was pointed out that a possible reason for the crash was that the autopilot system misinterpreted the trailer of a truck as free road due to unfavourable lighting conditions _cite_ . Current approaches for road detection use either cameras or LIDAR sensors. Cameras can work at high frame-rate, and provide dense information over a long range under good illumination and fair weather. However, being passive sensors, they are strongly affected by the level of illumination. A passive sensor is able to receive a specific amount of energy from the environment, light waves in the case of cameras, and transform it into a quantitative measure (image) . Clearly, the process depends on the amplitude and frequency of the light waves, influencing the overall result, while a reliable system should be invariant with respect to changes in illumination _cite_ . LIDARs sense the environment by using their own emitted pulses of laser light and therefore they are only marginally affected by the external lighting conditions. Furthermore, they provide accurate distance measurements. However, they have a limited range, typically between N and N meters, and provide sparse data. Based on this description of benefits and drawbacks of these two sensor types, it is easy to see that using both might provide an improved overall reliability. Inspired by this consideration, the work presented here investigates how LIDAR point clouds and camera images can be integrated for carrying out road segmentation. The choice to use a fully convolutional neural network (FCN) for LIDAR-camera fusion is motivated by the impressive success obtained by deep learning algorithms in recent years in the fields of computer vision and pattern recognition _cite_ . In summary, this work makes the following two main contributions: (i) A novel LIDAR-camera fusion FCN that outperforms established approaches found in the literature and achieves state-of-the-art performance on the KITTI road benchmark; (ii) a data set of visually challenging scenes extracted from KITTI driving sequences that can be used to further highlight the benefits of combining LIDAR data and camera images for carrying out road segmentation. The remainder of the paper is structured as follows: Sect.~ _ref_ gives a brief overview of related approaches that deal with the problems of road detection or sensor fusion. The FCN base architecture and the fusion strategies are presented in Sect.~ _ref_ . Section _ref_ describes the procedure to transform a sparse ND point cloud into a set of dense ND images. The experimental results and discussion are reported in Sect.~ _ref_ which is followed, in Sect.~ _ref_, by a summary and the conclusions.