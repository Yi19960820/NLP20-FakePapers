Human body pose recognition in video is a long-standing problem in computer vision with a wide range of applications. However, body pose recognition remains a challenging problem due to the high dimensionality of the input data and the high variability of possible body poses. Traditionally, computer vision-based approaches tend to rely on appearance cues such as texture patches, edges, color histograms, foreground silhouettes or hand-crafted local features (such as histogram of gradients (HoG) ~ _cite_) rather than motion-based features. Alternatively, psychophysical experiments~ _cite_ have shown that motion is a powerful visual cue that alone can be used to extract high-level information, including articulated pose. Previous work~ _cite_ has reported that using motion features to aid pose inference has had little or no impact on performance. Simply adding high-order temporal connectivity to traditional models would most often lead to intractable inference. In this work we show that deep learning is able to successfully incorporate motion features and is able to out-perform existing state-of-the-art techniques. Further, we show that by using motion features alone our method outperforms~ _cite_ (see Fig~ _ref_ (a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available. This paper makes the following contributions: