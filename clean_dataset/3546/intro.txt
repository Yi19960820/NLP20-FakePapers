Action recognition from depth sequences _cite_ has attracted significant interest recently due to the emergence of low-cost depth sensors. Human action refers to a temporal sequence of primitive movements carried out by a person _cite_ . Recurrent neural network (RNN) _cite_ is naturally suited for modeling temporal dynamics of human actions as it can be used to model joint probability distribution over sequences, especially in the case of long short-term memory (LSTM) _cite_ which is capable of modeling long-term contextual information of complex sequential data. RNN-based approaches become the dominant solution _cite_ for action recognition from depth sequence recently. However, these approaches require either skeleton joints _cite_ or hand-crafted depth features _cite_ as inputs in both training and testing. Skeleton-based action recognition assumes that a robust tracker can estimate body joints accurately in the testing stage. This often does not hold in practice, especially when a human body is partly in view or the person is not in an upright position. Hand-crafted features with heuristic parameters are designed for task-specific data. This often requires multi-stage processing phases, each of which needs to be carefully designed and tuned. An end-to-end trainable model from raw video frames _cite_ is desired to extract spatio-temporal features and model complex sequences in a unified framework. This learning pipeline typically combines a deep convolutional neural network (CNN) _cite_ as visual feature extractor and an RNN _cite_ to model and recognize temporal dynamics of sequential data. Unfortunately, these conventional end-to-end manners (CNN + RNN) are difficult to be applied to action recognition from depth sequences due to the fact that: N) Color and texture are precluded in depth maps, which weaken the discriminative power of the representation captured by the CNN model. N) Existing depth data of human actions are considered as a small-scale dataset compared to publicly available RGB image dataset. These conventional pipelines are purely data-driven that learn its representation directly from the pixels. Such model is likely at the risk of overfitting when the network is optimized on limited training data. To address the above-mentioned issues, we propose a privileged information-based recurrent neural network (PRNN) that exploits additional knowledge to obtain a better estimate of network parameters. This additional knowledge, also referred to as privileged information (PI) _cite_, hidden information _cite_ or side information _cite_, is only available during training but not available during testing. Our model aims to encode PI into the structure or parameters of networks automatically and effectively during the training stage. In this work, we consider skeleton joints as the PI in the proposed three-step training process (see Fig.~ _ref_) . A pre-training stage is introduced that taking both depth sequences and skeleton joints as input. The learned embedding layers construct intermediate distributions over the appearance of depth sequences and skeleton joints. As our method aims to utilize only depth sequences as input in testing stage, we then optimize our model by formulating the PI into an multi-task loss in learning step: a standard softmax classification loss as our primary task, and a regression loss as our secondary task, which learn the mapping parameters to predict the skeleton joints from depth appearance. However, We observe empirically that exploiting PI as a secondary task provides little help to improve the performance of primary task due to the gap between them. Finally, a bridging matrix is defined to connect two tasks by discovering latent PI in the refining step. We present a PI-based classification loss serving as a connector to maintain a consistency between latent PI and primary output distribution by penalizing the violation of the loss inequality. We enforces dependencies across regression and classification targets by seeking shared information. The bridging matrix, latent PI and network parameters are iteratively estimated and updated in an expectation-maximization (EM) procedure. This proposed learning process can provide greater discriminative power to model subtle depth difference, while helping avoid overfitting the scarcer training data. As we encode skeleton joints as PI, our model does not require a skeleton tracker in a testing stage, showing its better generalizability in a more challenging scenario, such as when a human body is partly in view or the person is not in an upright position. We evaluate the proposed PRNN against state-of-the-arts on the task of action recognition from depth sequences. We demonstrate that our approach can achieve higher accuracy on the three public benchmark datasets: MSR ActionND _cite_, SBU Interaction dataset _cite_ and Cornell Activity _cite_ . A larger performance gain can be obtained on our newly collected Blanket dataset, where actions captured from a challenging camera view-point and some actions are partially occluded by a blanket. We also compare with several variants of our model and show that each component consistently contributes to the overall performance.