\noindent Deep neural networks (DNNs) have revitalized the field of machine learning by achieving accuracy levels beyond human perception in a variety of image, video, text and speech processing tasks~ _cite_ . Today, they have emerged as the solution approach for challenging artificial intelligence problems and are deployed in several products and services, including Google's image and voice search~ _cite_, Facebook's DeepFace~ _cite_ and DeepText~ _cite_, Apple's Siri~ _cite_, to name a few. However, one of the key challenges to the ubiquitous adoption of DNNs is their computational demand, which far outstrips the capabilities of modern computing platforms. For example, ResNet-N~ _cite_, a state-of-the-art DNN for image recognition, requires _inline_eq_ N giga operations to classify a single N _inline_eq_ N image. As DNNs begin to pervade the spectrum of computing devices from high performance servers to low-power edge/IoT devices, the demands on compute efficiency are expected grow even more stringent. Consequently, there is an urgent need to explore new approaches to improve their implementation efficiency. Realizing this need, prior research efforts have explored several key directions. A majority of research efforts exploit the compute and communication characteristics of DNNs to efficiently parallelize them on distributed platforms~ _cite_ or design specialized accelerator architectures~ _cite_ . Other efforts leverage the intrinsic error resilience of DNNs to approximate selected computations~ _cite_ or utilize non-CMOS technologies whose device characteristics match the compute kernels of DNNs~ _cite_ . Complementary to the above efforts, we focus on a different design scenario and target deeply embedded systems such as wearables and IoT edge devices where the additional area/ cost imposed by custom-accelerators is prohibitive. For example, even a low power DNN accelerator such as Eyeriss~ _cite_ occupies _inline_eq_ area which is N _inline_eq_ larger than the _inline_eq_ occupied by an ultra low power Cortex AN core~ _cite_ . Accelerating DNNs on these low-power systems then comes down to accelerating them on the indispensable general-purpose processor (GPP) cores already present in these systems. We focus on improving the execution time of DNNs on GPPs by leveraging in different DNN data-structures,, features, weights and backpropagated errors. Sparsity in DNNs can be both static or dynamic depending on whether the zero values remain constant or vary across different inputs to the network. Sparsity in weights, introduced by pruning connections in the network after training, is static in nature. In contrast, feature and error sparsities, caused by the thresholding nature of the ReLU (Rectified Linear Unit) activation functions, are dynamic in nature. Prior efforts that exploit sparsity to accelerate DNNs can be grouped into two classes based on whether they exploit static or dynamic sparsity, as shown in Figure~ _ref_ . Specialized accelerators~ _cite_ have been proposed to exploit both forms of sparsity. However, these techniques are closely tied to a given accelerator architecture thereby preventing their applicability to GPPs. In contrast, static sparsity in weights can be exploited on GPPs through software-only approaches~ _cite_ that involve sparse encodings and sparse matrix multiplication routines. However, extending them to exploit the intermediate levels of dynamic sparsity (N \%-N \%) can be counter-productive because of the encoding overhead involved at runtime. For example, the sparse encoding overhead involved in performing sparseBLAS based matrix multiplication on the sparse convN layer features of AlexNet~ _cite_ causes a slowdown of N _inline_eq_ . We observe across N image-recognition DNNs that dynamic sparsity in features results in N \% of the computations being rendered redundant during inference, presenting a significant opportunity for improving performance. We believe our effort, \sagpp, is the first to successfully exploit dynamic sparsity in DNNs on GPPs. To that end, we propose micro-architectural and ISA extensions that are minimally intrusive and low-overhead. Moreover, since static sparsity is a special case of dynamic sparsity where the location of zeros remains constant across inputs, the extensions also allow \sagpp~to exploit static sparsity in weights. To exploit dynamic sparsity, GPPs need to be equipped to dynamically detect if the result of an instruction ({\em e.g.}, a load from a sparse data structure) is zero and if so, skip a set of future instructions that use its result. However, there are two key challenges: (i) the instructions to be skipped often do not immediately follow the instruction that determines whether they may be; moreover, the instructions to be skipped may be non-contiguous in the program, and (ii) to maximize performance benefits, the instructions to be skipped should be, as squashing the instruction in-flight would diminish the benefits by introducing a pipeline stall. To overcome the aforementioned challenges, we propose sity aware ore xtensions (\sagpp), comprised of two key micro-architectural enhancements. First, \sagpp~contains a (SpRF) to track which general purpose registers that contain a zero. We achieve this by augmenting the writeback stage of the processor to check if the update to a register is zero and appropriately modify the corresponding SpRF entry. Next, a (SASA) table is used to store instruction sequences and the conditions under which they can be skipped the registers in the SpRF that need to be zero for the instructions to become redundant. Whenever \sagpp~fetches an instruction, it uses the SASA table and the SpRF to whether the following instruction (s) can be skipped. If so, the program counter is modified to directly fetch the next irredundant instruction. In summary, the key contributions of this work are: The rest of the paper is organized as follows. Section~ _ref_ explains the sources of sparsity in DNNs. Section~ _ref_ details the key design principles of \sagpp~and demonstrates them in the context of an in-order pipelined processor. Section~ _ref_ shows \sagpp~in action using the ARM-BLAS GEMM routine as a case study. Section~ _ref_ describes the experimental methodology and the results are presented in Section~ _ref_ . Section~ _ref_ presents related research efforts and Section~ _ref_ concludes the paper. \vspace* {-Npt}