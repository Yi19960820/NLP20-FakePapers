Neural networks, particularly CNNs have broken all records recently in the computer vision research area. The growth of CNNs focused initially on the recognition of characters. Fukushima and LeCun were the initial pioneers. Independently they developed CNN based systems, some of which are still being used widely~ _cite_ . Large networks are often trained with large number of data samples to achieve good accuracies~ _cite_ . Still, scepticism over CNNs among the modern day computer vision scientists stems from the fact that one does not have a clear understanding of its inner working. Some studies show that a few (_inline_eq_) nodes are all that are actively contributing to classification~ _cite_ . They also suggest that large networks often overfit, but since the data is too large over-fitting often works as an advantage~ _cite_ . While it is reasonable to expect edge detectors and Gabor-like features in the lower-level filters and more sophisticated concepts at the higher levels, it is not clear as to why these filters adapt themselves in this manner. What is fairly clear though is that different datasets result in different sets of filters that are similar if the datasets are similar. It is only natural to ask, what role does the data itself play in such filters being learnt and how they compare with filters learnt from another dataset. In this paper we take the view that the filters learnt by networks when trained using a particular dataset represent the detectors for some in the data itself. In which case each layer is a mapping form the previous layer to the next layer that is constructed using combinations of these atomic structures in the first layer in order to minimize a cost. Let us first define to be the forms that CNN filters take by virtue of the entropy of the dataset it is learning on, analogous to dictionary atoms. Complex datasets have more and varied atomic structures. Consider the following thought experiment: Let's assume that all possible atomic structures reside in an universe _inline_eq_ . Suppose we have a set of three datasets _inline_eq_ and _inline_eq_ . Consider the system in figure~ _ref_ . The figure describes the configuration of the elements of _inline_eq_ . One would now recognize that _inline_eq_ is a more general dataset with respect to _inline_eq_ and _inline_eq_ . It is so because, while _inline_eq_ most of the atomic structures of _inline_eq_ and _inline_eq_, the latter do not contain as many atomic structures of _inline_eq_ . While this analysis is simplified for one layer, in typical CNNs, co-adaptation plays a major role in the learning of these atomic structures. Therefore, generality as defined by the overlap of areas in a layer-wise Venn diagram is impractical to obtain. In this paper we postulate that, the generalization performances of CNNs on one dataset re-trained on a network initialized by training using another, could be used to derive generality. We call this process of pre-training as . By prejudicing on the first dataset, we froze and unfroze layers and retrained the networks on the second dataset. By freezing layers we are making a network more obstinate and we call this process obstination . The more the layers are frozen, the more obstinate the feature extractor is, therefore the harder the classifier has to work. If the prejudice was general enough, the classifier shall still generalize fairly well enough. What this means is that if the prejudicing dataset is more general than the re-train dataset, the classifier can generalize better than vice versa. We developed a generality metric by comparing the gain in performances of networks of various obstination. Using a generality such as the one proposed, it becomes clearer as to what kind of datasets are to be used to prejudice CNNs with during transfer learning. We even discovered that samples with particular labels within a dataset alone are general enough. So, if we begin by prejudicing the network on only those and then moved on to the rest of the labels, we were able to learn the rest of the dataset with considerably less training samples while achieving comparable generalization performances. Off-the-shelf networks such as VGG, overfeat and various published Caffe model weights are trained on large scale image datasets such as Imagenet or PASCAL~ _cite_ . For instance, while these may work on applications such as human pose recognition or vehicle detection, they do not necessarily work on tasks involving medical images. This is because the datasets on which they are trained are not general enough to adapt to the representational requirements of medical images, which is on a manifold unique and disjoint form the manifolds of natural images. This is visualized in _inline_eq_ and _inline_eq_ from figure~ _ref_ . Even a large collection of natural images is not general enough to have networks trained that are suitable to medical images. In these cases, the prejudiced network often fails. For instance, on the Colonoscopy dataset discussed later a _inline_eq_ layer deep overfeat features, trained with a logistic regression performs poorer than a _inline_eq_ layer deep CNN trained from random initialization, which is in turn outperformed when initialized by a network trained on an endoscopy dataset. In this article we considered popular offline character recognition datasets and arrived at some interesting analysis and generalities. We also show that within the MNIST dataset, classes _inline_eq_ are general enough that we could learn the other classes with very few (even just one) samples, when prejudiced with networks trained on _inline_eq_ . We also considered more sophisticated datasets such as Cifar N and Caltech N against some medical image datasets for colonoscopy video quality~ _cite_ . This study led us to two major research insights: The rest of the paper is organized as follows: section~ _ref_ discusses related works, section~ _ref_ presents the design of our experiments, section~ _ref_ shows some results on the core-experiment and section~ _ref_ provides concluding remarks.