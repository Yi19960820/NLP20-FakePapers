In recent years, deep learning technologies achieved excellent performance and many breakthroughs in both academia and industry. However the state-of-the-art deep models are computational expensive and consume large storage space. Deep learning is also strongly demanded by numerous applications from areas such as mobile platforms, wearable devices, autonomous robots and IoT devices. How to efficiently apply deep models on such low power devices becomes a challenging research problem. The recently introduced Binary Neural Networks (BNNs) could be one of the possible solutions for this problem. Several approaches _cite_ introduce the usage of BNNs. These BNNs have the capability of decreasing the memory consumption and computational complexity of the neural network. This is achieved by on the one hand storing the weights, that are typically stored as N bit floating point values, as binary values, by binarizing the floating point values with the, to be of either _inline_eq_ or _inline_eq_, and storing several of them in a single N bit float or integer. Computational complexity, on the other hand, is reduced by using and for performing matrix multiplications used in convolutional and fully connected layers. Most of the publicly available implementations of BNN do not store the weights in their binarized form _cite_, nor use and ~ _cite_ while performing the matrix multiplications in convolutional and fully connected layers. The deep learning library Tensorflow _cite_ tries to decrease the memory consumption and computational complexity of deep neural networks, by quantizing the N bit floating point weights and inputs into N bit integers. Together with the minimum and maximum value of the weight/input matrix, _inline_eq_ less memory usage and also decreased computational complexity is achieved, as all operations only need to be performed on N bit values rather than N bit values. BMXNet stores the weights of convolutional and fully connected layers in their binarized format, which enables us to store N/N weights in a single N/N bit float/integer and use _inline_eq_ less memory. During training and inference we binarize the input to each binary convolution and fully connected layer in the same way as the weights get binarized, and perform matrix multiplication using bit-wise operations (and) . Our implementation is also prepared to use networks that store weights and use inputs with arbitrary bit widths as proposed by Zhou et al. _cite_ . The deep learning library MXNet _cite_ serves as a base for our code. MXNet is a high performance and modular deep learning library, that is written in C + + . MXNet provides Bindings for other popular programming languages like Python, R, Scala and Go, and is used by a wide range of researchers and companies.