In recent years, recurrent neural networks _cite_, such as standard RNN (sRNN), its variant Long Short-Term Memory (LSTM) _cite_, and Gated Recurrent Unit (GRU) _cite_, have been adopted to address many challenging problems with sequential time-series data, such as action recognition _cite_, machine translation _cite_, and image caption _cite_ . They are powerful in exploring temporal dynamics and learning appropriate feature representations. The structure of recurrent neural networks facilitates the processing of sequential data. RNN neurons perform the same task at each step, with the output being dependent on the previous output, {\it i.e.}, {{some} historical information is memorized. Standard RNNs have difficulties in learning long-range dependencies due to the vanishing gradient problem _cite_ . The LSTM _cite_ or GRU _cite_ architectures combat vanishing gradients through a gating mechanism. Gates provide a way to optionally let information through or stop softly, which balances the contributions of the information of the current time slot and historical information. There are some variants of RNNs with slightly different designs _cite_ . {Note a gate applies a single scaling factor to control the flow of the embedded information (as a whole) of the input rather than imposing controls on each element of the input.} They are not designed to explore the potential different characteristics of the input elements. Attention mechanisms which selectively focus on different parts of the data have been demonstrated to be effective for many tasks _cite_ . These inspire us to develop an \Outer-Attention Gate (\EleAttGn) to augment the capability of RNN neurons. More specifically, for an RNN block, an EleAttG is designed to output an attention vector, with the same dimension as the input, which is then used to modulate the input elements. Note that similar to _cite_, we use an RNN block to represent an ensemble of _inline_eq_ RNN neurons, which for example could be all the RNN neurons in an RNN layer. Fig.~ _ref_ ~ (a) illustrates the EleAttG within a generic RNN block. Fig.~ _ref_ ~ (b) shows a specific case when the RNN structure of GRU is used. The input _inline_eq_ is first modulated by the response of the EleAttG to output _inline_eq_ before other operations are applied to the RNN block. We refer to an RNN block equipped with an EleAttG as EleAtt-RNN block. Depending on the underlying RNN structure used ({\it e.g.}, standard RNN, LSTM, GRU), the newly developed EleAtt-RNN will also be denoted as EleAtt-sRNN, EleAtt-LSTM, or EleAtt-GRU. An RNN layer with such EleAttG can replace the original RNN layer and multiple EleAtt-RNN layers can be stacked. We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to action recognition. Specifically, for ND skeleton-based human action recognition, we build our systems by stacking several EleAtt-RNN layers, using standard RNN, LSTM and GRU, respectively. EleAtt-RNNs consistently outperform the original RNNs for all the three types of RNNs. Our scheme based on EleAtt-GRU achieves state-of-the-art performance on three challenging datasets, {\it i.e.,} the NTU~ _cite_, N-UCLA~ _cite_, and SYSU~ _cite_ datasets. For RGB-based action recognition, we design our system by applying an EleAtt-GRU network to the sequence of frame-level CNN features. Experiments on both the JHMDB _cite_ and NTU _cite_ datasets show that adding {\EleAttGn} s brings significant gain. The proposed \EleAttG has the following merits. First, \EleAttG is capable of adaptively modulating the input at a fine granularity, paying different levels of attention to different elements of the input, resulting in faster convergence in training and higher performance. Second, the design is very simple. For an RNN layer, only one line of code needs to be added in implementation. Third, the EleAttG is general and can be added to any underlying RNN structure, {\it e.g.}, standard RNN, LSTM, GRU, and to any layer.