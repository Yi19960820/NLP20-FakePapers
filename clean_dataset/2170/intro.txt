Establishing dense correspondences across similar images can facilitate a variety of computer vision applications including non-parametric scene parsing, semantic segmentation, object detection, and image editing~ _cite_ . In this semantic correspondence task, the images resemble each other in content but differ in object appearance and configuration, as exemplified in the images with different car models in~ (a-b) . Unlike the dense correspondence computed for estimating depth~ _cite_ or optical flow~ _cite_, semantic correspondence poses additional challenges due to intra-class appearance and shape variations among different instances from the same object or scene category. To address these challenges, state-of-the-art methods generally extract deep convolutional neural network (CNN) based descriptors~ _cite_, which provide some robustness to appearance variations, and then perform a regularization step to further reduce the range of appearance. The most recent techniques handle geometric deformations in addition to appearance variations within deep CNNs. These methods can generally be classified into two categories, namely methods for geometric invariance in the feature extraction step, e.g., spatial transformer networks (STNs) ~ _cite_, and methods for geometric invariance in the regularization step, e.g., geometric matching networks~ _cite_ . The STN-based methods infer geometric deformation fields within a deep network and transform the convolutional activations to provide geometric-invariant features~ _cite_ . While this approach has shown geometric invariance to some extent, we conjecture that directly estimating the geometric deformations between a pair of input images would be more robust and precise than learning to transform each individual image to a geometric-invariant feature representation. This direct estimation approach is used by geometric matching-based techniques~ _cite_, which recover a matching model directly through deep networks. Drawbacks of these methods include that globally-varying geometric fields are inferred, and only fixed, untransformed versions of the features are used. In this paper, we present recurrent transformer networks (RTNs) for overcoming the aforementioned limitations of current semantic correspondence techniques. As illustrated in~, the key idea of RTNs is to directly estimate the geometric transformation fields between two input images, like what is done by geometric matching-based approaches~ _cite_, but also apply the estimated field to transform the convolutional activations of one of the images, similar to STN-based methods~ _cite_ . We additionally formulate the RTNs to recursively estimate the geometric transformations, which are used for iterative geometric alignment of feature activations. In this way, regularization is enhanced through recursive refinement, while feature extraction is likewise iteratively refined {according to the geometric transformations} as well as jointly learned with the regularization. Moreover, the networks are learned in a weakly-supervised manner via a proposed classification loss defined between the source image features and the geometrically-aligned target image features, such that the correct transformation is identified by the highest matching score while other transformations are considered as negative examples. The presented approach is evaluated on several common benchmarks and examined in an ablation study. The experimental results show that this model outperforms the latest weakly-supervised and even supervised methods for semantic correspondence.