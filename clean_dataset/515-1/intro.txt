Human activity recognition is one of the main areas of research in computer vision where the goal is to classify what a human is doing in an input video or image. Group activity recognition is a subset of human activity recognition problem which focuses on the collective behavior of a group of people, resulted from the individual actions of the persons and their interactions. Collective activity recognition is a basic task for automatic human behavior analysis in many areas like surveillance or sports videos. In group activity recognition, it is crucial to take into account the individual actions of the people and their interactions as in many cases the group activity is formed by these actions and interactions. So, most group activity recognition models analyzes the personal activities either explicitly or implicitly. Some works try to predict individual actions and group activity in a joint framework using probabilistic graphical models _cite_ or neural networks that implement the functionality of graphical models _cite_ . Other approaches, use pooling methods like max pooling _cite_ or attention pooling _cite_ on the individual person representations to model the relation between individual humans and the collective activity. Another important factor in recognizing the group activity recognition is the temporal development of the individual actions and group activity. Various approaches use Recurrent Neural Networks (RNNs) to model individual actions and group activity over time _cite_ . This strategy provides a concrete way of modeling group activity in the video. Some existing works try to inject temporal data through Convolutional Neural Networks (CNNs) on optical flow fields calculated between two consecutive frames as an additional input to each time step of RNN _cite_ . Recently, multi-stream convolutional networks where temporal information is modeled by CNNs on optical flow fields outperformed RNNs in action recognition task _cite_ . To the best of our knowledge, none of the previous approaches study the group activity recognition using convolutional streams on the full range of the temporal data. When using multi-stream convolutional networks for group activity, a challenge is how to model temporal stream for multiple individuals in the input video. One can extract the sequence of images for each person according to its trajectory and compute optical flow for this sequence. Each sequence will be analyzed by a CNN on optical flow fields. This approach can be costly due to the computation of optical flow for multiple individuals and several forward passes of CNNs on the input optical flow field for all the persons in the video. Incorporating additional information other than the appearance features has positive effect on the performance of the activity recognition models. In _cite_, discrete pose labels are combined with action labels to form new action labels to improve accuracy. In _cite_, a pose class is injected into the model as sub-action information. As stated above, in _cite_, temporal cues obtained from the optical flow are given to the RNN. In this paper, we use the pose heatmap which contains detailed information about the body parts of the humans as novel information source in our model. In this paper, we propose a multi-stream convolutional framework for the task of group activity recognition which addressed all of the above issues. In this framework, new input modalities are easily incorporated into the model by the addition of new convolutional streams. Optical flow, warped optical flow, pose heatmap, and the RGB frame are four different modalities considered in this paper, each of which capturing a different aspect of the input video. Moreover, to consider both the individual actions and the global activity, each stream is composed of two distinct branches; one focuses on the individual regions of the actors for individual action or group activity recognition while the other processes the whole content of the input video to recognize group activities. Evaluation on Volleyball _cite_ and Collective Activity _cite_ datasets shows the effectiveness of our approach. The rest of the paper is organized as follows. First, in Sec. _ref_, the related works are briefly discussed. Details of our multi-stream framework are given in Sec. _ref_ . Sec. _ref_ is about the datasets. Our experimental results along with the analysis of the results are given in Sec. _ref_ . We conclude the paper in Sec. _ref_ .