Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images _cite_ also seen growing interest in semantic pixel-wise labelling problems _cite_ . However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if _inline_eq_ non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is _inline_eq_ of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (_inline_eq_ in our example) have the same features. This often results in predictions that appear blocky . This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages. First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers _cite_ . Second, ablation studies to understand the effects of features such as in _cite_ can be performed using the decoder stack. We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models _cite_ and unsupervised learning of feature hierarchies _cite_ . Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a _inline_eq_ layer SegNet with _inline_eq_ kernels and _inline_eq_ non-overlapping max pooling in each layer has a spatial context of _inline_eq_ pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs _cite_ . SegNet maintains a constant number of features per layer which is typically set to _inline_eq_ . This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair. In Sec. _ref_ we review related recent literature. We describe in detail the SegNet architecture in Sec. _ref_ along with its qualitative analysis. Our quantitative experiments with SegNet on several well known benchmark datasets are described in Sec. _ref_ . We also discuss the advantages and drawbacks of our approach including computational times. We conclude with pointers to future work in Sec. _ref_ . For most of our experiments, we use outdoor RGB road scene analysis _cite_ and indoor RGBD scene analysis _cite_ datasets to measure the quantitative performance.