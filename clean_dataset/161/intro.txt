Predominately, face recognition has been focused in the visible spectrum. This pertains to a large number of applications from biometrics, access control systems, social media tagging to person retrieval in multimedia. Among the main challenges in visible face recognition, the different lighting/illumination condition has proven to be one of the big factors for appearance change and performance degradation. Many prior studies _cite_ have stated better face recognition performance in the infra-red spectrum because it is invariant to ambient lighting. Relatively recently, few efforts have been devoted in the cross-modal face recognition scenarios, where the objective is to identify a person captured in infra-red spectrum based on its stored high resolution visible face image. The motivation for this lies in the night-time or low light surveillance tasks where the image is captured discretely or covertly through active or passive infra-red sensors. In fact there does not exist a reliable solution for matching thermal faces, acquired covertly in such conditions, against the stored visible database such as police mugshots. This poses a significant research gap for such applications in aiding the law enforcement agencies. In the infra-red spectrum, thermal signatures emitted by skin tissues can be acquired through passive thermal sensors without using any active light source. This makes it an ideal candidate for covert night-time surveillance tasks. As opposed to the visible spectrum wavelength (N _inline_eq_ to N _inline_eq_), the infra-red spectrum lies in four main ranges. Near infra-red `NIR' (N _inline_eq_-N _inline_eq_), short-wave infra-red `SWIR' (N-N _inline_eq_), mid-wave infra-red `MWIR' (N-N _inline_eq_) and long-wave infra-red `LWIR' (N-N _inline_eq_) . Since the NIR and SWIR bands are reflection dominant, they are more close to the visible spectrum. The MWIR and LWIR are the thermal spectrum and are emission dominant i.e. dependent on material emissivity and temperature. Skin tissue has high emissivity in both the MWIR and LWIR spectrum. Because of this natural difference between reflective visible spectrum and sensed emissivity in the thermal spectrum, the images in the two modalities are very different and have a large modality gap. This hinders reliable face matching across the two domains. It is, perhaps, for this reason that most of the earlier studies, aiming at cross-modal face recognition, relies only in visible-to-NIR face matching. While achieving very good results, NIR imaging use an active light source that makes it redundant for covert night-time surveillance. More recently, some attempts have been made in thermal-to-visible face recognition, indicating a significant performance gap due to the very challenging nature of the problem and the large modality gap. In this paper, we seek to bridge this gap by trying to model directly the highly non-linear mapping between the two modalities. Our contribution is a useful model, based on a feed-forward deep neural network, and its effective design steps in order to map the perceptual differences between the two modalities while preserving the identity information. We show that this mapping can be learned from relatively sparse data and works quite good in practice. Our model tries to learn a non-linear regression function between the visible and thermal data. The learned projection matrices capture the non-linear relationship well and are able to bring the two closer to each other. Another contribution is to further the best published state-of-the-art performance on a very challenging dataset (University of Notre Dame UND-XN) by more than N \%. Our results show that this accounts for bridging the performance gap due to modality difference by more than N \%. To the best of our knowledge, this is the first attempt in using deep neural networks to bridge the modality gap in thermal-visible face recognition. Figure _ref_ provides an overview of the approach. We will start by discussing some of the related work in section _ref_ . Section _ref_ will detail the presented approach. We will conclude in section _ref_ after presenting detailed experiments, results and implementation details in section _ref_ .