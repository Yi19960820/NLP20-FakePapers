Multi-task learning aims to improve the generality of performance by mutually utilizing information of other tasks _cite_ . By modeling multiple tasks in a single network, useful knowledge among tasks can be shared during training, and diversified training data tend to cancel out bias and noise. The most common way to achieve multi-task learning is to share parameters in feature representation layers of single-task convolutional neural networks (CNNs) and branch several top layers for task-wise prediction _cite_ Fig. \, _ref_ \, (a) . The shared layers, either by hard or soft sharing~ _cite_, learn common representations for all tasks, and they achieve better generalization because more data are harder to overfit. However, this sharing architecture is not very flexible, since sharing choices are discrete, and the number of shared layers must be fixed among all tasks. Particularly in upper layers, the parameter sharing can be too restrictive because feature representations must be specialized on each task~ _cite_ . To alleviate this, cross-stitch networks~ _cite_ were proposed as a more general CNN architecture for multi-task learning. As shown in Fig. \, _ref_ \, (b), it models shared representations by using element-wise linear combinations of activation maps from each task stream, while retaining individual network parameters. It is, however, limited in that it considers only the combination of channels with corresponding indices. Cross-stitching has been applied to multi-tasks that have annotations on the same dataset, but as we tend to have different learning datasets for different tasks, we may like to use successful single-task CNNs trained on each dataset, and combine them later for better performance. In this paper, we propose a cross-connected CNN, a new architecture for multi-task CNNs. As shown in Fig. \, _ref_ \, (c), we cross-connect intermediate layers of CNNs via convolutional layers. Our architecture streams to communicate with each other by exchanging their activation maps, while its novelty is in that the activation map passes through convolutional layers. The convolution layers learn the importance of each activation map for the other task, and which information to be sent to which destination. The proposed architecture is a generalization of cross-stitching, and it can further model mutual effects across channels. The cross-connection can be made across any layers in principle. This paper has the following contributions. (N) We propose a cross-connected CNN, a new architecture for multi-task learning. Convolutional layers that cross-connect two single-task CNNs can model cross-channel and cross-layer feature interaction between tasks, and the proposed model is a generalization of existing ones. (N) To our knowledge, this is the first attempt to tackle multi-task learning of object detection and semantic segmentation using different datasets between tasks.