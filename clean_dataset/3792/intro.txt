Pedestrian detection is a task that has several applications from driver assistance systems and surveillance to image and video understanding. There has been a significant research on pedestrian detection since the late Ns _cite_ and more recently using deep learning _cite_ and _cite_ . However, to the best of our knowledge, almost all works in this area have been limited to the frontal views from personal and closed-circuit television cameras (CCTVs) . In contrast, detecting pedestrians in top-down or aerial imagery is a relatively new area of research. Aerial pedestrian detection has several other applications like surveillance using Unmanned Aerial Vehicle (UAV), which provides a wider range of view, higher performance, search and rescue tasks, and human interaction understanding. In this paper, we investigate the problem of finding particular pedestrians who are doing one or multiple specific actions (e.g., calling and running) in aerial images (see Figure N) . This line of research is particularly applicable in scenarios where the location of an individual exhibiting a malbehavior needs to be identified. For instance, in an urgent situation, on the basis of people \textquotesingle s tweets, it might be very important to search for a person who is running and carrying something in a street. In this paper, we assume that objects of interest can be discriminated based on their single or multiple actions, and evaluate the proposed framework on the Okutama-Action dataset _cite_, which is an aerial dataset for concurrent human single and multiple action detection. Although aerial imagery provides a wider range of view for surveillance tasks, it brings different challenges in the literature. In top-down view, pedestrians as well as other objects are very small. In addition, the state of the art object detectors like Faster R-CNN _cite_, YOLO _cite_, SSD _cite_ as well as deep object classifiers like VGG _cite_ usually work with input images of less than one mega pixel (e.g., NxN and NxN), which makes the problem even worse. In other words, pedestrians and objects of interest sizes are limited to a few pixels and this makes the recognition of single and multiple actions impossible. Figure N shows how pedestrian's appearance fade when resolutions decreases. In this paper, we propose a two-step framework to generate objects of interest or object proposals from small size aerial images in the first step, and then we create a latent common sub-space and fuse high resolution object proposals with different possible actions in the second step. The final output of the framework is Yes or No for the question of "Is this probable pedestrian doing action X (and Y) ?". The proposed system helps to find particular targets in aerial images. This paper is organized as follows: In Section II, we review the literature of deep object detectors, multi-label classification, and visual question answering. Then in Section III, we explain our proposed framework. Section IV describes the experiments, their details, and results. Finally, in section V we summarize our conclusions.