Triplet loss is a metric learning method that has been widely used in many applications,, face recognition _cite_, image retrieval _cite_ and person re-identification _cite_ . A triplet usually consists of three samples: an anchor sample, a positive one with the same class to the anchor, and a negative one with the different class. The objective of triplet loss is to learn a metric that pushes the positive pairs closer while pulls the negative pairs away, thus the samples within the same class can be nearest to each other. One question is why we need the triplet loss? Actually, there are some alternatives such as the softmax loss, which is also popular. However, as the number of classes becomes extremely large, the fully-connected layer that connects to softmax loss will also become extremely large, thus the GPU memory cost will be unbearable with an usual batchsize, while the small batchsize will take too long to train. Another reason is that if only a few samples are available in each class, training with softmax loss is difficult, and Fig. _ref_ shows its influence on softmax and triplet loss with _inline_eq_ identities. The triplet performs much better when the number of samples in each class is small (_inline_eq_) . Though the advantage is clear, there are some challenges to use it. One big challenge is how to train triplet models effectively with large-scale data,, _inline_eq_ and _inline_eq_ identities are common cases in face recognition. The difficulty of scaling triplet is that the number of possible triplets is cubic in the number of samples, and most triplets are too easy that cannot help training. To reduce searching space, some researchers transferred the triplet loss into softmax loss _cite_, while some proposed the Online Hard Negative Mining (OHNM) _cite_ or batch OHNM _cite_ . Most studies focused on the small-scale case, while FaceNet _cite_ used an extremely large number of identities (_inline_eq_), but it suffered from long training time (a few months) . In the above methods, all of them consider all identities to sample the batch. It is widely accepted that hard triplets can help training because they can reduce the ambiguity of recognizing similar identities, and it indicates these hard triplets should better come from similar identities. However, sampling from all the identities cannot guarantee to include the similar ones, thus it will fail in generating effective hard triplets. Especially, in the large-scale face recognition with _inline_eq_ or _inline_eq_ identities, the probability of sampling similar identities with an usual batchsize is very tiny,, the batch with the size of _inline_eq_ is randomly sampled from _inline_eq_ identities in FaceNet _cite_ . Therefore, how to find the similar identities is the key to improve the training of triplet networks with large-scale data. In this paper, we consider the case of large-scale face recognition, and propose to train triplet networks with subspace learning. The basic idea is to generate a representation for each identity, and apply clustering on all the identities to generate clusters or subspaces, wherein identities are similar in each subspace. With the batch OHNM applied in each subspace iteratively, the proposed method can easily generate more effective hard triplets. Evaluations on the large-scale MS-Celeb-NM dataset with _inline_eq_ identities _cite_ show that the proposed method can largely improve performance and get more robust triplet models. Particularly, our single triplet network obtains the LFW _cite_ accuracy of _inline_eq_, which can be competitive to FaceNet's _inline_eq_ _cite_ with _inline_eq_ identities. This subspace learning with batch OHNM is our main contribution. In addition, given the fact that the MS-Celeb-NM dataset is noisy, we also design a noise removing trick to clean the training data at the beginning, and experiments show that it is able to handle different number of noises. Furthermore, consider that the number of training images is about _inline_eq_ after the cleaning, we use a two-layer hierarchical trick to retrieve a test image accurately and efficiently. Combined with the proposed triplet network, we have achieved the state-of-the-art performance on the MS-Celeb-NM competition,, _inline_eq_ without external data. The recall of the random and hard set is _inline_eq_ and _inline_eq_ respectively, in which _inline_eq_ has achieved the upper limit on the random set.