Recovering the dense ND shapes of objects from ND imageries is a fundamental AI problem which has many applications such as robot-environment interaction, ND-based object retrieval and recognition, \etc. Given a single image of an object, a human can reason the ND structure of the object reliably. However, single-view ND object reconstruction is very challenging for computer algorithms. Recently, a significant progress of single-view ND reconstruction has been achieved by using deep convolutional neural networks (CNNs) _cite_ . Most CNN-based methods reconstruct the object shapes using ND and ND convolutions in a ND encoder-ND decoder structure with the volumetric ND representation. The input to these CNNs are object images taken under unknown viewpoints, while the output shapes are often aligned with the canonical viewpoint in a single, pre-defined ND coordinate system such that shape regression is more tractable. Although promising results have been shown by these CNN-based methods, single-view ND reconstruction is still a challenging problem and the results are far from being perfect. One of the main difficulties lies in the object shape variations which can be very large even in a same object category. The appearance variations in the input images caused by pose differences make this task even harder. Consequently, the results from CNN-based methods are prone to missing some shape details and sometimes generate plausible shapes which, however, are inconsistent with input images, as shown in Figure~ _ref_ . In this paper, we propose an approach to improve the fidelity of the reconstructed shapes by CNNs. Our method combined traditional wisdom into the network architecture. It is motivated by two observations: N) while directly recovering all the shape details in ND is difficult, extracting the projected shape silhouette on the ND plane, \ie segmenting out the object from background in a relatively easy task using CNNs; N) for some common objects such as chairs and cars whose ND coordinate systems are well defined without ambiguity, the object pose (or equivalently, the viewpoint) can also be well estimated by a CNN~ _cite_ . As such, we propose to leverage the object silhouettes to assist the ND learning by lifting them to ND using pose estimates. Figure~ _ref_ is a schematic description of our method, which is a pure GPU-friendly neural network solution. Specifically, we embed into the network a single-view visual hull using the estimated object silhouettes and poses. Embedding a visual hull can help recover more shape details by considering the projection relationship between the reconstructed ND shape and the ND silhouette. Since both the pose and segmentation are subject to estimation error, we opted for a ``soft'' visual-hull embedding strategy: we first predict a coarse ND shape using a CNN, then employ another CNN to refine the coarse shape with the constructed visual hull. We propose a probabilistic single-view visual hull (PSVH) construction layer which is differentiable such that the whole network can be trained end-to-end. In summary, we present a novel CNN-based approach which uses a single-view visual hull to improve the quality of shape predictions. Through our method, the perspective geometry is seamlessly embedded into a deep network. We evaluate our method on synthetic data and real images, and demonstrate that using a single-view visual hull can significantly improve the reconstruction quality by recovering more shape details and improving shape consistency with input images.