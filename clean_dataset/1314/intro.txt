Convolutional neural networks (CNNs) _cite_ have been proved to be quite successful in modern deep learning architectures _cite_ and achieved better performance in various computer vision tasks _cite_ . By tying the kernel weights in convolution, CNNs have the translation invariance property that can identify the same pattern irrespective of the spatial location. Each neuron in CNNs is a scalar and can detect different (low-level details or high-level regional semantics) patterns layer by layer. However, in order to detect the same pattern with various variants in viewpoint, rotation, shape, etc., we need to stack more layers, which tends to ``memorize the dataset rather than generalize a solution'' _cite_ . A capsule _cite_ is a group of neurons whose output, in form of a vector instead of a scalar, represents various perspectives of an entity, such as pose, deformation, velocity, texture, object parts or regions, etc . It captures the existence of a feature and its variant. Not only does a capsule detect a pattern but also it is trained to learn the many variants of the pattern. This is what CNNs are incapable of. The concept of capsule provides a new perspective on feature learning via instance parameterization of entities (known as capsules) to encode different variants within a capsule structure, thus achieving the feature equivariance property and being robust to adversaries. Intuitively, the capsule detects a pattern (say a face) with a certain variant (it rotates N degree clockwise) rather than realizes that the pattern matches a variant in the higher layer. One basic capsule layer consists of two steps: capsule mapping and agreement routing, which is depicted in Fig. _ref_ (a) . The input capsules are first mapped into the space of their higher counterparts via a transform matrix. Then the routing process involves all capsules between adjacent layers to communicate by the routing co-efficients; it ensures only certain lower capsules which resemble higher ones (in terms of cosine similarity) can pass on information and activate the higher counterparts. Such a scheme can be seen as a feature clustering and is optimized by coordinate descent through several iterations. However, the computational complexity in the first mapping step is the main bottleneck to apply the capsule idea in CNNs; lower capsules have to generate correspondence for every higher capsule (e.g., a typical choice _cite_ is N capsules with N dimension, resulting in N million parameters in the transform matrix) . To tackle this drawback, we propose an alternative to estimate the original routing summation by introducing two branches: one is the branch that serves as the primary source from the direct contact capsule in the lower layer; another is the branch that strives for searching other pattern variants along the channel and replenishes side information to . These two branches are intertwined by their co-efficients so that feature patterns encoded in lower capsules could be fully leveraged and exchanged. Such a one-pass approximation is fast, light-weight and supervised, compared to the current iterative, short-lived and unsupervised routing scheme. Furthermore, the routing effect in making higher capsule have agreement with lower capsule can be extended as a direct loss function. In deep neural networks, information is inevitably lost through stack of layers. To reduce the rapid loss of information in nearby layers, a loss function can be included to enforce that neurons or capsules in the higher layer can be used for reconstructing the counterparts in lower layers. Based on this motivation, we devise an agreement feedback unit which sends back higher capsules as a feedback signal to better supervise feature learning. This could be deemed as a regularization on network. Such a feedback agreement is achieved by measuring the distance between the two distributions using optimal transport (OT) divergence, namely the Sinkhorn loss. The OT metric (e.g., Wasserstein loss) is promised to be superior than other options to modeling data on general space. This add-on regularization is inserted during training and disposed of for inference. The agreement enforcement has witnessed a unanimous gain in both capsule and vanilla neural networks. Altogether, bundled with the two mechanisms aforementioned, we {(i)} encapsulate the neural network in an approximate routing scheme with master/aide interaction, {(ii)} enforce the network's regularization by an agreement feedback unit via optimal transport divergence. The proposed capsule network is denoted as EncapNet and performs superior against previous state-of-the-arts for image recognition tasks on CIFARN/N, SVHN and a subset of ImageNet. The code and dataset are available .