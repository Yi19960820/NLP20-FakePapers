Image classification, as a foundational problem in computer vision, is receiving increasing attention in the research community. Although marked progress is achieved in this topic thanks to the great success of deep convolutional neural networks (CNNs) _cite_, existing approaches mainly focus on single-label image classification that considers the situation where an image would contain only one object. In contrast, multi-label image recognition shows more practical significance, as the real-world image is normally annotated with multiple labels and modeling rich semantic information is essential for the task of high-level image understanding. A straightforward method that extends CNNs to multi-label image recognition is to fine tune the networks pre-trained on single-label classification dataset (e.g., ImageNet _cite_) and extract global representation for multi-label recognition _cite_ . Though being end-to-end trainable, classifiers trained on global image representation may not generalize well to images containing multiple objects with different locations, scales, occlusions, and categories. An alternative way _cite_ is to introduce object proposals that are assumed to contain all possible foreground objects in the image, and aggregate features extracted from all these proposals to incorporate local information for multi-label image recognition. Despite notable improvement compared to global representation, these methods still have many flaws. First, these methods need to extract hundreds of proposal to achieve a high recall but feeding such a large number of proposals to the CNN for classification is extremely time-consuming. Second, an image usually contains only several objects, most of the proposals either provide intensely coarse information of an object or even refer to the same object. In this way, redundant computation and sub-optimal performance are inevitable, especially in complex scenarios. Last but not least, they usually oversimplify the contextual dependencies among foreground objects and thus fail to capture label correlations in images. In this paper, inspired by the way that humans continually move fovea from one discriminative object to the next when performing image labeling tasks, we propose an end-to-end trainable recurrent attention reinforcement learning framework to adaptively search the attentional and contextual regions in term of classification. Specifically, our proposed framework consists of a fully convolutional network for extracting deep feature representation, and a recurrent attention-aware module, implemented by an LSTM network, to iteratively locate the class-related regions and predict the label scores over these located regions. At each iteration, it predicts the label scores for the current region and searches an optimal location for the next iteration. Note that by ``remember" the information of the previous iterations, the LSTM can naturally capture contextual dependencies among the attentional regions, which is also a key factor that facilitates multi-label recognition _cite_ . During training, we formulate it as a sequential decision-making problem, and introduce reinforcement learning similar to previous visual attention models _cite_, where the action is searching the attentional location of each glimpse and performing classification on attentional regions, the state is the features regarding the current regions as well as the information of previous iteration, and the reward measures the classification correctness. In this way, the proposed framework is trained with merely image-level labels in an end-to-end fashion, requiring no explicit object bounding boxes. To the best of our knowledge, this is the first paper that introduces recurrent attentional mechanism with deep reinforcement learning into generic multi-label image classification. Compared to the recent hypothesis-regions-based multi-label recognition methods, our proposed method not only enjoys better computational efficiency and higher classification accuracy, but also provides a semantic-aware object discovery mechanism based on merely image-level labels. Experimental results on two large-scale benchmarks (PASCAL VOC and MS-COCO) demonstrate the superiority of our proposed method against state-of-the-art algorithms. We also conduct experiments to extensively evaluate and discuss the contribution of the crucial components.