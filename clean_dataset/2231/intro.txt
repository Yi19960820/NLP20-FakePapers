The problem of video classification based on semantic contents like human actions or complex events has been extensively studied in the computer vision community. The fact that videos are intrinsically multimodal demands solutions that can explore not only static visual information, but also motion and auditory clues. Key to the development of video classification systems is the design of good features. Popular feature descriptors include the SIFT _cite_, the Mel-Frequency Cepstral Coefficients (MFCC) _cite_, the STIP _cite_ and the dense trajectories~ _cite_, which can be encoded into video-level representations by bag-of-words (BoW) _cite_ or Fisher vectors (FV) _cite_ . In contrast to the hand-engineered descriptors, the deep neural networks that can learn features automatically from raw data have demonstrated strong performance in various domains. In particular, the convolutional neural networks (ConvNets) are very successful on image analysis tasks like object detection~ _cite_, object recognition~ _cite_ and image segmentation~ _cite_ . However, for video classification, most deep network based approaches (\eg, ~ _cite_) demonstrated worse or similar results to the hand-engineered features~ _cite_ . This is largely due to the high complexity of the video data. Unlike images that only have static visual appearance information, videos also contain temporal motions and auditory soundtracks. For example, a ``diving'' action video usually involves a sequence of atoms, such as ``jumping from a platform'', ``rotating in the air'' and ``falling into water'', accompanied by cheering or clapping sounds. Some approaches~ _cite_ only focused on the static frames and short-term motion clues captured by a few adjacent frames, which are apparently not sufficient. A few very recent studies attempted to use recurrent neural networks (RNN) to model long-term temporal information and achieved competitive performance~ _cite_ . Nevertheless, the audio information has rarely been exploited. In addition, most existing approaches fused the outputs of multiple networks in a very straightforward way~ _cite_, which could lead to sub-optimal performance. Realizing the above limitations, in this paper, we propose a multi-stream framework of deep neural networks to exploit the multimodal clues for video classification. Figure~ _ref_ illustrates the diagram of our approach. Three ConvNets are trained to model the static spatial information, short-term motion and auditory clues, respectively. The motion stream is computed on stacked optical flows over a short temporal windows and thus can only capture short-term motion. In order to model the long-term temporal clues, we employ a Recurrent Neural Network (RNN) model, namely the Long Short Term Memory (LSTM), on the frame-level spatial and motion features extracted by the ConvNets. The LSTM encodes history information in memory units regulated with non-linear gates to discover temporal dependencies. To combine the outputs from different networks, we develop a simple yet effective fusion method to learn the optimal fusion weights adaptively for each class. We propose to regularize the weight learning process using class relationships estimated without using additional labels. This helps inject class context into the final predictions and thus can significantly improve the results. Our contributions are summarized as follows: Incorporating the fusion method into the proposed multi-stream framework, we achieve superior performance on two popular benchmark datasets.