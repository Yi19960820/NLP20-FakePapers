The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets _cite_ and learned deep representations _cite_ . However, there has not yet been the same corresponding progress in natural sound understanding tasks. We attribute this partly to the lack of large labeled datasets of sound, which are often both expensive and ambiguous to collect. We believe that large-scale sound data can also significantly advance natural sound understanding. In this paper, we leverage over one year of sounds collected in-the-wild to learn semantically rich sound representations. We propose to scale up by capitalizing on the natural synchronization between vision and sound to learn an acoustic representation from unlabeled video. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about sound. Recent progress in computer vision has enabled machines to recognize scenes and objects in images and videos with good accuracy. We show how to transfer this discriminative visual knowledge into sound using unlabeled video as a bridge. We present a deep convolutional network that learns directly on raw audio waveforms, which is trained by transferring knowledge from vision into sound. Although the network is trained with visual supervision, the network has no dependence on vision during inference. In our experiments, we show that the representation learned by our network obtains state-of-the-art accuracy on three standard acoustic scene classification datasets. Since we can leverage large amounts of unlabeled sound data, it is feasible to train deeper networks without significant overfitting, and our experiments suggest deeper models perform better. Visualizations of the representation suggest that the network is also learning high-level detectors, such as recognizing bird chirps or crowds cheering, even though it is trained directly from audio without ground truth labels. The primary contribution of this paper is the development of a large-scale and semantically rich representation for natural sound. We believe large-scale models of natural sounds can have a large impact in many real-world applications, such as robotics and cross-modal understanding. The remainder of this paper describes our method and experiments in detail. We first review related work. In section _ref_, we describe our unlabeled video dataset and in section _ref_ we present our network and training procedure. Finally in section _ref_ we conclude with experiments on standard benchmarks and show several visualizations of the learned representation. Code, data, and models will be released. Sound Recognition: Although large-scale audio understanding has been extensively studied in the context of music _cite_ and speech recognition _cite_, we focus on understanding natural, in-the-wild sounds. Acoustic scene classification, classifying sound excerpts into existing acoustic scene/object categories, is predominantly based on applying a variety of general classifiers (SVMs, GMMs, etc.) to the manually crafted sound features (MFCC, spectrograms, etc.) _cite_ . Even though there are unsupervised _cite_ and supervised _cite_ deep learning methods applied to sound classification, the models are limited by the amount of available labeled natural sound data. We distinguish ourselves from the existing literature by training a deep fully convolutional network on a large scale dataset (NM videos) . This allows us to train much deeper networks. Another key advantage of our approach is that we supervise our sound recognition network through semantically rich visual discriminative models _cite_ which proved their robustness on a variety of large scale object/scene categorization challenges _cite_ . _cite_ also investigates the relation between vision and sound modalities, but focuses on producing sound from image sequences. Concurrent work _cite_ also explores video as a form of weak labeling for audio event classification. Transfer Learning: Transfer learning is widely studied within computer vision such as transferring knowledge for object detection _cite_ and segmentation _cite_, however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models _cite_ . Our method builds upon teacher-student models _cite_ and dark knowledge transfer _cite_ . In _cite_ the basic idea is to compress (i.e. \transfer) discriminative knowledge from a well-trained complex model to a simpler model without loosing considerable accuracy. In _cite_ and _cite_ both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound. _cite_ also transfer visual supervision into depth models. Cross-Modal Learning and Unlabeled Video: Our approach is broadly inspired by efforts to model cross-modal relations _cite_ and works that leverage large amounts of unlabeled video _cite_ . In this work, we leverage the natural synchronization between vision and sound to learn a deep representation of natural sounds without ground truth sound labels.