the increasing ubiquity of closed-circuit television cameras, the problem of person re-identification (re-id) has attracted considerable research attention. The purpose of re-id techniques is to match the images of pedestrians from disjoint camera views. The application of re-id techniques in intelligent video surveillance systems will be beneficial to enhance security in public areas. Conducting re-id over disjoint non-overlapping camera views is challenging because of the dramatic visual changes caused by variations in illumination, image quality, and especially viewpoint. In the existing literature, most re-id methods consist of a feature extraction process and a view-invariant discriminative transformation or metric _cite_ . Figure _ref_ displays a traditional re-id framework for addressing the cross-view problem. Conventional re-id approaches first extract view-generic features for different views. Then, a view-invariant model is learned to narrow the gaps between the images of the intra-class pedestrians and increase the distances between the inter-class samples. Three issues prevent traditional methods from solving the re-id problem. First, traditional features characterize pedestrians without any view-specific knowledge. View-generic features are inadequate for solving a re-id problem when dramatic changes in appearance occur between disjoint camera views. Second, feature extraction and view-invariant model learning are independent. During training, the view-invariant model propagates no information back to the view-generic features. View information, which is commonly exploited by discriminative model learning algorithms, is rarely utilized to improve the feature extraction process. Learning a model with excellent generalizability simply by metric learning is difficult because the view-generic features of the same person vary across different views. Third, traditional methods learn a view-invariant model shared by all views. Utilizing a shared model for all views ignores the discrepancies among different views. Compared with the shared model, view-specific models can cover stronger view-related information. With such information, view-specific models can achieve better performance than view-generic ones. However, the amount of computational resources grows dramatically with the increasing number of camera views. Currently, few studies have focused on the learning of view-specific models. Although view information may be useful for improving the performance of the existing methods, learning view-specific features or transformations for re-id is still under study. The current literature focuses on learning view-invariant but view-generic discriminative information. The existing approaches mainly integrate view information into metric learning methods by projecting the view-generic features onto a view-invariant common space _cite_ . To further exploit view-related information, some recent works _cite_ attempt to learn a view-specific dictionary or transformation for each view and project the view-generic features onto the corresponding view-specific spaces. Although view-specific models can minimize the discrepancies among changing viewpoints, the process of view-generic feature extraction ignores the view information hidden in the low-level features. The using of view-specific features may improve the performance of the existing frameworks. Nevertheless, few studies have explored the topic of view-specific feature extraction for re-id issue. Data-driven features are more effective than empirically designed features in exploiting view-specific information. Deep learning is the most effective technique to learn discriminative features from the training data. Deep networks have been successfully applied in various computer vision tasks, including image classification _cite_, face recognition _cite_, and action recognition _cite_ . In the last few years, deep learning-based approaches have been proved to be effective for re-id _cite_ . Deep models may exploit the view information and extract discriminative view-related features by using a back propagation algorithm. However, the existing approaches share the parameters of deep networks across all views, particularly for the layers that extract low-level features. Therefore, view-specific information may be ignored during feature extraction stage. According to the above discussion, we propose to extract view-specific features by using deep networks. To the best of our knowledge, our approach is the first to extract view-specific features by using deep networks for re-id. To narrow the gaps between features from changing viewpoints, we integrate a cross-view Euclidean constraint (CV-EC) into the proposed framework. The goal of CV-EC is to decrease the distances between the deep features of the same person from disjoint camera views. Together with CV-EC, we integrate another loss called the cross-view center loss (CV-CL) to improve the discriminative ability of the view-specific deep networks. Center loss has been proved to be effective for face recognition _cite_ by narrowing the margin between the samples and their corresponding class centers. In this paper, we extend the center loss to a view-specific version that better fits the re-id problem. Moreover, we propose an iterative optimization algorithm (ICV-ECCL) to learn CV-EC and CV-CL alternatively and optimize the parameters of the view-specific networks from coarse to fine. Finally, we extend CV-EC and CV-CL to a multi-view version to deal with the application using more than two cameras. The study makes the following contributions. The rest of this paper is organized as follows: In Section \uppercase, we review the related works. We then introduce the overall framework and the optimization algorithm in Section \uppercase . Section \uppercase presents the experimental results of the comparisons and the self-evaluation. Section \uppercase concludes the paper.