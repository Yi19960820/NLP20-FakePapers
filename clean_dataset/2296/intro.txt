recent years, artificial intelligence (AI) has flourished in many fields such as autonomous driving _cite_ _cite_, games _cite_ _cite_, and engineering applications _cite_ _cite_ . As one of the most popular topics, autonomous driving has drawn great attention both from the academic and industrial communities and is thought to be the next revolution in the intelligent transportation system. The autonomous driving system mainly consists of four modules: an environment perception module, a trajectory planning module, a control module, and an actuator mechanism module. The initial perception methods _cite_ _cite_ are based on the expensive LIDARs which usually cost tens of thousands of dollars. The high cost limits their large-scale applications to the ordinary vehicles. Recently, more attention is paid to the image-based methods _cite_ of which the core sensor, i.e. camera is relatively cheap and already equipped on most vehicles. Some of these perception methods have been developed into products _cite_ _cite_ . In this paper, we focus on the lateral control problem based on the image captured by the onboard camera. The vision-based lateral control methods can be divided into two categories: the end-to-end control methods and the perception and control separation methods. The end-to-end control system directly maps the observation to the desired output. In the context of vehicle control problem, a typical approach of the end-to-end learning is imitation learning in which a classifier _cite_ or regressor _cite_ is learned for predicting the expert's control command when encountering the same observation. However, since the predicted action affects the following observation, a small error will accumulate and lead the learner to a totally different future observation _cite_ . Thus the end-to-end control methods usually need a large dataset or data augmentation _cite_ process to enhance the coverage of the observation space. Otherwise, the learner will learn a poor policy. In contrast to the end-to-end control methods, the perception and control separation methods disassemble the control pipeline into a perception module and a control module _cite_ . The perception module takes the image as its input and extracts the features that are used to locate the vehicle on the track. The control module is responsible for making an optimal decision to follow the desired trajectory. Since the perception and control separation methods provide the flexibility that one can employ the state-of-the-art algorithm for perception and control, we design our vision-based lateral control algorithm in this framework. The main task for the perception module is to extract useful features from the image and locate the vehicle in the track. The previous works perceive the underlying features which include lane boundaries _cite_, distance to lane boundaries _cite_, vehicle poses _cite_, and road curvature _cite_, etc. from the image. By obtaining the distance to the lane boundaries and the heading angle difference with the lane heading direction, the vehicle can be located in the track coordinate. In fact, these features show strong visual correlation. For example, the lane boundaries can present the bending degree of the track, and the curvature estimation describes the bending degree with precise value, i.e. curvature. Thus, these two perception tasks are correlated and show some common features. However, the above works solve the perception tasks separately. To utilize the general features and improve the learning performance, we formulate the perception problem in the framework of the multi-task learning (MTL) convolutional neural network (CNN), which is able to exploit the shared information of different tasks and has been successfully applied to many fields like dangerous object detection _cite_ . Another important module is the control module whose objective is to generate the optimal or near-optimal control command that keeps the vehicle follow the trajectory made by the planning module. The popular control methods include linear quadratic regulator (LQR) _cite_, fuzzy logic _cite_, and model predictive control (MPC) _cite_ . However, the aforementioned control methods require system model. Since the vehicle is a strong nonlinear dynamic system running in an uncertain environment, it is hard to approximate an accurate model. Recent efforts try to employ model-free methods instead and learn from the raw sensors data _cite_ _cite_ . As a category of data-driven methods, reinforcement learning (RL) _cite_ which is inspired by the decision-making process of animals evaluates and improves its control policy by interacting with an environment. The agent is capable of learning an optimal or near-optimal policy from evaluative feedback based on the experience data _cite_ _cite_ . Recent years have seen many exciting RL applications in the context of autonomous driving such as adaptive cruise control _cite_, lane keeping _cite_, and obstacle avoidance _cite_ . In the RL framework, the vehicle lateral control is a continuous state and continuous action decision-making problem. Therefore, we employ the policy gradient algorithm to solve this problem. For the vision-based lateral control system, first the driver-view image is fed into the perception module based on the MTL neural network to predict the key track features, then a following RL control module maps these track features to a steering action and keeps the vehicle following the trajectory. We call it the MTL-RL controller. Note the desired trajectory is set to lane center line in this paper. The RL agent evaluates and improves its control policy in a trial-and-error manner which usually takes numerous samples to converge. Thus, it would be dangerous and costly to train an agent on a real vehicle. Additionally, training a deep neural network like CNN also needs a large set of samples. For the purpose of training and evaluating the perception and control algorithms, an autonomous driving environment which simultaneously integrates the image processing functions and the agent learning functions is developed. The Open Racing Car Simulator (TORCS) _cite_ is not only an ordinary vehicle racing game but also serves as an AI research platform for many years. The game engine provides high-quality traffic scenes and vehicle dynamics and kinetics models. It was the official competition software for the N IEEE World Congress on Computational Intelligence (WCCI) _cite_ and the N Simulated Car Racing (SCR) Championship _cite_ . The existing TORCS releases include official release _cite_, SCR release _cite_, and DeepDriving release _cite_ . The official TORCS provides functions like track definition, vehicle model, and basic software framework but AI research extensions. Thus, the SCR organizers release a version with various sensors support such as range radar and speed sensor, etc. The corresponding sensor data is the low-dimensional physical measurements. It can only provide a small fixed-size image (_inline_eq_) . Moreover, the employed user datagram protocol (UDP) data transmission tool is not secure because we cannot ensure all messages are received by the receiver e.g. the RL agent. Thus, this framework has limitations in image processing and communication. The DeepDriving release is specialized in image processing features like image capturing and labeling. However, it does not provide the control supports for the RL agent training and evaluating. Therefore, a new simulator which can provide perception and control supports is urgently needed. Motivated by this, we propose the visual TORCS (VTORCS), an OpenAI gym-like environment which provides multiple physical sensors, multi-view image processing, and plenty of easy-to-use RL utilities such as state and reward definition. As shown in Fig. _ref_, one can set up the VTORCS with various configurations such as different lane numbers, vehicle numbers, and track curvature etc. to obtain the desired simulation environment with different difficulty levels. At every time step, the agent can retrieve the physical or visual observations from the VTORCS and make a decision according to the underlying policy, then the action is sent back to the VTORCS where it finishes the one-step simulation. In the presented paper, we design a vision-based lateral controller by integrating an MTL perception module and an RL control module. In the perception module, in order to address the insufficient locating precision issue which may cause vehicle out of track shown later, we analyze the correlated tasks and introduce a track orientation classification task as the auxiliary task. In the RL controller module, a reward function based on the geometrical relationship is designed to guide the policy learning. Additionally, an autonomous driving simulator VTORCS is developed for the general algorithm implementation. The experiment results validate the promising performance of these two modules and the effectiveness of the vision-based lateral controller. The remainder of this paper is organized as follows. In section II, we introduce the VTORCS environment in details. In section III, we define the vision-based lateral control problem and introduce the background of RL and MTL. We give an overview of the proposed the vision-based lateral control framework in section IV. Section V and VI describe the MTL perception module and RL control module of the framework. The experiments are given in section VII. At last, we draw the conclusion and present future work in section VIII.