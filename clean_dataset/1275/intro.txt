One way to build a noise robust Automatic Speech Recognition (ASR) system is to incorporate complementary information from a different modality that is independent of noise in the speech. For example, in an audio-visual ASR (AV-ASR) system visual information from the speaker's lip region when combined with audio inputs have been shown to provide significant reduction in word error rates (WER) when the audio modality is corrupted by noise. This was inspired by the fact that human perception of speech is dependent on both auditory and visual senses as demonstrated by the famous McGurk effect _cite_ . Traditionally, AV-ASR systems were implemented using GMM/HMM models _cite_ . One way (called decision fusion method) is to model each modality by a separate GMM/HMM and at test time fuse the decisions of each stream by linearly combining the log-likelihoods to get the overall likelihood. In the feature fusion method, audio and visual features are combined usually by concatenation followed by a dimensionality reduction step. The fused features are modeled by a single GMM/HMM model. With the advancement of deep learning based techniques in speech recognition _cite_, corresponding AV-ASR systems based on deep learning have been proposed _cite_ . Deep learning based ASR systems tend to out-perform GMM/HMM models for several reasons. Firstly, GMM is a mixture model which acts as a "sum of experts" model, whereas DNN is a "product of experts". Also GMM/HMM systems require uncorrelated inputs and do not benefit from multiple frames of input whereas, this is not the case for a DNN _cite_ .Correspondingly, deep learning based AV-ASR systems have been shown to perform better than their GMM/HMM counterparts. Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems _cite_ . In decision fusion, each modality is modeled by a separate network. Mid-level fusion of features is also possible by fusing the hidden layer outputs of separate audio and visual features deep networks. So far, fusion models based on deep learning have been single task learning (STL) methods in which the fusion network predicts posterior probabilities of labels (usually context dependent tied HMM states) and the state space is common for both audio and visual modalities. An alternative approach, would be to assume different label set for the two modalities and train the network for two tasks simultaneously using a shared representation. This is called multi-task learning (MTL) _cite_ and has been successfully applied to various problems of NLP _cite_ and speech recognition _cite_ such as speech synthesis and multilingual acoustic modeling. One of the tasks is considered to be a primary task while the other is an auxiliary task. The auxiliary task helps in better feature estimation in the hidden layers and proper generalization of the model. Better generalization results in improved robustness to noise. Once the MTL model is trained, the parameters corresponding to auxiliary task are usually discarded. In this work, we explore the application of MTL to AV-ASR systems. We model the visual stream by a separate GMM/HMM whose states are used as classes for the auxiliary task. We compare our method with a baseline DNN-HMM based model. We show that MTL gives better performance, especially at higher noise levels. The paper is organized as follows: Section N describes the feature extraction pipe-line for audio and visual features. MTL is explained and is followed by a description of the primary and auxiliary tasks for AV-ASR. The training procedure and experimental results are discussed in section N. Section N discusses the relationship of our work with previous AV-ASR methods. Finally, we summarize our work in section N.