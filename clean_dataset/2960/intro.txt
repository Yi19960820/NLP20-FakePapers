It has been shown that spiking neural network (SNN) _cite_ is the solution to bridge the gap between performance and computational costs and theoretically, SNN can approximate any function as ANNs. Unlike conventional ANN, neurons in SNN communicate with each other via discrete events (spikes) instead of continuous-valued activations. Such system is updated asynchronously as event arrives, thus reducing number of operations required at each time step. Recent developments show that SNN can be emulated by neuromorphic hardware such as TrueNorth _cite_, SpiNNaker _cite_ and Rolls _cite_ with several orders of magnitude less energy consumption than by contemporary computing hardware. Furthermore, due to its event-based characteristic, SNN inherently fits to process input from AER-based (address-event-representation) sensors that have low redundancy, low latency and high dynamic range, such as dynamic vision sensor (DVS) _cite_ and auditory sensor (silicon cochlea) _cite_ . A recent work _cite_ reported that spiking stereo neural network implementation consumes roughly one order of magnitude less energy than a micro-controller implementation based on classic sum-of-absolute-differences (SAD) algorithm. Currently, one main challenge of SNN is to find an efficient training algorithm that can overcome the discontinuity of spiking and achieve comparable performance to ANNs. Conversion method, i.e., training a conventional ANN and building a conversion algorithm to map the weights to an equivalent SNN achieves the best performance by far. However, the conversion of a very deep network has never been addressed before. In this paper, we investigate the learning of deep SNN based on residual network _cite_, a cutting-edge CNN architecture that has achieved great performance across many datasets and allows network to go extreme deep. We convert a pre-trained residual network to its spiking version with the assumption that the converted ResNet still has its original advantages. In order to scale continuous-valued activations to fit SNN, we develop the shortcut normalisation to scale shortcut connection and maintain unit max firing rate across the SNN, i.e., neurons at each layer can reach the theoretically max firing rate (a spike is fired at each time step) . We also propose a layer-wise compensation approach to improve the approximation by reducing sampling error at each layer.