Image segmentation, a fundamental problem in computer vision, concerns the division of an image into meaningful constituent regions, or segments. In addition to having applications in computer vision and object recognition (Figure _ref_), it is becoming increasingly essential for the analysis of biological image data. Our primary motivation is to understand the function of neuronal circuits by elucidating neuronal connectivity _cite_ . In order to distinguish synapses and follow small neuronal processes, resolutions of \texttildelow Nnm are necessary in ND and provided only by electron microscopy (EM) . On the other hand, individual neurons often extend over millimeter ranges. This disparity of scales results in huge image volumes and makes automated segmentation an essential part of neuronal circuit reconstruction. Additionally, automated segmentation of EM images presents significant challenges compared to that of natural images (Figure _ref_), including identical textures within adjacent neurons, mitochondria and vesicles within cells that look (to a classifier) similar to the boundaries between cells, and elongated, intertwined shapes where small errors in boundary detection result in large errors in neuron network topology. The methods we introduce here, however, are generally applicable and extend to images of arbitrary dimension, which we demonstrate by segmenting both EM data and natural image data. A common approach in the field is to perform oversegmentation into small segments called, and then to merge these into larger regions _cite_ . A merging algorithm consists of a merging criterion, or policy, that determines which merges are most likely, and a merging strategy, that determines how to merge segments (for example, through simulated annealing _cite_, probabilistic graphical models _cite_, or hierarchical clustering _cite_) . Often, much effort is devoted to the generation of a pixel-level boundary probability map by training a classifier that predicts boundaries between objects from pixel-level features _cite_ . Meanwhile, oversegmentation and agglomeration are performed in a straightforward fashion, for example using watershed _cite_ to generate superpixels, and the mean boundary probability over the contour separating adjacent superpixels _cite_ as the merge criterion. Boundary mean has been a relatively effective merge priority function for hierarchical agglomeration because every merge results in longer boundaries along adjacent regions. Therefore, as the agglomeration proceeds, the mean becomes an increasingly reliable estimate of the merge probability. We hypothesized that agglomeration could be improved by using more information than just the boundary mean, despite the latter's desirable characteristics. A priority function could draw from many additional features, such as boundary variance and region texture. Using training data in which pairs of superpixels have been labeled as ``merge'' or ``don't merge'', we could then apply machine learning techniques to predict from those features whether two superpixels should be merged. With that simple approach, however, we found that the guaranteed effectiveness of the mean could easily disappear. Similarly to the case with the boundary mean, the region sizes progressively increase and so does the amount of evidence for or against a merge. However, we could encounter a combination of features for which we had no training data. To get around this problem, we developed an active learning paradigm that generates training examples across every level of the agglomeration hierarchy and thus across very different segment scales. In active learning, the algorithm determines what example it wants to learn from next, based on the previous training data. For agglomerative segmentation, we ask the classifier which two regions it believes should be merged, and compare those against the ground truth to obtain the next training example. By doing this at all levels of the agglomeration hierarchy, we ensure that we have samples from all parts of the feature space that the classifier is likely to encounter. Past learning methods either used a manual combination of a small number of features _cite_, or they used more complex feature sets but operated only on the scale of the original superpixels _cite_ . (We discuss two notable exceptions _cite_ in Section _ref_ .) We instead learn by performing a hierarchical agglomeration while comparing to a gold standard segmentation. This allows us to obtain samples from region pairs at all scales of the segmentation, corresponding to levels in the hierarchy. Although Jain et al. independently presented a similar approach called LASH _cite_, there are some differences in our approach that yield some further improvements in segmentation quality, as we explain later. We describe below our method for collecting training data for agglomerative segmentation. Throughout a training agglomeration, we consult a human-generated gold standard segmentation to determine whether each merge is correct. This allows us to learn a merge function at the many scales of agglomeration. We show that our learned agglomeration outperforms state of the art agglomeration algorithms in natural image segmentation (Figure _ref_) . To evaluate segmentations, we advocate the use of variation of information (VI) as a metric and show that it can be used to improve the interpretability of segmentation results and aid in their analysis. The ideas in this work are implemented in an open-source Python library called that performs agglomeration learning and segmentation in arbitrary dimensions.