Image captioning _cite_ has emerged as an important testbed for solutions to the fundamental AI challenge of grounding symbolic or linguistic information in perceptual data _cite_ . Most captioning systems focus on what refer to as {\em concrete conceptual} descriptions, that is, captions that describe what is strictly within the image, although recently, there has been growing interest in moving beyond this, with research on visual question-answering _cite_ and image-grounded narrative generation _cite_ among others. Approaches to image captioning can be divided into three main classes _cite_: This paper focuses on the third class. The key property of these models is that the CNN image features are used to condition the predictions of the best caption to describe the image. However, this can be done in different ways and the role of the RNN depends in large measure on the mode in which CNN and RNN are combined. It is quite typical for RNNs to be viewed as `generators'. For example, suggest that `the RNN is trained to generate the next word [of a caption] ', a view also expressed by . A similar position has also been taken in work focusing on the use of RNNs as language models for generation _cite_ . However, an alternative view is possible, whereby the role of the RNN can be thought of as primarily to encode sequences, but not directly to generate them. These two views can be associated with different architectures for neural caption generators, which we discuss below and illustrated in Figure~ _ref_ . In one class of architectures, image features are directly incorporated into the RNN during the sequence encoding process (Figure~ _ref_) . In these models, it is natural to think of the RNN as the primary generation component of the image captioning system, making predictions conditioned by the image. A different architecture keeps the encoding of linguistic and perceptual features separate, merging them in a later multimodal layer, at which point predictions are made (Figure~ _ref_) . In this type of model, the RNN is functioning primarily as an encoder of sequences of word embeddings, with the visual features merged with the linguistic features in a later, multimodal layer. This multimodal layer is the one that drives the generation process since the RNN never sees the image and hence would not be able to direct the generation process. While both architectural alternatives have been attested in the literature, their implications have not, to our knowledge, been systematically discussed and comparatively evaluated. In what follows, we first discuss the distinction between the two architectures (Section _ref_) and then present some experiments comparing the two (Sections _ref_ and _ref_) . Our conclusion is that grounding language generation in image data is best conducted in an architecture that first encodes the two modalities separately, before merging them to predict captions.