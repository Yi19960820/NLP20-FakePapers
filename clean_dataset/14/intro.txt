Object recognition is a highly active topic in computer vision and can be particularly useful for mobile devices as regards retrieving information about objects on the fly. Visually impaired persons can also benefit from these systems . MirBot is a smartphone app that users can train to recognize any object. The objects are categorized according to lemmas (such as chair, dog, laptop, etc.) selected from the WordNet lexical database . A user can employ MirBot to take a photograph and select a rectangular region of interest (ROI) in which the target object is located. The image, the ROI coordinates and a series of associated metadata are then sent to a server, which performs a similarity search using k-Nearest Neighbors (kNN) and returns the class of the most likely image, as shown in Figure _ref_ . The app users can validate the system response in order to improve the classification results for future queries, and this feedback allows the database to grow continuously with new labeled images. MirBot is designed as a pedagogical game in which a simulated robot can be trained in order to involve users in an automatic learning process. As pointed out in _cite_, developing machine learning tasks through games has proven to be a successful approach to make users enjoy labeling data. From the users' perspective, the main distinctive feature of MirBot with regard to other apps is that it allows them to train a personal image search system, thus making the dataset dynamic and user driven. This work extends the contents of the MirBot system for object retrieval introduced in _cite_, which initially used handcrafted visual descriptors. The main contributions of this paper with regard to the initial work are a detailed description of the user interaction process, the statistics related to the database gathered after four years of usage, a new classification methodology based on CNN features (neural codes) obtained from pre-trained and fine-tuned models, the study of PCA compression on different neural networks, the inclusion of metadata to complement the neural codes and the evaluation results and conclusions. When a user submits a photograph, visual descriptors are extracted and compared to the existing prototypes in the dataset in order to predict the class of the object. In the initial MirBot version _cite_, both local features and color histograms were extracted and combined to obtain the most likely class. In this work, several convolutional neural network (CNN) features have been added to these descriptors for use in evaluation. The gap between the results obtained using handcrafted descriptors and features extracted from convolutional networks led the traditional image descriptors in MirBot to be replaced with neural codes in June N. As pointed out in _cite_, finding images within large collections is an important topic for the computer vision community. Recent progress in object recognition has been built upon efforts to create large-scale, real-world image datasets that are crucial for developing robust image retrieval algorithms, in addition to considering the large amount of data required in recent deep neural networks . One of the main contributions of MirBot is a dataset with a similar structure to that of ImageNet, with the exception that all the images are gathered with smartphone cameras and stored with their associated metadata and with regions of interest. In October N we had _inline_eq_ validated images distributed in _inline_eq_ classes. Although the MirBot dataset still cannot be considered a very large collection, it is incremental and grows continuously thanks to its users' feedback. One important difference with regard to other datasets such as _cite_ and _cite_ is that, rather than employing images downloaded from the Internet, users take pictures specifically for object recognition. This signifies that MirBot images are focused on the target objects and gathered with minimum occlusions and plain backgrounds. Our team reviews the new images on a weekly basis in order to avoid inappropriate, unfocused or wrongly labeled samples, thus ensuring good quality data. The MirBot dataset also includes a series of metadata that could be used to constrain the search space. These metadata, which are detailed in _cite_, are extracted from the smartphone sensors (angle with regard to the horizontal, gyroscope, flash, GPS, etc.), and have reverse geocoding information (type of place, country, closest points of interest, etc.) and EXIF camera data (aperture, brightness, ISO, etc.) . All the images are stored with their associated metadata. We have evaluated the performance using these metadata in order to complement the image information. This work begins by describing the user interaction interface in Section _ref_, and the methodology (Section _ref_) used for similarity search on the server side. The evaluation results are described and discussed in Section _ref_, followed by the conclusions in Section _ref_ .