Human action recognition _cite_ _cite_ _cite_ _cite_ has attracted much attention in recent years due to its potential applications in automatic video analysis, video surveillance, sports event analysis and virtual reality etc. Still image classification _cite_ has gained great success in recent years, whereas human action recognition remains as a problem especially in realistic videos like movies, sports videos and daily-life consumer videos. The problem is caused by some inherent characteristics of action videos such as intra-class variation, occlusions, view point changes, background noises, motion speed and actor differences. In early researches of action recognition, people have designed effective hand-crafted descriptors (Stip _cite_, MoSift _cite_, DT _cite_, iDT _cite_) to capture the spatial and temporal information from video sequences for action recognition. Among these descriptors, improved Dense Trajectories (iDT) has dominated video analysis owing to its good performance. Despite the good performance, iDT has its weakness in huge computation costs _cite_ and large disk affords _cite_ . For example, it takes NGB and NGB memory respectively to store the extracted features of HMDBN _cite_ and UCFN _cite_ datasets. Due to the constrains of hand-crafted features and the success of deeply learned features of images, researchers have tried to generate video representations from deep ConvNets. A series of attempts like ND-CNN _cite_, Deep ConvNets _cite_, Two-Stream ConvNets _cite_ have been proposed. However, unlike image classification _cite_, deep video ConvNets did not make great progress over traditional local descriptors like iDT. We find that there are mainly two reasons that hinder the performance of deep video representations. Firstly, current video ConvNets is relatively shallow compared with image ConvNets, which limits its capability of capturing the complex video action information. In image, it only contains ND spatial information like low-level color, texture features and high-level object concepts. While in video, it contains more complex information like scene context, interacting objects, human pose and motion speed. In our common sense, the more complex problem usually need more effective and powerful model to deal with. From Table _ref_ we can see that in the large-scale image tasks like ImageNet classification, deeper and more effective models can usually achieve higher performances. However, the representation capacity of current video ConvNets is constrained by their depth. For example, ND-CNN _cite_ only contains N weighted layers (N convolutional and N fully-connected layers), while Deep ConvNets _cite_ and Two-Stream ConvNets contain N layers (N convolutional and N fully-connected layers) . As to image, VGG contains N-N layers (N groups of convolutional and N fully-connected layers) and GoogleNet contains N layers (N Inception modules), which is much deeper and more informative. Secondly, temporal information of videos is not properly utilized to pool and encode the video sequences. Pooling and coding are two key factors in both hand-crafted and deep feature representations for action recognition. Pooling spatio-temporal descriptors with Bag of Features (BOF) technique is a widely used approach for action recognition _cite_ _cite_ . Recently, improved Dense Trajectories features with Fisher vector encoding has been the main paradigm for local feature based video representation _cite_ . In this paradigm, local descriptors are aligned and pooled along the trajectories with high motion salience and then encoded by effective Fisher vector. However, temporal information is not well utilized in deep video representation and thus constraints the performance improvement. For deep video representations _cite_, video frames are regarded as still images and inputs of the trained ConvNets to extract features of the fully-connected layer. Average pooling across frames are then used to get the video features. As an improvement, Xu et al. _cite_ use multi-scale pooling on the pooling _inline_eq_ layer to get latent concept descriptors and encode them by VLAD. However, in above methods, no temporal variations are used in the pooling and coding phase. In ND-CNN _cite_ and Deep ConvNets _cite_, they directly used the ND video cubes and modified the image ConvNets for video classification. Though motion information can be embedded in video cubes, it takes large computational costs and the improvements are not significant. The most successful architecture which competes the state-of-the-art performance of improved Dense Trajectories is the Two-Stream ConvNets _cite_ . It is composed of two neural networks (namely spatial nets and temporal nets) aiming to capture the discriminative appearance features and motion features in one framework. Unlike deep image ConvNets which overwhelmed the other feature engineering methods, deep video representation needs to be well improved. How to properly utilize the intrinsic characteristics of videos to pool and encode should be important and essential. Motivated by above discussions, we propose an efficient video representation framework. We get the benefits from two state-of-the-art ConvNets: VGGNet _cite_ and temporal nets from Two-Stream ConvNets _cite_ . In our framework, Trajectory pooling and line pooling are used together to pool the extracted convolutional layers and the new proposed frame-diff layers to get local descriptors. We then use VLAD to encode the pooled local descriptors and form the final representations. We illustrate our framework in Figure _ref_ . For spatial ConvNets, we extract the convolutional layers and frame-diff layers from the trained VGGNet. The frame-diff layers are generated from original convolutional layers and the goal is to capture the motion information in consecutive frames. For temporal ConvNets, we extract convolutional layers from the optical flow nets of Two-Stream ConvNets. We use detected trajectories points from improved Dense Trajectories and line points to locate certain interesting points on convolutional feature maps. The responses from feature maps of one frame are stacked across the channel to form descriptors. Then the descriptors from the same line or trajectory are average pooled. At last, we choose the VLAD encoding strategy to aggregate these local descriptors for final video features and use multi-class linear SVM for action recognition. We conduct experiments on two public action datasets: HMDBN _cite_ and UCFN _cite_ . We get the state-of-the-art results on UCFN and comparable to the state-of-the-art on HMDBN. The rest of this paper is organized as follows. In Section N, we briefly review the hand-crafted features and deep learning methods. In section N, we propose the convolutional layer extraction and pooling framework in detail. Encoding strategy is described in Section N. Then we report the experimental results and discussions in Section N. Finally, we conclude our paper in Section N.