localization plays a vital role in medical image analysis, facilitating the automatic process for registration, classification, and segmentation _cite_ . Besides speeding up the interpretation, it contributes to visualization and assessment-based applications. However, accurate landmark localization in ND medical images is a challenging problem because of high inter-patient variations in terms of size, shape, and orientation, as well as the variations and artifacts caused by different parameter settings. Machine learning approaches are becoming more and more common to solve the localization problem under such variation. Standard approaches suggest classification or regression-based model in order to localize the landmarks. However, all of the previous learning approaches are mainly exploitative and may behave inconsistently for an exceptional test data. Long-term reward-oriented reinforcement learning (RL) algorithms offer ways to balance between exploration and exploitation, yielding a noteworthy performance in various fields of image processing _cite_ . With a few instances of implementation in object localization in terms of a bounding box, RL-enforced landmark localization is rarely found. \par Value function approximation (e.g., deep Q-Network (DQN) _cite_) is a widely used method to solve the RL problem for large state and/or action spaces, suggesting an indirect behavior learning. Compared to such value-based methods, explicit behavior learning by directly approximating the policy function _cite_ has the advantage of a better convergence. However, direct policy search method suffers from the high variance problem _cite_ . Actor-critic RL performs direct policy approximation while utilizing an additional value function approximator to reduce the variance, thus taking advantage of both the policy and value-based methods _cite_ . Nevertheless, a good exploration in order to obtain the optimal policy in a large space is challenging. Despite remarkable propositions and improvement to maintain the balance between exploration and exploitation, RL practically faces problem to successfully learn a task in a large state and/or action space and requires many trials _cite_ . \par In this paper, we formulate the landmark localization problem as a sequential decision-making problem in RL, where an agent initiated at a random position inside a ND medical image (i.e., volume) observes the current state and takes subsequent actions to move towards the target landmark. We suggest learning the policy function directly using an actor-critic approach because of its advantage over pure policy or value-based approach. To ensure a successful behavior learning within a significantly smaller number of trials, we introduce a partial policy-based reinforcement learning model where multiple sub-agents learn assigned micro-tasks to successfully learn the original task. Partial policies with respect to the micro-tasks are obtained by projecting the original policy onto smaller sub-action spaces, enabling a disintegration of the complex decision problem into a set of simpler problems. We allow independent actors to update the corresponding partial policy functions each utilizing its own value function (i.e., critic) . Fig.~ _ref_ shows a schematic illustration of the localization process using the partial policies for a ND case. \par