Image inpainting is a technique that aims to restore the damaged images or fill in the missing parts of images with visually plausible contents _cite_ . It allows to remove distracting objects or retouch undesired regions in photos _cite_ . It can be extended to other image generative tasks such as super-resolution _cite_ . And it is an important step in many graphics algorithms, e.g., generating a clean background plate or reshuffling image contents _cite_ . Due to the inherent ambiguity and complexity of natural images, image completion is a challenging problem. Recent years, Deep learning has been applied to this troublesome task attributed to the outstanding performance. Some image generative networks are designed for hole-filling based on Convolutional Neural Networks (CNN) . _cite_ trained an encoder-decoder CNN (Context Encoder) . It is a pioneering model. _cite_ designed global and local context discriminators to distinguish real images from completed ones using Generative Adversarial Network (GAN) framework. However, a simple fact currently is that compared with various types of networks in the domain of image classification and recognition where the models vary from squeezing AlexNet for FPGA and embedded deployment _cite_ to very large scale networks with several hundred layers _cite_, networks for image inpainting are few. Moreover, some models even come from the domain of image segmentation, super resolution, and image style transfer. _cite_ used the DCGAN model architecture from _cite_ . The generative model used in _cite_ and _cite_ is based on that in _cite_ which is for image-to-image translation task. In addition, there are two main problems which current image completion faces to. First, these methods often create boundary artifacts, distorted structures and blurry textures inconsistent with surrounding areas because of insufficient cognitive understanding and ineffectiveness of convolutional neural networks in modeling long-term correlations between contextual information and the hole regions _cite_ . For example, the target is a dog, but the completed result does not follow the vision cognition. It does not look like a dog visually and much details are artifacts. The filter used by traditional CNNs is a generalized linear model (GLM) . Therefore, it is implicitly assumed that the features are linearly separable for extraction, but the actual case is often difficult to be linearly separable. Furthermore, most generative networks give up pooling and are limited with _inline_eq_ convolutional kernels. This is obviously not possible to fully utilize its learning ability and cognitive understanding due to using only a single type of receptive fields. Given a dog in an image, from our human vision cognition, it is always a dog no matter where the target is, big or small it is, and rotated or not it is. Vision cognition keeps invariable. Specifically, deep inception learning is adopted to utilize more complex structures to abstract the data within diverse receptive fields and explore enough cognitive understanding. Micro neural networks are build within a layer and the inpainting network can be constructed by stacking multiple of these layers for efficient high-level feature extraction by means of the ability of nonlinearity of inception layer. Another problem is that previous deep learning approaches focused on rectangular regions located around the center of the image _cite_ . Several works for random inpainting are still limited to regular shape masks and rectangle region is the most used form. _cite_ created an irregular mask dataset for train inpainting networks using the research work in _cite_ . However it is insufficient for arbitrary completion and free-style inpainting. It is necessary to include regular, special, and any other style shapes apart from irregular masks. To address this limitation, methods to create diver masks for model robustness of arbitrary completion are introduced in this paper. Combined with the above constructed network, free-style inpainting is finally realized. Our contributions are summarized as follows. First, a novel generative network architecture using inception modules is proposed to enhance the abstraction ability of feature. The constructed network significantly improves completion results. To the best of our knowledge, we are the first to adopt inception learning for image inpainting. Moreover, approaches for generating diverse masks are provided and a relevant mask dataset is created. A variety of comparative experiments are performed on benchmark datasets. High-quality inpainting results are achieved and results demonstrate that our model is robust for arbitrary completion including on regular, irregular, and custom masks as shown in Figure _ref_ .