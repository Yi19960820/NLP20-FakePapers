Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing ND convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep ND CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf ND networks for a ND CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating _inline_eq_ convolutions with _inline_eq_ convolutional filters on spatial domain (equivalent to ND CNN) plus _inline_eq_ convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-ND Residual Net (PND ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our PND ResNet achieves clear improvements on Sports-NM video classification dataset against ND CNN and frame-based ND CNN by N \% and N \%, respectively. We further examine the generalization performance of video representation produced by our pre-trained PND ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques.