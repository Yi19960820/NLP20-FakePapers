In principle, co-salient object detection~ _cite_ is defined as the problem of discovering the common and salient foregrounds from an image group containing multiple images at the same time. It has a wide range of applications on computer vision tasks, such as image or video co-segmentation~ _cite_, object localization~ _cite_, and weakly supervised learning~ _cite_ . In order to detect co-salient regions precisely, we need to focus on two key points: N) how to extract effective features to represent the co-salient regions; N) how to model the interactive relationship between images in a group to obtain the final co-saliency maps. For N), feature representation in the co-saliency detection task should not only reflect the individual properties of each image itself, but also express the relevance and interaction between group images. For N), we know that images within a group are contextually associated with each other in different ways such as common objects, similar categories, and related scenes. The co-saliency detection job tries to use this information to find the target saliency maps, so we can utilize the consistency information within these image groups and capture an interaction between the images so that they mutually reinforce and enhance each other's saliency regions. For tackling lots of challenges, we need to design a model that can extract robust features that reflect the individual properties of each image as well as features that represent the group-wise information such as group consistency, object interactions and, to a minor extent, the objects that are present in only single images but not the rest of the images. A series of approaches have been proposed from different points of view. Some methods~ _cite_ consider that the co-salient objects appearing in the group images should share a certain consistency in both low-level feature and high-level semantic feature~ _cite_, however, they do not model the interaction between the group-wise features and single image features, which can contain information that can improve the results. Some approaches detect the single-image individual saliency and the common salient regions of a group in a separate manner~ _cite_ and, they also detect the intra-image and inter-image saliency separately from other information priors, such as the objectness~ _cite_, the center priors~ _cite_, and the border connectivity~ _cite_ . Usually, calculating the intra-image and inter-image saliency separately is incapable of well capturing the intrinsic semantic interaction information among images within each group, which is important to the co-saliency detection quality. Motivated by this observation, we propose a co-saliency deep model based on a fully convolutional network (FCN) with group input and group output. Our aim is to make use of all the information available and create a robust and effective network. Our model needs to take into account both the image properties and the intra group information while processing the co-saliency results. We design our network to be fully convolutional, this allows it to fully benefit from the local relationships between the pixels in an image, it is also designed deep enough to have a large receptive field. The network will extract the semantic features of the images, then will be divided into two branches. Namely, one processes each image individually and the other takes into account all the image group, the branches are later merged. This allows the network to learn features not only from the individual image properties, but also from the intra group properties, leveraging the shared and unique information between the images, resulting in accurate co-saliency maps. Our deep model takes a data-driven learning pipeline for capturing the collaboration and consistency intra image group, and is trained end-to-end. The main contributions of this work are summarized as follows: First, we propose a unified group-wise deep co-saliency detection approach with group input and group output, which takes advantage of the interaction relationships between group images. The proposed approach performs feature representation for both single image (e.g., individual objects and unique properties) and group consistency (e.g., common background and similar foreground), which generally leads to an improvement in the performance of the co-saliency detection. Second, we set up an end-to-end deep learning scheme (FCN) to jointly optimize the process of group-wise feature representation learning and the collaborative learning, leading to more reliable and robust co-saliency detection results. The collaborative learning process combines the group-wise saliency and single image saliency in a unified framework that model a better interaction relationships between group images.