The visual analysis of humans has a broad spectrum of applications as diverse as autonomous vehicles, robotics, human-computer interaction, virtual reality, and digital libraries, among others. The problem is challenging due to the large variety of human poses and body proportions, occlusion, and the diversity of scenes, angles of observation, and backgrounds humans are pictured against. The case, which is central and intrinsic in many scenarios like the analysis of photographs or video available on the web, adds complexity as depth information is missing for Nd reconstruction. This leads to geometric ambiguity and occlusion which are difficult to palliate compared to situations where multiple cameras are present. A detailed analysis at both Nd and Nd levels, further exposes the need for both measurement and prior-knowledge, and the necessary inter-play between segmentation, reconstruction, and recognition within models that can jointly perform all tasks. As training is essential, a major difficulty is the limited coverage of current datasets: Nd repositories like LSP _cite_ or MPI-II _cite_ exhibit challenging backgrounds, human body proportions, clothing, and poses, but offer single views, carry only approximate Nd joint location ground truth, and do not carry human segmentation or body part labeling information. Their size is also relatively small by today's deep learning standards. In contrast, Nd datasets like HumanEva _cite_ or N _cite_ offer extremely accurate Nd and Nd anatomical joint or body surface reconstructions and a variety of poses captured under multiple viewpoints. Human N is also large-scale. However, being captured indoors, the Nd datasets typically lack the background and clothing variability that represent a strength of the Nd datasets captured in the wild. An open question is how one can leverage the separate strengths of existing Nd and Nd datasets towards training a model that can operate in realistic images and can offer accurate recognition and reconstruction estimates. In this paper we propose one such deep learning model which, given a monocular RGB image, is able to fully automatically sense the humans at multiple levels of detail: figure-ground segmentation, body-part labeling at pixel level, as well as Nd and Nd pose estimation. By designing multi-task loss functions at different, recursive processing stages (human body joint detection and Nd pose estimation, semantic body part segmentation, Nd reconstruction) we are able to tie complete, realistic training scenarios by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model component training (complex Nd image data with no body part labeling and without associated Nd ground truth, or complex Nd data with limited Nd background variability), leading to covariate shift and a lack of model expressiveness. In extensive experiments, including ablation studies performed using representative Nd and Nd datasets like LSP, HumanEva, or N, we illustrate the model and show that state-of-the-art results can be achieved for both semantic body part segmentation and for Nd pose estimation.