In the age of deep learning and big data, we face a situation where we train ever more complicated models with ever increasing amounts of data. We have different models for different tasks trained on different datasets, each of which is an expert on its own domain, but not on others. In a typical setting, each new task comes with its own dataset. Learning a new task, say scene classification based on a pre-existing object recognition network trained on ImageNet, requires adapting the model to the new set of classes and fine-tuning it with the new data. The newly trained network performs well on the new task, but has a degraded performance on the old ones. This is called {\em catastrophic forgetting} ~ _cite_, and is a major problem facing life long learning techniques _cite_, where new tasks and datasets are added in a sequential manner. Ideally, a system should be able to operate on different tasks and domains and give the best performance on each of them. For example, an image classification system that is able to operate on generic as well as fine-grained classes, and in addition performs action and scene classification. If all previous training data were available, a direct solution would be to jointly train a model on all the different tasks or domains. Each time a new task arrives along with its own training data, new layers/neurons are added, if needed, and the model is retrained on all the tasks. Such a solution has three main drawbacks. The first is the risk of the negative inductive bias when the tasks are not related or simply adversarial. Second, a shared model might fail to capture specialist information for particular tasks as joint training will encourage a hidden representation beneficial for all tasks. Third, each time a new task is to be learned, the whole network needs to be re-trained. Apart from the above drawbacks, the biggest constraint with joint training is that of keeping all the data from the previous tasks. This is a difficult requirement to be met, especially in the era of big data. For example, ILSVRC~ _cite_ has N classes, with over a million images, amounting to N GB of data. Yet the AlexNet model trained on the same dataset, is only N MB, a difference in size of three orders of magnitude. With increasing amounts of data collected, it becomes less and less feasible to store all the training data, and more practical to just store the models learned from the data. Without storing the data, one can consider strategies like using the previous model to generate virtual samples (i.e. use the soft outputs of the old model on new task data to generate virtual labels) and use them in the retraining phase _cite_ . This works to some extent, but is unlikely to scale as repeating this scheme a number of times causes a bias towards the new tasks and an exponential buildup of errors on the older ones, as we show in our experiments. Moreover, it suffers from the same drawbacks as the joint training described above. Instead of having a network that is jack of all trades and master of none, we stress the need for having different specialist or expert models for different tasks, as also advocated in~ _cite_ . Therefore we build a Network of Experts, where a new expert model is added whenever a new task arrives and knowledge is transferred from previous models. With an increasing number of task specializations, the number of expert models increases. Modern GPUs, used to speed up training and testing of neural nets, have limited memory (compared to CPUs), and can only load a relatively small number of models at a time. We obviate the need for loading all the models by learning a gating mechanism that uses the test sample to decide which expert to activate (see Figure _ref_) . For this reason, we call our method {\em Expert Gate} . Unlike _cite_, who train one Uber network for performing vision tasks as diverse as semantic segmentation, object detection and human body part detection, our work focuses on tasks with a similar objective. For example, imagine a drone trained to fly through an environment using its frontal camera. For optimal performance, it needs to deploy different models for different environments such as indoor, outdoor or forest. Our gating mechanism then selects a model on the fly based on the input video. Another application could be a visual question answering system, that has multiple models trained using images from different domains. Here too, our gating mechanism could use the data itself to select the associated task model. Even if we could deploy all the models simultaneously, selecting the right expert model is not straightforward. Just using the output of the highest scoring expert is no guarantee for success as neural networks can erroneously give high confidence scores, as shown in _cite_ . We also demonstrate this in our experiments. Training a discriminative classifier to distinguish between tasks is also not an option since that would again require storing all training data. What we need is a task recognizer that can tell the relevance of its associated task model for a given test sample. This is exactly what our gating mechanism provides. In fact, also the prefrontal cortex of the primate brain is considered to have neural representations of task context that act as a gating in different brain functions~ _cite_ . We propose to implement such task recognizer using an undercomplete autoencoder as a gating mechanism. We learn for each new task or domain, a gating function that captures the shared characteristics among the training samples and can recognize similar samples at test time. We do so using a one layer under-complete autoencoder. Each autoencoder is trained along with the corresponding expert model and maps the training data to its own lower dimensional subspace. At test time, each task autoencoder projects the sample to its learned subspace and measures the reconstruction error due to the projection. The autoencoder with the lowest reconstruction error is used like a switch, selecting the corresponding expert model (see Figure _ref_) . Interestingly, such autoencoders can also be used to evaluate {\em task relatedness} at training time, which in turn can be used to determine which prior model is more relevant to a new task. We show how, based on this information, Expert Gate can decide which specialist model to transfer knowledge from when learning a new task and whether to use fine-tuning or learning-without-forgetting~ _cite_ . To summarize, our contributions are the following. We develop Expert Gate, a lifelong learning system that can sequentially deal with new tasks without storing all previous data. It automatically selects the most related prior task to aid learning of the new task. At test time, the appropriate model is loaded automatically to deal with the task at hand. We evaluate our gating network on image classification and video prediction problems. The rest of the paper is organized as follows. We discuss related work in Section _ref_ . Expert Gate is detailed in Section~ _ref_, followed by experiments in Section _ref_ . We finish with concluding remarks and future work in Section _ref_ .