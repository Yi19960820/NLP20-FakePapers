Video captioning and semantic video description has generated increasing attention in recent years due to encouraging results observed for similar problems such as image captioning~ _cite_ and question and answering systems~ _cite_ where typical encoder-decoder frameworks are employed. Learning and encoding visual feature representations for video analysis and specifically video captioning is challenging. Some of these challenges come from the complexity and nature of the data where video frames displayed as images in a time sequence add a temporal dimension to a much larger challenge of recognizing and detecting objects in a per frame basis. Because of the variable length of its temporal component, a paradigm that accommodates for an unconstrained sequence is necessary to process video frames. One way to accomplish this is through a feature encoding mechanism that is two-fold: First, visual CNN features are extracted from every frame using a CNN model previously trained on large single-image datasets such as ImageNet. The second step in the process is to transform or encode these CNN features into one single feature vector that is trained on video samples with a decoder conditioned on the encoder features and a cost function that relies on the provided labeled data in a supervised learning manner. Our method improves upon recent visual feature encoding methods by introducing a novel encoding framework consisted of an encoder which is jointly trained with multiple decoders in a multi-task fashion. Our encoder consists of a bi-directional recurrent neural network trained with multiple and separate decoders that use labeled samples to project the textual features into a semantic space where a distance metric can be used to compare semantic distances from different samples and captions. We use a semantic distance function that allows us to select captions being input to each decoder during training. Thus, given a set of different training labels for a single video/segment, we find the pair with the farthest semantic distance among all possible pairs. Intuitively, pairs that are farthest apart in the semantic space complement the meaning of each other while allowing us to obtain a centroid of a potential cluster in semantic space that will enable us to capture the full meaning of all the captions. This also avoids biasing the encoder weights towards a single sample, providing improved generalization. Our method is designed to work not only with multiple annotations per video but also single captions. Our objective function allows for a regularization term that will leverage multiple caption scenarios and augment training samples when there is only a limited number of training data. Thus, our method does not depend on a large number of training labels and can handle datasets with limited number of annotations such as LSMDC. Our proposed method shows improvements over the current baseline in public datasets that contain multiple or single annotations per video such as MSVD~ _cite_, MSR-VTT~ _cite_, TRECVID~ _cite_ and LSMDC~ _cite_ . Contributions: In essence, the contribution of this paper can be summarized in the following: In this paper, we first give a brief introduction to the problem and discuss some issues with current methods and how our method helps overcome these challenges. We also give a brief literature review of past and current trends. We then explain our method in detail. We end with a discussion of our results and conclusion.