Explaining how deep neural networks work is an open and challenging task. Certain insights about their underlying mechanisms can help users and researches to explain their operation, identify potential strengths and weaknesses, and help them to easily understand how they work, operate and infer. In addition, network optimization will be an easier process for the user, as it will be easier to identify errors and imperfections in terms of architecture and accuracy. \par The need for better model understanding has become even more urgent, as the use of deep learning techniques is already extended to areas such as autonomous vehicles, robotic industrial systems, medical devices, and security systems. Inevitably, in all these systems there will be a human interaction (autonomous vehicle-pedestrian, industrial robot-worker) resulting in critical decisions for the autonomous systems which can even influence the lives of people interacting with them. These questions and concerns have been established and have recently become widely known following various open letters co-signed by distinguished scientists and academics _cite_ . The better understanding of the emerging generation models of deep convolutional networks will help the scientific community to bring further developments with the ultimate goal in the end-user confidence. \par Various works have been recently proposed for such understanding targeting at different methodologies and goals. A visualization method by projecting the feature activations back to the input pixel space has been proposed, showing which patterns from the training set activate the feature map _cite_ . This approach utilizes a fully supervised convolutional network model together with deconvolutional network for the necessary mapping. A framework for explaining the transfer learning of deep networks by analyzing their contraction and separation properties have been recently also proposed _cite_ . This explanation was achieved by separating variations of certain operators with a wavelet transform at different scales. Insights about the captured invariances by deep network representations are also given in _cite_ where a progressive and more invariant and abstract image notions are formed throughout the network _cite_ . Using invertible deep networks is another approach for such analysis since several properties can be back-tracked from the feature space to the input space _cite_ . \par The purpose of this research is to help the end user understand in a more practical way the rationale behind the decisions made by the used deep convolutional network. Decisions, recommendations, classifications, or actions produced by such a system should be as comprehensible as possible to the human user so that he can more easily trust and evaluate them. This is achieved by various sensitivity analyses based on Gestalt principles aiming to define how deep networks perceive patterns and organize the raw input data.