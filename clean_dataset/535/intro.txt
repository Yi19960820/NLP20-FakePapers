As the number of layers of neural networks increase, effectively training its parameters becomes a fundamental problem (_cite_) . Many obstacles challenge the training of neural networks, including vanishing/exploding gradients (_cite_), saturating activation functions (_cite_) and poor weight initialization (_cite_) . Techniques such as unsupervised pre-training (_cite_), non-saturating activation functions (_cite_) and normalization (_cite_) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still lead to a hard to train model. Recently, models such as Residual Networks (_cite_) and Highway Neural Networks (_cite_) permitted the design of networks with hundreds of layers. A key idea of these models is to allow for information to flow more freely through the layers, by using shortcut connections between the layer's input and output. This layer design greatly facilitates training, due to shorter paths between the lower layers and the network's error function. In particular, these models can more easily learn identity mappings in the layers, thus allowing the network to be deeper and learn more abstract representations (_cite_) . Such networks have been highly successful in many computer vision tasks. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (_cite_ _cite_ _cite_ _cite_) . This agrees with the increasing depth of winning architectures on challenges such as ImageNet (_cite_ _cite_) . Increasing the depth of networks significantly increases its representational capacity and consequently its performance, an observation supported by theory (_cite_ _cite_ _cite_ _cite_) and practice (_cite_ _cite_) . Moreover, _cite_ showed that, by construction, one can increase a network's depth while preserving its performance. These two observations suggest that it suffices to stack more layers to a network in order to increase its performance. However, this behavior is not observed in practice even with recently proposed models, in part due to the challenge of training ever deeper networks. In this work we aim to improve the training of deep networks by proposing a layer design that builds on Residual Networks and Highway Neural Networks. The key idea is to facilitate the learning of identity mappings by introducing a {\em gating mechanism} to the shortcut connection, as illustrated in Figure~ _ref_ . Note that the shortcut connection is controlled by a gate that is parameterized with a scalar, _inline_eq_ . This is a key difference from Highway Networks, where a tensor is used to regulate the shortcut connection, along with the incoming data. The idea of using a scalar is simple: it is easier to learn _inline_eq_ than to learn _inline_eq_ for a weight tensor _inline_eq_ controlling the gate. Indeed, this single scalar allows for stronger supervision on lower layers, by making gradients flow more smoothly in the optimization. We apply our proposed network design to Residual Networks, as illustrated in Figure~ _ref_ . Note that in this case the layer becomes simply _inline_eq_, where _inline_eq_ denotes the layer's residual function. Thus, the shortcut connection allows the input to flow freely without any interference of _inline_eq_ through the layer. We will call this model Gated Residual Network, or GResNets. Again, note that learning identity mappings is again much easier in comparison to the original ResNets. Note that layers that degenerated into identity mappings have no impact in the signal propagating through the network, and thus can be removed without affecting performance. The removal of such layers can be seen as a transposed application of sparse encoding (_cite_): transposing the sparsity from neurons to layers provides a form to prune them entirely from the network. Indeed, we show that performance decays slowly in GResNets when layers are removed, when compared to ResNets. We evaluate the performance of the proposed design in two experiments. First, we evaluate fully-connected GResNets on MNIST and compare it with fully-connected ResNets, showing superior performance and robustness to layer removal. Second, we apply our model to Wide ResNets (_cite_) and test its performance on CIFAR, obtaining results that are superior to all previously published results (to the best of our knowledge) . These findings indicate that learning identity mappings is a fundamental aspect of learning in deep networks, and designing models where this is easier seems highly effective.