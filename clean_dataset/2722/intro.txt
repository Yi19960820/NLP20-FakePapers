The problem of detecting active speakers in video is a central one to several applications. In video conferencing, knowing the active speaker allows the application to focus on and transmit the video of one amongst several people at a table. In a Human-Computer-Interaction (HCI) setting, a robot/computer can use active speaker information to address the correct interlocuter. Active speaker detection is also a part of the pipeline in video diarization, the automatic annotation of speakers, their speech and actions in video. Video diarization is useful for movie sub-titling, multimedia retrieval and for video understanding in general. Traditionally, visual active speaker detection has been done using lip motion detection ~ _cite_ . However, facial expressions and gestures from the upper body, movement of the hands, etc., are all cues that can be utilized to assist with this task, as shown in ~ _cite_, where better detection results are achieved using spatio-temporal features extracted from the entire upper body, compared with just lip motion detection. Another powerful idea we borrow from~ _cite_, is to use audio to supervise the training of a video based active speaker detection system. In that work, a microphone array is used to get directional sound information (assumed to be speech sounds), and based on this input, upper body tracks are associated with speak/non-speak labels. These labels are then used to train an active speaker classifier using video only. However, the presence of reverberation and background noise prevents perfect active speaker identification using directional audio alone, which subsequently affects the training of the video-based classifier. Additionally, in the vast majority of videos, such as the millions of Youtube videos available online, in videos from films and TV series, only a single channel of sound is available, with no directional information. Even in those cases where N channels of audio are available, the relative position of the camera and the microphones varies, and no calibration information is available, making it impossible to apply the method of~ _cite_ . In the absence of directional information, we propose to use Voice Activity Detection (VAD) ~ _cite_ to tell us when there is someone speaking in a frame. If there is only one person in the frame, then this can be used to train the video-based classifier directly. However, when this is not the case, the problem becomes one of simultaneously associating the voice activity with one of the people in the frame, and learning the classifier (Figure _ref_) . That's the challenge we address in this work. Moreover, there's an additional challenge. Investigating our trained classifier, we find that it has some bias: it works better for some speakers, compared to others. We identify two reasons for this. First, the way people gesticulate while speaking varies a lot from person to person. Indeed, a person-specific model typically outperforms the generic model. Second, there is the domain shift problem: the change of data distribution between training and test data. We address both by extending our previous scheme to an online learning setting that, starting from a generic classifier, gradually adapts to a specific person. To this end, we retrain the model with an incrementally increasing number of training samples coming from a new video of a previously unseen person at each iteration. The online training is also weakly supervised by VAD from audio. The generic classifier is used to label and pick the training samples for each speaker and temporal continuity constraints allow the classification performance to improve in spite of imperfectly labelled training data from the generic classifier. Our method is completely unsupervised, in the sense that there is no supervision/labelling. We use audio to supervise the learning. This supervision comes "for free" with the video, but is only partial-VAD tells us that one of the persons in the frame is speaking, but not who. As opposed to~ _cite_, who use full supervision from directional audio, we use weak supervision from VAD. This work can be seen as an example of how the availability of multi-modal data allows us to learn a model without the need for supervision, by transferring knowledge from one modality to another. The remainder of the paper is organized as follows. We discuss prior work in this area in Section _ref_ . We discuss the use of audio for active speaker detection in Section _ref_, with subsections _ref_, _ref_ and _ref_ discussing the weakly supervised learning with Latent SVMs, speaker specific classification and online learning, respectively. Experimental results are discussed in Section _ref_ and concluding remarks and potential for future work in Section _ref_ .