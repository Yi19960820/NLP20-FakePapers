\noindent Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks _cite_ . Very recent methods also use deep networks for the proposal step _cite_, sometimes sharing features between localization and classification. Differences exist in the detailed architectures in the way calculations are shared over layers, scales, spatial regions etc. (see section _ref_ for a detailed analysis) . Another criterion is the coupling between hypothesis creation and confirmation/classification. Earlier works create thousands of hypotheses per image, sometimes using low level algorithms (e.g. R-CNN _cite_), leaving the burden of validation to a subsequent classifier. Current work tends to create very few proposals per image, which satisfy a high degree of ``objectness''. In this work we focus on the localization step, targeting cases where the existing methods tend to give weak results: \noindent Similar to recent work, the proposed method localizes bounding boxes by direct regression of (relative) coordinates. The main contribution we claim is a new model which performs spatially local computations, efficiently sharing parameters spatially. The main challenge in this case is to allow the model to collect features from local regions as well as globally pooled features in order to be able to efficiently model context. Similar to models like YOLO _cite_ and Single-Shot Detector _cite_, our outputs are assigned to local regions of the image. However, in contrast to these methods, each output is trained to be able to predict objects in its support region, or outside. Before each gradient update step, we match predictions and ground truth objects. Each output of our model directly sees only a limited region of the input image, which keeps the overall number of trainable parameters low. However, outputs get additional information from outside regions through context, which is collected using spatial ND recurrent (LSTM) units. This spatial context layer proved to be a key component of our model. We propose the following contributions: \noindent The paper is organized as follows: the next section briefly outlines related work. Section _ref_ discusses properties and trade-offs of deep models related to convolutions, poolings and subsampling, which will be related to our proposed model. Based on these conclusions, a new model is introduced in section _ref_ . \noindent Earlier (pre-deep learning) work on object recognition proceeded through matching of local features _cite_ or by decomposing objects into mixtures of parts and solving combinatorial problems _cite_ . Early work on deep learning first extended the sliding window approach to deep neural networks. To avoid testing a large number of positions and aspect ratios, R-CNN _cite_ introduced the concept object proposals, created by separate methods, followed by convolutional networks to classify each proposal. The concept was improved as Fast R-CNN _cite_ and Faster R-CNN _cite_ . Erhan et al. proposed Multibox _cite_, which performs direct regression of bounding box locations instead of relying on object proposals. After each forward pass, network outputs are assigned to target ground-truth boxes through a matching algorithm. YOLO _cite_ and the Single-Shot Detector _cite_ can be seen as variants of this concept, they will be discussed in more detail in section _ref_ . Some recent work strives to detect and localize objects with pixel-wise precision, which somewhat blurs the boundaries between object detection and semantic segmentation _cite_ . Methods which learn to segment without pixelwise ground truth have also been proposed _cite_ . Pixelwise segmentation is not needed in our application, where the segmentation step is performed in a latter stage jointly with recognition (recognition results will be given in the experimental section) . Context through spatial ND-recurrent networks has been proposed as early as in _cite_ . However, up to our knowledge, no method did use it for object localization. Similarly to our method, inside-Outside-Nets _cite_ contain ND spatial context layers collecting information from N different directions. However, the hidden transitions of recurrent layers are set to identity, whereas our model contains fully-fledged trainable ND-LSTM layers. Moreover, localization is performed as ROI proposals with selective search, the deep model being used only for classification and bounding box correction, whereas we do not require a region proposal step. Our model directly performs bounding box regression. Other recent work uses ND recurrent networks for semantic segmentation _cite_ . CNNs have been used before for text detection, for instance in _cite_, a Fully convolutional network (FCN) is used to classify each position of a salient map as text or non-text. In _cite_, a YOLO-related method is proposed for the detection of text in natural images but only few objects are present in the images. The problem of dataset sizes has been addressed before, with strategies reaching from external memories _cite_ and unsupervised learning, for instance by learning feature extraction from physics _cite_ .