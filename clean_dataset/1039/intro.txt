Despite the tremendous success of deep convolutional neural networks (CNNs) _cite_, their training remains to be notoriously difficult both theoretically and practically, especially for state-of-the-art ultra-deep CNNs. Potential reasons accounting for such difficulty lie in multiple folds, ranging from vanishing/exploding gradients _cite_, to feature statistic shifts _cite_, to the proliferation of saddle points _cite_, and so on. To address these issues, various solutions have been proposed to alleviate those issues, examples of which include parameter initialization _cite_, residual connections _cite_, normalization of internal activations _cite_, and second-order optimization algorithms _cite_ . This paper focuses on one type of structural regularizations: orthogonality, to be imposed on linear transformations between hidden layers of CNNs. The orthogonality implies energy preservation, which is extensively explored for filter banks in signal processing and guarantees that energy of activations will not be amplified _cite_ . Therefore, it can stabilize the distribution of activations over layers within CNNs _cite_ and make optimization more efficient. _cite_ advocates orthogonal initialization of weight matrices, and theoretically analyzes its effects on learning efficiency using deep linear networks. Practical results on image classification using orthogonal initialization are also presented in _cite_ . More recently, a few works _cite_ look at (various forms of) enforcing orthogonality regularizations or constraints throughout training, as part of their specialized models for applications such as classification _cite_ or person re-identification _cite_ . They observed encouraging result improvements. However, a dedicated and thorough examination on the effects of orthogonality for training state-of-the-art general CNNs has been absent so far. Even more importantly, how to evaluate and enforce orthogonality for non-square weight matrices does not have a sole optimal answer. As we will explain later, existing works employ the most obvious but not necessarily appropriate option. We will introduce a series of more sophisticated regularizers that lead to larger performance gains. This paper investigates and pushes forward various ways to enforce orthogonality regularizations on training deep CNNs. Specifically, we introduce three novel regularization forms for orthogonality, ranging from the double-sided variant of standard Frobenius norm-based regularizer, to utilizing Mutual Coherence (MC) and Restricted Isometry Property (RIP) tools _cite_ . Those orthogonality regularizations have a plug-and-play nature, i.e., they can be incorporated with training almost any CNN without hassle. We extensively evaluate the proposed orthogonality regularizations on three state-of-the-art CNNs: ResNet _cite_, ResNeXt _cite_, and WideResNet _cite_ . In all experiments, we observe the consistent and remarkable accuracy boosts (e.g., N in CIFAR-N top-N accuracy for WideResNet), as well as faster and more stable convergences, without any other change made to the original models . It implies that many deep CNNs may have not been unleashed with their full powers yet, where orthogonality regularizations can help. Our experiments further reveal that larger performance gains can be attained by designing stronger forms of orthogonality regularizations. We find the RIP-based regularizer, which has better analytical grounds to characterize near-orthogonal systems _cite_, to consistently outperform existing Frobenius norm-based regularizers and others.