Line art colorization plays a critical role in the workflow of artistic work such as the composition of illustration and animation. Colorizing the in-between frames of animation as well as simple illustrations involves a large portion of redundant works. Nevertheless, there is no automatic colorization pipelines for line art colorization. As a consequence, it is performed manually using image editing applications such as Photoshop and PaintMan. Especially in the animation industry, where it is even known as a hard labor. It's a challenging task to develop a fast and straightforward way to produce illustration-realistic imagery with line arts. Several recent methods have explored approaches for guided image colorization _cite_ . Some works are mainly focused on images containing greyscale information _cite_ . A user can colorize a greyscale image by color points or color histograms _cite_, or colorize a manga based on reference images _cite_ with color points _cite_ . These methods can achieve impressive results but can neither handle sparse input like line arts, nor color stroke-based input which is easier for users to intervene. To address these issues, recently, researchers have also explored more data-driven colorization methods _cite_ . These method colorize a line art/sketch by scribble color strokes based on learned priors over synthetic sketches. This makes colorizing a new sketch cheaper and easier. Nevertheless, the results often contains unrealistic colorization and artifacts. More fundamentally, the overfitting over synthetic data is not well handled, thus the network can hardly perform well with "in the wild" data. PaintsChainer _cite_ tries to address these issues with three models (with two models not opened) proposed. However, the eye pleasing results were achieved in the cost of losing realistic textures and global shading that authentic illustrations preserve. In this paper, we propose a novel conditional adversarial illustration synthesis architecture trained fully on synthetic line arts. Unlike existing approaches using typical conditional networks _cite_, we combine a cGAN with a pretrained local features network, i.e., both generator and discriminator network are only conditioned on the output of local features network to increase the generalization ability over authentic line arts. With proposed networks trained in an end-to-end manner with synthetic line arts, we are able to generate illustration-realistic colorization from sparse, authentic line art boundaries and color strokes. This means we do not suffer from overfitting issue over synthetic data. In addition, we randomly simulate user interactions during training, allowing the network to propagate the sparse stroke colors to semantic scene elements. Inspired by _cite_ which obtained state-of-the art results in motion deblurring, we fuse the conditional framework _cite_ with WGAN-GP _cite_ and perceptual loss _cite_ as the criterion in the GAN training stage. This allows us to robustly train a network with more capacity, thus makes the synthesized images more natural and real. Moreover, we collected two cleaned datasets with high quality color illustrations and hand-drawn line arts. They provide a stable training data source as well as a test benchmark for line art colorization. By training with the proposed illustration dataset and adding minimal augmentation, our model can handle general anime line arts with stroke color hints. As the loss term optimizes the results to resemble the ground truth, it mimics realistic color allocations and general shadings with respect to the color strokes and scene elements as shown in Figure _ref_ . We trained our model with millions of image pairs, and achieve significant improvements in stroke-guided line art colorization. Extensive experimental results demonstrate that the performance of the proposed method is far superior to those of the state-of-the-art stroke-based user-guided line art colorization methods in both qualitative and quantitative evaluations. In summary, the key contributions of this paper are summarized as follows.