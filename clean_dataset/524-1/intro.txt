Recognizing objects at vastly different scales is a fundamental challenge in computer vision. (for short we call these) form the basis of a standard solution _cite_ (Fig.~ _ref_ (a)) . These pyramids are scale-invariant in the sense that an object's scale change is offset by shifting its level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels. Featurized image pyramids were heavily used in the era of hand-engineered features _cite_ . They were so critical that object detectors like DPM _cite_ required dense scale sampling to achieve good results (\eg, N scales per octave) . For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) _cite_ . Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale _cite_ (Fig.~ _ref_ (b)) . But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet _cite_ and COCO _cite_ detection challenges use multi-scale testing on featurized image pyramids (\eg, _cite_) . The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which, including the high-resolution levels. Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (\eg, by four times _cite_), making this approach impractical for real applications. Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time _cite_, which creates an inconsistency between train/test-time inference. For these reasons, Fast and Faster R-CNN _cite_ opt to not use featurized image pyramids under default settings. However, image pyramids are not the only way to compute a multi-scale feature representation. A deep ConvNet computes a layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition. The Single Shot Detector (SSD) _cite_ is one of the first attempts at using a ConvNet's pyramidal feature hierarchy as if it were a featurized image pyramid (Fig.~ _ref_ (c)) . Ideally, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost. But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (\eg, convN \_N of VGG nets _cite_) and then by adding several new layers. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. We show that these are important for detecting small objects. The goal of this paper is to naturally leverage the pyramidal shape of a ConvNet's feature hierarchy while creating a feature pyramid that has strong semantics at all scales. To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig.~ _ref_ (d)) . The result is a feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale. In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids without sacrificing representational power, speed, or memory. Similar architectures adopting top-down and skip connections are popular in recent research _cite_ . Their goals are to produce a single high-level feature map of a fine resolution on which the predictions are to be made (Fig.~ _ref_ top) . On the contrary, our method leverages the architecture as a feature pyramid where predictions (\eg, object detections) are independently made on each level (Fig.~ _ref_ bottom) . Our model echoes a featurized image pyramid, which has not been explored in these works. We evaluate our method, called a Feature Pyramid Network (FPN), in various systems for detection and segmentation _cite_ . Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark _cite_ simply based on FPN and a basic Faster R-CNN detector _cite_, surpassing all existing heavily-engineered single-model entries of competition winners. In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by N points; for object detection, it improves the COCO-style Average Precision (AP) by N points and PASCAL-style AP by N points, over a strong single-scale baseline of Faster R-CNN on ResNets _cite_ . Our method is also easily extended to mask proposals and improves both instance segmentation AR and speed over state-of-the-art methods that heavily depend on image pyramids. In addition, our pyramid structure can be trained end-to-end with all scales and is used consistently at train/test time, which would be memory-infeasible using image pyramids. As a result, FPNs are able to achieve higher accuracy than all existing state-of-the-art methods. Moreover, this improvement is achieved without increasing testing time over the single-scale baseline. We believe these advances will facilitate future research and applications. Our code will be made publicly available.